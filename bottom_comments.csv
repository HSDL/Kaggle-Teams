2408, ----
2435, ----
2438, ----
2439, ----
2442, ----
2445, ----
2447, ----
2448, ----
2452, ----
2454, ----
2464, <p>At a granular level most of them (us) are from Melbourne</p>
2464, <p>I'm interested as well</p>
2464, <p>I suspect its to do with the Twitter API and not the twitteR package. By the way the twitter package has a limit of 3200 tweets not 1500 (maximum as dictated by Twitter). Usually this problem doesn't occur if you use the 'userTimeline' function but I think that's not what you're after</p> <p>The way I get around this is to run the searchTwitter code during non-peak hours. For me its lunch time or midnight. Seem to be able to obtain more tweets but never quite the maximum somehow.</p>
2464, <p>[quote=Aditya Shankar;43423]</p> <p>RF&lt;- randomForest(response~.data=train.dataset mtry=best mtry from above ntree=1600 keep.forest=TRUE importance=TRUEtest=test.dataset)</p> <p>[/quote]</p> <p>Good suggestion. I would even tune down the ntree. Usually 500 is sufficient for convergence. Beyond that you start to overfit (RF is not mean to overfit but it still does. That's why you go with RRF package). Just do plot(RF) to check at which point you start to achieve convergence&nbsp;</p>
2464, <P>Hi&nbsp;<BR><BR>On top of&nbsp;Michelangelo's question I would like to also add this question of How the 25% of the test data is being selected? I presume its random sampling but just wanted to confirm that its the case. <BR><BR>Thanks!</P>
2464, <P>Hi all <BR><BR>First and fore most congratulations to Jeremy Howard for winning the contest and the top 5 contestants. <BR><BR>Also well done to everyone in the top 10 as well. It was a very very very close match with just a tiny fraction that sets us all apart. And kudo's to everyone who tried their best. <BR><BR>This was my first competition and I've enjoyed it thoroughly so thanks to Anthony for organising this. I've learned alot during the course of this competition and would love to hear everyone else's story and their experience in this competition.<BR><BR>Thanks<BR>Eu Jin</P>
2464, Hi all <BR><BR>Thanks for sharing with us your approach and solutions Jeremy Quan Sun and Vsh. <BR><BR>Actually I was wondering if anyone did a social network analysis&nbsp;using the PersonIDs to extract featuers such as hubs authorities etc? <BR><BR>I had the idea that extracting such features could represent the spase data in&nbsp;the&nbsp;training set so&nbsp;I extracted knn hubs and centrality scores but&nbsp;wasn't a strong predictor. I suspect I did something wrong. So just wondering if anyone&nbsp;did&nbsp;do this and if&nbsp;any success at all?&nbsp;&nbsp;<BR><BR>Thanks<BR>Eu Jin
2464, <p>Hi</p> <p>&nbsp;</p> <p>Just wondering when will the final results be announced?</p>
2464, Hi   First and foremost congratulations to UCL for such a convincing win. Can't wait to hear their No Free Hunch post. I'd also love to hear from the rest of the competitors as well  I came 5th and only used the features provided as I've have absolutely no knowledge about image processing or text analytics and have no idea what the features meant on the instructions. So my strategy were the following stages:  1) Without domain knowledge the only levers I could pull were the choice of algorithms and the framing of the data. So the key issues I saw were that a) there were too few instances (2 paragraphs for each author) to train the algorithm but there were way too many predictors (6480) b) the target has way too many categories (54 authors) for the algorithm to predict for each instance.   2) I started out by tackling problem (b) by transforming the data to predict just a single author. For example I started off with author 31 change the target to "1" if author 31 wrote 0 if author 31 did not. And trained the algorithm on all 6480 predictors to predict which paragraph author 31 wrote in the test set. Did this for all the 54 authors (it was tedious work).   Outcome: Unfortunately this didn't work very well as there were sometimes multiple paragraphs written by each author and doing this for each author at a time there were often crossovers which ends up with 1 paragraph written by 10 or more authors!   3) Back to square one I proceeded to tackle problem (a) by combining both train and test factor analysing the predictors based on group as described on the instruction and extracted on eigen value 1. Reduced the predictors down to about 300 train the algorithm to predict the target as multiple categories (53 authors)  Outcome: This worked very well as it reduced training time and improve accuracy in prediction despite the target being multiple categories. My plan was to then apply what did on stage 1.   3) With the succes of tackling problem (b) I was out of time unfortunately so I couldn't used the approach I used previously for stage 1. So instead i used an ensemble of the different algorithms to produce my final submussion that gave me the highest score in the final moments.   The algorithms I used were - Logistic BayesNet SVM RandomForest Boosting  For unknowns I set them all to zero.   Thanks Eu Jin
2464, Wow I totally forgot about GLM and it got such a high AUC score!    Zach given that the codes are revealed to us all I think we'll all be up for a vicious scramble for the miniscule % difference at the end of it. =)  This competition is a real learning opportunity. Keep the benchmark rolling in.
2464, I'll guess 0.999  Phil (Sali Mali) has the secret recipe of course. But I suspect someone might discover it (is it possible Phil? =P). However the prospect would probably need to first figure out which of the 200 variables are being used to generate the target.   
2464, <p>[quote=Eu Jin Lok;2754]</p> <p>I'll guess 0.999 Phil (Sali Mali) has the secret recipe of course. But I suspect someone might discover it (is it possible Phil? =P). However the prospect would probably need to first figure out which of the 200 variables are being used to generate the  target.</p> <p>[/quote]</p> <p>I still believe we can achieve 0.999. And I think it is possible if everyone work together by posting their ideas and findings. I'm more interested now to see someone get to the 0.999 score anyone.</p>
2464, Wow you've got some really powerful variables there Ockham. I tried using them and took me from 92 up to 96. I don't know how you managed to identify those variables but you're definitely close to finding the secret ingredient.
2464, <p>[quote=Yasser Tabandeh;2933]</p> <p>Excellent variables!</p> <p>Try&nbsp;a SVM solver like PEGASOS&nbsp;</p> <p>[/quote]</p> <p>Like Yasser I used Pegasos as well. My best submissions all came from machine learning techniques such as SVM NN and Perceptrons.</p> <p>&nbsp;</p> <p>Funily enough TKS's attribute selections did not work well for me using machine learning techniques. It was massively overfitted (accuracy dropped by 8% when applied on test). Ockham's selection was on the dot for me. I am now wondering what happens if  GLMnet was applied on Ockham's selection. Will try this now but if anyone has done this or has some insights to whats going on would love to hear it.</p>
2464, <p>Thanks Wu</p> <p>&nbsp;</p> <p>Thats really interesting. That means that the 2 sets of variables (TKS's and Ockham's) does not generalise accross different techniques. Any thoughts?</p>
2464, @Wu Or use the Caret package and use the train function to tune the SVM. It will help you find the best parameters for the SVM model - degree and nu)
2464, Hi WIlliam I am having the same issues as well. I suspect target practice has its own variable selection which generates it different to target leaderboard. I keep getting overfitting issues when I reduce the variables used. I used some variant of kernel  methods in Weka to rank the features (to be consistent with Pegasos) and found that around 80 variables got me AUC of 0.97 by 10-CV using Pegasos. But as soon as I submit it AUC was 0.91. Ensembles should help you get a better score but I don't think  thats how the target_leader board was being generated?
2464, <p>Hi Phil</p> <p>&nbsp;</p> <p>On the first point:</p> <p>I can't speak for everyone but personally publication in a journal is not an incentive but a hassle for me. It takes alot of time and effort to write a proper publication and I just don't have time as I'm working full time. However I'm happy to do like  a presentation to a seminar or conference and talk about how I did it (for instance in the R meetup group then record it and host the video in Kaggle?) Maybe give us an option between the two?</p>
2464, <p>@TeamSMRT</p> <p>Very well said. I'll do a competition with no prize at all for learning purposes. My thoughts are: setting a prize (whatever it may be) that is valuable enough to attract as many competitors but not too high that everyone becomes too competitive and refuses  to share information. This competition I reckon has found the right balance.</p> <p>&nbsp;</p> <p>On point 2: Would be nice to predict multiple categories or a continuous scale for a change. Alot of the competitions on Kaggle is about predicting a binary outcome.</p>
2464, <p>@Mark</p> <p>Your code is not crap. Its just not as good as when applied to the test which is the case for many of us here. I had a quick trial yesterday the 64 variable set had really strong predictive power on the train. When applied to the test however it drops.  But I've experienced this many times on different alogrithms and different variable selections they overfit.&nbsp;This is part of the challenge of this contest. To find the right variable set and algorithm which&nbsp;is translated across both train and test.&nbsp;</p> <p>Thanks for posting the codes and keep trying.&nbsp;=)</p> <p>&nbsp;</p> <p>Eu Jin</p>
2464, <p>Hi Phil</p> <p>A checky question if you don't mind. Does that mean that Ockham's list of variables were the correct answers to part 2? Oh and who blew the whistle!?</p>
2464, <p>[quote=Sali Mali;3391]</p> <p>6) Also in the email you need to include<br> &nbsp;&nbsp; &nbsp;Your real name and those of your team members. You won't win any money if you don't supply your real name.<br> &nbsp;&nbsp; &nbsp;The team names of the 3 contestant who you think contributed most to the forum<br> &nbsp;&nbsp; &nbsp;The names of the teams that you think will finish in the top 5 winner first 5th place last.</p> <p>[/quote]</p> <p>&nbsp;</p> <p>There is one particular contestant which I felt has contributed alot and gone the extra mile. Can I vote for him/her 3 times?</p>
2464, <p>[quote=Sali Mali;3453]</p> <p>[quote=Eu Jin Lok;3452]</p> <p>There is one particular contestant which I felt has contributed alot and gone the extra mile. Can I vote for him/her 3 times?</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Well as long as it is not yourself I don't see why not. I guess that happens a lot on Australian Idol.</p> <p>[/quote]</p> <p>&nbsp;</p> <p>LOL!</p>
2464, <p>@Phillips</p> <p>Yes I found the same as well but I was kinda expecting that. I has suspected that the Public AUC tends to favour fewer features as indicated through my 10-fold CV score. I don't think that reveals anything about the patterns in the data thou other than  the Public AUC sample was just slightly different to the Private AUC sample. The private AUC was consistent with my 10-fold CV score as expected. Unless you're talking about more than 0.05 difference...then that means its overfitted.</p>
2464, <p>[quote=Zach;3796]</p> <p>[quote=Cole Harris;3787]</p> <p>I am guilty of overfitting the modeler. I was certain there were ~100 variables.</p> <p>[/quote]</p> <p>I was pretty sure the final model had all 200 variables...</p> <p>[/quote]</p> <p>I was also very convinced that the model had to had more than 100 variables. At 100 variables I scored an AUC&nbsp;of 95 through 10-fold&nbsp;CV and at 70 variables an AUC of 97. I was so sure they were overfitted so i took the safer option&nbsp;of 140 variables at  90 AUC. I'd love to find out if my variable selection at 70 would have actually given me AUC of 96 on test.... I'd be so shattered if it did!!! Having said that I'm very happy with&nbsp;the outcome certainly exceeded my expectations&nbsp;and had lots of fun learning  too!</p>
2464, Nope I don't think anyone has. Should be in the next 10 minutes or so or else it would be 5th of April when the official data release date (based on UTC time)!
2464, Might be death based on the profiles....i am however confused with the CharlsonIndex. Is it correct that they are dates? 
2464, <p>[quote=cmgast;2159]If you see dates for the Charlson Index that's probably Excel "helpfully" trying to interpret a "1-2" as "2-Jan".[/quote] Yes thanks heaps!</p>
2464, <p>Hi</p> <p>I'm creating a post on any questions relating to the Members_Y1 data. So anyone with questions relating to this data can just slot it in here. Hope that helps manage to number of post in the forum</p> <p>I'll start off with a dumb question...the AgeAtFirstClaim i'm getting some values of 1/10/2019 (Oct-19). Is this correct and what does it mean?</p> <p>Thanks</p> <p>EJ</p>
2464, Yes spot on. Thanks heaps
2464, <p>﻿﻿Hi all</p> <p>I'm struggling to keep up with all the topics in the forum at the moment. They are all important&nbsp;but&nbsp;there are already 29 topics in the forum&nbsp;only just after&nbsp;2 days&nbsp;into the competition. I can't speak for the rest but would it be possible to organise the topics by groups? Say for example a group for any questions relating to the competition rules one for the Claims data one for the Members data etc.</p> <p>I think this would be really helpful&nbsp;moving forward especially for those who join&nbsp;the competition at a later stage.&nbsp;Should make it easier to look for relevant information.</p> <p>Thanks - Eu Jin&nbsp;&nbsp;</p>
2464, <p>Hi</p> <p>&nbsp;</p> <p>I discovered quite a few Male patients whom I believe were pregnant (PRGNCY) ages from 0-9 up to 80+. For instance memberID 11832375. I googled male pregnancies to check and so far this is still an unachievable feat. Perhaps I've mis-intrepreted the information? If so can someone please clarify?</p> <p>Thanks</p> <p>EJ</p>
2464, On the contrary Anthony I searched the web an apparently this is an achievable feat! I think the medical term is called "Intersex" which may explain the erectile dysfunction... =)  I can't say that it is the case for this data set but certainly an outlier that is probable. Or the explanation may be as simple as the fact that the male patient insist on being classified as female.  Which leads me to wonder how are the claims data being collected in the US? For example is it through a computer system where the doctors key in the patient details and the patient swipes swipes the Health Care card for each claim. Is it at that point where we get the data like we have now?  Thanks Eu Jin   
2464, p does not need to be integer. Anthony confirmed:   http://www.heritagehealthprize.com/forums/default.aspx?g=posts&t=437
2464, Hi Chaseshaw  There's an existing post on this topic with more information:     http://www.heritagehealthprize.com/forums/default.aspx?g=posts&t=391  Eu Jin 
2464, Hi   The following information cannot be provided due to privacy issues. Anthony mentioned in one of the post if only i can find it....
2464, Confirmed anything more than 15 days is truncated to 15 days for privacy issues as its so rare it can be identified.   Also you need to read this thread to find out more about this response variable. It is a little ODD;  http://www.heritagehealthprize.com/forums/default.aspx?g=posts&t=389
2464, its uploaded now. Its a seperate file which you can download in the data section
2464, <p>@mkwan</p> <p>It is based on their first claim in between Y1 and Y3 so i'd say 21-30. A similar post over here:</p> <p>http://www.heritagehealthprize.com/c/hhp/forums/t/578/age-at-first-claim-how-is-it-handled-over-time-and-category</p>
2464, <p>[quote=Silentpartnr;5353]</p> <p>Questions:</p> <p>1. How much data am I missing in the claims file from Releases 2 and 3 (ie. how many rows should there be in each of these files; and</p> <p>[/quote]</p> <p>Don't worry about Release 1 and 2.&nbsp;Release 3 has all the data you need for this contest and you should get 2668990 rows&nbsp;(Y1 Y2 and Y3 claims). And from that you need to predict the Days In Hospital in for Y4. In other words you need to predict 1 year  ahead.</p> <p>[quote=Silentpartnr;5353]</p> <p>2. If when I open the claims data file Excel 2010 automatically opens them what are some of the other programs others may be using to access the full data file.</p> <p>[/quote]</p> <p>The simplest and fastest way to verify your row numbers is to use notepad&#43;&#43;. But further down the track you may want to consider better tools such as R which is highly recommended.</p> <p>&nbsp;</p> <p>All the best</p> <p>Eu Jin</p>
2464, <p>Sorry decided to move post to a new topic</p> <p>=)</p>
2464, <p>Hi</p> <p>Alot of us have absolutely no knowledge in this field what-so-ever but I believe great discoveries can come from all walks of life.</p> <p>I'd love to give this a go and may I ask if anyone knows of any good articles to get me started?</p> <p>Here's one I found:</p> <p><a href="http://www.eclipse.net/~cmmiller/DM/">http://www.eclipse.net/~cmmiller/DM/</a></p> <p>Thanks Eu Jin</p>
2464, @ tdk Thanks! Your own webpage had lots of useful information too!
2464, <p>Hi</p> <p>Just a quick note on the &quot;Mapping Dark Matter contest&quot; the results have changed. The scoring on the withold data set was inconsistent&nbsp;to the leaderboard (more details visit the forum) which resulted in a large shift in standings (or positions). They've  now addressed this issue and the leaderboard standings&nbsp;are now true reflections of the actual final standing - the leader on the leaderboard was the actual winner overall and the RMSE scores was consistent.</p> <p>There were some slight shifts in standings for the other&nbsp;competitors but they weren't too far off just 2 or 3&nbsp;ranks. As William mentioned when everyone's score&nbsp;is so close together you'd expect to see some slight changes. But at 3000 random selected cases  it is still a robust sample and&nbsp;I doubt the standings will change much. Unless of course a competitor has overfitted their model then their standings will surely drop. =)</p> <p>EJ</p> <p>&nbsp;</p>
2464, <p>Hi Bo</p> <p>&nbsp;</p> <p>I did not achieve the benchmark score with Anthony's script. And yes the sampsize call function means 100000 rows randomly sampled from training data. Usually done to improve processing speed. </p>
2464, <p>Hi Sir</p> <p>&nbsp;</p> <p>Its quite common to use codes&nbsp;96 97 and 98 to mean others don't know and refused to say and you see this all the time on survey data or census data. They are not coded as missing since they actually do mean something especially those who &quot;refused to  say&quot;.</p> <p>&nbsp;</p> <p>EJ</p>
2464, <p>[quote=Godel;5888]</p> <p>What is the method to deal with these kinds of observations then? I mean should we just ignore these observations or do some transformation? It would be great if somebody could provide some insight on this.</p> <p>Thanks.</p> <p>[/quote]</p> <p>Hi Godel&nbsp;</p> <p>Knowing how to deal with data issues like these is part of the competition. Real world data are really messy have a look at the HHP contest data. What you are seeing here is a great example of a data that may have been obtain from a survey / census hence  the 96 and 98 values. Everyone will have different ways of dealing with it depending on how they see it so there's no one true method. Personally I kept those values and did some transformation.&nbsp;</p>
2464, <p>[quote=Sali Mali;5928]</p> <p>The answer to what 98 may mean is in the data. Try building a model to find what are the characteristics of those whose value is 98 - when you figure this out all will become a bit clearer although in a way also a bit confusing!</p> <p>(hint - what may be a reason why it is impossible to go past due?)</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Yes I agree! I look at the data and I get it...and yet....I don't get it!!! </p>
2464, <p>[quote=elad bensal;5866]</p> <p>what does it mean values as 98 or 96 ???</p> <p>[/quote]</p> <p>Hi Elad</p> <p>&nbsp;</p> <p>96 and 98 are usually attribute to values such as Don't know Refused to Say Not relevant etc:</p> <p><a href="http://www.kaggle.com/c/GiveMeSomeCredit/forums/t/867/numberoftime30-59dayspastduenotworse-96-and-98">http://www.kaggle.com/c/GiveMeSomeCredit/forums/t/867/numberoftime30-59dayspastduenotworse-96-and-98</a></p>
2464, <p>Hi Bogdanovist</p> <p>The front page states - &quot;predicting the probability that somebody will experience financial distress in the next two years&quot;. To me thats quite clear what we're aiming to predict unless you are after something abit more specific</p> <p>For those&nbsp;new to&nbsp;credit scoring they can always do <a title="this" href="http://lmgtfy.com/?q=credit&#43;scoring"> this</a>&nbsp;</p> <p>=)</p>
2464, <p>[quote=gisleyt;5835]</p> <p>So from the task description:</p> <p>&quot;Credit scoring algorithms which make a guess at the probability of default are the method banks use to determine whether or not a loan should be granted.&quot;</p> <p>And we should make a new estimate of a probability of default yet we do not know how the gold standard is computed since the training data only provides binary scores.</p> <p>I fully agree with BarrenWuffet on this. To me it would only make sense to predict a probability (or a credit score) if the training data provided such a score.</p> <p>[/quote]</p> <p>Hi gisleyt</p> <p>You can choose to predict a binary score of&nbsp;1 or&nbsp;0 (definite yes or no) outcome. However due to the AUC measure you are more likely to get a lower score compared to if you submitted probabilities.</p> <p>There was a good discussion on the AUC metric in a previous competition and I reckon it would be a good start to understand the concept behind the AUC link is here:</p> <p><a href="http://www.kaggle.com/c/socialNetwork/forums/t/173/auc-calculation">http://www.kaggle.com/c/socialNetwork/forums/t/173/auc-calculation</a></p> <p>&nbsp;</p> <p>&nbsp;</p>
2464, <p>Could also be that its 2 months away from finishing so everyone's not going to go the whole mile yet. For me thou I'm just plain lazy.&nbsp;&nbsp; =)</p> <p>Having decided that I will put more effort this weekend on this project out comes a new competition on Kaggle......</p> <p>&nbsp;</p> <p>Oh and well done on coming 4th on the Dunnhumby contest Wil!</p>
2464, <p>[quote=Tanstaafl;7055]</p> <ol> <li>It seems like it was a bad idea to post the benchmark program as everyone is easily copying it and slightly tweaking parameters which will lead to an unskilled random winner. Why was this done? </li><li>I guarantee that everyone who is simply using this random forest technique is over-fitting noisy data which will not be useful in practice for this sponsor. </li><li>Why are the submissions requiring percentage results instead of boolean. Some people using sigmoid functions or svm will have different meanings for their percentages and threshold cutoffs. </li></ol> <p>&nbsp;[/quote]</p> <ol> <li>The benchmark in this case is a very simple and well known model. By posting the benchmark and the script the following is implied -- If the new algorithms here can't beat the benchmark then you might want to go back to the drawing board because there's  no point in using the new algorithm-- The benchmark aims to drive people to do better and acts as a measure for how you are performing compared to the current best practice. </li><li>Only the top 5 winners on the private leaderboard will be considered. So anyone who has fallen in the ovefitting trap will not get to the top 5. I certainly hope I haven't and have done so to make sure I haven't </li><li>I am not the best person to answer this but it is all due to the AUC metric. </li></ol>
2464, <p>[quote=Xiaonan Ji;5884]</p> <p>Is there something that I missed? I would assume this variable is between 0 and 1 but appearently there are many values that are greater than 1. Any suggestion? Thanks.</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Hi Xiaonan</p> <p>&nbsp;</p> <p>There was a similar discussion on this:&nbsp;</p> <p>http://www.kaggle.com/c/GiveMeSomeCredit/forums/t/874/errors-in-data</p>
2464, <p>[quote=Will Fourie;6019]</p> <p>Thanks guys. If I make up an infinite number of new variables as derivatives of the existing ones and stick them all into a model and some come up as predictive (by sheer number) is this not &quot;over-fitting&quot; a model too? And what about multicollinearity?  Maybe my definition is too broad (or wrong lol).</p> <p>[/quote]</p> <p>Deriving and modifying variables is an essential skill in this competition. Its one other way to give you a competitive edge over a pure algorithmic approach. If you keep deriving variables without any thought behind it chance are those variables won't  help at all. If in the off chance you found one that works then there is always a reason behind it. The data tells it as it is. =)</p>
2464, <p>To be politically correct:</p> <p>[quote=just1passerby;7169]</p> <p>A whole bunch of people lagging behind vsu seems to be overacting their anger [and jealousy?] as I see this whole thing.</p> <p>[/quote]</p> <p>The people venting their anger here are top competitors who have done well in Kaggle we don't feel inadequate in any way&nbsp;&nbsp;</p> <p>[quote=just1passerby;7169]</p> <p>..Vladimir seems to have a good point of limiting [total # of submissions during competition] rather than [# of submissions per day].</p> <p>[/quote]</p> <p>It was Soil who suggested this not Vladimir.</p> <p>[quote=just1passerby;7169]</p> <p>&nbsp;What is the practical/intellectual difference between [100 submissions done by 5 teams owned by 1 person who spent 10 days doing intense work] and [100 submissions done by 1 team owned by 3 persons who spent 50 days submitting 2 csv files each day]??!!!</p> <p>[/quote]</p> <p>Fact:</p> <p>Melbourne Uni Contest:</p> <p><a href="http://www.kaggle.com/c/unimelb/Leaderboard">http://www.kaggle.com/c/unimelb/Leaderboard</a></p> <p>uqwn - 27 submissions</p> <p>vyatka - 34 submissions</p> <p>grisha - 45 submissions</p> <p>Uni melbourne contest has a limit of 2 submissions per day. Grisha's account suggests that he joined 22 days early</p> <p>Another example:</p> <p>RTA contest:</p> <p><a href="http://www.kaggle.com/c/RTA/Leaderboard">http://www.kaggle.com/c/RTA/Leaderboard</a></p> <p>uqwn: 26 submissions</p> <p>vyatka: 48 submissions</p> <p>grisha: 65 submissions</p> <p>I think it was 2 submissions limit as well but either way the winner made 25 submissions in total.</p> <p>&nbsp;</p>
2464, <p>[quote=Alec Stephenson;7229]</p> <p>The big learning experience for me is how strong a team can be if the skills of its members complement each other. Rather like an ensemble in fact. None of us would have got in the top placings as individuals.</p> <p>[/quote]</p> <p>It was a prefect blend of skill and knowledge and coincidence brought us together at the most curcial time in this contest. The 3 of us had something completely different to offer.</p> <p>At an early stage I used GBM RandomForest Multi-layer perceptrons Mars Mutinomial Logit and many more which I cannot remember all implemented through the caret package in R (other than GBM and Random Forest). GBM worked best for me. At the mid point  I had spent most of my time trying to get SMOTE to work no success unfortunately. I was alittle dissapointed that SMOTE did not work as it took a large portion of my time. It is a solution in search of a problem and based on literature this was the prefect  problem for it. If you are interested give it a go perhaps you might be able to solve it.</p> <p>I've attached my R code with the SMOTE and modelling component in it. I'd love to hear your feedback of what worked and what didn't:</p> <p>Some of my learnings:</p> <p>1) Always clean your data. Whilst cleaning the data it helped me get a better understanding of the data and extract new features. I made sure the data was in its absolute best condition before modelling. A small mistake at this level can be costly.</p> <p>2) Visualisation is key. I was luckly enough to have worked on excel this time which allowed me do quick plots to see patterns in the data as I was cleaned it. If I had been using SQL I would have missed alot of the key features I derived.</p> <p>3) Documentation and planning will ensure a structured and methodical path in analysis. In a long and large contest information management is key. You want to spend more time in knowledge discovery so by documenting what you found and make a plan you  save alot of time.</p> <p>It was a fun experience. Thank you all for participating. =)</p> <p>Regards</p> <p>Eu Jin</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p>
2464, <p>Hi </p> <p>What are the rules around the use of external data? </p> <p>Thanks<br> Eu Jin</p>
2464, <p>Hi there</p> <p>&nbsp;</p> <p>I'd like to get some clarification and more understanding of the scoring metric - categorization accuracy. I can't seem to find anything about it on the web. Are you able to post the code (R preferably) as well?&nbsp;</p> <p>&nbsp;</p> <p>Thanks</p> <p>Eu Jin</p>
2464, <p>[quote=Ali Hassaïne;8665]</p> <p>Hi Eu Jin</p> <p>Thanks for your interest</p> <p>It's very simple.</p> <p>Categorization accurary = number of correctly identified documents / total number of documents</p> <p>We call it also in our field: the identification rate.</p> <p>Ali</p> <p>[/quote]</p> <p>Thanks Ali! Who would have thought the beauty of simplicity. That means a 100% is the perfect score.&nbsp;</p>
2464, <p>....thrirded</p> <p>It was an amazing event. I'd also like to add that despite competing against different teams all under one roof it was a very bonding experience. </p> <p>The thing I regretted most in this event is not spending more time to get to know the other participants. </p> <p>There should be an after-party event for this! </p>
2464, <p>here's the R evaluation metric code if anyone's interested:</p> <p>RMSLE &lt;- function(PA) {<br> &nbsp; sqrt(sum((log(P&#43;1)-log(A&#43;1))^2)/length(A))<br> }<br> <br> <br> </p>
2464, <p>For dimension reduction</p> <p>Areduce &lt;- A %*% Vreduce</p>
2464, <p>[quote=Gxav (Xavier Conort);13346]</p> <p>Even after the updates I still find the data weird. There is a huge gap between my cv score and the public leaderboard score. The distributions of my predictions for the test sets look also suspect (particularly for Act 2 5 8 and 15).<br> Have you done any test yourself? My code is pretty simple with no risk of overfitting. Are you sure the test data are correct? Are the test samples generated randomly?</p> <p>[/quote]</p> <p>Yup I agree with Gxav. My CV scores doesn't match up with the leaderboard. I don't know why yet but I am of the same opinion as Gxav. May be worth checking the test data again? </p>
2464, <p>[quote=Will Fourie;13820]</p> <p>I think the competition windows are too short.&nbsp; I'm a full time predictive modeller I study part time and I'm married - so you see I don't have much spare time for the Kaggle comps (at least not as much time as I'd like to give them).&nbsp; LOL.</p> <p>Compounding the problem:&nbsp; I'd like to attempt several of the competitions but given my time constraints and the competition time frames I end up choosing a single competition based on the prize pool problem complexity and time remaining.&nbsp; And so I've  chosen this competition over the CareerBuilder competition - but really I'd like to attempt both!</p> <p>Could I recommend competition windows of at least 3 months in the future?&nbsp; How do other Kagglers feel?</p> <p>&nbsp; [/quote]</p> <p>&nbsp;</p> <p>Wow I never thought I'd hear this.</p> <p>A year ago I had faced a different type of problem. There was hardly any new competitions and I was attempting every single one of them and the prizes were very small. There were on average 3 at a given point in time (excluding HHP) and the dataset was  not as complex</p> <p>So my answer to your question is that I'm impartial or rather I'm not sure what's best. I certainly echo your sentiment and would like longer time frames for big comps but keep in mind that moving forward there's going to be alot of competitions happening  all at the same time like what we're seeing now. I am also seeing that competitions becoming more specialised:</p> <ul> <li>ASAP = NLP (natural Language Processing) </li><li>US census = GIS (Geographical information systems) </li><li>Facebook = SNA (Social Network Analysis) </li><li>GEFComp = Time Series forecasting </li><li>etc.. </li></ul> <p>And its going to evolve even further</p> <p>Personally I don't think I can handle more than 3 competitions even if the time frames are long...its too much pressure. Like many here I work munge breath sleep and speak data. Surely too much of something can never be good? So maybe the better stragtegy  is to start specialising in competitions I know I can do well in. After all the data scientist are spreaded across specialise field so I suppose its a better chance of winning!</p> <p>Keen to hear other perspectives too. =)</p>
2464, <p>[quote=Jason Tigg;15792]</p> <p>I am not in this competition but I noticed that users kaggle rating points have already been modified according to their private scores in this competition&nbsp;</p> <p>[/quote]</p> <p>Wouldn't that just be the public score? In my user profiled it had assumed the public as the final standing hence scores calculated from there? no?&nbsp;</p> <p>In either case I predict:&nbsp;</p> <p>1) Data Robot&nbsp;</p> <p>2) Berry</p>
2464, <p>[quote=Dale Lehman;13736]</p> <p>Also as long as I've got the forum space here why the separate public and private leaderboards?&nbsp; I don't really understand what the purpose is of having separate ones.</p> <p>[/quote]</p> <p>Hi Dale&nbsp;</p> <p>Its to ensure participants don't overfit their models. Here's one example where it really can happen:&nbsp;</p> <p>http://www.kaggle.com/c/AlgorithmicTradingChallenge/leaderboard/public</p> <p>If you switch between public and private leaderboard scores you can see how different the rankings are. Essentially as a host you want your solution to be reliable and scallable so the private leaderboard is an unbias measure of performance&nbsp;</p>
2464, <p>Hi there</p> <p>I just wanted to clarify the rules. As a non-US citizen/PR I am not eligible to win the prize. But if I enter into a team which has at least 1 US-citizen/PR he/she as an individual within a team is eligble to win the full prize/rewards. Correct?</p> <p>&nbsp;</p> <p>Thanks</p> <p>Eu Jin</p> <p>&nbsp;</p>
2464, <p>Sorry wrong post</p>
2464, <p>Just my opinion:</p> <p>&nbsp;</p> <p>For the long term view pick up Python or R. If you want to work in agile mode in data analytics move on from SAS and pick up open source tools. SAS has licencing issues on many competitions in Kaggle and as you will encounter in your career only large corporate firms can afford SAS.&nbsp;</p> <p>&nbsp;</p> <p>R and Python are very different languages and comes with very different benefits. But once you've acquired one the other becomes less difficult to learn as you'll transfer your strong 'programming principals' from one to the other.</p> <p>&nbsp;</p> <p>It'll be a steep learning curve but in future you will be better off. Can't take short cuts in data analytics. =)</p> <p>&nbsp;</p> <p>&nbsp;</p>
2464, <p>[quote=rcarson;46028]</p> <p>[quote=Iancss;45993]</p> <p>Sorry to disturb you.</p> <p>I am confused that whether the prediction is an &quot;binary value&quot; or a &quot;float value&quot;?</p> <p>For example in my model it predicts that customer A have 90% probability to buy again.</p> <p>Then what is the prediction value? &nbsp; &quot;0.9&quot; or &quot;1&quot; (Float Number or Binary Number)?</p> <p>[/quote]</p> <p>sorry forget to quote you.</p> <p>[/quote]</p> <p>Not quite. It is not necessary to submit probabilities strictly speaking. You&nbsp;can&nbsp;submit any real-valued number as a prediction. The AUC is sensitive to&nbsp;rank so you can essentially submit a ranking of the IDs.&nbsp;</p> <p>EDIT: By the way here's a good&nbsp;AUC summary&nbsp;from a different comp on Kaggle:&nbsp;</p> <p>http://www.kaggle.com/c/stumbleupon/details/evaluation</p>
2464, <p>[quote=nhc;48747]</p> <p>I face this errors...</p> <p>Can anyone help me &quot;how to solve this problem?&quot;</p> <p>ValueError: time data '1' does not match format '%Y-%m-%d'</p> <p>[/quote]</p>  <p>Can you elaborate abit more about the error? I'm going to take a stab here and say that its because the test and train history data has different date format. You'll have to treat them both seperately</p>
2464, <p>[quote=LetterRip;49166]</p> <p>I'm pretty sure it is only the individuals who have redeemed the offer that are in the data set we are give.</p> <p>So of the original 100 customers we only see the 60 that have redeemed the offer.</p> <p>[/quote]</p>  <p>Yes and to add to that of the 60 customers above some of them make repeated purchases (purchase the product again without the coupon). This is what we are trying to predict. Not who redeems the offer.</p>  <p>The question of 'how do we know whether the customer redeem the offer?' would be useful to know&nbsp;but I don't see the use of it for this competition. Any reason why you want to know this?&nbsp;</p>
2464, <p>[quote=Kyle_BA;50323]</p> <p>There is an argument of cheating and successful random submissions falling into the top positions but surely these models would require validation via the client?</p> <p>Regards</p> <p>[/quote]</p> <p>Its a very awkward question to answer transparently. Season competitors will answer you in a very terse manner. Why?&nbsp;Because we don't want to&nbsp;teach you to cheat&nbsp;</p> <p>Cheaters are common place in Kaggle competitions many of them have made it to the top 10 and in contention to win prizes. The 1 week deadline is to&nbsp;restrict cheaters who are contenders to win. Its a serious issue because they are really hard to detect. A few &quot;less skilled&quot; cheaters can be found on the leaderboard. Can you see them?&nbsp;</p> <p>With 3 more days to go I only have 15 submissions left. I certainly would hope I have another extra 5 submissions. But like William says gotta be honest since everyone else is honest too. &nbsp;</p>
2464, <p>@kmcd</p> <p>Touche!</p>
2464, <p>@Trisco7</p> <p>Not sure what you mean there but the Target / label doesn't change. It remains 0 and 1. But what Triskelion is advising is to submit probabilities of between 0 and 1. Where 1 is 100% confidence that the likelihood of the label is 1.</p>
2464, <p>But there can only be one winner. Who will it be? =P</p>
2464, <p>[quote=Trisco7;50557]</p> <p>What is the difference between private and public leader boards?</p> <p>[/quote]</p> <p>Taken from the page:&nbsp;https://www.kaggle.com/c/acquire-valued-shoppers-challenge/leaderboard/public</p> <p>This leaderboard is calculated on approximately 20% of the test data.<br>The final results will be based on the other 80% so the final standings may be different.</p> <p>Also if you are finding that your scores are miles lower don't be surprised. Have a look through some of the post here:&nbsp;</p> <p>https://www.kaggle.com/c/acquire-valued-shoppers-challenge/forums/t/9630/72-roc-in-train-set-but-54-in-test-set</p>
2467, <p>I am wanting to predict a set of data for submission but I don't know where that set is which I am required to predict. Where can I find the set of data that is required to predict?</p> <p>&nbsp;</p> <p>Many thanks</p> <p>Steven Mark Ford</p>
2467, <p>Aaaah! Thanks Thomas I see it now.</p> <p>-Steven</p>
2467, <p>Am I allowed to offload computations onto some CUDA cores? The particular hardware I will be using costs around $80?&nbsp;</p> <p>I ask this in light of what was said on the prize page &quot;...so that individuals trained in computer science can replicate the winning results.&quot;</p> <p>As CUDA programming is not very common so someone trained in computer science might not be able to replicate the result with out doing substantial additional training?</p> <p>I don't want to go through the extra effort of porting my code to CUDA if I could potentially be disqualified.&nbsp;</p>
2467, <p>Could you please highlight the diff?</p>
2467, <p>2 Questions:</p> <p>1. Why are you asking this?</p> <p>2. What differences are you experiencing?</p>
2478, ----
2479, ----
2487, ----
2488, ----
2489, ----
2496, <p>Since you have other (hidden) data for final results. You shouldn't care too much about exploration of test data. I just experimenting for now and I don't want to wait 12 hours to test other solution. I think time interval in 10-30 minutes will be good compromise.  Thank you.</p> <p>UPD: Or second option is just provide some small subset from fourth year. In this case participants could write their own scoring system to check effectivness of solution at home.</p>
2496, <p>[quote=alexanderr;3749]</p> <p>If you could test against year 4 many times a day you could get a higher score using trial and error algorithms instead of showing real understanding of the data.The winning team must have an algorithm that does more than win-it must have an algorithm that  works every year it is needed and not just for one data set in one year.</p> <p>[/quote]</p> <p>Sure on TEST data. But the final checking will be performed on some other SECRET data. And your HI-score for current TEST won't be good for SECRET data.</p> <p>It's actually the orginizers should decide but I think some of users just lost an interest if they need to wait a day to check the real solution. I've just checked some trivial one to ensure about output data and now need to wait a day to check the real  one... So will be back in one day ... probably.</p>
2499, ----
2509, ----
2518, <p>Statistics student here What is the best method to reduce the categarical independent variable/ reduce the level of indepenedent variable in the training data set.</p>
2518, <p>Yes please I would like to know both the answers</p> <p>1. How to reduce the variable?</p> <p>2. How to reduce the level in a variable?</p>
2518, <p>I am a new bie in this data mining field. I have started working on this competition to learn new things. My computer is 32 bit machine and has 3.24 GB RAM. I am facing memory issues while training the data set. Can anyone please help me on this...</p>
2518, <p>Thanks a lot..........</p>
2518, <p>Thank you Leustagos  I read about ff package. It is very useful...</p>
2549, ----
2551, <p>Hi</p> <p>is it intentional that the meteorological forecasts are available only until 2012 06 26 while predictions need to be made until 2012 07 02.</p> <p>thanks</p> <p>Indrė</p>
2551, <p>Hi Pierre</p> <p>the file that was corrected is benchmark.csv right? <br> The one that I downloaded two days ago ended with a line<br> 753620120702000.2410.4180.010.1210.6670.0660<br> the current one available on the competition website ends with <br> 7488 2012062812 0.226 0.069 0.227 0.011 0.329 0.079 0<br> so now the predictions need to be made until 2012 06 28. Is that correct?</p> <p>One more question it seems that the submission format has changed the currect version is in tab format whereas the previous one was in csv. Which format should be used?</p> <p>thanks<br> Indrė</p>
2551, <p>Hi</p> <p>I have a question regarding the evaluation measure. The rules say:</p> <p>The RMSE is calculated as the square root of the average of square forecast errors.&nbsp;</p> <p>Is the average over 7 wind farms is taken first and then the square root or RMSE for each farm is calculated separately and them the average is taken?</p> <p>thanks</p> <p>Indrė</p>
2551, <p>Hi</p> <p>in relation to releasing the code should we also save all random seeds used so that the exact submission is reproducible?</p> <p>Indrė</p>
2551, <p><span style="line-height: 1.4">Is each observation an individual or a transaction?&nbsp;</span></p>
2551, <p>Are all the loans reported at the same time of their lifecycle? For instance are they all reported at the time of granting the loan?&nbsp;</p>
2551, <p>Regarding&nbsp;Loss Given Default is this value calculated as a percentage from the initial loan amount or the remaining amount at the time of default?</p>
2551, <p>Could 0 loss include loans that have defaulted but were fully recovered (e.g. from a collateral)?</p>
2552, ----
2554, ----
2555, ----
2558, ----
2559, ----
2564, <p>I am confused about the meaning of Yhat.</p> <p>From the competition notes: &quot;We expect the participants to produce a score between -Inf and &#43;Inf large positive values indicating that A is a cause of B with certainty large negative values indicating that B is a cause of A with certainty.&nbsp;Middle range  scores (near zero) indicate that neither A causes B nor B causes A.&quot;</p> <p>But as you say this is *ternary* problem: if a large positive value indicates A is a cause of B with certainty a large negative value can only mean A is not a cause of B.</p> <p>It seems I should produce *two* numbers: Z1 to indicate the degree that A is a cause of B and Z2 to indicate the degree that B is a cause of A. Obviously consistency requires they cannot both be large.</p> <p>As an example consider a ternary probabilistic classifier and let Z1=logit P(A-&gt;B) and Z2=logit P(B-&gt;A).</p>
2564, <p>Hi Isabelle</p> <p>Thanks for your reply--I know you must be pretty busy.</p> <p>I really like this challenge I am just concerned the scoring system is not evaluating actual beliefs.</p> <p>Here is an example: suppose I think P(A-&gt;B) = 0.6 and P(B-&gt;A) =0.1. Then under your suggestion Yhat = logit 0.6 - logit 0.1 = 2.6. But I would get the same score if my beliefs were P(A-&gt;B) = 0.79 and P(B-&gt;A) = 0.21 or P(A-&gt;B) = 0.3 and P(B-&gt;A) = 0.03. While  these always favour A-&gt;B over B-&gt;A in the last case it much more likely that there is no direct causal connection at all!</p> <p>I have no problem turning a ternary classification problem into two one-against-the-rest classification problems it just seems I should provide a number for each problem.</p>
2564, <p><span>Same issue : submission &quot;pending&quot;...</span></p>
2564, <p>I saw somewhere that the leaderboard is calculated using only 30% of the submitted data so more saturation is expected.</p>
2589, ----
2602, ----
2606, ----
2609, ----
2658, ----
2667, Hi all<BR><BR>When I submit the form asks me to enter a description of 600 charaacters or fewer.&nbsp; However when I click on your submissions my description gets cut off.&nbsp; How can I view my full submission description?
2667, I am getting dead links to all of my .csv submission files before today.&nbsp; I just submitted my seventh submission which I can retrieve but my first six are not accessible.<br mce_bogus="1">
2667, The bug is not fixed.  I can assure you I did not get a 0.  There was something wrong with the way my entry was submitted.<br mce_bogus="1">
2667, The Public MAE given on the Submissions page for my 2nd submission is still "Scoring..." instead of an actual number like 0 which is reported on the Leaderboard page.  For my first submission 0.238571 is reported as was also given on the leaderboard so that is correct.  At least in my case I contend something did go wrong in the scoring process.  Perhaps I did get a 0 and the submission page just needs to reflect this.
2667, Disregard
2667, <p>Please pardon me if it has already been answered but I have been scouring the posts and not found an explanation. &nbsp;In the original training data the domain1 score appears to be the sum of all four traits for both raters which would lead to a scale of  0-24. &nbsp;The explanation for the essay set says that content (trait 1) is doubled which would lead to a scale of 0-30. &nbsp;While I can certainly train for a scale of 0-24 as is in place now I just want to make sure that the human scoring does not suddenly change  on either validation or the not yet released test set.</p>
2667, <p>I organize a local <a href="http://www.meetup.com/Front-Range-PyData/" target="_blank"> &quot;Python for Data Analysis&quot; user group&nbsp;</a> and we'd like to use Kaggle as a way for people to experiment with and learn the Python libraries. &nbsp;In fact we're planning to dedicate our next meetup to working together on a competition.</p> <p>Are there any competitions that allow large team sizes (and allow new people to join the team after it's started)? &nbsp;Ideally it'd be a competition with a low barrier to entry (in terms of data size and general complexity). &nbsp;Any suggestions for using Kaggle  in this type of setting?</p> <p>Dan</p> <p>PS We could pair off into smaller teams but we'd like for the presenter at the meetup to distribute some &quot;getting started&quot; code without breaking any rules.</p>
2667, <p>I'd be interested in seeing the new pages and hopefully giving useful feedback.</p>
2667, <p>I started a very similar thread (a long time ago) on the Heritage health prize forum. &nbsp;You can find it at&nbsp;http://www.heritagehealthprize.com/c/hhp/forums/t/805/project-management-software-for-data-analysis</p> <p></p> <p>I think you are looking for much more than this but I've gotten a long way with</p> <p>1) Source control</p> <p>2) Programmatically naming all output files with names that include the date/time they were produced.</p> <p></p> <p>It's easy to pull the source code that produced any output file (by pulling the version of the code that was current in source control at that time.)</p>
2667, <p>What's the protocol for proposing new competitions?</p> <p>I'd like to suggest a competition for early detection of forest fires from satellite images remote sensing devices and weather data. &nbsp;I haven't thought this through much and the devil is always in the details. &nbsp;But I'm curious whether this is something  Kaggle would potentially be interested in hosting.</p> <p>I also may have contacts at organizations that hold this data if that helps.</p> <p></p> <p>Of more general interest is the protocol for proposing/developing new contests available anywhere?</p> <p></p>
2667, <p>Check out&nbsp;<a href="http://blaze.pydata.org/docs/latest/index.html">Blaze</a> which extends Numpy and Pandas to include out-of-core computation.</p> <p>It's being developed by Continuum.io so it may be easier to install if you are using their Anaconda distribution than if you are using some other Python distribution.</p>
2667, <p>Hi</p> <p>I'd like to chat with some users to figure out how we can improve your Kaggle experience. For anyone willing to answer a couple questions and share your feedback over a short phone call drop me a private message or email me at dan@kaggle.com</p> <p>Thanks for your help!</p> <p>Dan</p>
2667, <p>While the relationship isn't 1:1 it seems obvious from what I've seen in the industry that Kaggle rank correlates to salary.</p>  <p>Personally I've received numerous job contacts on the basis of kaggle performance (including a couple jobs I've accepted).  It's also been part of my personal pitch when I've interviewed and it's been clear that this has made an impression.  </p>  <p>And I'm not unique in this regard.  There are A LOT of people (including OP) with more impressive kaggle results than me.  And I've heard people with both better and worse ranks tell me about how it has benefited them professionally.</p>  <p>Presenting yourself and your work well may make a bigger difference but casual empiricism suggests that Kaggle results buy you a lot of professional credibility.</p>  <p><em>Disclaimer: I used to work at Kaggle. It was one of those jobs I took after being noticed for performance in Kaggle competitions.</em></p>
2667, <p>How do you think data science will look different in 10 years from what it looks like today?</p>
2667, <p>Amulet: &nbsp;In principle you are right. &nbsp;There may be some disadvantage (in the long run) to taking a milestone prize. &nbsp;Anyone concerned about winning the milestone prize can specify a mediocre submission as their entry for the milestone prize to ensure they  don't win.</p> <p>Whatever disadvantage Market Makers incurred from publishing their first milestone paper wasn't great enough to stop them from winning the second milestone. &nbsp;Similarly Edward and Willem took second place in both milestones so far. &nbsp;</p> <p>Either the disadvantage from winning a milestone is smaller than it appears or those teams are just very impressive (or as I suspect both).</p>
2667, <p>As I understand the data a member may have DIH&gt;=1 for year 2 with no year 2 claims in the claims table if they were not an HPN member in year 3.</p> <p>&nbsp;</p> <p>If they aren't a member in year 3 than HPN doesn't have DIH for year 3.&nbsp; So HPN wouldn't include their Y2 claims data.</p>
2667, <p>When I merged the claims table and the members table I didn't find any members without claims. &nbsp;Are others finding the same thing? &nbsp;If so why is that? &nbsp;Surely there were members who didn't use medical care.</p> <p>&nbsp;</p> <p>Thanks</p> <p>Dan</p>
2667, <p dir="ltr">@FrogEater: Thanks. That makes sense.</p> <p dir="ltr">@arbuckle: Removing members without claims will create a selection bias when HPN applies this to a broader population. For a simple example consider a sub-population with a set of characteristics X. Assume that 99% of this sub-population does  not use medical care in a 5 year period. The remaining 1% use much more medical care than average in the 5 year period. In the data used for this competition group X will appear to be an especially high risk group even though in the broader population of  HPN members they are an especially low risk group. This doesn't make the competition any harder but it may affect the ability to generalize what HPN learns from the competition. Of course the winning algorithm can later be applied to a full dataset that  includes people who used no medical care. Whether a winning algorithm will perform equally well in that more heterogeneous population is an open question.</p>
2667, <p>@Signipinnis I don't know if I understand your first point.&nbsp; <strong>If</strong> we were interested in predicting hospitalization for <strong>all</strong> insurees we'd need to know the size of this omitted group (and their hospitalization rates in the target year).&nbsp;</p> <p>For the reasons that arbuckle kindly explained I was thinking in terms of the wrong population... and HRN is explicitly interested in the subpopulation with claims prior to the target year.</p>
2667, <p>I'm trying to test some distributional assumptions for the days in hospital.&nbsp; I know the Kolmogorov-Smirnov test is underpowered when the parameters of the distribution are estimated from the data... Can anyone suggest&nbsp;other tests?</p> <p>I'm contemplating modeling days in hospital as a zero-inflated poisson.&nbsp; Anyone else going in this direction?</p>
2667, <p>Hi</p> <p>I had a model where I used the previous year's claims table to predict days in hospital. &nbsp;Adding the previous year's &quot;days in hospital&quot; to the model significantly improved the fit in the training data (from year 3) and it improved the fit in year 3 data  I set aside for cross validation. &nbsp;But it signifantly worsened the fit in the year 4 target.</p> <p>I of course used the year 2's days in hospital to forecast year 3 and used year 3's days in hospital when forecasting year 4. &nbsp;I imputed 0's for individuals who did not appear in the previous years days in hospital table.</p> <p>Would anyone care to speculate if there's some sort data issue explaining this surprising pattern?</p> <p>&nbsp;</p> <p>Thanks</p> <p>Dan</p> <p>&nbsp;</p>
2667, <p>Thanks for the response alexanderr... but I think I was unclear about what I was imputing to be 0.</p> <p>I was imputing the 0's on the days in hospital for the year in which that data is missing not for the year after it is missing. &nbsp;In this sense I am not making a judgment about whether going to the hospital was good for one's health or bad for one's health.  &nbsp;&nbsp;I just thought DIH that were missing were necessarily 0. Looking back to the various posts about the DaysInHospital table this does not appear to be the case. &nbsp;</p>
2667, <p>That's interesting Karan.&nbsp; You've either found better transofrmations or you have different other covariates... because dsfs and paydelay&nbsp;never proved useful (e.g. improved fit) for me</p>
2667, That's a good improvement Karan. Since you mentioned pregnancy: I'd point out that the data has the condition code is PRGNCY for some males as well as rather elderly folks. Since the code probably means something different for a 60 year-old dude than  it does for a 20-something woman I made interaction terms for male*pregnancy and over50*pregnancy. That improved my score a similar amount.
2667, <p>Karan: &nbsp; I think you are correct that a RF should find important interactions if those interactions can be formed given the structure of your data. I'm still hand tuning a parametric model at this point.</p> <p><br> <br> Regarding dsfs and paydelay: I already have counts of claims in the model. I would expect dsfs to have explanatory power without controlling for claim count because it's a reasonable proxy for # of claims. If you have a lot of claims you likely have some far  apart as well as some that are close together. I just didn't find those had power controlling for everything else. Maybe we just have different controls.<br> <br> I inserted paragraph breaks in this post using the link to the html editor and typing the html control codes. There's probably an easier way. Maybe an admin will chime in on that. &nbsp;:)</p>
2667, <p>Anyone else getting an error&nbsp;</p> <p>&quot;Field index must be included in [0 FieldCount[. Specified field index was : '2'. Parameter name: field Actual value was 2.&quot;</p> <p>&nbsp;</p> <p>Looking at my submission file I don't see anything that looks obviously in error (e.g. extra quotation marks mislabeled fields etc.)</p>
2667, <p>I've moved on to new projects. &nbsp;As a last experiment I want to see if I can significantly improve your score by incorporating my predictions. &nbsp;I've read about boosting but haven't tried it yet. &nbsp;I want to send someone my predictions and see if is useful  to them. &nbsp;If this helped someone get in the money I wouldn't want ask for any part of it.</p> <p>-----</p> <p>Overview of my algorithm:</p> <p>My algorithm was fairly straightforward and I think it was different from what most people here are using. I created variables from the data that I thought would be predictive. &nbsp;I then ran an OLS regression on training data</p> <p>DIH=beta*variables</p> <p>I used the fitted values from that regression as an index of predicted health usage. I ran a very simple non-parametric estimator to map the index to predictions that minimizes rmsle.</p> <p>-------</p> <p>What I was going to do next (In case anyone cares):</p> <p>I'd like to include a quite a few more variables (e.g. more dummy vars for specific vendor more interaction terms) but I think I have a method to reduce overfitting when I do so. &nbsp;I would have included these variables in a multi-level estimation framework  that shrinks imprecise estimates towards group means. &nbsp;I was going to use methods from Gelman and Hill's book. &nbsp;This incorporate &quot;regression to the mean&quot; to reduce overfitting. &nbsp;I was going to implement this in PyMC but you could do it in R too. &nbsp;I thought  this was a really good idea (and I thought it was the big advantage of using a regression in the first stage rather than random forests.) &nbsp;I don't have time to follow it through but hopefully the idea interests someone.</p> <p>-----</p> <p>How to take me up on the offer:</p> <p>If kaggle says I can make my predictions or my code publicly available I'll do so. &nbsp;I cleaned the data in stata and did estimation in python. &nbsp;If I'm only allowed to give it to one team I'd like to see if it helps someone that already has a better algorithm  than me. &nbsp;Drop me a line though.</p> <p>I'm out... have fun predicting.</p>
2667, <p>B</p> <p>My linear model has quite a few vars. If I don't hear an objection from kaggle about it I will clean up my source code put together and better explanation and make my work avail as a zip file. I thought this was an intersting project so let me know if  you have any questions or want to chat about anything. </p> <p>Right now I only have time to type of a list of what is in the model</p> <p>counts of the number of charlson1 charlson2 charlson3 and charlson4 observations <br> whether the charlson1 charlson2 charlson3 and charlson4 observations in the previous year are positive.<br> the charlson index of the last claim in previous year<br> age<em>male dummies and age</em>female dummies/intercepts (omited group is ageMissing)<br> Number of days for each specialty<br> A count of observations and total number of days in in-patient hospital<br> a count for claims in each other place of service<br> counts of claims for each primaryConditionGroup<br> Counts for Inpatient claims in each primary condition group with at least 50000 claims and 5000 in-patient claims<br> claimstruncated</p> <h1>of claims for each procedure</h1> <p>lagged Days in hospital (imputed based on gender and age if not available)<br> dummy for lagged days in hospital == 0<br> Dummy for lagged days in hospital &gt;=10<br> pregnant<br> pregnant<em>labTests<br> pregnant</em>whether baby was delivered (based on in-patient stay)<br> pregnant<em>tests</em>whether baby was delivered<br> preg<em>Male<br> preg</em>age==20-29<br> preg<em>age==30-39<br> preg</em>age==40-49<br> preg<em>age==50-59<br> preg</em>age==60&#43;<br> preg*ageMissing<br> outpatient claims for primaryconditiongroup msc2a3<br> peds claims for primaryconditiongroup msc2a3<br> suplos<br> number of claims for primary condition group codes and procedure combinations with many observations: I converted primary condition group and procedure groups to numbers based on alphabetical order. The list is<br> pcg 3 procedures 2 3 5 and 13 (this makes 4 variables)<br> pcg 12 procedures 24 58<br> pcg 20 proceudre 4<br> pcg 23 procedures 2347<br> pcg 27 procedures 23457<br> pcg 28 procedures 23<br> pcg 38 procedures 2 3 5<br> pcg 42 procedure 11<br> pcg 44 procedure 3<br> Counts of claims for each specialty<br> count for place of service==in-patient<br> counts of claims with each pcp who had </p> <p>Counts of claims for a group of vendors and pcp's that had enough observations to estimate this precisely. Those turned out to be (ignore the trailing Cou and Co characters)<br> pcp1303Cou<br> pcp2136Cou<br> pcp2448Cou<br> pcp2469Cou<br> pcp3394Cou<br> pcp4025Cou<br> pcp4313Cou<br> pcp4523Cou<br> pcp5300Cou<br> pcp9524Cou<br> pcp10164Co<br> pcp11148Co<br> pcp13281Co<br> pcp16757Co<br> pcp18175Co<br> pcp18880Co<br> pcp20090Co<br> pcp20893Co<br> pcp21146Co<br> pcp21579Co<br> pcp22193Co<br> pcp23056Co<br> pcp26051Co<br> pcp27467Co<br> pcp30569Co<br> pcp30870Co<br> pcp32724Co<br> pcp33193Co<br> pcp33303Co<br> pcp33843Co<br> pcp35565Co<br> pcp35832Co<br> pcp36452Co<br> pcp36955Co<br> pcp36990Co<br> pcp37301Co<br> pcp37759Co<br> pcp37796Co<br> pcp38110Co<br> pcp38583Co<br> pcp38762Co<br> pcp39372Co<br> pcp39946Co<br> pcp40607Co<br> pcp41370Co<br> pcp42381Co<br> pcp43790Co<br> pcp44164Co<br> pcp44537Co<br> pcp46162Co<br> pcp46795Co<br> pcp47414Co<br> pcp48905Co<br> pcp51763Co<br> pcp56126Co<br> pcp59950Co<br> pcp62284Co<br> pcp62871Co<br> pcp63771Co<br> pcp64709Co<br> pcp70119Co<br> pcp70171Co<br> pcp70222Co<br> pcp70553Co<br> pcp70686Co<br> pcp71040Co<br> pcp71847Co<br> pcp72000Co<br> pcp72351Co<br> pcp73550Co<br> pcp73982Co<br> pcp74354Co<br> pcp75037Co<br> pcp75876Co<br> pcp76634Co<br> pcp77134Co<br> pcp78718Co<br> pcp80381Co<br> pcp80533Co<br> pcp81146Co<br> pcp82373Co<br> pcp86472Co<br> pcp86510Co<br> pcp86658Co<br> pcp86723Co<br> pcp87960Co<br> pcp88511Co<br> pcp88661Co<br> pcp89127Co<br> pcp90868Co<br> pcp91972Co<br> pcp92411Co<br> pcp93075Co<br> pcp94201Co<br> pcp94891Co<br> pcp96614Co<br> pcp98627Co<br> pcp98900Co<br> pcp99068Co<br> pcp99196Co<br> vendor9717<br> vendor2610<br> vendor3194<br> vendor3556<br> vendor6476<br> vendor1110<br> vendor1224<br> vendor1403<br> vendor1526<br> vendor1648<br> vendor2400<br> vendor2518<br> vendor2536<br> vendor2862<br> vendor3066<br> vendor3274<br> vendor3698<br> vendor4254<br> vendor4725<br> vendor4913<br> vendor4962<br> vendor5054<br> vendor5597<br> vendor5606<br> vendor6178<br> vendor7063<br> vendor7850<br> vendor7912<br> vendor9722</p>
2667, <p>I don't know how many variables are on my list... but if I'd continued working on this I'd aim to have a lot more.</p> <p>The OLS regression takes under 10 seconds to run on my laptop. &nbsp;So computation is a non-issue. &nbsp;Maybe some of those vendor codes are overfitting but I think the hierarchical model (or any shrinkage estimator) would reduce overfitting. &nbsp; &nbsp;</p> <p>Of course it's easy to talk about &quot;what I would have done&quot; as I walk out the door ;)</p>
2667, <p>Uri</p> <p>My apologies. &nbsp;I cut and pasted parts of that list so it didn't contain much explanation:</p> <p>&nbsp;</p> <p>1)Charlson1234(there are no 1234 and I guess that he means to the 4 different options for charlson index)</p> <p>That's correct.&nbsp;</p> <p>2)age male dummies and age female dummies but not age missing?</p> <p>I have a constant in the regression. &nbsp;In this case that represents the age missing group. &nbsp;I could have included age missing and excluded the constant.</p> <p>3) a count of observations and total number of days in in-patient hospital(what is the meaning of total number of days (it can be days of stay and it can be days in hospital in previous year)</p> <p>The count is the number of claims. &nbsp;The days in in-patient hospital is the sum of the days for individual claims.&nbsp;</p> <p>4)lagged days in hospital(does it mean days in hospital in previous year?)</p> <p>Yes</p> <p>5)What is the difference between Pregnantwhether baby was delivered and Pregnanttestswhether baby was delivered?</p> <p>The latter is pregnant*# lab tests*delivered. &nbsp;The first variable doesn't include the lab tests term.</p> <p>6)preg*ageMissing(does it mean preg and age missing)?</p> <p>Yes</p> <p>&nbsp;</p> <p>Mark:</p> <p>You are correct. &nbsp;It is the days in hospital listed for the year before we want to predict.</p>
2667, <p>Uri</p> <p>When I said previous year that may have been confusing. &nbsp;I meant the year of the claims (the year prior to what we are predicting). &nbsp;If I am predicting Y3 I want to know whether the last claim in Y2 was &quot;serious.&quot; &nbsp; In this sense perhaps I should have  said current year. &nbsp;That resolves the problem of lacking data since we only estimate y[n] when there are claims in y[n-1]. &nbsp;</p> <p>If there are multiple claims tied for last based on dsfs I base this variable on the highest charlson index for those claims. I'm trying to capture health towards the end of the year. &nbsp;If someone has a claim of severity 1 and a claim of severity 3 their  condition is at least as bad as someone who has only a single claim of severity 3 in that month.</p>
2667, <p>Uri</p> <p>Your code looks right to me. &nbsp;</p> <p># of days is from length of stay. &nbsp;I picked an integer whenver I was given a range. &nbsp;For example 1-2 weeks was 10 days. &nbsp;</p> <p>The stuff in blue is supposed to be&nbsp;</p> <p># of claims for each procedure group.</p> <p>&nbsp;</p> <p>Not sure what happened with the formatting to cut off some words and make it a big blue font.&nbsp;</p>
2667, <p>Uri</p> <p>I don't do anything special with length of stay for those claims. I include a variable for number of supLos claims and I hope that offsets the measurement error problem for observations with supLos claims.</p> <p>If computing power were unlimited you could break down SupLos for each person/year observation by the type of claim it is associated with. The regression algebra would work out so the coefficient on the &quot;days&quot; variables would be the effect for non-supLos  claims and ((days*coefficient on days)&#43;)# supLos claims * coefficient on supLos claims)) would be the estimate of that category for supLos claims.</p> <p>Given that there are something like 11000 supLos claims I haven't put in the time to drill down too far into this. I think including a count of supLos claims for each person/year combo is a good first step.</p>
2667, <p>I apologize Uri. Looking at my code I created inpatient * pcg for every primary condition group.</p> <p>The 50000-5000 rule was for interaction variables between primary condition group and procedure group. I created pcg<em>condition for every pcg/procedure combination with 5000 claims in the pcg/procedure intersection and 50000 for that pcg that are not  that procedure. Some of those interaction terms didn't turn out to be very predictive so I cut some of them on an ad-hoc basis. This left the pcg-procedure combination combos in the bottom of my orignal list. But there were pcg</em>inpatient combos for everything.</p>
2667, <p>The non-parametric (2nd step) estimator was something I wrote myself. &nbsp;I don't think it has a name but it's vaguely similar to an m-estimator. &nbsp;In the end it was a bunch of extra code and I don't think it had any significant advantage over LOESS or an  m-estimator. &nbsp;</p> <p>That estimator is just creating a function f such that f(index) minimized rmsle. &nbsp;If I were starting over I'd run OLS in the first stage with a ton of variables (like I did) and then use LOESS or an m-estimator in the 2nd stage. &nbsp;You should use log(1&#43;daysinhospital)  rather than daysinhospital as the realized values you are try to match in the 2nd stage. &nbsp;</p> <p>If you use my general strategy <strong>finding good explanatory variables in the first stage will make a much bigger difference than choosing the &quot;optimal&quot; non-parametric estimator for the 2nd stage</strong>. &nbsp; &nbsp;</p> <p>Whether you use the estimator I wrote LOESS or an m-estimator won't affect your score materially. &nbsp;So why not start with LOESS which is widely available and relatively easy to understand.</p> <p>To recap I suggest</p> <p>1) Throwing a bunch of powerful explanatory variables into OLS in the first stage. &nbsp;</p> <p>2) Make predictions which I will call &quot;index.&quot;</p> <p>3) Use LOESS to create a function f that gives a good fit to&nbsp;log(1&#43;daysinhospital)=f(index)</p> <p>4) Apply your OLS parameters and LOESS function f to the target data</p> <p>5) Submit</p> <p>6) Find better explanatory variables and repeat.</p>
2667, <p>It seems a lot of people are concerned they might win a progress prize they don't want.</p> <p>My understanding is that you can choose which submission is considered for the prize. If you don't want the progress prize (and everything that comes with it) make a submission where every prediction is 1. For anyone concerned NOT winning the prize should  be very easy. </p> <p>Personally I'd be surprised if those at risk of winning a progress prize did this.</p>
2667, <p>I think a submission queue (either pure FIFO queue or something that users can reorder) would be a great feature.</p>
2667, <p>Are there any project management tools for data analysis (something that integrates a version control system keeps track of relationships between data files and source code etc.)?</p> <p>While I'm at it what larger data analysis communities forums are there to ask this sort of question?</p>
2667, <p>Jeff</p> <p>I'm doing something close to what I think you are suggesting (using Mercurial).&nbsp; I only use version control on my source code and I recreate intermediate data files from source when necessary. Do you put your data in version control?&nbsp;&nbsp;</p> <p>When you say&nbsp;committing links the data&nbsp;and source is that something beyond that you committed them at the same time?&nbsp; Unless you always run all your code at the same time this still requires some care.</p> <p>For instance when&nbsp;I&nbsp;modify the source that creates&nbsp;a data file I want a warning&nbsp;if I try to use&nbsp;that data file before updating&nbsp;it by re-running&nbsp;my&nbsp;modified source.&nbsp; Can git do that?</p>
2667, <p>Jeff</p> <p>The makefile is a great suggestion. I don't know how I've never thought about using those for statistical work.</p> <p>I was hoping to change my workflow in a way that goes beyond reproducability (though that's important too). I'm hoping for something that improves my efficiency along the way. If you have other workflow suggestions I'd love to hear them.</p> <p>I just followed Allan's links and Project Template looks useful. I'm also about to buy Long's Stata book. </p> <p>Thanks guys!</p>
2667, <p>My first model was a zero-inflated poisson regression which is very close to what I think sfin is suggesting. This worked poorly for me but my mistake would be easy to fix. So I'll explain my mistake.</p> <p>First I estimated the model. <br> Then I calculated the conditional pmf of days in hospital for each person. <br> Finally I calculated each person's expected value of days in hospital from that pmf.</p> <p>That expected value was my submission. But the loss function does better if you submit the expected value of (e^E[log(daysinhospital&#43;1])-1.</p> <p>Those aren't the same because logs aren't linear. So if you calculate a pmf for each possibility make sure you convert days in hospital to logs and then expand it back out after calculating the expected values.</p>
2667, <p>Will those who don't win find out where they were ranked (i.e.. whether they were in 5th place 15th place or whatever.)</p> <p>Thanks.</p>
2667, <p>There are some extremely smart people working at kaggle (for evidence check out Jeff Moser's blog at http://www.moserware.com/).</p> <p>It seems reasonable to assume they know how to generate a random sample.</p>
2667, <p>@Outsider: &nbsp;In the &quot;Give me some credit&quot; competition some teams <a href="http://en.wikipedia.org/wiki/Overfitting" target="_blank"> overfit</a> the leaderboard data. &nbsp;Overfitting occurs even on randomly drawn samples and avoiding it&nbsp;is one of the basic challenges of predictive modeling (both in these competitions and outside them).</p> <p>As you suggest it's possible (and likely) that some teams will do significantly worse on the final score data than on the public leaderboard data. &nbsp;That isn't a flaw in the competition. &nbsp;The teams that move up in the final rankings just did a better job  of a certain part of modeling (avoiding overfitting) than the teams that move down.</p> <p>@JermeyA: I'm fairly confident that the hospitalization data that kaggle uses to create your leaderboard score is the same every time you submit.</p>
2667, <p>I think there's a very wide spectrum between &quot;rewriting a model daily&quot; and &quot;randomizing output.&quot; &nbsp;We have many submissions that similar to each other. &nbsp;If we have a model that we like but we find a few new variables that we think will be predictive of the  target we will re-run the model with the extra explanatory variables. &nbsp;That's not starting from scratch but it's not randomly tweaking coefficients either.</p> <p>That said the ensembling process described in Chris's link is probably the #1 factor explaining the high submission counts for all the top teams.</p>
2667, <p>Esla</p> <p>That is a very interesting phenomenon that the predictions are all different but they generate similar scores. If they are using a validation dataset it would be interesting to compare their scores on the validation dataset. Assuming they are not doing  that (and k-fold validation is introducing too much complexity) it would be interesting to see how much their R^2 or RMSE differ on the training dataset.</p> <p>Another interesting experiment would be to have some of the student teams merge and &quot;ensemble&quot; their submissions. Since the scores are so similar you could reasonably ensemble the best submissions from multiple teams using the simple of their respective  submissions. If their underlying predictions are actually disimilar I think they'd be pleasantly surprised to learn how well this works.</p>
2667, <p>1) It's data that you are given not something you estimate. It indicates whether the claims information for the previous year is truncated (presumably to protect patient confidentiality).</p> <p>2) These are used to help create a chronology of events for each patient. As you noticed each drugcount labcount and claim has it's own dsfs. If you have two labcount records the labcount record with the higher dsfs value is the later lab count. Drug  counts and lab counts are aggregated... my recollection is that each record is the aggregate number of drug or lab counts for a month but I may be remembering incorrectly (and it may be aggregated for a 2 month period).</p> <p>3) No. DaysInHospital_Y2 is the number of days that the patient spent in the hospital in the 2nd year. The idea is to build a model where you use claims from year K to predict hospitalization in year K&#43;1. </p> <p>A common workflow is to train/estimate a model using hospitalization in the 2nd year as an outcome to be predicted from Y1 claims and year 3 hospitalization as an outcome to be predicted from Y2 claims. Once you've trained those models you can use claims  data from the third year to predict hospitalization in the 4th year. We aren't given Days<em>In</em>Hospital4 and that is what the competition is judged on.</p>
2667, <p>Amulet:&nbsp;To predict Y4 you should only use features from Y3 (and prior years if you choose).</p> <p>Imagine being an insurer on Dec 31 of Y3 trying to set insurance premiums for Y4. &nbsp;You don't have any data from Y4 data yet so you will try to forecast Y4 outcomes (like days in hospital) from previous years' data.</p>
2667, <p>Have Kaggle and/or Heritage determined when they will announce the final prize winners? &nbsp;If so when should we expect to hear?</p> <p>&nbsp;</p> <p>If the date has not been set can you give us a rough estimate?</p> <p>&nbsp;</p> <p>Thanks for the putting together this great learning experience.</p>
2667, <p>Hi Rie<br> I'm trying to use the RGF code you linked to in your paper. When I try to compile the code by running the makefile in os x I get the error</p> <p><br> Dans-Laptop:rgf1.2 dan2$ make<br> /bin/rm -f bin/rgf<br> g&#43;&#43; src/tet/driv_rgf.cpp src/com/AzDmat.cpp src/tet/AzFindSplit.cpp src/com/AzIntPool.cpp src/com/AzLoss.cpp src/tet/AzOptOnTree_TreeReg.cpp src/tet/AzOptOnTree.cpp src/com/AzParam.cpp src/tet/AzReg_Tsrbase.cpp src/tet/AzReg_TsrOpt.cpp src/tet/AzReg_TsrSib.cpp  src/tet/AzRgf_FindSplit_Dflt.cpp src/tet/AzRgf_FindSplit_TreeReg.cpp src/tet/AzRgf_Optimizer_Dflt.cpp src/tet/AzRgforest.cpp src/tet/AzRgfTree.cpp src/com/AzSmat.cpp src/tet/AzSortedFeat.cpp src/com/AzStrPool.cpp src/com/AzSvDataS.cpp src/com/AzTaskTools.cpp  src/tet/AzTETmain.cpp src/tet/AzTETproc.cpp src/com/AzTools.cpp src/tet/AzTree.cpp src/tet/AzTreeEnsemble.cpp src/tet/AzTrTree.cpp src/tet/AzTrTreeFeat.cpp src/com/AzUtil.cpp -Isrc/com -Isrc/tet_tools -O2 -o bin/rgf<br> src/tet/AzTree.cpp: In member function ‘void AzTree::finfo(AzIFarr* AzIFarr*) const’:<br> src/tet/AzTree.cpp:232: error: call of overloaded ‘abs(double&amp;)’ is ambiguous<br> /usr/include/stdlib.h:146: note: candidates are: int abs(int)<br> /usr/include/c&#43;&#43;/4.2.1/cstdlib:174: note: long long int __gnu_cxx::abs(long long int)<br> /usr/include/c&#43;&#43;/4.2.1/cstdlib:143: note: long int std::abs(long int)<br> make: *** [all] Error 1</p> <p><br> Hopefully it's just a matter of changing the type declarations but I'm years out of practice in C&#43;&#43;.<br> Thanks for any help<br> Dan</p>
2667, <p>I'm skeptical that a final submission can do well (either in the public leaderboard or the final award data) without some degree of overfitting. &nbsp;The ridge regression technique described (and similar ensembling techniques) uses information from the leaderboard  in making the final blend. &nbsp;Any time you use a meaningful amount of information from one dataset you'll likely have some degree of overfitting.</p> <p><span style="line-height:1.4em">But since the only distinction between the leaderboard data and the private data is due to random sampling the information you learn from leaderboard scores is too valuable to be thrown away entirely by not incorporating  it into your final blend.</span></p> <p><span style="line-height:1.4em">So I don't think it's a question of whether to overfit but of how much to overfit.</span></p> <p>&nbsp;</p> <p>Seems like this competition was a great learning experience for all... I'd love to meet other players over beer some time and chat about the whole thing.</p>
2667, <p>I'm looking at the training_set_rel2.tsv file and some of the punctuation marks are showing up as weird characters I've never seen before. &nbsp;Essay sets 1 and 2 seem fine.</p> <p>Essay 5979 is the first where I have a problem. The third sentence starts with the character ﾓ</p> <p>Almost all of the quotation marks and apostophres are being shown as weird symbols from this point on. &nbsp;Is anyone else getting this problem? &nbsp;Do we know what characterset is being used so I can convert it to unicode?</p> <p>Thanks</p> <p>Dan</p>
2667, <p>Thanks Ben</p> <p>That was what I assumed from the answer to&nbsp;<a title="this link" href="http://stackoverflow.com/questions/4685568/importing-file-with-unknown-encoding-from-python-into-mongodb">this StackOverlow question.</a>&nbsp; However the python function that decodes Windows-1252  is still returning unusual characters. &nbsp;Here was my attempt to decode essay 7000. &nbsp;The first command is before decoding and the second is after decoding. &nbsp;</p> <p>&nbsp;</p> <pre style="padding-left:60px">In [157]: essays[7000][essay_content]<br>Out[157]: 'To a cyclist the surrounding setting can either cause triumph or despair. The cyclist was given very old directions. He was given back roads that are abandoned now. These towns had no people in them normally that would not matter but he \x93was traveling through the high deserts of California in June.\x94 (@NUM1). If there was shade a breeze and @NUM2 weather he would be fine but he is pedaling a bike in a desert during the summer. A \x93ghost town\x94 with no good water could have killed him. A cyclist needs to know their surroundings and be prepared for what nature throws at them.'<br><br>In [158]: essays[7000][essay_content].decode('cp1252')<br>Out[158]: u'To a cyclist the surrounding setting can either cause triumph or despair. The cyclist was given very old directions. He was given back roads that are abandoned now. These towns had no people in them normally that would not matter but he \u201cwas traveling through the high deserts of California in June.\u201d (@NUM1). If there was shade a breeze and @NUM2 weather he would be fine but he is pedaling a bike in a desert during the summer. A \u201cghost town\u201d with no good water could have killed him. A cyclist needs to know their surroundings and be prepared for what nature throws at them.'</pre> <p style="padding-left:60px">&nbsp;</p> <p>The unusual characters are converted but they aren't converted to anything sensible. &nbsp;If anyone has ideas for what's going on here I'd appreciate it.</p> <p>Thanks!</p> <pre>&nbsp;</pre>
2667, <p>Oops. &nbsp;I should have caught that.</p> <p>Thanks so much!</p>
2667, <p>Essays 6320 9369 and 10055 all threw exceptions when I put essay text in python strings (called text) and call</p> <p>text.decode('cp1252')</p> <p>I'm quickly double checking the other essays but I think everything else looks fine.&nbsp; I'm not inclined to worry about these three essays... just mentioning it in case it's useful to anyone else who runs into similar issues later on.</p> <p>&nbsp;</p>
2667, <p>@Ben: &nbsp;I've also had trouble loading the file. &nbsp;Apparently lots of other people aren't having problems so I'm not sure what the deal is. &nbsp;I'm using python on linux... so linux is a common factor between us.</p> <p>I got rid of the exceptions by loading the text with latin1. &nbsp;I also found tabs in some of essay text which is a problem in a tab delimited file... not sure if that's related to reading the file using latin1 instead of the suggested 1252. &nbsp;Good luck...  let us know if you stumble on any good solutions.</p>
2667, <p>I haven't used the RF packages in Matlab so maybe this is only of limited use. FWIW I noticed in other competitions that I got much better scores from the randomForest library in R than from using random forests in Python's scikit.learn. </p> <p>Regarding your experience: the fact that you are normalizing the score array to (01) suggests the Matlab is treating this as a regression problem. Instead it should be trying to solve this as a classification problem. If the dependent variable is stored  as a factor variable the R implementation automatically handles this correctly. Is there an option in the Matlab package to explicitly use classification?</p> <p>Also how are you normalizing the score to (01)? Because of the loss function you should probably bound your predictions away from 0 and 1 (e.g. set a lower bound of .01 instead of 0). Is your normalization procedure keeping the mean prediction around  .19?</p> <p>In general the need to normalize would be a red flag for me.</p>
2667, <p>Hi Forecasters</p> <p>I just created my first submission for this contest. &nbsp;Since I'm doing this for fun and a learning experience I thought I'd sharemy code in the hopes that it's useful to others.</p> <p>We also have a &quot;Python for Data Science&quot; meetup group near Denver and I'll be presenting this code among other things at the upcoming meetup to give people a chance to see pandas and sklearn in action. &nbsp;In case anyone else is in central Colorado more info  on the meetup group is available at&nbsp;http://www.meetup.com/Front-Range-PyData/</p> <p>Again I hope this code is useful or interesting. &nbsp;If you have any thoughts or feedback I'd love to hear that too.</p> <p>Good luck in the competition</p> <p>Dan</p> <p>&nbsp;</p> <p>PS You can also check this out from a public bitbucket repository at&nbsp;https://bitbucket.org/dansbecker/global-energy-forecasting-competition-2012-wind-forecasting</p>
2667, <p>In the case of NikitSaraf and myself that is likely because I made my code publicly available in a previous post (http://www.kaggle.com/c/GEF2012-wind-forecasting/forums/t/2645/check-out-my-code)</p>
2667, <p>I'm looking at the &quot;most popular&quot; benchmark and it appears different from my calculations.</p> <p>For example in python I do</p> <p style="padding-left:30px">train = pd.read_csv(my_dir&#43;&quot;train.csv&quot;)</p> <p style="padding-left:30px">like_counts =&nbsp;train.groupby('event').interested.sum()</p> <p style="padding-left:30px">like_counts.sort()</p> <p>&nbsp;</p> <p>The second most popular event appears to be&nbsp;2529072432 with 94 likes. &nbsp;If this is the 2nd most popular event I wouldn't expect it to be listed later the 2nd for any user. &nbsp;But we find this is frequently the case. &nbsp;For instance consider the line</p> <p>23465780[955398943L 602394192L 2529072432L 1600413013L 3017074183L 2414358105L 3807695143L]</p> <p>where it is listed after an event with only 12 likes.</p> <p>This is just one example but I'm finding lots of instances like this.</p>
2667, <p>Thanks for the quick response... though that makes me want to verify I understand the metric correctly. &nbsp;The rules say&nbsp;</p> <p>&quot;<span>Events should be ordered from the ones in which you predict the user will be most interested to those in which the user will be least be interested.&quot;</span></p> <p>Can I infer from your response that &quot;interested&quot; in the rules refers to the probability of attending rather than probability of saying they are interested?</p> <p>Thanks</p> <p>Dan</p> <p><span><br> </span></p>
2667, <p>If anyone wants to team up for any of the new &quot;representation learning&quot; challenges get in touch.</p> <p>My primary goal for these challenges is to learn more about theano and pylearn2 so I'd want to team up with someone else using these packages.</p>
2667, <p>Anyone want to team up for this competition (or one of the other representation learning competitions)? &nbsp;</p> <p>&nbsp;</p> <p><span style="font-size:14px; line-height:1.4em">I've been interested in semi-supervised learning for a while but I haven't done much with it. &nbsp;</span></p> <p><span style="font-size:14px; line-height:1.4em">I have a minimal understanding of theano. &nbsp;I've been meaning to check out pylearn2 but haven't gotten around to it. &nbsp;So gaining experience with pylearn2 is one of my major goals for this competition.</span></p> <p>&nbsp;</p> <p>Let me know if you're looking to use similar tools for one of these competitions and want to team up.</p>
2667, <p>I'm experimenting with Sigmoid units by modifying the supplied benchmark code for 2 layer networks with rectifier units but I'm getting a surprising result. &nbsp;My validation error keeps getting stuck at 0.800000011921.</p> <p>&nbsp;</p> <p>I've tried a wide range of parameters to the SIgmoid layer but it doesn't appear to affect my output. &nbsp;I'm currently initializing it in the yaml file as</p> <pre>            !obj:pylearn2.models.mlp.Sigmoid {<br>                layer_name: 'h1'<br>                dim: 1875<br>                monitor_style: 'detection'<br>                sparse_init: True<br>            }</pre> <p>&nbsp;</p> <p><span style="font-size:14px; line-height:1.4em">though I've tried many other ways to initialize it. &nbsp;</span></p> <p><span style="font-size:14px; line-height:1.4em">Is the Sigmoid benchmark publicly available? Anyone (and by &quot;anyone&quot; I'm guessing I'm just asking Ian:) have any advice?&nbsp;</span><span style="font-size:14px; line-height:1.4em">Also is there a better venue  where I should ask pylearn2 questions in the future?</span></p> <p><span style="font-size:14px; line-height:1.4em">Thanks!</span></p>
2667, <p>With regards to my original issue Ian's proposed solution fixed it.</p>
2667, <p>When you first start an MLP it wonders around for a some iterations before it starts making progress. &nbsp;The amount of wandering depends to some extent on the initialization parameters (the irange or sparse_init) though the initial momentum setting and learning  rate settings play a huge role too.</p> <p>You've likely also set the mlp to stop whenever it goes sufficiently long without making progress. &nbsp;If your final score is 0.81 then it's tripping this condition during the wandering period. &nbsp;</p> <p>In some cases I've played with the momentum and learning rate to get an mlp that starts walking towards the goal sooner and that has solve the problem. &nbsp;A more reliable solution would be to set the termination criteria to be more patient and hope the  mlp starts making progress before it hits the termination criteria. &nbsp;</p> <p>To allow more patience before termination you need to increase N in the termination criteria. &nbsp;So that might look like</p> <pre>            !obj:pylearn2.termination_criteria.MonitorBased {<br>                prop_decrease: 0.<br>                channel_name: &quot;valid_y_misclass&quot;<br>                N: 50<br>            }</pre>
2667, <p>Jose</p> <p>R's nnet package was originally written a long time ago and there's been a shift towards the use of bigger neural networks (both in terms of more hidden layers and more units in each hidden layer).&nbsp; This trend has been especially dramatic recently with  the success of &quot;deep learning.&quot;&nbsp; This shift has been driven by improvements in both hardware and in the underlying algorithms.&nbsp;</p> <p>As an example of &quot;algorithmic change&quot; larger networks are more sensitive to how the weights are initialized and our knowledge about how to initialize weights has improved over time.</p> <p>Similarly larger datasets allow more precise estimation of the weights in large networks.&nbsp; So the ideal network topology for a large dataset will have more units than the ideal topology for a smaller dataset.</p> <p>This competition gives a reasonably large amount of unlabeled data and modern NN techniques allow unlabeled data to be incorporated in training (e.g. using autoencoders).&nbsp; As a result larger networks seem to work well for this problem.</p> <p>I think pylearn2 caters more to this type of problem whereas the nnet package in R is intended for problems that don't for example leverage unlabeled data.&nbsp; As a result the sizes of networks used are going to be much different.</p>
2667, <p>Domcastro: &nbsp;You can make it run by setting a sufficiently high MaxNWts when you call nnet.</p>
2667, <p>I'm not sure which line in dataset.py you've changed to&nbsp;</p> <p><span>preprocessor.input_to_h_from_v(X)</span></p> <p>so it's hard to debug it. &nbsp;Pylearn2 is built on theano which adds a lot of complexity to normal python programming. &nbsp;Fortunately pylearn2 saves you from having to muck about too much in the raw python/theano code by using separate model specification files.</p> <p>You write model specifications in yaml files and then call train.py on that yaml file. &nbsp;If you've already trained a model you may have already done that. &nbsp;Otherwise start by<span style="font-size:14px; line-height:1.4em">&nbsp;looking at the yaml files in the&nbsp;scripts/icml_2013_wrepl/black_box  and modifying those model specifications as desired. &nbsp;There is also a README in that directory with other relevant details.</span></p> <p>It's conceivable to do everything directly by modifying the library (the py files) but this is making things much harder on yourself.</p>
2667, <p>Joerg</p> <p>I think it's likely that the learning rate in that example (0.01) is too low for the algorithm to make any progress away from it's starting point. &nbsp;It's worth experimenting with that value and see if it progresses.</p> <p>As an aside a lot of teams in this competition are using pylearn2. &nbsp;It's a library that implements a lot of the algorithms in those examples (and does so on top of theano.) &nbsp;Working in theano is a comparatively low-level programming language experience.  &nbsp;If you want to make things easier on yourself pylearn2 is great.&nbsp;</p>
2667, <p>Hi</p> <p>I've been meaning for a small collaborative project to test out the tools at dominoup.com</p> <p>&nbsp;</p> <p>Any python programmers interested in joining me as a team for this project? &nbsp;I'm not intending to put too much effort into this just enough to test the collaboration tool. &nbsp;But I still think it'd be fun. &nbsp;Drop me a line if interested.</p>
2667, <p>Upgrading packages is more reliable in Anaconda. &nbsp;I've had errors with packages in pip where I somehow ended up with incompatible versions of different packages. &nbsp;</p> <p>Even for versions that are supposed to work together you can get run-time incompatibilities if the packages were installed using different underlying compilers.</p>
2667, <p>[quote]</p> <p>In case you are taking a classification+regression two step approach I want to mention when you are trying to make the final prediction for&nbsp;the loss given default one may better varying the threshold/cutoff to first determine which one is a defaulter and then apply the LGD regressor you built to them with the purpose to minimizing the overall MAE.</p> <p>[/quote]</p>  <p>Assuming the probability of default is greater than 0.5 how are you calculating a prediction given the lgd prediction. &nbsp;</p> <p>I think there is a good theoretical justification for&nbsp;estimating&nbsp;the 10th 20th 30th ... quantiles of&nbsp;loss given default for each borrower and then&nbsp;choose the right percentile based on pr[default] to get the median expected loss. &nbsp;For example if there is a 40% likelihood that loss is 0 I would take the 10th percentile of this borrower's lgd distribution.</p> <p>However the results from this method haven't been as good as I expected.</p>
2667, <p>Tantrev</p> <p>Do you realize you are accusing people of cheating without any support for your accusation?</p> <p>The few players that are far ahead have a history of doing very well in many competitions most of which didn't have this type of data leakage issue. Their performance here is likely just a reflection of the same skill that's landed them at the top of the previous competitions.</p> <p>Even if there were cheating plenty of us are playing it fair. So the appropriate remedy isn't for the admins to help you take advantage of the data leakage. If you can't trust the other players there are plenty of Kaggle competitions that haven't had this type of issue.</p>
2667, <p>James King:</p> <p>I agree with you that there may be some leakage that is not in the prohibited variables and the administrators have explicitly said everyone can use everything still in the data.</p> <p>However for context this conversation is specifically talking about the columns that have been deleted and that we are prohibited from using. &nbsp;I'm surprised anyone would argue we are allowed to use the prohibited columns. &nbsp;</p> <p>&nbsp;</p> <p>Given the disagreement in this forum I hope Kaggle will explicitly announce whether we are allowed to use these columns in any way.</p> <p>Though I'm fairly confident I know what the response will be.</p> <p>&nbsp;</p>
2667, <p>For python users: I've been plotting with the pandas plotting functions for a while (and previously with matplotlib). &nbsp;But I recently tried out seaborn and I think it's really nice.</p> <p>In case it inspires others to discover this nice plotting library I put up a&nbsp;<a href="https://www.kaggle.com/users/9028/danb/forest-cover-type-prediction/plotting-with-seaborn-adios-matplotlib">very simple example</a>. &nbsp;Something like this would have been quite a headache in&nbsp;matplotlib... and I think it looks nicer than what I've seen in bokeh or in pandas plotting.</p> <p>What libraries are others using for plotting in python these days?</p>
2667, <p>Hi</p> <p>I'd like to chat with some users&nbsp;to figure out how we can improve your Kaggle experience. For anyone willing&nbsp;to answer a couple questions and share your feedback over a short phone call drop me a private message or email me at dan@kaggle.com</p> <p>Thanks for your help!</p> <p>Dan</p>
2667, <p>I think the hard part about an hypothesis test here is that there are so many possible models and so many possible&nbsp;hypotheses we could use.</p> <p>On the simplest side you could conceive of a model where all kaggle competitors have the same level of skill in which case it's reasonable to pool our predictions together. &nbsp;This would allow an hypothesis like&nbsp;</p> <p>&quot;Kaggle competitors' are usually better at estimating odds than vegas bookies.&quot; &nbsp;Each prediction we make is better than the bookie prediction if our prediction assigned a higher probability to the event that actually occurred than what is implied by vegas odds.</p> <p>This yields a bunch of 1's and 0's and the hypothesis you want to test is that our process would systematically yield 1's more than 50% of the time. &nbsp;Since the event is binary you could get standard errors from the binomial distribution.</p> <p>The variance would be n*p*(1-p) and under the null hypothesis p=0.5.</p> <p>The t-statistic is &nbsp;(fraction of 1's - 0.5) / sqrt(variance)</p> <p>Slightly more sophisticated would be to look at how much better or worse our individual predictions are (rather than just saying &quot;1 they are better&quot; or &quot;0 they are worse.&quot;) &nbsp;I think you could do&nbsp;this with&nbsp;a likelihood ratio test (comparing the likelihood functions from our predictions to those of bookies).</p> <p>If you have nothing better to do with your time you could write down a model that allows you to test more&nbsp;interesting/realistic hypotheses.&nbsp;I&nbsp;could imagine a model where competitors' scores vary due to both variation in skill and due to luck. &nbsp;Treating skill as a latent variable you could estimate&nbsp;a distribution for a competitors' skill conditional on his final score. &nbsp;I'd think you could then test hypotheses like &quot;the best Kaggle competitors are better than the bookies.&quot;&nbsp;I haven't thought through the details I just think something like this is possible.</p>  <p>PS Given my performance on this competition so far you might take my thoughts with a grain of salt. &nbsp;If Arizona loses you should ignore me completely.</p>
2667, <p>Though surprising to see that people don't like <em>br</em> and <em>isn</em>.  </p>  <p>Amazon users and their unsophisticated palettes.</p>
2711, ----
2730, ----
2732, ----
2742, ----
2748, ----
2749, ----
2752, ----
2758, ----
2762, ----
2780, <p>[quote=Paul Duan;25371]</p> <p>Hi everyone</p> Since it seems we have a lot of Coursera students with us (of which I'm also a fervent user) I wanted to share some simple starter code in Python to help those who are new at machine learning. You might also want to read&nbsp;<a>Foxtrot's excellent post about beating  the benchmark using Vowpal Wabbit.</a>&nbsp;In contrast this code uses Python with scikit-learn and aims at giving a base on which to expand for those who want to be a little bit more hands-on or have more flexibility in the algorithm design. It provides an example  on how to design a simple algorithm including performing some pre-processing training a logistic regression classifier on the data and assessing its performance through cross-validation. I also added some comments to point at where to go next. The script  assumes you have train.csv and test.csv in a folder named data in the same location as the classifier.py file. The strategy itself is essentially the same as Foxtrot's ie. training a linear model on the original data with nothing else changed excepted for  the one-hot encoding. In this case the model used is a regularized logistic regression. This will net you an AUC score of .885 -- have fun! Edit: forgot to remove an import in the original file. Use classifier_corrected.py instead. <p>[/quote]</p> <p></p> <p>I receive this error</p> <p>AttributeError: 'module' object has no attribute 'OneHotEncoder'</p>
2780, <p>This is really a very easy way and smart way to make a simple model based on searching patterns. Thank you for sharing the idea.</p> <p>Mohamed</p>
2780, <p>Would you please update your code after fixing it. I am really interested in it. Thanks in advance.</p> <p>Mohamed</p>
2780, <p>[quote=Dylan Friedmann;26318]</p> <p>Just tried it. &nbsp;My best model gives .891 this method gave .828</p> <p></p> <p>my code if anyone is interested in doing this in R.... one thing that could be improved is to only apply the probability if the number of entries that person has is above a certain number... might overcome some limitations of not knowing the resource. &nbsp;personally  I'm not going to presue this any further though :p</p> <p></p> <p><a>https://gist.github.com/dylanjf/5879356</a></p> <p>[/quote]</p> <p></p> <p>Would you please update your code after fixing it. I am really interested in it. Thanks in advance.</p> <p>Mohamed</p>
2780, <p>[quote=Karan Sarao;26290]</p> <p>On the train file create a concat string of all variables from MGR_ID till Role_Code (i.e. exclude only Resource column you may want to introduce a seperator in the concat  for eg. CONCATENATE(C2&quot;_&quot;D2and so on.)). Now pivot out the concat string and  in the summation field create average of ACTION variable.</p> <p>Create the similar concat string in the test file. Vlookup the Average value of the ACTION variable against the concat column. You will get under 4K '#N/A' i.e. combinations which dont exist in the train file. Simply replace them with .9421 (Average value  of ACTION field). Create the submission file and .825 on the leaderboard without any modeling.</p> <p>If you want to improve this score simply replace the #N/A rows only with you previous model's estimate chances are if you previous model is around .88 the new submission will beat it.</p> <p>Rgds!</p> <p></p> <p></p> <p>[/quote]</p> <p></p> <p>This is really a very easy way and smart way to make a simple model based on searching patterns. Thank you for sharing the idea.</p> <p>Mohamed</p>
2780, <p>Hi Mathworks and Kaggle Team</p> <p>It is noticed that your benchmark does not detect &nbsp;pack overlap at all. I am sure that most of submitted solutions have pack overlap but not detected by your benchmark. for such a problem you have two solutions</p> <p>1- ask competitors to upload their code</p> <p>2- fix your benchmark code for a more complex but accurate one</p> <p>&nbsp;</p> <p>I wish if you take this post seriously to make the competition fair enough for all competitors.</p> <p>&nbsp;</p> <p>Mohamed Ali</p>
2780, <p>I can see that your modification does not accommodate package rotation. it is obvious to me that it considers file not dimension intact.</p>
2780, <p>Thanks. removed</p>
2780, <p>Thanks</p>
2780, <p>Thanks for sharing</p>
2780, <p>I have added a featured post in my <a href="http://market-analysis.net/santander-customer-satisfaction/">blog</a> for this great competition. Please share your ideas opinion &amp; any questions there so we can discuss it as a community.</p>  <p>Also I welcome you to write any analysis articles there as this blog is intended to be a Kaggle community to discuss problems and there solutions</p>
2799, ----
2831, ----
2840, ----
2860, ----
2863, ----
2888, ----
2889, ----
2895, ----
2917, ----
2935, ----
2947, ----
2954, ----
2958, ----
2959, ----
2963, ----
2969, ----
2975, <p>Whatever the form of database you're using for this loading all URL's will take too much time. Even if you manage to get it you'll have large query execution times.</p> <p>I would strongly suggest that you sample the training set first and use that subset for building a model and only run your model calibration on the full data set once you're pretty confident that it produces reasonable results. Of course this comes at the expense of the extra work of coming up with a good sampling method to maintain the structure of the original dataset in the reduced set. Hope that helps.</p>
2975, <p>Sampling a percentage of days in the training period is one way to do it. What it provides you with is a data subset that is rich in user information since you'll probably be preserving a large number of users but that reduced dataset will not be as rich in pointing out temporal variations (short period of time relative to the original training set). On the other hand you could decide to take a sample of users instead of days say 5% of the unique users in the dataset with all their associated queries over the entire period of time. This gives you the advantage of having the full view over the time period provided but with a reduced number of users.</p> <p>So it really depends on what your model is attempting to explore. If you're looking at temporal or time-dependent characteristics of querying habits for users then you might be better off with a longer period of time to work with hence sampling a set of users. If you're more interested in detailed user preference analysis and how that relates to query relevance then you might want to have a large enough set of users and sampling a short period of time with a large number of users might be a better way of doing this.</p> <p>One more thing to bear in mind if you decide to sample is to watch out for your model's robustness. You want to make sure that your model performs on the full dataset close to how it performs on the reduced one. This is important because you're inevitably losing information in the sampling process. One way you could do that is to repeat the sampling process several times (say producing 5 reduced datasets) and comparing the performance of your models between these different subsets. You want the spread between performance across the different data samples to be as small as possible to ensure robustness.</p> <p>Hope that helps</p> <p>&nbsp;</p>
2975, <p>This is probably a misunderstanding on my part but shouldn't the sum of probabilities at every level in the decision tree sum to 1? It doesn't seem to adhere to this rule.</p> <p>Thanks</p>
2975, <p>Great that makes sense now. Thanks a lot :)</p>
2984, <p>Yes please. I'd like to have the true ratings for test data to evaluate my model for research purpose.</p>
3043, ----
3046, ----
3064, ----
3065, <p>Hi Dan</p> <p>I have a question about the chronological order of rows for each bond_id. &nbsp;You said they are in time order. &nbsp;Do they appear from the oldest to the newest (most recent) or the other way around?</p> <p>Thanks</p> <p>Austin</p>
3065, <p>This may be pedantic but I'll ask anyway. The &quot;is_callable&quot; column is described as indicating whether or not the bond is callable by the holder. My understanding is that bonds are callable by issuers and puttable by bondholders. Can someone clarify?</p> <p>Thanks<br> Austin</p>
3066, ----
3080, ----
3090, ----
3108, ----
3112, ----
3126, ----
3152, ----
3154, ----
3175, ----
3193, ----
3199, ----
3209, <p>+1 forget about deadline and fail to submit. If those who provided benchmark submission or rather all-zeros submission when competition starts can take part but me not I think such deadline is not good idea.</p>
3209, <p>Hi Kiran</p> <p>can you clarify where you got days.csv and what its contents are.</p> <p>what are the test_dates.</p> <p>I really appreciate your help</p> <p>Thanks</p> <p>&nbsp;</p> <pre class="x_prettyprint">cutoff_days &lt;- read.csv (paste (cv_path 'days.csv' sep='/') header=TRUE stringsAsFactors = FALSE)<br>	cutoff_days$cutoff_time &lt;-  as.POSIXct (cutoff_days$selected_cutoff_time tz=&quot;UTC&quot; format = &quot;%Y-%m-%d %H:%M:%S&quot; origin=&quot;1970-1-1&quot;  )<br><br>	for (myDate in test_dates) {<br>		print (paste (&quot;for myDate&quot; myDate sep=&quot;:&quot;))<br><br></pre>
3209, <p>Dear Moderator</p> <p>is this competition still open to new enrollees</p> <p>Thanks</p> <p>Kiran</p>
3209, <p>Are multiple submissions allowed per user.</p>
3209, <p>Dear Moderator</p> <p>can you please provide the format for the weather station file.</p> <p>Can you please explain the difference between Cutoff_benchmark file and estimated_arrival file.</p> <p>Are these the expected values for the given Scheduled_arrived file</p> <p>Thanks</p> <p>Kiran</p>
3209, <p>Does anyone know if Gates are assigned by a pattern or in other words are they driven by the parameters we have.</p> <p>A gate could be assigned arbitrarily.</p> <p>As opposed to the arrival time which can be predicted to a marginal error rate.</p> <p>Has anyone been successfull in Modelling/Predicting/Mining the Gate Code.</p> <p>Thanks in advance</p>
3209, <p>Does the Evaluation Data Set contain all the files provided in Initial Training Set</p>
3209, <p>Dear all</p> <p>being a newbie to this problem. What makes this such a complicated problem as opposed to a simple regression prediction.</p> <p>What are the undercurrents that could make one deviate from the results.</p> <p>Is this a Data Analysis or a Data Simulation problem.</p> <p>Is it the volume of the data or the complexity of the data that could be an issue</p> <p>Any comments that are like don't do this would be greatly appreciated.</p> <p>I am Not looking for any solutions thus not violating any rules of the competition</p> <p>just the pitfalls and trying to understand the problem domain</p> <p>Thanks</p>
3209, <p>What does the adsiairway convey &nbsp;for a flight plan</p> <p>Thanks</p> <pre>19;0;&quot;SJN5&quot;<br>19;1;&quot;J144&quot;<br>19;2;&quot;V246&quot;<br>19;3;&quot;VEENA2&quot;</pre>
3209, <p>Dear Admins</p> <p>what is the UTC time for the start date/time for each test day and end date/time for each test day.</p> <p>There are way too many numbers floating around</p> <p>for example for the set 11/12/2012 what would the numbers be</p> <p>Thanks</p>
3209, <p>Hi</p> <p>can we use aipot's Lat Long and does the Waypoint data have the entire flight journey from start to finish before flight</p> <p>Thanks</p>
3209, <p>Dear Admin are all the times in the PublicLeaderboard Augmented set files UTC&nbsp;</p> <p>Thanks</p>
3209, <p>Good Evening Ben:<br> Can you confirm that the timezone of all files in both PublicLeaderboard Augmented and also in InitialRelease (training) folder is in UTC?</p> <p>This is a crucial factor to make any submission</p> <p><span style="font-size:14px; line-height:1.4em">Thanks</span></p> <p>&nbsp;</p>
3209, <pre id="x_yui_3_7_2_1_1359951109416_2130">Good Evening Ben </pre> <pre id="x_yui_3_7_2_1_1359951109416_2130">Can you clarify what information for a flight will be available Pre flight and inflight and landing.</pre> <pre id="x_yui_3_7_2_1_1359951109416_2130">for example asdiwaypoint has the &quot;planned&quot; waypoints for the flight  the asdiposition has snapshots of the plane flights is this in flight? the groundspeed in each row is that the speed for the travel from this long lat to the next one or is it the snapshot speed. ex: receivedcallsignaltitudegroundspeedlatitudedegreeslongitudedegreesflighthistoryid 2012-11-12 01:00:12-08AIP51382300031346.4799995422363-111.800003051758280326212 When a flight is inflight will the asdiposition have all the information or just the entries to the lang lat where the flight is currently in midair.  Has the ground speed been adjusted to windspeed turbulence and other weather related changes.  Is there a possibility of error in the Weather and hazard information provided and does it need to be filtered. </pre> <pre id="x_yui_3_7_2_1_1359951109416_2130">Is it possible that there are no flight plan entries even if the flight is &quot;really&quot; in the air. In other words can we rely on the asdiposition being available for </pre> <pre id="x_yui_3_7_2_1_1359951109416_2130">every flight in the air.</pre> <pre id="x_yui_3_7_2_1_1359951109416_2130">What information does the pilot have available during flight from this dataset ike ATSCC advisories etc?</pre> <pre id="x_yui_3_7_2_1_1359951109416_2130">Please clarify my concerns  Thanks drivetheresult</pre>
3209, <p>Hi Admins</p> <p>given half of the teams have joined in January. Would it be possible for the teams to submit a preliminary Model as a Hash or&nbsp; Psuedo code on Feb 11th&nbsp; and submit the final code on March 14th</p> <p>Thanks</p>
3209, <p>Dear Admin is this contest still open and what is the latest to submit to the leaderboard</p> <p>Thanks</p>
3209, <p>Dear Admins is this contest open</p> <p>what is the latest a submission needs to be made for being considered</p> <p>thanks</p>
3211, ----
3235, ----
3238, ----
3250, ----
3272, ----
3273, ----
3288, ----
3289, ----
3294, ----
3300, ----
3316, <p>Just wondering since the&nbsp;<span>&nbsp;</span><a href="http://www.kaggle.com/c/ChessRatings2/Details/Evaluation">Capped Binomial Deviance</a>&nbsp;metric does not provide a clear understanding of how well the top competitors are predicting the test data set.</p> <p>Cheers!</p>
3316, <p>James YetiMan thank you for sharing! It is very helpful!</p>
3316, <p>Hi</p> <p>I was going through step by step and notice that in some occurences in the training data set I caught what I would consider to be issues in regards to the accuracy of the SalaryNormalized column.</p> <p>Is there a particular formula/algorithm that is used to Normalize the Salary? (and does is it dependent on the SourceName). I pasted an example record below.</p> <p>Thank you in advance.</p> <pre>Id                                                         69039698<br>SalaryRaw                    Up to 60000 per annum basic &#43; package<br>SalaryNormalized                                              30000<br>SourceName                                             cwjobs.co.uk</pre>
3316, <p>Thank you for the explanation. Since the validation results must be evaluated agains the same normaliser it is useful to understand that the reasoning behind the calculation.</p> <p><span style="line-height:1.4em">For the record I did find over 8000 records affected by this issue in the training dataset. It is defintely adding some noise.</span></p>
3316, <p>The scores on the leaderboard have changed for the better (unless I am going crazy ;) so is it safe to assume that the evaluation normalizer was fixed but the training file was not revised?</p>
3316, <p>Hello</p> <p>Just thought I would point out that for record_id:&nbsp;72418656</p> <p>LocationNormalized is oddly: &quot;2013-02-03T20:56:08&quot;</p> <p>Maybe it ought to be corrected in the original dataset even though it is an easy to fill in by hand since the raw location is London.</p> <p>Thanks!</p>
3316, <p>UTF-8 seems to the work best for me as well. I have not seem many double-byte type characters though it seems that the training/test dataset was well cleaned up in the first place.</p>
3316, <p>Congrats to the prizewinners! I had tons of fun!</p> <p><span style="line-height:1.4em">I am going to have to find the time to write a blog post about my complete approach because I might be the only crazy one who chose to not use machine learning to solve this problem. My solution is 100% arithmetic and actually  requires no training at all :)</span></p> <p><span style="line-height:1.4em">The hacker in me thought a real-time solution would be an elegant approach for Adzuna since job postings would be flowing in at high volume and that fluctuation in the different markets would require continuous retraining  of the model. I used only a text similarity using the Title &amp; FullDescription field and used GA to evolve a solution to find the best weights and thresholds to calculate the predicted salary based on the similarity scores returned.</span></p> <p>At the end of the public competition though I realized that my solution had some room for improvement if I had introduced machine learning at one stage though to improve accuracy. On my self-generated testing sets I noticed that my solution approached ~3000  accuracy on 92% of the test data sets but exhibited a much larger error (~19500) on the remaining 8% which weighted badly in the final score. If I had managed to filter properly those high error margin candidates it could have improved the final accuracy.</p> <p>Can't wait to hear what others have done and especially how they leveraged the location data (since I pretty much did not use it at all)</p>
3316, <p>@gggg touché :)&nbsp;</p> <p>I meant that my solution did not include a particular ML model. GA was purely used to investigate the search space to guide me towards the best arithmetic formula based on the similarity score measure I was using. It is not dependent on the data provided  by Adzuna (meaning I did not rerun GA when I was given the private test set to predict). I hope this clears things out a little.</p>
3316, <p>Ben</p> <p>Is there anyway to know if our final submission once uploaded is at least valid (meaning no error occured when processing it?)</p> <p>Thanks!</p>
3316, <p>[quote=Ben Hamner;21762]</p> <p>Your final ranking will be based solely on test set performance (so you need to make a submission against this set if you'd like to receive a final ranking).</p> <p>[/quote]</p> <p>So final ranking is different from prize eligibility right? Somebody's final rank could be 3rd (and earned the corresponding Kaggle points) but end up not qualifying for the 3rd prize (money) if they altered their model to generate the final test set prediction  correct?</p> <p>It is relevant to me in particular since I did not have time to finish the development of my model which is at the moment only based on leveraging content from FullDescription and Title and I might have time to alter it before next deadline to increase  its accuracy.</p> <p>Thank you!</p>
3316, <p>To echo Alec. I think it would be great if Kaggle displayed a &quot;prize contestant&quot; leaderboard which would be simply show the ranking based on the public set for all of the competitors who submitted models. This way it would help clear things out.&nbsp;</p> <p>This is the first competition I have a chance to devote enough time so I was actually surprised that the private ranking was not based on the model I had submitted but run by Kaggle against a new test set since it is supposed to be self contained and provide  reproduceable results. Giving the opportunity to contestant to submit their own private test prediction leaves the door open to continue to improve the accuracy of your model (why shouldn't you?)</p> <p>Personally I don't care about the prize money I only care about the challenge and the chance to work on a interesting problem and refine a particular algorithm/approach and get somewhat some public recognition along the way if successful.</p>
3316, <p>@beluga. I hear you. I think it would nice to get an official answer from Kaggle in regards to whether it is against the rules to continue to optimize your model at this point or if simply it disqualifies your from being an actual prize winner (and/or earn  Kaggle points). I am a newby here and don't seem to be getting a clear answer based on this thread ;)</p>
3316, <p>@Cole Harry I very much like what you describe. It would allow a lot of us to submit the &quot;prize&quot; test prediction right away and then keep working on improving our model if we have time to make a second submission (not counting for the prize).</p> <p>I also think it would be certainly nice for Kaggle to compute the accuracy of the final test predictions submitted (on a sample/fraction of it) and notify you via email privately to make sure you did not slip on a banana peal as @beluga suggested earlier.&nbsp;</p>
3316, <p>@willkurt</p> <p>For what it is worth I am starting to believe the Kaggle points are awarded based on when the public board was frozen. I made some &quot;toy&quot; entries in the past in a few contests and it appears I was awarded some points even though I never submitted a model.  But again it might have been because for a lot of competition the public board score is based on a subset of the predicted test dataset and therefore there is no confusing 2nd phase and whether or not the private ranking will affect your points compared to  the public one.</p>
3316, <p>Quick question have the images been preprocessed at all?</p>
3316, <p>Hi</p> <p>I stepped through a good 500 pictures by hand and noticed that a lot of those that are getting misclassified are due to the different crop/angle/scale. This includes side shots partial face shots etc... This defineltly ups the challenge since a lot of  scaling/rotation/centering is going to be required or even detection when the shot is a side shot instead of a frontal shot.</p> <p>Also I am not sure if anybody noticed but some pictures are thumbnails from shutterstocks.com which were grabbed obviously as a screenshot and have letters &quot;shutterstock.com&quot; watermarked over those ;)</p> <p>If anybody else wants to share what they found during the analysis/preprocessing stage in this thread that would be great. I am eager to learn what preprocessing tricks / strategies can be used in this challenge.</p> <p>Thanks!</p>
3316, <p>@gallamine you can install the latest from git using the following command if you are using conda:</p> <p>conda pip install git&#43;git://github.com/Theano/Theano.git</p>
3321, ----
3337, ----
3338, <p>@bhm - awesome book link. Any idea on where Chapter 3 is?</p>
3338, <p>yep you need an l not an i to start the name of that package.</p> <p><br> try install.packages('lme4')</p>
3338, <p>In case you want to see the status of the benchmark model at different iterations as it optimizes add the verbose=TRUE option to the lmer() call.</p> <p>&nbsp; rasch = lmer(correct ~ 1 &#43; (1|user_id) &#43; (1|question_id)<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; data=training[training$track_name==track <br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; c(&quot;correct&quot;&quot;user_id&quot;&quot;question_id&quot;)] <br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; family=binomial REML=FALSE verbose=TRUE)<br> <br> I found it interesting to watch the iteration output of the model as it slowly converged.</p>
3338, <p>So when you say that you use an SQL db as a backend are you connecting the db with R or do you use the db to get the relevant data and then import that in R. Essentially - is there an R library that let's us connect with an SQL db and write &quot;sql type&quot; code to extract and load a subset of data directly in R?</p> <p>&nbsp;</p> <p>Thanks</p>
3338, <p>I'm trying to use RMySQL to read data from the Test and Training sets that I've uploaded on an sql server. With the following query I'm trying to see if every block of 300 consecutive records in Test actually has the same sequence ID. (I know it says it does but I'm still checking):</p> <p>sequences = 0<br>for (i in 0:90023)<br>{<br> pos = i*300+1<br> check_query = paste('select * from TestData limit 300 offset 'pos sep='')<br> check_test = dbGetQuery(con check_query) &nbsp; &nbsp;##con is the SQL connection<br> sequences = c(sequencesunique(check_test$SeqID))<br> print(i) ## prints the block being printed<br>}</p> <p>What I notice is that it starts off rather fast but then the reading of the test data really slows down as 'i' starts increasing. It becomes less then 1 block/second as 'i' approaches 19000.</p> <p>Now if my code has run for the first 19400 blocks and I rerun it beginning from 19000 then it runs really fast for the first 400 blocks and then slows down again. <strong>This seems like</strong><strong>&nbsp;R stores the results of dbGetQuery and is hence able to retrieve them really fast. Does anybody know if I can&nbsp;<em>flush</em> this memory since I just need the variable&nbsp;<em>sequences</em>? Can that help in speeding the code?</strong></p> <p><span style="line-height: 1.4">Please let me know if this question doesn't make sense.</span></p> <p>&nbsp;</p> <p>Thanks</p> <p>&nbsp;</p> <p>&nbsp;</p>
3338, <p>Very curious to know what does Kaggle mean by cheaters...</p>
3338, <p>I have just joined the competition and am trying to make sense of the post here so apologies if I repeat something here but in line with tantrev's question... I believe the original dataset had some leakage and hence a new one was posted. So I guess if somebody still uses the original one and if they happen to win then by visually screening their code (or by other means) it will be thoroughly verified that they did not use the leaked data fields correct?</p> <p>So in essence since I'm using the latest dataset I don't need to worry about any advantage to somebody who already has the former one. Right?</p> <p>&nbsp;</p> <p>Thanks</p>
3338, <p>Hi Everyone</p> <p>I am a beginner in text mining using python and I built off Abhishek's code of &quot;beating the benchmark&quot;. I am trying&nbsp;to better understand CountVectorizer and TfidfVectorizer and in the process have 7 questions that I have embedded in my python notebook.&nbsp;</p> <p>I deliberated as to whether pose questions in such format in this forum&nbsp;than breaking them down but they made sense in their entirety and I think others can benefit from it too.</p> <p>Would appreciate any explanations that the experienced people here can give.</p>  <p>Best</p> <p>Kunal</p>
3342, <p>[quote=Jeff Moser;5686]</p> <p>[quote=Jose M.;5664]</p> <p>When will the 1st milestone ranking be made public?</p> <p>[/quote]</p> <p>It's available <a href="http://www.heritagehealthprize.com/c/hhp/Leaderboard/milestone1"> here</a>. Expect a more official announcement in the next day.</p> <p>[/quote]</p> <p>I was able to get to this link yesterday and read the pdf's. Now I can't! No where on the site I can find a link to Milestone 1 leaderboard or the papers. Its rather annoying. One would think that this information would be made available easily accessible...  in the interest of contest popularity!</p>
3342, <p>On a 64bit Windows system with Revolution R 64 bit this works for me (I have 8GB of RAM not sure if this might become a bottleneck for you) It takes a few minutes to read in from CSV but the read in from xdf is couple of seconds at most... once you have  the dataframe you can export it to whatever you wanna use this data in - note that all car charactersitics will be the same for the same submodel so you can effectively break the table in 2 a look up and the main - making the data transfer to other programs  smaller.</p> <p>rxTextToXdf( inFile = &quot;C:\Users\InsuranceContest\train<em>set.csv&quot; <br> outFile = &quot;C:\Users\InsuranceContest\train</em>set.xdf&quot; rowSelection = NULL <br> transforms = NULL append = &quot;none&quot; overwrite = FALSE <br> stringsAsFactors = TRUE columnDelimiters = NULL<br> rowsPerRead = 500000)</p> <p>data &lt;- rxXdfToDataFrame(file = &quot;C:\Users\InsuranceContest\train_set.xdf&quot; varsToKeep = NULL varsToDrop = NULL rowVarName = NULL <br> rowSelection = NULL transforms = NULL blocksPerRead = 1 reportProgress = 2 maxRowsByCols = 9e&#43;09)</p>
3342, <p>Apologies if it is posted alredy!</p> <p>Thanks.</p> <p>&nbsp;</p> <p>Edit: Of course! I didn't read the description!! Please ignore this.</p>
3342, <p>why did you withdraw from the competition?</p>
3342, <p>Trying to download the data I get this warning:</p> <p>*********</p> <div class="x_x_main" dir="ltr">You attempted to reach&nbsp;<strong>kaggle2.blob.core.windows.net</strong> but the server presented an expired certificate. No information is available to indicate whether that certificate has been compromised since its expiration.  This means Google Chrome cannot guarantee that you are communicating with<strong>kaggle2.blob.core.windows.net</strong>&nbsp;and not an attacker. Your computer's clock is currently set to Friday February 22 2013 8:30:37 PM. Does that look right? If not you should  correct the error and refresh this page.</div> <div class="x_x_main">You should not proceed&nbsp;<strong>especially</strong>&nbsp;if you have never seen this warning before for this site.</div> <div class="x_x_main">*********</div> <div class="x_x_main">Is anyone else having this issue?</div>
3342, <p>Update: it seems to be fixed now.</p>
3342, <p>Hi Admin:</p> <p>&nbsp;Why is that my submission from last night missing from the leaderboard? As far as I can tell the format is correct I didn't get an error message and the count of submissions remaining for the day went down by one. What am I missing? This is what my data looks like:</p> <p>customer_IDplan<br>100000012113132<br>100000022023122<br>100000031021022<br>100000042011123</p> <p>And there are 55717 rows in it including the header.</p> <p>Thanks and hope to hear from you soon -&nbsp;</p>
3342, <p>Looks like it has showed up now thanks.&nbsp;</p>
3342, <p>Based on your answer above&nbsp;</p> <p><em>The test set could just have:</em></p> <p><em>Quote1</em><br><em>Quote2</em></p> <p><em>(<strong>or any of the quote subsequences up until the purchase</strong>).</em>&nbsp;</p> <p>I was under the assumption that a sequence of&nbsp;</p> <p>Quote2<br>Quote3<br>Quote4</p> <p>in the test set was pausable.</p>
3342, <p>Thank you</p>
3346, ----
3353, <p>Data is like a shaggy dog. It needs grooming every now and then!</p>
3353, <p>For the members I make it the following:<br> 51482 Females<br> 43966 Males<br> 17552 Unspecified<br> 113000 Total</p>
3353, <p>Anyone who studies the Milestone descriptions cannot but be impressed by the cleverness the persistence and the comprehensiveness of the methods used. As time goes on you see the best scores on the leaderboard shaving 0.001 or thereabouts off the previous  best score. However it occurs to me that these methods will not be practical in real life situations because they are a composite of several algorithms some of which take considerable time to compute. Is the situation going to arise where the method(s) that  finally win accordin to the RMSLE objective function will NOT be the ones actually used in practice? To me the rules seem to have assumed that each entry will be based on ONE algorithm. Has this issue been discussed anywhere in the forum?</p>
3353, <p>Along the same lines has it been established that all members in the members table were members during year 1?</p>
3353, <p>Surely any serious method of predicting hospitalisation would use the patient's weight Body Mass Index and smoking history as well as age and sex. </p>
3353, <p>In another thread called &quot;2nd Milestone Winners&quot; I posted the following now 24 days ago:-</p> <hr> <p>Anyone who studies the Milestone descriptions cannot but be impressed by the cleverness the persistence and the comprehensiveness of the methods used. As time goes on you see the best scores on the leaderboard shaving 0.001 or thereabouts off the previous  best score. However it occurs to me that these methods will not be practical in real life situations because they are a composite of several algorithms some of which take considerable time to compute. Is the situation going to arise where the method(s) that  finally win according to the RMSLE objective function will NOT be the ones actually used in practice? To me the rules seem to have assumed that each entry will be based on ONE algorithm. Has this issue been discussed anywhere in the forum?</p> <hr> <p>So far no one has responded to this comment on that thread. Since then I have searched and read the forum to see if the issue of a single algorithm has been discussed but found nothing. And I have re-read the Rules a few times. It is noteworthy that the  rules expect a single algorithm.<br> For example:<br> In the Introduction:-<br> &quot;The winning team will create an algorithm...&quot;<br> In Rule 5:-<br> A &quot;Prediction Algorithm&quot; is the algorithm used to produce the data in an Entry taken as a whole (i.e. its particular total configuration) but does not include individual components of the Prediction Algorithm or tools used for analysis or development of the  Prediction Algorithm.</p> <p>In Rule 12:-<br> Entries will be judged based on the degree of accuracy of their Prediction Algorithm’s predictions of DaysInHospital for Y4... </p> <p>Once an Entry is selected as eligible for a prize the conditional winner must deliver the Prediction Algorithm’s code and documentation ...</p> <p>If this issue has been settled and I have missed it would someone (Kaggle Admin?) tell me where and what the conclusion was.</p>
3353, <p>Thanks guys (ChipMonkey DavidC and Chris Raimondi) for your comments. Let me say right away that of course I accept Kaggle Admin's ruling as to whether the milestone winners' methods comply with the rules. (I might use the same techniques myself if my  methods get good enough.)<br> However I still disagree that a combination of several algorithms should be considered &quot;AN algorithm&quot;. It should be called &quot;a method&quot;. Have a look at Market Makers' paper describing their method for their winning round 1 milestone.<br> Here is what they write:-</p> <hr> <p>There were four underlying algorithms used in our models all of which are freely available in the R language for statistical computing. Online references for each algorithm are given in the hyperlinks below.</p> <ol> <li>Gradient Boosting Machines ... </li><li>Neural Networks... </li><li>Bagged Trees... </li><li>Linear Models... </li></ol> <hr> <p>I have deleted the hyperlinks for clarity. Market Makers go on to use &quot;ensembling&quot; which blends the results of the different algorithms.<br> Surely you guys wouldn't argue that a combination of four such different algorithms is AN algorithm. If you do I would have to give away my geographical location and say you are a mob of &quot;bush lawyers&quot;!</p>
3353, <p>Oleg My definition of an algorithm is the same as the dictionary definitions I have seen which are all singular ie refer to &quot;a procedure&quot; or &quot;a method&quot;. Ok I can see that some people could classify a procedure that embodies the use of four totally distinct  algorithms (eg Market Makers' milestone 1 solution) as fitting this definiton and is therefore &quot;an algorithm&quot;. It is just that that is not my position.</p>
3353, <p>Guys I only joined Kaggle a few days ago and thought I would try to come up to speed on this credit problem for practice. Then I discovered what surely must be dirty data yesterday. I thought what seemed to be missing decimal points in colum 'C' might have  been an artifact of the download but a second download revealed the same data of course. It would be good if Kaggle Admin would make some statement whether the data really is dirty and we have to live with it or not! I don't have time to clean it up now so  I am going to look at a problem with more time until it closes. </p>
3353, <p>In real life surely the volume of securities exchanged at a trade would be important and used for prediction of recovery.</p>
3353, <p>I notice in the diagram &quot;Liquidity Replenishment.jpg&quot; posted by Capital Markets CRC that bid and ask prices are the same at a trade (of course as they should be). But in the testing data when a trade takes place eg at time 49 the bid is always less than  the ask. I haven't looked at the training data (I'm almost not game to download such a big file) and it may be different.<br> Further the p_value in row 2 for security 13 is 6963537884 pounds. Should the data dictionary say pence for this column?</p>
3353, <p>Thanks William and Christopher.</p>
3353, <p>Hi William (or anybody)&nbsp; I am starting to get the hang of this!&nbsp; Am I correct in thinking if the trade is initiated by the buyer (initiator = 'B') then the trade price is the ask price ie ask49 and if the trade is initiated by the seller (initiator = 'S')  then the trade price is the bid price ie bid49?</p>
3353, <p>Many thanks Christopher. I wonder what these guys pay the exchanges to let them do all these shenanigans. Not what I would pay I bet.</p>
3353, <p>I have a question about the data. Comparing the values in the column 'trade<em>vwap' with the values in the column 'trade</em>volume' you see that the values in 'trade<em>vwap' do not vary much while those in 'trade</em>volume' do. This doesn't make sense  if the meaning of these values is as given in the data dictionary. Since each row relates to a different trade shock event shouldn't values in 'trade<em>vwap' vary like those in 'trade</em>volume'. So what are the values in 'trade_vwap'? </p>
3353, <p>Quick. At this late stage I think I am having trouble unpacking &quot;training.zip&quot;.&nbsp; Can anyone tell me how many rows there are?&nbsp; I get variable result around 544111 to 544351 !!</p>
3353, <p>On the question of profitability it would be helpful if someone would tell us what are the typical financial arrangements between an exchange and a HFT organisation. I understand the exchange pays the HFT organisation for keeping the market &quot;ticking over&quot;.  They certainly could not afford to pay the brokerage of a retail trader.</p>
3353, <p>Hi Kaggle Admin&nbsp; When are the winning methods going to be published?</p>
3353, <p>Congratulations to all the winners and thanks to the sponsors and Kaggle for an interesting competition.&nbsp; What a marvellous high score the winners achieved!&nbsp; My approach was to use 9 bandpass filters from 70Hz to 310Hz and record their magnitude and time  of occurrence.&nbsp; Then I fed that data into a neural net.&nbsp; Although on the face of it you would think these data carried nearly all the important information I was surprised to find it scored less than all zeros!!!&nbsp; With the shortening of the time to finish  I did not have time to go any further.&nbsp; I used basic desktop languages.&nbsp; Looks like I will have to get R or something.&nbsp; Could I plead for someome to compile a list of acronyms so I can look them up and see what hey are and do.</p>
3353, <p>Isabelle&nbsp; Is that a typo at the beginning of</p> <p>- A is a cause of A means A=f(B noise) [denoted A &lt;- B]</p> <p>in your response to LoveSaves?</p>
3353, <p>What do you gurus say if we substitute an altimeter for a thermometer -- the type of altimeter that is based on air pressure.&nbsp; Then we use the altimeter to determine the height of the city.</p>
3353, <p>It is my understanding that the data in this competition are not time-series.</p>
3353, <p>The rules for this competition are the messiest of all the competitions I have been interested in.&nbsp; The framework setting up the competition&nbsp;is also messy some being outside the Kaggle site.&nbsp; Early contributions to the forum by the organisers contained  confusing typos.&nbsp; Altogether in my opinion it has not been up to Kaggle standard.</p> <p>Nevertheless it is a very interesting problem.&nbsp; I don't expect to score high enough to have to submit my solution but since I am using 'fossil' desktop languages to do the computing I doubt that the organisers would be able to run&nbsp;my programs anyway.</p>
3353, <p><span color="#000000" style="color:#000000"><span face="Times New Roman serif" style="font-family:'Times New Roman'serif"></span></span></p> <p>Like I have written previously this competition is a mess.&nbsp; Kaggle Admin should take control of it.&nbsp; Where are you Kaggle Admin?</p> <p><span color="#000000" style="color:#000000"><span face="Times New Roman serif" style="font-family:'Times New Roman'serif"></span></span></p>
3353, <p>I agree with Sali Mali that some Kaggle competitions have been very unstable in the description the data and the rules for a time after they have been launched.&nbsp; I sympathize with the people setting this competition because the&nbsp;flight of a commercial aircraft is a very complex&nbsp;phenomenon.&nbsp; (If this competition wasn't so interesting I would put it away for a couple of weeks and hope things were more stable when I come back!)&nbsp;</p> <p>I fancy this competition is too big for my humble desktop and my fossil languages so I am making the following observation in case it might be useful to someone else.&nbsp; The structure of the flight would seem to be amenable to 'Dynamic Programming'.&nbsp;&nbsp;Further if the simulator is to be used to evaluate the cost function of answers submitted and if it accepts starting point&nbsp;and destination information&nbsp;and if it is made available for use by competitors then it could be used to evaluate the cost function at intermediate points and using Dynamic Programming give an answer that should be the best in the competition.</p> <p>Am I wrong?</p>
3353, <p>Has anyone noticed there are 89 flights (rows) in 'testFlightsRev3.csv' with zero ground speed -- all above 21000 ft altitude.</p> <p>Also the flight with FlightHistoryId = 303137309 has no history record in&nbsp;'test1_flighthistory.csv'.</p> <p>Have these two issues been discussed anywhere?&nbsp;</p>
3353, <p>Am I the last or the first to find out that you don't have to submit all 14989 flights (testFlightsRev3.csv) to the evaluation process?&nbsp; Is this deliberate?&nbsp; If so it enables 'gaming' the system by submitting only those flights that have the best results.&nbsp; This doesn't seem compatible with the competition's objectives.</p>
3353, <p>Thanks Alessandro.&nbsp; I should have realised your explanation.&nbsp; However I'm sure all my flights finish within 75 miles so I don't know why my submission(s) result in such terrible scores.</p>
3353, <p>Hi Admins&nbsp; Is it still the case that the submitted flights as in the Milestone Phase&nbsp;need only go to within 75 miles and &lt;= 18000 ft of the arrival airport?&nbsp; Then the simulator takes over performs the landing and adds the extra cost to that of the submitted flight?&nbsp;</p>
3353, <p>Hi Admin&nbsp; What has happened to the data page?&nbsp; On Oct 15 there were the following (amongst others):</p> <p>- testFlightsRev3.csv</p> <p>- sampleSubmissionRev2.csv</p> <p>- FlightQuest Simulator-Rev4.zip</p> <p>&nbsp;</p> <p>testFlightsRev3 is referred to&nbsp;under 'basic structure of fq2/Getting Started' but I can't find it anywhere now.&nbsp; What&nbsp;has happened (or am I very confused)?&nbsp; Where is it?&nbsp;</p> <p>Also on the data page in the Guide to Files the last bullet point:</p> <p>-test1_flighthistory_rev1: revised version of file flighthistory.csv. See this <strong>forum thread</strong> for an explanation of the revision.</p> <p>The link under the words <strong>'forum thread'&nbsp;</strong>just takes you back to 'Competition Details'!</p>
3353, <p>Thanks joycenv.&nbsp; I have the&nbsp;current files but it was the out-of-date descriptions that confused me.&nbsp; It is a complex problem so anything you can do to&nbsp;make the description accurate would be helpful.</p>
3353, <p>I'm afraid my download of 'FQ2DataRelease2_AggregateTrain' (.zip version) stopped at 99% with 1 second remaining).&nbsp; Would it possible to break this file up into smaller pieces please.</p> <p>Thanks in anticipation.</p> <p>A second try was successful.</p> <p>&nbsp;</p>
3353, <p>Hi Charango&nbsp; This rule was mentioned in Basic Structure of Flight Quest2/Getting Started/Paragraph 5 where it says</p> <p>&quot;One constraint worth mentioning here is the maximum allowed number of instructions for each flight is 200 instructions&quot;</p> <p>Good Luck</p>
3353, <p>Anil airspeed is not an input in the ascent model. Fuel consumed seems to depend on altitude and weight.&nbsp; I took it that the ascent model describes&nbsp;the typical aircraft in&nbsp;ascent and for a given weight the fuel consumed depends on altitude reached.&nbsp; It would be possible with some algebra (and possibly some differentiation) to determine the airspeed at a particular altitude but this is not in the ascent model.&nbsp; Have I missed something?</p>
3353, <p>Thanks Anil.&nbsp; I'm afraid I have missed the subtleties of line 37 in the FuelModel.&nbsp; id speculated on the reality of the ascent/descent/ascent etc path.&nbsp; I have been told that in certain motor competitions to get the maximum distance for a given amount of fuel&nbsp;the optimum is to flatten the accelerator to get to a high speed and then to coast and then to repeat this.&nbsp;&nbsp;The efficiency of the internal combustion engine (work done per unit of fuel) is greatest under high load.&nbsp; I don't know much about the efficiency of a jet engine but it is conceivable it has this sort of characteristic.&nbsp; Not sure the passengers would think much of it!</p> <p>&nbsp;</p>
3353, <p>To all those interested in the peculiarities of the fuel consumption for the GE model of a typical commercial aircraft please see the attached graph.&nbsp; Fuel consumption (lbs/hr) is plotted on the y-axis and altitude (ft) on the x-axis.&nbsp; Series 1 2 and 3 are for cruising at 300 400 and 500 knots respectively.&nbsp; Series 4 is for the aircraft ascending.&nbsp; The aircraft weight is 130000 lbs.&nbsp; All points on the ascent curve are for an aircraft weighing this amount.</p> <p>The calculation ignores line 37 in FuelModel.fs <br>&quot;let ascentFuelBurn = max rawAscentFuelBurn cruiseFuelBurn&quot;</p> <p>It appears that the model has a peculiarity in that at low altitudes ( 10000 to 12000 ft)&nbsp; the ascent fuel burn rate is less than the cruise fuel burn rate for an aircraft cruising at 500 knots.&nbsp; There are probably intersection points for lower altitudes and lower speeds also.</p>
3353, <p>Thanks joycenv.&nbsp; That's a nice improvement.&nbsp; (But now FQ2 competes with Christmas!!!)</p>
3353, <p>skwales nick</p> <p>According to the competition 'Description'</p> <p>Work performed during unsanctioned hours decrease an elf's productivity:</p> <p>p=p&nbsp;&#8242;&nbsp;&#8727;(0.9)&nbsp;m&nbsp;&nbsp;</p>
3353, <p>Nick&nbsp; I am referring to skwales' para</p> <p>The above are the *minimum* required work durations that the elves have to work based on their productivity.&nbsp; <strong>Since productivity increases while an elf works during sanctioned time</strong> one could try making the elf &quot;pad&quot; its work duration in order to boost productivity say while waiting for the next toy to arrive.&nbsp; The rules are designed to prevent this &quot;padding&quot; to artificially boost productivity.</p> <p>I agree with your first post about the ambiguities.&nbsp; I have struck such issues in previous Kaggle competitions.&nbsp; The rules are not gone over carefully enough.&nbsp; Perhaps there is more than one person editing the rules.</p>
3353, <p>apologies to Nicholas Hamilton and skwales for my comments above.&nbsp; I was confused about sanctioned and unsanctioned.</p>
3353, <p>Everything looks correct to me too.&nbsp; What does elf 728 do next?</p>
3354, <p>When viewing User profiles there is an issue with the back button. If you have clicked through the tabs on a user's profile the back button does not properly return to you the previous tab. (The count of the number of backs to exit a user's profile is  maintained there is just no change in the which tab is shown.)</p> <p>Google Chrome 19.0.1084.56 m</p>
3354, Indy Actuaries were invited although we have not made a submission yet. We were invited individually but will be competing as a team. (It would be hard to say we didn't collaborate sitting 100ft. from each other). If you follow us you know that we will  make a couple early submissions to test the submission process and hopefully answer some of the question Sergey outlined in his first post. After that though we will train and test internally and submit final models at the end. We don't do this to mask our  leader board position until the end. It is just when we finish our ensembling. As far as who else is lurking those in the completion know but I will them reveal themselves. I am not sure of how they determined the invitations but I would hypothesize that  being an Actuary would be a good predictor of getting invited.
3354, <p>This public/private thing is tough to know what I can divulge. I think I can announce that I finally opened the 4GB dataset in SAS today and look at some data fields. As such I think I have developed our best strategy yet.</p> <p>The 3 steps to our secret strategy:</p> <p>Step 1) Look at what data we received<br> Step 2) Understand what those data fields mean<br> Step 3) Copy Xavier's answers when he is not looking</p>
3354, <p>I thought our LogRegression benchmark entry was pretty solid. &nbsp;At least we can say we are beating SirGuessalot.</p>
3354, <p>Some solutions in R:</p> <p>glmnet <br> Pros: Fastest<br> Ridged and Lasso will help reduce multicolinearity problems<br> Cons: Assumes linear relations <br> Does not model interactions internally</p> <p>randomForest<br> Pros: Models interactions<br> Models features non-linearly<br> Fast<br> Extra trees will not add to overfitting<br> Cons: Predictions bounded by high and low values of examples</p> <p>GBM<br> Pros: Most accurate<br> Models interactions<br> Models features non-linearly<br> Cons: Too many trees leads to overfitting<br> Slower<br> More difficult to tune parameters</p> <p>I will make a warning about this analysis. I know nothing about your data though if there is significant natural variation in the features you may need consider my warning. The issue is with predicting 20000 different responses based on 300 observations  and 25 features. Statistically some of the natural variation in the 25 features will likely be found as significant for one or more of the 20k responses even though it may have no correlation what-so-ever. In linear terms if you are 99.99% confident that  was a relationship between a feature and the response. There is still a 1 in 10000 that there really is no relationship between the two. So testing against 20K responses increases the chance of identifying a relationship when there is none.</p> <p>As for what I would based on your paragraph explanation I would run glmnets for each of the 20k responses perhaps including 2nd degree interactions or splines of some of the features. It is quick; It is explainable; It will be more accurate than OLS.</p>
3354, <p>[quote=Jason Tigg;15811]</p> <p>[quote=jcnhvnhck;15672]</p> <p><strong>Ideas</strong></p> <p>1. Prohibit new entrants from entering a competition close to the competition end.</p> <p>[/quote]</p> <p>Personally am opposed to 1. I often only enter competitions in the last 2 weeks. In fact my last 2 competitions I only entered with roughly 8 days and 15 days to go. Of course this depends on your definition of &quot;close to end&quot;. I am also opposed to any system  of authentication that requires either payment or credit card activation to me this is contrary to the open to all ethos.</p> <p>[/quote]</p> <p>Following Kaggle's golf theme of scoring and masters competitions consideration should be given to creating two classes of competitors; Pros and Amateurs. &nbsp;There could be advantages to earning a professional status such as the ability to enter competitions  at anypoint. &nbsp;Amateurs would be unable to enter near the end of competition. (Maybe they can be invited into a team already established.) &nbsp;</p>
3354, <p>Sorry for the long quote but this is too humorous in retrospect.</p> <p>&nbsp;</p> <p>[quote=Berry;15767]&nbsp;</p> <p>Possible frauds (that are already mentioned)</p> <p>1-&nbsp;&nbsp;&nbsp; Socket puppets</p> <p>2-&nbsp;&nbsp;&nbsp; Masers</p> <p>3-&nbsp;&nbsp;&nbsp; Fake account merging (with fake people) to increase your submissions</p> <p>&nbsp;</p> <p>Being realistic it is very hard to detect these frauds.&nbsp; How can you prove team merging is not a fraud (people in teams are real people!). What you gonna do with fake accounts. For most of people with a modicum education in computer science or mathematics  it should be easy to design undetectable submissions. I believe under mild conditions it is possible to prove that detecting illegal submissions is an <strong>NP-complete problem</strong>. i.e. one can submit as many as submission as he/she wants and be sure it is exponentially hard to detect who was responsible for it!</p> <p>We can look at this problem from another angle: There are 3035 entries in MERK. Each submission has 6 digits; so each submission equals to 20 bits (2^20 ~ 10^6). Thus about 60k bits of data is out! In MERK you need to predict 50K data points! What it means  leaderboard data is out with 1 bit of precisions (clearly without considering the bias). Who has this data: all of participants every one has one piece of it! This can get worse in other competitions!</p> <p>&nbsp;</p> <p>Multiple accounts make the competition unfair and <strong>NON-SCIENTIFIC</strong>; but it is not possible to get ride of them! As I said finding fake accounts is a NP-complete problem. If anyone interested I can describe how to make submissions that it  is impossible (at least exponentially hard) to DETECT them (not that I have used it myself)</p> <p>&nbsp;</p> <p>And even I don’t like whole idea of learning by leader board using your own legit submissions! It is just not a scientific approach! But it is necessary to win any competition. In some competitions that have non-RMSE score (Ilike MERK) you might need to  spend a long time to find optimal combination of models! Waste of time! Waste of time!</p> <p>&nbsp;&nbsp;</p> <p>I suggest that Kaggle puts<strong> less information</strong> in the kept-out (public leaderboard).</p> <p>For some competitions data that you have to predict is just <strong>different</strong> than what they release. In such situations more submissions&nbsp;dramatically help!</p> <p>If the kept-out data is just <strong>sampled from training data</strong> (<strong>uniformly</strong>! <strong>no time difference</strong> or so..) then effect of more submissions is marginal and possibly useless! &nbsp;</p> <p>&nbsp;</p> <p>MERK data description officially says that:</p> <p><em>“We find construction of test sets&nbsp;by random sampling&nbsp;gives R2's that are much&nbsp;too optimistic.”&nbsp;</em></p> <p><strong>It means</strong>: it is not possible to win the competition without using leaderboard scores! (Waste of time and talent to use leaderboard scores in your models!!!)and so calls for many fake-illegal submissions.</p> <p>[/quote]</p> <p>Truly the best paragraph of them all.</p> <p>[quote=Berry;15767]</p> <p>Multiple accounts make the competition unfair and&nbsp;<strong>NON-SCIENTIFIC</strong>; but it is not possible to get ride of them! As I said finding fake accounts is a NP-complete problem. If anyone interested I can describe how to make submissions that it  is impossible (at least exponentially hard) to DETECT them (not that I have used it myself)</p> <p>[/quote]</p> <p>Maybe Berry should've used his exponentially hard to DETECT method and then maybe he could have kept his standings and winnings. &nbsp;Instead he is now ranked 6498th with no prize money to show for it.</p> <p>The next best quote of this thread.</p> <p>[quote=Glider;15769]</p> <p>[quote=Berry;15767]</p> <p>For most of people with a modicum education in computer science or mathematics it should be easy to design undetectable submissions.</p> <p>[/quote]</p> <p>You'd be surprised...</p> <p>[/quote]</p>
3354, <p>[quote=Sergey Yurgenson;15961]</p> <p>Keep in mind that with unlimited number of submission (even with reduced number of decimal points):<br> 1. It will be possible to find split between public and private test data<br> 2. Somebody will try to submit 10 random submissions per second:<br> a. To crash system<br> b. To win (there are always clueless people around)</p> <p>[/quote]</p> <p>Or 2c. To add the public test data into their training set.</p>
3354, <p>I stated earlier in the thread about pros and amateurs having different qualifications to late entry into a competition. Hell the threshold for being a pro maybe a single top 10% or 25% entry. This would be a barrier to entry for most new accounts but  would eliminate sock puppets at the end of competition which is when most of them show up. </p> <p>Some self debate on this topic:</p> <p>Problem: People will circumvent this by making sock puppet accounts earlier.<br> Response: Few cheaters set out to cheat from the beginning they are lazier and will cheat as a an opportunity presents itself. In the beginning of the competition there is no incentive to cheat and set up sock puppets because their main account will have  plenty of submissions before the deadline. It is near the end of the deadline that procrastination has caught up with them and they require more submissions than are left for their main account.</p> <p>Problem: Sock puppet accounts will become smarter.<br> Response: Sure someone could create accounts and submit strong enough models to earn pro status and then use them to compete multiple times in multiple contests. And this doesn't happen now? I am sure Kaggle has other methods to tracking similar accounts. I  will not even try to speculate on how but Berry is an example of someone thinking they were smarter than them.</p> <p>Another observation about sock puppet accounts:<br> I doubt Kaggle is addressing this because in the end the puppets related to prize winners are all that matter. The sock puppet accounts inflate the number of competitors in a competition. This number is used to calculate the total number of point earn for user  rankings. </p> <p>From this Merck thread<br> http://www.kaggle.com/c/MerckActivity/forums/t/2837/really-swift-learning<br> [quote=Sergey Yurgenson;15632]</p> <p>Number of teams by day:</p> <p>October 1 : 167</p> <p>October 2: 168</p> <p>October 3 : 177</p> <p>October 4 : 183</p> <p>October 5: 186</p> <p>October 6 : 190</p> <p>October 7 : 195</p> <p>October 8: 199</p> <p>October 9 : 203</p> <p>October 10: 211</p> <p>October 11: 234</p> <p>October 12 : 239</p> <p>October 13 : 243</p> <p>October 14 : 243</p> <p>October 1 5: 262</p> <p>now : 284</p> <p>[/quote]</p> <p>In the last two weeks of the competition there was a 70% increase in the number of competitors. While some are real account many probably were not and this will artificially increase user ranking scores.</p>
3354, <p>I would like to see benchmarks for the public board score for the milestone prize winners. This would make a good target to judge progress.</p>
3354, <p>Since this patient's Place of Service is &quot;Other&quot; and they have 4-8wks for almost every DSFS you can assume that they are in a nursing facility or some long term care facility and the LOS is different than inpatient length of stay. &nbsp;Often times these types  of care facilities bill the insurance company monthly. &nbsp;So you would see multiple stays of length 1 month (4weeks) if they spend more than one month there. &nbsp;&nbsp;</p> <p>I am finally getting around to exploring this data after such a long time. &nbsp;So I am not an expert on &quot;this&quot; data set but in my experience this is typical behavior.</p>
3354, <p>I currently get 38.9% on spend and 40.5% on date.</p>
3354, <p>Andy</p> <p>Yes those marginal percentages are from the 17.97 entry on the leader board. At first I was a little amazed at the correlation but now I understand it. </p> <p>These marginal rates are based on the entire training set. I understand the overfitting pitfalls with that but I was not concerned based on the current methodology. My score on the full training set is less than the leaderboard so I expect my final score  to be lower than 17.97.</p>
3354, <p><span>I am gathering code and writing my process for Kaggle. &nbsp;As I am doing that I thought I would share findings and methodology as I go along.</span></p> <p><span>I handled my data manipulations and spend predictions in SAS. &nbsp;I used R to run&nbsp;Generalized Boosted Regression Modeling (GBM package) to predict the visit date. &nbsp;I also used JMP to visualize the data along the way. &nbsp;</span></p> <p><span>&nbsp;</span></p> <p><span>First I focused on predicting spend amounts. &nbsp;Testing was done on the actual next spend amounts in the training data regardless of the next visit date. &nbsp;I tried a suite of median statistics: entire year most recent 3 months and recent 17 spend (based  on roobs forum discussion). &nbsp;</span></p> <p><span>Then I tried some time series projections. &nbsp;I used Croston's exponential smoothing for sparse data to develop projections. &nbsp;This is really projecting usage of groceries and produced strange results due to small purchases after a long time period and  large purchases after a short time period. &nbsp;I modified my formulas to predict needed inventory levels i.e. how much does a customer need to refill their pantry. &nbsp;None of these time series methods outperformed the median estimates so I abandoned this line  of reasoning.</span></p> <p><span>Finally after looking at the distribution of claims realized that the range covered by the median did not cover as much as other $20 ranges could. &nbsp;The final methodology used in the spend prediction is written below. &nbsp;This is from the documentation  I am preparing for Kaggle and will discuss the date methodology in later post.&nbsp;</span></p> <p style="padding-left:30px"><span>Visit_Spend Methodology</span></p> <p style="padding-left:30px"><span>All presented methods use the same spend amounts.&nbsp; The amounts will differ based on the projected day of the week for the shopper's return but the methodology is the same.&nbsp; A members next spend amount was developed on historical  data only.&nbsp; There was no training a model on data past March 31 2011.&nbsp; Training data are used later to optimize method selection.&nbsp;</span></p> <p style="padding-left:30px"><span>The chosen method optimizes the results based on the testing statistic for this competition.&nbsp; The metric for determining if the projected visit spend amount is correct was being with $10 of the actual spend amount.&nbsp; Maximizing  the number of spends within the $20 window was accomplished by empirically calculating the $20 range that a customer most often spends.&nbsp; I termed this window the Modal Range.&nbsp; Typically it is less than both the mean and the median of a customer's spending  habits.&nbsp; &nbsp;Predictions were further enhanced by determining a modal range for each day of the week. &nbsp;&nbsp;In the final submissions these values were also front weighted by triangle weighting the dates from April 1 2010.&nbsp; (A spend on April 1 2010 has a weight  of one and a spend on March 31 2011 has a weight of 365.)</span></p> <p style="padding-left:30px"><span>The projected visit spend was based off the day of the week of the projected visit date.&nbsp; In cases where the customer does not have enough experience on the return day of the week their overall modal range is assumed.&nbsp; The  training data were used to develop credibility thresholds for selecting a customer's daily modal range verse their overall modal range.&nbsp; The thresholds were hard cutoffs.&nbsp; If the customer did not have enough experience on a particular day of the week the  overall modal range was projected. &nbsp;The overall modal range was not front weighted like the daily ranges.</span></p> <p>Future considerations would have included replacing the thresholds cutoffs with a blending of the daily modal range and the overall modal range based on experience.</p> <p style="padding-left:30px">&nbsp;</p> <p>EDIT: Added language that the fallback overall modal range was not weighted.</p>
3354, <div> <p><span>Sorry it took me so long to finish the methodology explanation for the visit_date.<span>&nbsp;&nbsp;</span>In addition I will upload my code at the end of this file along with the documentation for the winning submission.</span></p> <p><span>&nbsp;</span></p> <p><span>This post will go through the evolution of my projections for visit_date. &nbsp;In the beginning of the competition I focused entirely on the spend amount for each customer.<span>&nbsp;&nbsp;</span>Every customer was given a return date of April 1st; since April  1st had the highest probability in the training data.<span>&nbsp;&nbsp;</span>After I developed the spend methodology I began to build a visit date methodology.<span>&nbsp;&nbsp;</span>I created estimates in pieces and slowly combined them.</span></p> <p><span>First I projected a customer would return on their highest probable day of the week (April 1-7).<span>&nbsp;&nbsp;</span>Next I project the most probable date of return based on their most common number of days between visits.<span>&nbsp;&nbsp;</span>If the projection  was before April 1st I said they would return on April 1st.<span>&nbsp;&nbsp;</span>Then I combined the weekday probabilities and the number of days between visits probabilities.<span>&nbsp;&nbsp;</span>The date with the combined highest probability was selected.<span>&nbsp;&nbsp;</span>(The  weekday probability was no longer limited to April 1-7.)<span>&nbsp;</span></span></p> <p><span>Other variations included allowing for the most common number of bays between visits to be selected from only those greater than the number of days already passed between the last visit and April 1st.<span>&nbsp;&nbsp;</span>The highest probable weekday was  also front weighted using a triangle weight damped by the cube root.<span>&nbsp;&nbsp;</span>(April 1 2010 = (1)^(1/3) &amp; March 31 2011 = (365)^(1/3)).<span>&nbsp;&nbsp;</span>The combination of date projections were also limited to dates in the first week (April 1-7). This  method produced a score of 17.70.<span>&nbsp;</span></span></p> <p><span>Next I included priori information that the date of return followed the distribution of return dates in the training set.<span>&nbsp;</span>Including this with the customer's weekday and time between visits probabilities produced the submission on the  public leaderboard of 17.97 (18.01 on the private board).<span>&nbsp;&nbsp;</span>If more than nine days had already passed since the last visit I used the highest probable weekday instead of the combination of probabilities.<span>&nbsp;&nbsp;</span>I attempted to optimize the  splits between methods by chosing the best method given certain criteria.<span>&nbsp;&nbsp;</span>While some of these mixtures preformed better on the private leaderboard none of them scored better on the public board.<span>&nbsp;&nbsp;</span>I stalled here for awhile.</span></p> <p><span>In a discussion with a co-worker about the random forests he mentioned boosting models and explained how they differed.<span>&nbsp;&nbsp;</span>I had used random forest model for other competitions and decided to use his advice and try a boosting model for  the date projection.<span>&nbsp;&nbsp;</span>When creating all the previous projections above I had gathered a large set of statistics describing a customer's shopping habits.<span>&nbsp;&nbsp;</span>I compiled these together and ran them through R's &quot;gbm&quot; package.<span>&nbsp;&nbsp;</span>These  variables are listed in the winning submission's documentation attached to this post.<span>&nbsp;&nbsp;</span>I felt the variables that would describe a visit on April 1st would not be the same as those for a visit on April 2nd and so on.<span>&nbsp;&nbsp;</span>So I created  dummy variables for each day from April first through the ninth.<span>&nbsp;&nbsp;</span>(The ninth was chosen because there was a large drop in visits in the training data on the tenth.)<span>&nbsp;&nbsp;</span>A separate Bernoulli GBM model was created for each possible date  of return.<span>&nbsp;&nbsp;</span>Each model took 18-24 hours to run.<span>&nbsp;&nbsp;</span>This was because I ran the model with 5-fold cross validation and very low shrinkage with a large number of trees.<span>&nbsp;&nbsp;</span>After all the models were complete I had a score for  each customer and each possible date of return.<span>&nbsp;&nbsp;</span>For the winning submission I selected the date with the maximum score.<span>&nbsp;&nbsp;</span>This scored 17.77 on the public board and 18.67 on the private board.<span>&nbsp;&nbsp;</span>I also ran the gbm scores  through a nominal logistic regression.<span>&nbsp;&nbsp;</span><span>&nbsp;</span>This method scored better on the training data but only scored a 17.43 on the public board.<span>&nbsp;&nbsp;</span>I was surprised and decided the regression was leading to overfitting.<span>&nbsp;&nbsp;</span>I  did not include this model for consideration at the end of the competition.<span>&nbsp;&nbsp;</span>It scored an 18.83 on the private board.<span>&nbsp;</span></span></p> <p><span>I enjoyed this competition and data.<span>&nbsp;&nbsp;</span>Developing a strategy for projecting future shopping habits based on such sparse historical data was challenging.</span></p> <p><span>Let me know if you any questions.</span></p> <p>&nbsp;</p> </div>
3354, <p>D'yakonov Alexander posted his methodology under the forum topic: &quot;Thanks&quot; started by Domcastro.</p> <p>Good luck with the project.</p>
3354, <p>Someone is gaming the system. Around 120th place there are a bunch of jibberish names that all ran (imo) GBMs with different parameters on the raw competition data.</p>
3354, <p>Anthony's post about rules and regulations explains what happened. There were a few people at the top running multiple accounts. It would be against the spirit of things and the rules to take a winning model and submit it under multiple accounts to win all  the prizes. </p> <p>I would be interesting to see the final results of those removed teams. I would assume by running multiple accounts and fitting to the leaderboard would just produce an overfit model not a good one. Their private score may have been much worse.</p>
3354, <p>Another way to control submissions from the same people through multiple accounts would be to require code submissions along with the answers. There is plenty of plagiarism software developed for identifying similar code structures and variables. This additional  hurdle would add complexity to cheating without adding a tremendous work load on Kaggle. </p> <p>Hell there are a lot of smart data scientist here. Kaggle may even sponsor a competition to create a better plagiarism algorithm. </p>
3354, <p>I like DUW's idea.</p> <p>This would encourage competition. Players would enter the competitions earlier and submit more models earlier. There is that psychology change from &quot;Allowed 2 entries per day&quot; to &quot;Lose 2 entries per day&quot;. </p> <p>It doesn't solve the problem of multiple accounts but that is more complicated.</p>
3354, <p>Down Under Wonder:</p> <p>1) The whole point of limiting the number of submissions per day is to promote competition. Competitors who were doing well on the leaderboard would not work as hard to improve because players would withhold a bunch a submissions until the end of the competition.  This would limit the time for a competitor to develop a new model.</p> <p>2) 30% is arbitrary but there are issues with increasing it. There are also issues with increasing it to 50%. This would only leave 50% for the private scores. By reducing the size of the private set they would increase the variability of the data used  to determine the winner.</p> <p>3) As a data science you should have an idea of which models you developed are the best. Our fifth place model was not one that scored well on the public board but i knew it was a solid model and chose it as one of our five. Is five appropriate? 10 20?</p>
3354, <p>I would like a more stable public score but value a stable private score more. None of my highest public board submissions is my highest private board. But the models that did the best on the private board were the ones I felt should be the best. </p> <p>In the Dunnhby shopping challenge I placed second. I submitted a model that was a slight variation to my winning model that I expected to be better. It scored much lower on the public board and I did not chose it as one of my five. That solution would've  tied me for first. I could be bitter about not having all my submissions judged but I should've had more faith in what I knew was better and not put faith in the board. The public board is there to inform people when there models are completely not on basis  with the rest of the competition not to inform on granular detail improvements. </p>
3354, <p>@Fuzzify</p> <p>Oblique RF (obliqueRF) has an implementation in caret but I think I had trouble with feeding it the outcome variable as a factor. &nbsp;I ran the oRF using both the pls and ridge methods (pls runs faster). &nbsp;It is <span>definitely&nbsp;</span>much slower than RF which is expected because of the required computation needed at each node and I only ran 500 trees. &nbsp;The out-of-fold log.loss was ~.45 for the pls method and ~.46 for the ridge method. &nbsp;The ridge was more unstable  (as shea mentioned above) and probably needed more trees. &nbsp;I ended up just adding more repeated CVs at the ~30 fold level.</p> <p>I also experimented with Regularized RF (RRF). &nbsp;I tried to optimize the coefReg using the caret package. &nbsp;My optimal coefReg was 0.5. &nbsp;Plugging ahead I ran 18k trees multiple times and still the predictions only had a ~.70 correlation. &nbsp;(18k trees was a  36hour run time for the ~30 fold cv.) &nbsp;In retrospect the higher coefReg was much more unstable and I should've stuck with 0.8 (the default). &nbsp;</p> <p>These methods are <span>definitely&nbsp;</span>slow. &nbsp;Your &quot;stalled for hours&quot; was just it working. &nbsp;If I recall correctly the oRF function took 3 hours for a single 500 tree run on a single core. &nbsp; I was running 7 <span>simultaneous&nbsp;</span>models at a time. &nbsp;So I am not sure if your questions about multiple core is asking if oRF can build a single model on multiple cores or how we used multiple cores to build multiple models. &nbsp;If your question is the former I can't  help. &nbsp;If it is the latter my code is below.</p> <pre>superman &lt;- makeCluster(7)<br>registerDoSNOW(superman)<br>getDoParRegistered(); getDoParName(); getDoParWorkers();<br><br>###Run 28 oRF.pls models<br>oRF.pls.cvs &lt;- foreach(<br>  i=1:nfolds<br>  .packages='obliqueRF'<br>  .verbose=TRUE<br>  ) %dopar% {<br>    #i&lt;-1L<br>    train.flag &lt;- (fold.ids != i)<br>    test.flag &lt;- (fold.ids == i)<br>    <br>    ###Pass the gbm the out of fold data too to save time<br>    trash.oRF &lt;- obliqueRF(<br>      x=as.matrix(train[train.flag])<br>      y=as.numeric(outcome[train.flag])<br>      mtry=250<br>      ntree=500<br>      training_method=&quot;pls&quot;<br>      )<br>    oRF.fold.pred &lt;- predict(trash.oRFtrain[test.flag]type=&quot;prob&quot;)<br>    return(oRF.fold.pred[2])<br>  }<br><br>stopCluster(superman)<br>stop.time &lt;- date()</pre>
3354, <p>As mention previously the oRF using &quot;Ridge&quot; was more unstable than the &quot;pls&quot; option. I had access to 16gb of ram and that might have been needed to complete the oRF using the fast ridge method. </p> <p>I just submitted some of these models.</p> <p>oRF(method=&quot;ridge&quot;)<br> public = 0.48319<br> private = 0.42485</p> <p>oRF(method=&quot;pls&quot;)<br> public = 0.47188<br> private = 0.40843</p> <p>RRF(coefReg=0.5) - I think coefReg should've stayed at 0.8. This was very unstable.<br> public = 0.59007<br> private = 0.52145</p> <p>Feature selection prior to running these model would've definitely helped with run time. </p>
3354, <p>@rkirana</p> <p>You should review your error metric. Make sure it is using LN not Log10. This will produce the differences you are seeing in the RMSLE calculation between your CV and the leaderboard.</p>
3354, <p>Yes log1p would be correct. That is natural log based.</p> <p>Did you create 12 models one for each month? Or did you stack the months create one model and include a month variable? If you did the latter you have to make sure you do the CV fold creation before you stack the variables. </p> <p>Although I would think the overfitting would be even higher for example if you included information for months 1-4 in the calculation for month 5.</p>
3354, <p>[quote=Bob Castleman;11945]</p> <p>I was curious why there is so little separation between the random bench mark and the leader. Seems to me that 5 points isn't really all that much.&nbsp; And shouldn't random be much lower than that?<br> <br> [/quote]</p> <p>The distribution of the target variables are close to normally distributed around a mean value. So when a random order is chosen there is a high likelihood that the value of a target out of order is similar to the value of the target in the correct order.  &nbsp;This is why the random benchmark scores well.<br> <br> This is the same reason for the separation between the random bench mark and the leaders. &nbsp;I would think there was more value in predicting the largest values since the information from those data are entered into the MCAP earlier but I was not able to produce  any improvements by doing this. &nbsp;</p> <p>&nbsp;</p> <p>[quote=Bob Castleman;11945]</p> <p><span>Doesn't this tend towards overfitting the data?</span><br> <br> [/quote]</p> <p>Yes but that is a part of the machine learning. &nbsp;There are techniques and algorithms to help reduce overfitting. &nbsp;In a linear model framework the quickest way to reduce overfitting is to apply a ridging factor to the beta coefficients. &nbsp;Extensive research  has gone into the subject and elasticNets are fairly robust in producing estimates that generalize well. &nbsp;An elasticNet is a combination of ridging and lassoing a regression. &nbsp;Being new to ML I would suggest reading about GLMNet. &nbsp;<a href="http://www.jstatsoft.org/v33/i01/paper">http://www.jstatsoft.org/v33/i01/paper</a>&nbsp;&nbsp;</p> <p>[quote=Bob Castleman;11945]</p> <p><span>Finally after this is competition closes will we have access to the actual data that scored our models?</span><br> <br> [/quote]</p> <p>Kaggle typically does not release the full set but the submission functions remain active so you can continue to test your algorithms and get feedback. &nbsp;While the leaderboard is only a portion of the test data after the compeition ends you will also receive  scores for the full test data for each submission.</p>
3354, <p>This thread has been derailed. Back to Will's original issue -&gt;</p> <p>I understand that the time requirements on these competitions can be intensive especially to compete for prize money. You should consider forming teams with some other competitors. Splitting the work load with other can allow to you compete in multiple  (or shorter) competitions. There are probably good and bad teammate in the Kaggle space and finding good ones is part of the challenge. I am lucky to have a good teammate locally and another in training but if I was approaching this here is what I would  do.</p> <p>Solicit the maximum number of teammates possible (I think 8.) for a competition with a small to medium size purse. Form the team with the understanding that any winnings are split evenly no matter the perceived value or amount of work done by individuals.(I  am not sure Kaggle will do otherwise anyway.) From these 8 hopefully you find someone you worked well with and then branch off to also compete in other competitions.</p>
3354, <p>Edit: &nbsp;The above post make most of this void.</p> <p>&nbsp;</p> <p>The feature descriptions for this competition are masked so I am not entirely sure of how much information of test molecules are typically known. But if I make my own assumptions...</p> <p>Merck and most pharma companies test thousand of molecules for activities. The point here would be to improve the success rate of finding molecules with a certain activation. That being said they already have the features for each molecule prior to testing  for activity. The distribution of test molecules would be known and optimizing models to train on similar molecules would make sense.</p> <p>Judging by the temporal grouping present in the train and test data there are classes of molecules that are tested for activity. Which is some of the challenge of this competition the shifts between the train and test is due to a new class of molecule  being testing and we need to predict how it will react.</p> <p>In many real world applications this approach would not make sense. You would need enough test points to determine the distribution of the test data set. Training a model will take time so there would not be instant results for the new test data. Pharma  molecule testing would not be limited by these needs. There is more of a financial incentive to waiting and training a model to predict which molecules to test than testing them all.</p> <p>As Shea alluded to above we have used the distribution of the test features to train model optimized for the test data but we have not used feedback from the leader board to influence our modeling decisions. Activity 3 for example the train and test molecules  are fairly different. Having 15 activities help mask some of the gaming that can be result from using leader board results. If you follow the HHP competition to compete at the top it appears you have to include some information derived from the leader board  into your model. Maybe this will end up being part of the key to winning this competition which will produce models are that are not useful for Merck in real application. I hope this is not the case but considering that finik was behaving badly and that  there are 15 activities. There could be more slave accounts in the depths of the player ranking or just dummy submissions that were testing outcomes of individual activities to optimize the model.</p>
3354, <p>Also the averaging of 15 different R^2s washes out some of the issues with this. An N of 15 is not perfect for central limit theorem but is a start in that direction. There will be a reduction in variance just based on the fact that the evaluation metric  is the mean of the 15 activities. Although in reality the N is different than 15 since the variance of the public R^2 for each activity is not homogeneous. The variance due to sampling in an over sampled activity such as 1 or 6 will be less than those from  under sampled activities like 4. </p> <p>Or in simpler terms a 0.1 change in R^2 for a single activity only has a 0.00666... effect on the final evaluation metric. Now considering some may change positively or negatively the overall impact may not be that large. Even considering a 0.0 R^2 would  have a total effect of 0.04-0.6 on the final metric. (Although an R^2 of 0.0 would be useful in building investment portfolios.)</p>
3354, <p>Any one else curious enough to create an unofficial private leaderboard? &nbsp;</p>
3354, <p>I don't see why not. The purpose of this competition is to explore visualizations of the leaderboard. I did something similar in my submission (there would be a black dash line for the currently logged in user). I think that is important for any visual;  to not only provide a view of the competition at the top but tell a story to each individual as how they relate to the top of the board. </p> <p>If you do develop something that is user specific I would like to see examples of the same competition from different user's view points in your submission.</p>
3354, <p>Take it one step at a time.&nbsp;</p> <p>&nbsp;</p> <p>A good starting point is Friedman and Hastie's paper on &quot;<a title="Regularization Paths for Generalized Linear Models via Coordinate Descent" href="http://www.stanford.edu/~hastie/Papers/glmnet.pdf">Regularization Paths for Generalized Linear Models&nbsp;via  Coordinate Descent</a>&quot;.&nbsp;</p> <p>And then play with the <a href="http://cran.r-project.org/web/packages/glmnet/glmnet.pdf"> GLMNet Package</a> in R.&nbsp;</p>
3354, <p>Here is the R code I used. This reads the data into a list. The data strings start with a space so the [-1] drop the first term (NA).</p> <p>&nbsp;</p> <pre>train.pairs.raw &lt;- read.csv(paste0(dir.path'CEdata_train_pairs.csv'))  train.pairs &lt;- lapply(1:nrow(train.pairs.raw) function(x) {   tmp.A &lt;- as.numeric(unlist(strsplit(as.character(train.pairs.raw[x&quot;A&quot;])&quot; &quot;)))[-1]   tmp.B &lt;- as.numeric(unlist(strsplit(as.character(train.pairs.raw[x&quot;B&quot;])&quot; &quot;)))[-1]   return(matrix(c(tmp.Atmp.B)nrow=length(tmp.A)ncol=2)) }) </pre>
3354, <p>Most real data will have artifacts also due to experimental design. In the altitude example causation can be a tricky thing. As you rise in altitude pressure and temperature drop. So there would be correlation between temp and pressure with causation  from an increase in altitude. While temp and pressure will have a number of other influential factors (error term in this model) altitude will not. In the experimental design temp and pressure values will be measured every 500m or every t time interval as  a weather ballon accends. This pattern of measurement will provide an artifact flagging which variable the designer thought was the cause. In truth it is all more complex. Altitude does not effect pressure gravity does and altitude is just a piece to that  equation. Then pressure also has an effect on temperature. So where measurements from a weather ballon may predict that pressure and temp are correlated but cause by something else data from a laboratory experiment on PVRT( if I recall my physics correctly)  would show there is causation.&nbsp;</p>
3354, <p>The &quot;Edit Notebook&quot; button takes you a the last successful version of the notebook. This is not necessarily the most recent version of the notebook. The user then has to wait until the most recent version completes before they can modify it.</p>  <p>Could you add functionality to edit the most recent version regardless of completion status? Possibly aborting the script to avoid multiple instances running simultaneously. </p>  <p>Thanks Neil</p>
3354, <p>One of the hidden gems of using the Kaggle Notebook for exploration and development is that you can write and run code from your phone. It would be nicer if the Run/Stop Minus and Plus buttons were larger and easier to press.</p>  <p>These buttons are also small when viewed from the desktop so I do not believe it is a rendering issue with the phone browser.</p>  <p>I am using the Chrome browser on an iPhone 6.</p>  <p>Neil</p>
3354, <p>When developing a notebook I don't necessarily need or want the current form of my script to run in its entirety. It would be nice to have functionality to save the current version of the script without the engine running the entire script in the back ground.</p>  <p>I would propose a 'save' and 'submit' button. The save button would be associated with a development(unreleased) version of the notebook and the submit button would commit the code to your notebook as an officially released version.</p>  <p>Thanks Neil</p>
3354, <p>Yes averages and sums are models even if they are very basic ones.  Just calculate these values ignoring the current observation. These data are 37mil records though so I would just use k-folds sampling to make features for the hold out sets. </p>
3354, <p>I believe you will need to explicitly change the variables to factors using the as.factor() function.</p>
3362, ----
3364, ----
3366, <p>I am totally new in machine learning and hence my questions might seem absolutely naive to most of you. I am listing my questions below</p> <p>1. In the log loss definition N is the number of samples. Does N mean the number of features of number of molecules? Again since I am new in this field this question might look very dumb.</p> <p>2. Is the main idea of this kind of approach is to find logloss values of the test data assuming a true value (1 or 0) and then iterate the true value such that the logloss value matches the one that I calculated from the training file? The problem with  this method is to match the logloss value of each row of the test data to several logloss values of the training data set. Not sure if I was able to explain the question.</p> <p>3. Can you please suggest any online machine learning literature or book that is free to download and discusses about logloss metric to predict responses</p> <p>&nbsp;</p> <p>Thanks</p> <p>Ghosh</p>
3366, <p>thanks for your reply. I will go through the website you had suggested and&nbsp; most likely things will become more clear afterwards. I have one final question. If N is the number of samples = number of molecules then we need to calculate the log loss for each  feature of each molecule?</p>
3366, <p>ok thanks a lot. That clears some confusion. Sorry for the dumb questions. Trying this for the first time.</p>
3366, <p>thanksfor clarifying further. this realy helps me understnad the basics. </p>
3370, <p>hahahahahaha</p>
3370, <p>[quote=Bluefool;99128]</p>  <p>I don't understand?</p>  <p>[/quote]</p>  <p>On the left it shows the top (as in kaggle ranks) kagglers you have beaten in any competition. </p>  <p>On the right the top ones you have yet to beat (and they have beaten you). It also tells you in which comps they are playing right now.</p>
3370, <p>Although I did not have a chance to know him very well I consider it  a life-changing experience to have met him and I will always treasure it.  He fought for what he liked and loved to the very end and he was always positive and brave.  I am really devastated about the news but it may be that this world was too small for a man like like him.</p>  <p>Rest in Peace - Marios   </p>
3370, <p>Hi everyone</p>  <p>IMHO there are pros and cons with this update Some pros are: 1) You can evaluate somebody's contribution outside position-performance and understand some of the other softer skills (like willingnes to share) .  2) You can track your activity better 3) it can boost networking as it promotes openness and the use of kernels 4) There is more clarity about how well some has done and with who?</p>  <p>The main con is that :</p>  <p>It is not very fair I think and this needs to be amended IMHO. I guess discussions and kernels could give some tiny points but I don't see how some people are not grand masters (like Jahrer) and some others are because have many upvotes. Being popular should not make you a master  this is a technical field above all. I could write jokes and take upvotes. I think Kernels and discussion upvotes need to be removed from the equation in respect to the ranks or could be used for low ranks only (in my view) . I also I think the top 10 kaggle achievement is underestimated (while it is very hard to get). There could be a rule with how many people this is achieved by (in the same team but not ignored. For example and Inspired from the performance of my colleagues in Avito comp  I think it is kind of unfair not to become Masters (although with the old criteria they could have with a 10% and top 10 position)</p>  <p>Another con (logistically) is that kaggle members are far less than before! </p>  <p>I kind of liked the prize winner and top 10 badges!</p>
3370, <p>[quote=Myles O'Neill;126932]</p>  <p>I think there is a misunderstanding of the system here. Competition tiers are in no way influenced by forum posts or kernels. </p>  <p>[/quote]</p>  <p>When you click on a person you can see a main rank on the top Right . My understanding is that this needs to reflect the competition rank otherwise it is misleading.</p>  <p>[quote=Myles O'Neill;126932]</p>  <p>The reason the users you highlighted aren't Grandmasters is because we added a requirement that Grandmasters have at least 1 Gold result as a solo competitor. This was a piece of feedback we took from the community when we asked for feedback on the current system.</p>  <p>[/quote]</p>  <p>I think the grand master status needs to be refactored taking these accounts into account. This is just my humble opinion - they have won many competitions and I think you should not penalise someone (that much) for preferring to play in teams (especially when they have been prize winners multiple times). </p>
3370, <p>[quote=Myles O'Neill;126973]</p>  <p>On each tab it shows your tier for the work on that tier - not your overall tier. </p>  <p>[/quote]</p>  <p>Hi Myles</p>  <p>How is the overall tier computed? (is it the highest of the 3 you mentioned  namely competition kernel discussion)?</p>
3370, <p>[quote=Myles O'Neill;126977]</p>  <p>At the moment we have 0 users on the site who have reached Master or Grandmaster in Kernels/Discussion without also reaching it in Competitions. So at least for highly ranked users it is close to equivalent to competition tier.</p>  <p>[/quote]</p>  <p>What about : <a href="https://www.kaggle.com/inversion">https://www.kaggle.com/inversion</a>   ?</p>
3370, <p>[quote=Myles O'Neill;126982]</p>  <p>I meant for the two groups collectively. Inversion is an example of a user who has posted enough amazing and helpful comments to be considered a Grandmaster.</p>  <p>[/quote]</p>  <p>I don't agree with this rule. Everybody can start chasing upvotes. I can ask 10 of my friends to go backwards and like all my comments in order to become master in that category . This rule is very dodgie in my opinion. Same with the kernels. I totally disagree. I regard this rule a show stopper.. You should not be able to become  kaggle's highest tier  especially when the most popular comments are these: </p>  <p><a href="https://www.kaggle.com/c/rossmann-store-sales/forums/t/17898/leaderboard-shakeup/101439#post101439">https://www.kaggle.com/c/rossmann-store-sales/forums/t/17898/leaderboard-shakeup/101439#post101439</a></p>  <p>(<strong>Link</strong> contents may have been <em>sneakily</em>  changed since the time of my posting... to make me look like a whiny! haha!)</p>  <p>In the spirit of fairness and respecting science please come up with some other criteria to define a grand master if you feel Inversion falls in that category. I also feel he does  but not because of the votes. </p>  <p>Grand masters should be proven on the battlefield!</p>
3370, <p>[quote=inversion;126990]</p>  <p>Maybe you can team up with him on a competition and he can write some forum posts for you. Win/Win.</p>  <p>[/quote]</p>  <p>Apparently I would be losing my forum tier badge if I was to do that! haha</p>
3370, <p>[quote=inversion;126995]</p>  <p>Truth be told I do plan on getting my final competition gold badges in the next 2 months. So regardless of whether or not the kaggle folks make tweaks the situation should correct itself soon.</p>  <p>[/quote]</p>  <p>If kaggle does not correct it  my trust lies in you :)</p>
3370, <p>[quote=&#924;&#945;&#961;&#953;&#959;&#962; &#924;&#953;&#967;&#945;&#951;&#955;&#953;&#948;&#951;&#962; KazAnova;126983]</p>  <p>I don't agree with this rule. Everybody can start chasing upvotes. I can ask 10 of my friends to go backwards and like all my comments in order to become master in that category . </p>  <p>[/quote]</p>  <p>I think we can all see the problem here . All I needed was 50 comments and ask some colleagues and friends to like some of my comments. I went (shamelessly) to the process of doing that because I think it is important to not compromise the effort and struggle to compete and get to grand master status versus posting popular comments . I could make some dummy comments to reach 500 and and ask people to like them to become grand master within a couple of hours (it took some minutes to become master and get to the 2nd place but I was already quite high - 8th) but I had enough! <strong>The score I had before asking for assistance was 472. Not sure if kaggle could manually reset this. IF not I will ask people to down vote me back!</strong> I only wanted to expose the potential threat of having masters just by comments - you dont need that many. </p>  <p>In any case we should not be able to like comments older than 2-3 days to avoid abuse (just like I did) and put a limit to to how many you can give per day (and a person can get per day?) . Also I think votes should be restricted based on the ranks.For example maybe Novices should not be able to vote Juniors maybe a few votes and so on. </p>  <p><img src="https://kaggle2.blob.core.windows.net/forum-message-attachments/127166/4584/from 8th to 2nd in an hour.jpg?sv=2012-02-12&se=2016-07-16T14:51:26Z&sr=b&sp=r&sig=5I26zfBcseYJCqeD5RjpX3YpeAZnSBFgZN6jFGs4DGE%3D" alt="from 8th to 2nd in an hour" title></p>
3370, <p>[quote=beluga;127168]</p>  <p>First of all thanks for choosing my profile :)</p>  <p>[/quote]</p>  <p>I thought you were behind of all this...haha!</p>
3370, <p>[quote=dune_dweller;127131]</p>  <p>So am I! Maybe Kaggle should start putting numbers like Top 0.1% there =).</p>  <p>One more thing I like is that I used to be one of those pesky new masters from overly large teams and with the new system I can be a master even with only my solo results. I'm fine being ways away from grandmaster status for now =).</p>  <p>[/quote]</p>  <p>[quote=Bluefool;127134]</p>  <p>Oh no  you've all ruined my ego trip!</p>  <p>[/quote]</p>  <p>revenge is a dish best served cold - haha!</p>
3370, <p>[quote=Bluefool;127197]</p>  <p>Hi Admins There is deliberate malicious down-voting happening. Please name and shame</p>  <p>[/quote]</p>  <p><img src="https://kaggle2.blob.core.windows.net/forum-message-attachments/127200/4591/200_s.gif?sv=2012-02-12&se=2016-07-16T16:53:57Z&sr=b&sp=r&sig=Iiz50sdlrfoAZCrwLf%2BqtK9PLsQ9TMS7DnHSnTklaZI%3D" alt="Haha. Let the game begin! " title></p>  <p>(its not me only asked for upvotes not downvotes!)</p>
3370, <p>[quote=Bluefool;127202]  I don't want to stop writing on the forums because people don't want me to be Discussion GM. [/quote]</p>  <p>Embrace the challenge!  we can beat Abhishek together!</p>
3370, <p>[quote=inversion;127205]</p>  <ul> <li>[Inversion's] most loved forum member</li> <li><p>[Inversion's] most hated forum member</p>  <p>:-)</p></li> </ul>  <p>[/quote]</p>  <p>+1 the more gossipy  the better!</p>
3370, <p>[quote=Bluefool;127241]</p>  <p>My Team Women Power comments were more directed at the men in the team. I will always blame the &quot;wolf in sheep clothing&quot; men - I was just cross with Dune Dweller for allowing it to happen. She should have metaphorically punched you (in my world). Dune Dweller will succeed on her own and doesn't need &quot;pity help&quot;</p>  <p>[/quote]</p>  <p>Now I am tempted to give you -1  because you are rehearsing the same things . I did not help her  we helped each other . Our team was fuelled by Women power! </p>
3370, <p>[quote=Bluefool;127245]</p>  <p>The team was 75% men? stop being cliche [/quote]</p>  <p>When we formed the team we were equal. and we were well above the kaggle's averages. </p>
3370, <p>Also big thank you to the people that started downvoting my top ranked posts ( I will be 472 back in no time!) and started upvoting Bluefool and inversion!</p>
3370, <p>You know for every vote you remove from me I can add 5 right?! Come out clean whoever you are! Meet me in the back yard! Arrr</p>
3370, <p>[quote=Epilogue;127287]</p>  <p>Just thinking what would happen if people of UK could have given an opportunity to &quot;up vote/ down vote&quot; the Brexit outcome !!!</p>  <p>[/quote]</p>  <p><img src="https://kaggle2.blob.core.windows.net/forum-message-attachments/127292/4592/brexit.gif?sv=2012-02-12&se=2016-07-16T19:58:48Z&sr=b&sp=r&sig=NnBgs%2B9dO8XjqhEi17hKu5OHujv%2FBcNwXk5PfxuoBCo%3D" alt="enter image description here" title></p>
3370, <p>[quote=Triskelion;127310]</p>  <p>My activity looks like this and I think for others it looks different:</p>  <p>Tested with and without log-in on Mac and Windows</p>  <p>[/quote]</p>  <p><strong>I RECEIVED PERMISSION FROM Triskelion TO POST THIS IN ORDER TO FIX THE BUG.</strong> </p>  <p><img src="https://kaggle2.blob.core.windows.net/forum-message-attachments/127317/4594/tris_mismatch.gif?sv=2012-02-12&se=2016-07-16T21:45:56Z&sr=b&sp=r&sig=Z0fWTtiqYvdc%2FBcguQsoF3w%2BedU2qaqXJWBQB%2Fdln9Q%3D" alt="enter image description here" title></p>
3370, <p>[quote=Justfor;127322]</p>  <p>Kazanova- This picture and topic is not nice! Sorry I really suggest to delete it!</p>  <p>[/quote]</p>  <p>I don't see why . I am showing a bug in Trsikselion's account. I received permission from him to post that . You don't like the joke?! </p>  <p>Edit strange correlation but if it upsets in that way I will remove it. The main concept of the joke is that he cannot see his activity (like it does not exist but I can!)</p>
3370, <p>[quote=barisumog;127455]</p>  <p>[quote=anokas;127382]</p>  <p>Somehow it feels like this forum post boils down to &quot;your opinion is wrong and nothing is changing&quot;.</p>  <p>[/quote]</p>  <p>I'm sorry but I had the exact same (familiar) feeling when I read it.</p>  <p>[/quote]</p>  <p>+1 . [quote=Myles O'Neill;127367]</p>  <p>some members of the community (cough &#924;&#945;&#961;&#953;&#959;&#962; Sergey Zach Bluefool etc...cough) jumped to abusing it</p>  <p>[/quote]</p>  <p>Name and shame!  Could you at least tell us who started down voting us with such hate?!</p>  <p>Joking aside we have spent significant amount of time sacrificing our yesterday's lunch break(!) and my colleagues' (of which I now owe them a favor!) in order to (in my view) prove a couple of points and protect kaggle (again in my view) .</p>  <p>and because some people may not know this and I don't want to be considered a cheat We did this experiment on purpose as I specified here <a href="https://www.kaggle.com/forums/f/15/kaggle-forum/t/22208/kaggle-progression-system-profile-redesign-launch/127166#post127166">from 8th to 2nd in an hour</a>:</p>  <p>Penalizing Bluefool that actually helped you come to counter measures does not seem a fair path of action. </p>  <p>To the points of the discussion and this is the last time I do this as I think you may have already come  to certain conclusions about the direction you are going. :</p>  <ol> <li><p>This is sad to hear I think it demotes kaggle's reputation in predictive modelling although it will definitely attract more users... . It will lose in quality and gain in quantity. Obviously as a user that have spent huge amount of time in competitions and improving myself  sand and blood in the Arena  fighting against university peers that would say '' kaggle is not scientific''  I can't but feel deprived of at least some arguments against it. As a counter suggestion you could make the overall tier different than the rest with clear description as to why a person is grand master on the profile. People already using kaggle (like recruiters) will be confused the way things have been framed. Also it does feel to me that this was made to make inversion a grand master. I would suggest at least to increase the requirements so that at least it does not seem it was given.</p></li> <li><p>These are good measures. Votes will still be abused though obviously not at the same rate as before. </p></li> </ol>  <p>3</p>  <p>[quote=Myles O'Neill;127367]</p>  <p>For competitions specifically we changed the requirement to become a Master from 1 Top 10 performance + 1 top 10% performance to needing to receive 3 silver medals. There has been concern from a number of users that this requirement was way easier and would see a lot of extra masters in the tier. This is actually very far from the truth and the new master system is actually considerably harder to achieve in the old system than the new.Under the old system we had 828 Masters on Kaggle. Under the new system only 515 Masters qualify.</p>  <p>[/quote]</p>  <p>The concerns are very valid. <strong>You are making a critical mistake here assuming offline models will result to online more accurate predictions. People were not doing it  because they did not have to do it</strong> (like the votes). Please don't make me prove it to you!  Take it from me it is much easier to become a master with the new system than before.  For me  adjusting the formula for a master status would have to allow some people like <a href="https://www.kaggle.com/carloshuertas">NxGTR</a> (before AVito)  that they were just unlucky to clinch a top 10 spot but I did not have in mind making it that much easier (and not surprising you are  not hearing many complains from ''kagglers''). I think the changes already made to golden badges formulation kind of  accounts for that and the 3 silver rule could be overwritten with 1 golden and 1 silver (or 1 golden 2 silver i think is better). The equivalent would be 7 or 8 silver medals without the golden one. </p>  <p>Also I think the solo rule is not right . I can totally see not awarding golden badges to people getting to top 10s with big teams(like more than 45 people). But getting to top 10swith 2 or 3 people consistently should award grand master status after a while...</p>  <ol start="7"> <li>Having discussed with many people. It would be good to if others ( other than the profile user) are not able to see this  It can cause trouble in working environments :/.</li> </ol>
3370, <p>Apparently the Datarobot guys are laughing at  Bluefool's posts! (revives the old times!) </p>  <p>Also + 2 to Mario (I cannot up vote) . </p>  <p>(+1 for the post and +1 for his awesome small name :)) </p>
3370, <p>[quote=Gilberto Titericz Junior;127536]</p>  <p>Kaggle Please can you move the Master's competition results from the bottom to the top of the profiles?   They used to be the ultimate challenge at Kaggle so place it were they deserve  thanks!  ;-D</p>  <p>[/quote]</p>  <p>Gilberto I think they have said they will do that already</p>  <p>[quote=Myles O'Neill;127367]</p>  <h3><b>6: Competition Medal Issues</b></h3>  <p>We've received a few miscellaneous complaints about the way we are handling competition medals and ordering of competition results on profiles. We've talked through them internally and I'll give you the results for each case:</p>  <ul> <li><strong>We will now be awarding medals for Master-Only competitions</strong> (even though they don't award points)</li> </ul>  <p>[/quote]</p>
3370, <p>[quote=Radu Stoicescu;127542]</p>  <p>It is easier for some it is harder for others. It is easier for me.</p>  <p>[/quote]</p>  <p>Its 3 'submit' buttons pressed at the right time (possibly with some minor changes).  </p>
3370, <p>[quote=Radu Stoicescu;127552]</p>  <p>I challenge you to grab a top 5% spot with the submit button even with minor changes.</p>  <p>[/quote]</p>  <p>Santander has some around there.</p>  <p>Liberty Mutual too.</p>  <p>But even what you already posted shows that the scripts get you quite close - it does not take THAT much more </p>
3370, <p>[quote=Radu Stoicescu;127561]</p>  <p>Santander is an edge case for many reasons. And then you would have had to know which of the 10s (I counted 70) of versions of the script will score highest on the leaderboard.  [/quote]</p>  <p>You challenged me for a top 5% and I gave you some.</p>  <p>[quote=Radu Stoicescu;127561] And still the highest script submission only got a bronze medal <a href="https://www.kaggle.com/aliamiraldan">https://www.kaggle.com/aliamiraldan</a> [/quote]</p>  <p>And so many more probably just copied the scripts internally  made some changes(like add another model with the same input data and average them) and scored a silver.</p>  <p>We can agree to disagree . </p>  <p>We will see if future will prove me wrong (and see how many new masters we get after each comp  versus the past) </p>  <p>There is no comparison to scoring top 10 in a competition . I agree doing so with many members should be blocked too as I mentioned before. </p>
3370, <p>[quote=Radu Stoicescu;127568]</p>  <p>I disagree. You spent too much on the top there is life down here too. It takes a lot of work to get from top 25% to top 10% and then to top 5%. It might not seem like much to you...</p>  <p>[/quote]</p>  <p>I disagree too (e.g. I know how hard it is) . </p>  <p>Been there done that . I have played 80 competitions.</p>  <p>i started kaggle with top 50-60% (in competitions of 1000 members) . My first was Amazon. And back then we did not have scripts. You can check my profile. </p>
3370, <p>[quote=Radu Stoicescu;127580]</p>  <p>[quote=&#924;&#945;&#961;&#953;&#959;&#962; &#924;&#953;&#967;&#945;&#951;&#955;&#953;&#948;&#951;&#962; KazAnova;127571] You challenged me for a top 5% and I gave you some. [/quote]</p>  <p>You haven't because there are none. I have challenged you to publish a script that ends up with a silver medal it doesn't even have to be your version it can be a forked script from yours.</p>  <p>[/quote]</p>  <p>You said :</p>  <p>[quote=Radu Stoicescu;127552]</p>  <p>I challenge you to grab a top 5% spot with the submit button even with minor changes.</p>  <p>[/quote]</p>  <p>i gave you this  without ''the minor changes'' part (because I CANNOT track the ''minor changes part'') . </p>  <p>298/5123 =5.8%</p>  <p>299/5123 =5.8% too</p>  <p>[quote=Radu Stoicescu;127580]</p>  <p>But I am arguing that getting a silver takes much more than running a script but you keep being dismissive.</p>  <p>[/quote]</p>  <p>A bit more (as I stated before) . </p>  <p>I know many cases (colleagues or friends)  that have done that  (including me) . I have recent masters  recent seniors that have done it . I can give you that this is not true for each competition   in a decent time span it can be achieved with small changes on top of public scripts.  </p>
3370, <p>[quote=Radu Stoicescu;127611]</p>  <p>What for someone might look like a little bit of work to transform a public script in a silver medal for people trying to get a silver medal looks like a lot of work. Probably kagglers that can easily turn a script into a silver medal are all busy at the top of the leaderbord anyway and are &quot;Masters&quot; if not &quot;Grand Masters&quot; already.</p>  <p>[/quote]</p>  <p>I know many examples of people not falling to that (experienced) category having made it with a public script and some work on it (not much) . I guess we will have to disagree . </p>  <p>The overall point of this discussion is that it is very easy (compared to before) to become a master now. </p>  <p><strong>Check how many people became masters in Santander (with the old criteria)...and lets compare the number after this competition</strong>  <a href="https://www.kaggle.com/c/talkingdata-mobile-user-demographics">TalkingData Mobile User Demographics</a> .  (I am picking this one because it will also be quite popular) </p>  <h2><strong><em>And if its not significantly more ...I will apologise to you publicly.</em></strong></h2>
3370, <p>[quote=Ben Hamner;127634]</p>  <p>I'm skeptical about the point that the new system is easier than the old: 40% fewer users qualify as masters under the new system.  [/quote]</p>  <p>Ben </p>  <p>the strategy can change drastically now that the rule has changed - before there was no incentive to do so...it is similar with the votes. Now you will be able to see 10+ members team for positions 40 and 50 in popular competitions.  The ranking points there are not much anyway...however a badge that gets you a master?</p>  <p>I might be wrong...I guess future will tell. </p>
3370, <p>[quote=littus;127676]</p>  <p>Hi Kaggle  </p>  <p>if you're missing old user badges as do I check my small <a href="http://alonescience.com/kaggle">application</a> (batteries included)</p>  <p>It could be a little buggy but it works.</p>  <p>[/quote]</p>  <p>I like it!</p>  <p>shame I cannot up vote (I am temporally blocked)</p>
3370, <p>[quote=Radu Stoicescu;127660]</p>  <p>I strongly disagree with you when you are saying that a silver medal is one submit click away. I worked hard for my two silver medals (I am prouder of my Airbnb bronze than my BNP silver) and the data shows that there are few if any who are gaining their silver medals through public scripts. And if they do they do so when they are already experienced not their first 2-3 silvers.</p>  <p>[/quote]</p>  <p>I said 1 click away plus a little work - my point was that it is much easier than before to become a master . I know people that got many top 10% (with the old system) like that . </p>  <p>After 4 competitions you are 1 silver away to get your master status. It took me 1 year and 20 competitions to hit my master status. </p>  <p>(with the new system I would have become after 9 competitions and 4 months and back then we did not have scripts  but we did have some 'beat the benchmark' though that helped)</p>  <p>I had worked really hard in all these 20 competitions.</p>  <p>I think the data shows it is much easier   But it may be that you are really talented and I was really crap- the future will tell us!</p>  <p>Lets just say we disagree  for now to end the discussion.</p>
3370, <p>@kaggle </p>  <p>I am very pleased with these changes - thank you listening !</p>  <p>My question in 2 is what happens if someone is grand master in 2 or more categories. Will both be shown?</p>
3370, <p>Hi everyone I dont know why I am getting this :</p> <ul class="x_x_submission-warning"> <li class="x_x_error">Column 'user_id' was not expected (Line 1 Column 1) </li><li class="x_x_error">Column 'business_id' was not expected (Line 1 Column 9) </li><li class="x_x_error">Required column 'RecommendationId' could not be found </li></ul> <p>Did something change when I was not looking?</p> <p>Thanks</p>
3370, <p>very good points raised I totally agree.</p>
3370, <p>Just to verify that I am the same person...</p>
3370, <p>To further clarify  because I noticed a thread about private sharing of code (which I hope was not ignited by this one) right after I posted mine. There was absolutely no sharing I have an issue with my laptop and I had to use my friend's (flatmate). &nbsp;I did not notice it was already logged in as per her account name.</p>
3370, <p>@Zero Zero you could try this one from scikit Learn http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html</p> <p>as per the documentation:</p> <p>&quot;&nbsp;Linear Support Vector Classification.<br>Similar to SVC with parameter kernel=&#8217;linear&#8217; but implemented in terms of liblinear rather than libsvm so it has more flexibility in the choice of penalties and loss functions and <strong>should scale better (to large numbers of samples)</strong>&quot;</p> <p>Actually the liblinear implementation it is much faster than libsvm.</p> <p>&quot;This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.&quot;</p>
3370, <p>Great competition Congrats to the winners.</p> <p>What I found really interesting is this competition is that the approach was much more important that the sophistication in deriving features (to achieve a 0.895 score). I used the exact MFCC coefficients provided as I had no experience with signal &nbsp;extraction.</p> <p>Iterration 1:</p> <p>I created 64 features average absolute average Stdev and Absolute stdev for each one of the coeffs. I solved multiple Yes/no &nbsp;models (87 in fact!) for each bird with random forests. That brought me to 0.860 on the leaderboard</p> <p>Iteration 2:</p> <p>Instead of solving multiple yes/no i converted the problem to multiclass and duplicated records that had many lables. E.g. if in a record X and Y bird appear i would create 2 records one with X one with Y . the exact same features with extremely randomized trees gave &nbsp;AUC around 0.888. I also tried ANN svm logistic regression knn and naive bayes.</p> <p>iteration 3:</p> <p>continuing from step 2 I started a second model in the submission formthat is is bird x in song y (87 rows for each song)?. where I found the best weights based on previous models taking into account that certain models were better in predicting specific birds E.g. I think bird 66 could only be some somewhat predicted with ANNs. any other was giving me AUC of less than 0.5 (experience from iteration 1) !! . This last-phase optimization gave me uplift 0.06.&nbsp;</p> <p>Regards</p>
3370, <p>Hi dnaeye</p> <p>&nbsp;</p> <p>I am not an admin but I think it is OK since it is under GNU license.</p> <p>Plus whatever you do with the console can be replicated using the java source code.&nbsp;</p>
3370, <p>Around 0.800</p>
3370, <p>yup!</p>
3370, <p>Kind of.&nbsp;I think it was useful to train on the actual results of the past tournaments as labels where&nbsp;such &quot;surprises&quot; occurred and were part of the training logic- and I think that was an advantage over the&nbsp;&nbsp;semi-unsupervised approaches that some people may have undertaken. A good boost also came from&nbsp;&nbsp;looking at a team historically not only its recent or even last year's standings. Looking at the average % of wins in last 5 seasons to the the predicted one was kind of predictive.&nbsp;</p>
3370, <p>I trained on&nbsp;previous tournament results using data for up to the point of the tournament *(e.g for Tournament&nbsp;B I used season A and Season B to make predictions. For Tournament C I used seasons AB and C...so on). I did not use external data (apart from the metrics provided by Jeff). &nbsp;My Approach is ml (Random Forests) only.&nbsp;</p>
3370, <p>I thought I would start the common (and usually very productive) end-of-contest discussion about what was tried what actually worked what external resources were used etc.</p> <p>I am particularly interested because I am a big fan of&nbsp;NCAA (and NBA)  although I am a little disappointed&nbsp;that Kentucky lost (:( .&nbsp;</p> <p>First of all I think Jeff made this competition really interesting-with his passion about the game as well as&nbsp;the ratings' system- he made the predictive power of our models (mine for sure) much better than what it would have been.</p> <p>In terms of the approach I took I trained specifically on the past tournament results (so not much data) but I thought this was the actual framework you'll be tested on and generally my perception about the playoffs is that predicting wins in the regular season is different than predicting wins for the playoffs (e.g. see Miami 2006 Championship and the rather mediocre regular Season results). &nbsp;</p> <p>My features are :</p> <ul> <li><span style="line-height: 1.4">Difference of average points&nbsp;</span><span style="line-height: 1.4">between the teams in last 135 seasons prior to the tournament</span></li> <li><span style="line-height: 1.4">Difference of win %&nbsp;</span><span style="line-height: 1.4">between the teams in last 135 seasons prior to the tournament</span></li> <li><span style="line-height: 1.4">proportion</span><span style="line-height: 1.4">&nbsp;of times the left team won the right in the last 135 years in their match-ups (if any)&nbsp;</span></li> <li><span style="line-height: 1.4">The Actual Teams as inputs</span></li> <li><span style="line-height: 1.4">Difference in Seeds at the end of the regular seasons</span></li> <li><span style="line-height: 1.4">The ratings provided by Jeff as average ranks from all the different sources</span></li> </ul> <p>I used Random Forests to train that and I was getting cross-validations of Log-Loss around 0.543 to 0.557 and roc curves of 0.800 on average.</p> <p>So my cross-validation results are really close to my final standings.</p> <p>I guess 2 outcomes are really interesting from this competition:</p> <ol> <li>The rating companies do a very good job! They were always adding predictive power to my models so it is interesting to know what kind of information they utilise .</li> <li>Still I think there is room for a significant improvement with an approach such as this or other's.</li> </ol> <p>So what other people have tried??</p> <p>Also Congrats to MGF for a great win and thank you to William for being so responsive  removing cheaters and providing graphical updates!</p>
3370, <p>I would also add the yelp's one int o the mix where you could go and find the ratings on of the businesses you did not know the rating from the web (not to mention all the crawling) ! I also&nbsp;feel this is kind of unfair given the time I've put in this and other competitions where there was&nbsp;room for &quot;handpicking&quot; opportunities but I am not going to lose my sleep about it!</p>
3370, <p>Don't care what my prediction says (which I haven't looked) I support&nbsp;Kentucky!</p>
3370, <p>Although it is quite late in the competition still I found that these columns are the same with a near one&nbsp;in the training set and therefore can be removed as they don't add value:</p> <p>f764f736f700f701f702f678f326f327f328f318f319f320f310f311f312f302f303f304<br>f294f295f296f265f266f267f255f256f257f245f246f247f235f236f237f225f226f227<br>f195f196f197f185f186f187f175f176f177f165f166f167f155f156f157f126f127f128<br>f116f117f118f106f107f108f96f97f98f86f87f88f37f38f33f34f35</p>
3370, <p>I did not I just saw that they are identical&nbsp;with other columns so there is no point keeping them all.</p>  <p>For example (assuming my data set is not faulty) f325f326f327f328 are identical.&nbsp;</p>  <p>There is no point keeping them all in . You can keep one of these (they do not add new information).&nbsp;</p>  <p>Also columns like&nbsp;f37f38f33f34f35 take only the value of 0 . They do not add information either.</p>  <p>cheers</p>
3370, <p>Quite similar things here. I made my own implementation of KNN to avoid scaling problems as wellbut still I had size problems. In the end I did not have much time (took me one week to load the data!) and only found the categories that were not found in the knn benchmark (surprisingly they were around 150K out of 325K). Then I tried to solve the problem:</p> <p>&nbsp;A<em>ssuming the category appears what is the closest neighbor in the test set.&nbsp;</em>I used simply&nbsp;Euclidean distance of the highest 4 features (no time for more)  after making the Idf transformation plus scaling . Then I combined the results with the knn benchmark. It was surprisingly good enough to give me some small uplift and reach my&nbsp;current position.</p>
3370, <p>While I have no expectations (for a high rank)&nbsp;in this competition I second you Gilles that this is taking TOO long and I can understand your frustration. I think there was a reasonable amount of time prescheduled to account for the code-testing and still it has taken over 2 weeks to finalize the ranking. I think the kaggle admins should take over this one...</p>
3370, <p>First things first. This is not wrong. I have used AUC optimization for my models as well. As others have mentioned it is good&nbsp;to tackle this problem by optimizing each of the options separately in different models rather than the whole string of options as one model (e.g. 1003212). Categorization accuracy may be hard in this problem that is so dominated by the last observed option value when it comes to features' selection hyper parameters' optimization etc. Having said that&nbsp;AUC may be a good option for optimizing the overall discrimination of your model towards your options. Although AUC is measured for a binary problem (i.e. 1 and 0) you can transpose your problem as 10 for each one of the the possible outcomes in each one of your options. &nbsp;For example option A takes three values 012. Your average AUC should be the combination of 3 AUCs:</p> <p>1)&nbsp;The auc for the probability of option A to be 0</p> <p>2) the auc for the probability of option A to be&nbsp;1 and</p> <p>3) &nbsp;the auc for the probability of option A to be&nbsp;2.&nbsp;</p> <p>Don't get me wrong The final evaluation metric in this competition is Categorization Accuracy  but it may be better to optimize for AUC or something similar in some occasions when you build your model especially when it comes down to assessing the value of a feature.&nbsp;</p>
3370, <p>That is not a good comment.</p>
3370, <p>:)</p>
3370, <p>I find this multi-AUC to give the bets results.&nbsp;</p> <p>I get roughly for &nbsp;a 10-fold Cross-Validation</p> <p>Multi-Auc: Option A: 0.962 and // Categorization error (opposite of accuracy) : 7.17%</p> <p>Similarly</p> <p>Option B: 0.952 // 6.61%</p> <p>Option C: 0.978//6.92%</p> <p>Option D: 0.978//5.1%</p> <p>Option E: 0.962//6.12%</p> <p>Option F: 0.971//7.21%</p> <p>Option G: 0.95//13.63%</p>
3370, <p><strong>Try open it with Notepad. If it looks OK in notepad (with the zeros in the beginning) then your submission is fine</strong>. This happens because in Excel once opened immediately assesses the column as numeric and assumes the starting zeros should not be there. If you want excel to understand this as &quot;text&quot; you need to format the whole column as text . In that case you will need to paste your predictions to an excel column that already expects &quot;text&quot; input.</p>
3370, <p>Congrats to the winners!</p> <p>I joined fairly late and I did not have much time to experiment a lot however I immediately saw that there was something going on with the cities as I could barely&nbsp;beat benchmark without them&nbsp;and got top 50 in private with them only with G ! In short :</p> <ol> <li><span style="line-height: 1.4">I created a sample training set with Silogram's suggestion (thanks Silogram!) to replicate the distribution in terms of counts. &nbsp;</span></li> <li><span style="line-height: 1.4">Created dummies of cities (all of them) some time-constrained variables (e.g. how much the cost of the options changed through the time span shift in ages) starting values for all of the options and proportion of changes in each one of options's values (e.g. in A option value 1 was chosen 70% of the times form X customer).</span></li> <li>Run Neural networks with <a href="http://www.heatonresearch.com/encog">encog </a> Random forests (my own implementation) as multilabel Linear SVM (<a href="http://www.csie.ntu.edu.tw/~cjlin/liblinear/">LibLinear</a>)&nbsp;and&nbsp;Random forests for each one of the labels separately (many binary problems) for each one of the options.</li> <li>used the previous inputs of these models into a new random forest to make my final predictions for each input.&nbsp;</li> </ol>
3370, <p>Marat</p>  <p>It takes a little bit of time but you need to automate what you do (e.g. everything in code). Also save everything you've done in the past. I am spending less than half the time on a competition in comparison to when I started kaggling and I am scoring much better because everything is automated and to some extend more optimized . I also don't repeat stuff that did not work many times in the past so  which also saves lots of time. After while the important question will be how much and how fast your computer can handle the data rather than how much time you have :) . Don't get me wrong you always need to spend time to understand the data but even that gets faster the more you play.</p>
3370, <p>We'll get it next time Gert. Maybe we can team up in KDD and get it together!</p>
3370, <p>Is it possible to know who is actually providing the data?</p>
3370, <p>Hi Maverick</p>  <p>I am no admin but this was discussed a number of times in the past and answer was &quot;yes you can&quot;. They don't mind you using SAS as long as you release the SAS code if you win.&nbsp;</p>
3370, <p>Don't worry Mike. Experience says Kaggle admins will kick them when the competition ends.</p>
3370, <p>What should they do? Link kaggle accounts with bank accounts ? My experience with&nbsp;online lending&nbsp;tells me&nbsp;that no matter how complicated and demanding you make the application process (with SMS&nbsp;verification email verification car-identification) cheaters (e.g. fraudsters) will keep coming through when money is at stake...I guess there should be a &quot;refer a cheater&quot; element in kaggle . Also the top 10 (not just the winners) should&nbsp;provide the code they used to achieve their score (since it counts for master's status)  including how they derived their optimum solution (e.g. how they cross-validated how they removed//added features // tuned hyper-parameters etc ). &nbsp;</p>
3370, <p>I am no admin however <strong>I don't think</strong> using SAS is a problem because I've seen similar answers before . It is the CODE that you need to make publicly available (e.g. your SAS script) . I think some of the first guys in Wallmart competition used SAS as well. So SAS or MATLAB should be fine. &nbsp;</p>
3370, <p>That weird feeling when your best friend is also your enemy...</p>
3370, <p>[quote=Alexander Larko;50237]</p> <p>On Kaggle competition - anyone who's thirty!</p> <p>[/quote]</p> <p>I need 3 more years for that! Damn so close!</p>
3370, <p>I am also interested to know!</p>
3370, <p>I am game!</p>
3370, <p>I like it!</p>
3370, <p>[quote=Guocong Song;50516]</p> <p>Did you guys make cross-validation work? I had been frustrated with that all the time in the competition...</p> <p>[/quote]</p> <p>Did you try splitting via offer? E.g train on all offers but one and test on that one. That worked fairly well for me with a little bit more tweaking. E.g to say there should be at least X uplift in order to trust it. For me the was was 0.002 (from AUC 0.610 to AUC 0.612).&nbsp;</p>
3370, <p>[quote=auduno;50580]</p> <p>One problem&nbsp;I noticed sometimes when cross-validating was that although the classifier had good AUC on each offer individually the classifier overall had much much poorer total AUC. I'm thinking that this was because the relative probabilities of the different offers was not aligned somehow but that's just a guess. Did anyone find a principled way to deal with this? I looked around for some theoretical approach to this but could not find any.</p> <p>[/quote]</p>  <p>You needed 2 models! One to optimize for individual (offer-specific) &nbsp;AUCs and one for the general AUC :) At least this is what we did. Also scaling helped in this one.</p>
3370, <p>[quote=auduno;50589]</p> <p>[quote=KazAnova;50581]</p> <p>You needed 2 models! One to optimize for individual (offer-specific) &nbsp;AUCs and one for the general AUC :) At least this is what we did. Also scaling helped in this one.</p> <p>[/quote]</p> <p>Ah I considered that but didn't get enough time to try it unfortunately. Did you blend the two models to get final result or did you use the second model to adjust the relative predictions of the offer-specific models?</p> <p>[/quote]</p>  <p>Blend!</p>
3370, <p>[quote=auduno;50740]</p> <p>&nbsp;how did you optimize for individual offer-specific AUCs? Most of the offers in the testset were not in the training set so did you just optimize for those that were in the training set</p> <p>[/quote]</p>  <p>For those in the training set only. When I optimized for individual AUCs I was ignoring the sample size of the offer and I was just averaging the results of the AUCs of the different offers ( 1-out format) When we were optimizing for the total AUC we were appending the results of each offer's prediction&nbsp;in an array and calculating the total AUC once all results had been appended. I hope that helps.&nbsp;</p>
3370, <p>Hi everyone!</p> <p>What a great feeling to finally get the 1st prize (as well as the Master's status that was our target in this one).</p> <p>I would like to thank kaggle for the wonderful competitions and my team mate Gert for his valiant effort and wit! Also congrats to the rest of the teams that played fairly (and thank you for not beating us as there was nothing to improve for quite some time!)</p> <p>We will wait a couple of days before posting our solution in order to contact with Kaggle first (forgive us it is the first time!)</p> <p>Generally speaking what was really important in this one was to find a way to cross validate(1st problem!) and retain features (or interactions of them ) and then again there was the big difference between the offers in the training and test set (2nd problem!).</p> <p>For the first one we generally used a 1-vs-rest offers' approach to test the AUC&nbsp;and sometimes even derivatives of that. For the second (problem) we tried to maximize the with-in offers' auc (how well the offers score individually irrespective of the rest) and the total AUC&nbsp;(e.g. how the different offers blend together) as separate objectives.</p> <p>We used 3 (conceptually) different approaches (and some other minor blends):</p> <p>1. Train with similar offers <br>2. Train with whether the customer would have bought the product anyway<br>3. Assume that some features work for all offers in the same way (like: if you bought the product before that increases the probability of becoming/staying a repeater)</p> <p>More coming soon...</p>
3370, <p>Hi guys thanks for you kind words. Don't worry we will explain what we did once we sort out the competition output requirements. It will help us to structure our thought-process as well! About the size of the data what really helped was to make a separate .csv file for each customer and put all their transactions in it. That way we could manipulate it at will. I will post a code for that although it was really straight forward since the file was sorted by customer and date. All you had to do was:</p> <p>1) open a reader</p> <p>2) stream each line</p> <p>3) paste the new line in a file (named as customer_id.csv) for as long as the customer was the same.</p> <p>and 4) switch to new file once the customer is different and so on.</p> <p>That way you have kind of done the indexing yourself and it is very easy to aggregate a file with 200-300 lines of transactions!</p> <p>:)</p>  <p>I don't think I used more than 1 GB of Ram &nbsp;to create my training and test set.</p>
3370, <p>[quote=Trisco7;51581]</p> <p>Hi KazAnova</p> <p>Many congrats for your win.</p> <p>[/quote]</p> <p>Thx :)</p> <p>[quote=Trisco7;51581]</p> <p>Hi KazAnova</p> <p>Can you please explain what do you mean by similar offers? Is it on the basis of category or offer value?</p> <p>[/quote]</p> <p>The initial idea about this was to&nbsp;create clusters of offers that either look like each other feature-wise (e.g. unsupervised learning ) or conceptually-wise  because they share same category same popularity they were listed together in the offer.csv(!) etc and train on these.</p> <p>However it did not work very well.</p> <p>At the end we (mostly Gert) created unsupervised features that (I speculate) approximate that and dropped them into his model.</p> <p>[quote=Trisco7;51581]</p> <p>Hi KazAnova</p> <p>Then you said &quot;I don't think I used more than 1 GB of Ram to create my training and test set.&quot; These sets are not the individual customer_id.csv files. Am I right?</p> <p>[/quote]</p> <p>Both Gert and me we created different sets. Here I was referring to the set I created which is a mixture of aggregate measures&nbsp;that map the relationship of the customer with the item (e.g how many times the customer bought from the same category in the last 306090120150180360 days in the past) - similar to what Triskelion did.- as well as features that show how good the customer is in general (how many visits he's made from how many different&nbsp;distinct&nbsp;categories he's bought from-e.g. cardinality measures)</p> <p>Generally we are still waiting to finalize things with kaggle and also see how much we are allowed to share with you guys. We haven't forgotten about you though :)</p>
3370, <p>[quote=bpeng;63732]</p> <p>KazAnova</p> <p>We are wondering is there a complete version of the solution?</p> <p>[/quote]</p> <p>Sorry for the huge delay...I guess I do not know how to best handle this but it is quite difficult for me to post anything because I am currently working in an area very similar to this and I am bound by strict contractual commitments to not share this kind of methods&nbsp;:/ .&nbsp;</p>
3370, <p>[quote=Bluefool;64275]</p> <p>Kazanova - I had to get my 2nd place solution code removed from the forum. The anonymous sponsors didn't want us posting it</p> <p>[/quote]</p> <p>Thanks for informing me about that. It seems there are too many barriers for this one...</p>
3370, <p>This code should not have been available now at the end of the competition...Nevertheless since now it is available I don't see how it is not eligible to use it since you can take that piece of code and chunk into your own. Needless to say that if you are not near winning (like me :))  you can just use it and none will ever know...spooky! also since yesterday (8 hours) 10 people have passed me on the leaderboard!</p>
3370, <p>[quote=puffin;53782]</p> <p>We don't want to argue about semantics&nbsp;</p> <p>[/quote]</p> <p>I made up my mind I will use this as&nbsp;I see good improvement in my cv score. Thumbs up for finding that. You get my vote and thanks.</p> <p>Oh you cannot vote anymore?? I do not see and flag :/</p>
3370, <p>[quote=BreakfastPirate;53790]</p> <p>If I have created a new feature named E.A.T.I.T (Eta Average Tau Integrated by Time) can I have both CAKE and EATIT in my final submission?</p> <p>[/quote]</p>  <p>Having it for breakfast you mean :P?</p>
3370, <p>[quote=Giulio;53806]</p> <p>&nbsp;I still think you shouldn't have posted this.</p> <p>[/quote]</p> <p>I am not sure about that. This is a scientific competition if you find something that improves the overall discrimination of the model or gives extra scientific evidence for the subject-matter is good to make it known. The truth is if you do not score considerably well people may not take into account the value of what is done ( I think now the organizers will take a closer look of that cake)- In other words this sharing facilitates the purpose the competition was set up for ! For some of us this 'cake' may&nbsp;mean extra kaggle points(...I have not used it yet) yes but I still believe it adheres to the spirit of this competition. I mean. &nbsp;It is not a ground-breaking beat_the_benchmark script that gets you to top 10%.&nbsp;&nbsp;it gives +0.002 tops in my cvs (which might not be validated). Although I agree the timing could have been better.</p>
3370, <p>[quote=Algoasaurus;53810]</p> <p>[quote=Giulio;53806]</p> <p>&nbsp;... you're putting many folks in a place where they need to decide whether staying true to months worth of own work is ethically worthwhile while tens of people are jumping ahead of you thanks to somebody else's work.</p> <p>[/quote]</p> <p>Exactly Giulio. Thank you for speaking so eloquently to these points.</p> <p>[/quote]</p>  <p>I don't know why I am so involved in this I guess I am a big fan of sharing (although I 've lost 15 LB positions in the last 14 hours!) Nothing personal really.I respect all opinions but I guess I do not really agree with that statement either. Did you write the data mining package you used yourself ? Did you write Gradient Boosting yourself (I have done so in case you ask for me and I do not feel is cheating that you may use XGBoostscikitR etc )? To make you feel better do not try to chunk it in your code if there are moral issues with that find a better way to use it and make it your own. Same way you do with all the nice free packages you use. &nbsp;</p>
3370, <p>I am a little disappointing in this competition because we&nbsp;would have finished much higher (around top 30 -3.726) &nbsp;if my teammate remembered he was the leader and had picked the correct submissions! I finally did not use cake. Saw improvements on my cv but not on the leader board.It was too last minute to do more checks! For me there were not improvements from the cake in private leader board as well (same score).</p> <p>We&nbsp;did similar things with most people (e.g. GBM classification with weight)&nbsp;</p> <p>but i also found experimentally that if you transform your weight into a polynomial and run regression with it as a target it can perform competitively (but not as well as classification) - at least in my cvs.It definitely added positively to the blend.&nbsp;</p> <p>This is the relevant code in python format .</p> <p><code>sum_wpos = sum( W[i] for i in range(len(W)) if label[i] == 1.0 )<br> sum_wneg = sum( W[i] for i in range(len(W)) if label[i] == 0.0 ) <br> print (&quot;Creating new target from weights...&quot;)<br>new_target=[(W[i]*(sum_wneg/sum_wpos) +5.0) if label[i]==1.0 else - ((W[i]*W[i]*W[i])/5.0 +1.0*W[i] + 100.0) for i in range(0len(W))]</code></p> <p>W is the weight label&nbsp;is 1 if 's' else 0 for 'b'</p> <p>in scikit a good regressor for this I think would be</p> <p><code>&nbsp;ensemble.GradientBoostingRegressor(loss='ls' learning_rate=0.03 n_estimators=120 subsample=0.9 min_samples_split=30min_samples_leaf=20 max_depth=10 random_state=1 max_features=10)</code></p>
3370, <p>+1  (PS &nbsp;BTW i just noticed how to vote!)</p>
3370, <p>[quote=ACS69;54083]</p> <p>Funny how the people who moan the most are the same people who have annoyed others by &quot;Beating benchmark&quot; posts and sharing on the forums when it specifically said not to (eg Walmart).</p> <p>They're moaning about &quot;New Trends&quot; of late minute sharing - well their &quot;New Trend&quot; was also annoying to us old Kagglers - but we sucked it up. Now they can't handle &quot;New Trends&quot; lol</p> <p>Well done KesterLester - what you shared was interesting - don't listen to the moaners - seemingly they're the only ones allowed to share.</p> <p>[/quote]</p>  <p>ouch! Hit below the belt that was (with my Yoda accent)!</p>
3370, <p>[quote=ACS69;54086]</p> <p>[quote=KazAnova;54084]</p> <p>[quote=ACS69;54083]</p> <p>Funny how the people who moan the most are the same people who have annoyed others by &quot;Beating benchmark&quot; posts and sharing on the forums when it specifically said not to (eg Walmart).</p> <p>They're moaning about &quot;New Trends&quot; of late minute sharing - well their &quot;New Trend&quot; was also annoying to us old Kagglers - but we sucked it up. Now they can't handle &quot;New Trends&quot; lol</p> <p>Well done KesterLester - what you shared was interesting - don't listen to the moaners - seemingly they're the only ones allowed to share.</p> <p>[/quote]</p> <p>ouch! Hit below the belt that was (with my Yoda accent)!</p> <p>[/quote]</p> <p>Well truth hurts.</p> <p>[/quote]</p> <p>I remember a relevant stumble upon conversation you must have been waiting 1 year now for redemption! I generally feel you need to give credit to people who are willing to share (many of which participate in this thread)  I for one have improved a lot because of that so that I feel next time is my turn to contribute (if I can) .</p> <p>On the other hand I feel last minute hacks/tricks/information is not that great (but not bad either)&nbsp;because not everybody can integrate them effectively and exploit their full potential and may have a different&nbsp;scope than knowledge&nbsp;(e.g. I can support you if you give me 20% of your earnings &nbsp;with my trick/hack etc outside the team-merging rules) ...I am NOT&nbsp;saying that this is the case here but is something to be cautious in general (e.g. about last-minute mana potions) .</p> <p>Also I feel us (kagglers) deciding these rules is generally a nice thing hopefully leading to fewer misunderstandings in the future. Having said that and since you are at a more senior level than many of us (as an early adopter) I was really wondering what is your input in this kind of discussion and the points raised?&nbsp;</p>
3370, <p>[quote=Mark Vicuna;54106]</p> <p>[quote=barisumog;54102]</p> <p>But this is a competition.</p> <p>[/quote]</p> <p>Then it should be treated as such and rules made to 'even the playing field' so no golden feature or golden code is used to the unfair advantage to other competitors.</p> <ul> <li><span style="line-height: 1.4">No training data other then the set(s) provided</span></li> <li><span style="line-height: 1.4">You can only use sponsor approved libraries.</span></li> <li>Etc.</li> </ul> <p>In competitions where certain competitors are given advantages because they have access to 3rd party provided materials the sports made rules to make them fair.</p> <ul> <li><span style="line-height: 1.4">In motor sports all tires come from certain manufactures and are randomly assigned.</span></li> <li><span style="line-height: 1.4">In swimming competitions certain suits are swimsuits are denied because only certain people had access to them.</span></li> </ul> <p><span style="line-height: 1.4">Kaggle is setup like an academic exercise as you are supposed to bring these 'golden' &nbsp;materials and its up to you when you share them even if its not fair to the other competitors.</span></p> <p>[/quote]</p> <p>golden features and code is ok from my point of you if you can find them yourself. The competitions can never be completely fair to everybody. For example someone who uses a 256 GB RAM server has an advantage over the average user with a 4-8GB laptop as it was to have shaquille oneal in your team ( to continue with sports!). However to limit the available usable resources does no sound sensible from the point of view of the organizers. For us it is a competition but for them is a way to get a better solution to their problem...The competition is &nbsp;a means to get a better solution to the problem not to give us money . Having said that my mentality is play and learn solve problems collaborate with others share &nbsp;and if you are lucky (and good enough) win some cash as well which at the end of the day is very little compared to what you can do in the market right now if you have the skills (see DataRobot). &nbsp;</p>
3370, <p>[quote=Giulio;49641]</p> <p>BTW- does Kaggle hold you responsible only for submissions you choose?</p> <p>[/quote]</p>   <p>I hope not . If you know the results in the test set  you can tweak your models so that you optimize for the actual labels...and make it look &quot;fair&quot;.</p>
3370, <p>Apparently :p. It was too good to be true I guess!</p>
3370, <p>Miaoski</p> <p>I have a feeling you might not last long enough to see the end in this one...stilI can't help but &quot;admire&quot; what you've done up there on the top :P</p>
3370, <p>number 10</p>
3370, <p>He thinks he/she will have 9&nbsp;lives in this competition :P</p>
3370, <p>I saw many teams being formed last minute? Again miaoski's team is a good example and an advocate of cooperation.</p>
3370, <p>[quote=Kyle;50586]</p> <p>KazAnova:</p> <p>miaoski is one of my team member.&nbsp;</p> <p>Giulio:</p> <p>maybe you should read the rules first.</p> <p>[/quote]</p> <p>Yeah right! Tell&nbsp;us more about rules Kyle ;)</p>
3370, <p>Congrats to the winners!</p> <p>I don't know if anybody mentioned that  but what I found quite useful was to make separate models for:</p> <p>was fully funded (fully_funded)<br>had at least one teacher-acquired donor (at_least_1_teacher_referred_donor)<br>has a higher than average percentage of donors leaving an original message (great_chat)<br>has at least one &quot;green&quot; donation (at_least_1_green_donation)</p> <p><span style="line-height: 1.4">donations from three or more non teacher-acquired donors</span></p> <p>one non teacher-acquired donor gave more than $100&nbsp;</p> <p>the project received a donation from a &quot;thoughtful donor&quot;</p> <p>and using them as meta-features in another GBM model using project and resources' data</p>
3370, <p>[quote=Giulio;50694]</p> <p>WOW just noticed cheaters are being removed!</p> <p>[/quote]</p> <p>and I can still see a couple.</p>
3370, <p>same here</p>
3370, <p>my cv for my current score gives f1= 0.814</p> <p>I also believe top 10 will be the same.</p>
3370, <p>Great competition with a thrilling finish! That was quite some shake-up for the top 5 positions I guess we (that made many submissions) ended up overfitting the leaderboard al ittle bit. Big credit to Alexander and antip for the impressive late finish proving themselves leaders in this field for one more time (as in wikipedia) . I personally (since I come from Greece and have studied in Thessaloniki ) really wanted to take that first place . Maybe next time!&nbsp;</p> <p>We made separate models for each one of the labels.</p> <p>In trems of software we used the Java implementation of Liblinear  but we changed it a little bit to make it accept our sparse matrices for faster performance and made seperate models for L2 L1 regressions and L2 L1 linear SVMs &nbsp;all with hard regularizations (less than one). In fact the L1 svc with some scalling and c=0.19 of the initial set <strong>would finish top 10 on its own</strong> (you may want to consider it as it is very implementable)!0.77554</p> <p>In terms of how we assigned the labels we always took the highest score for each observation and assigned its corresponding label plus any other that was above a threshold X (subject to cross-validation).</p> <p>We also used neural networks from Encog on 2000 svd components created from the initial data and they were quite useful as they optimize for a multi-label error (e.g multi-objective not each label independently) .</p> <p>I also codded up from scratch random forests in Java for multi label problems that&nbsp;accept sparse input and I managed to finish it time to run it but it was really slow so I had to assign sub-optimal parameters and wait for 1 week to finish! they scored around 0.60 but they did contribute in the final blend.</p> <p>For the blend I used Scikits ExtraTrees and Random Forests to get my final score. It is a shame that my selection of final submissions was poor as well :( Congrats to the winners (really happy for you).</p>
3370, <p>[quote=Julian de Wit;50673]</p> <p>My goal of the competition was to learn proper blending.</p> <p>[/quote]</p> <p>Well you did more than that you got your master status as well congrats!</p>
3370, <p>We 've used scikit-Logistic Regression with L1 Regularization. We used all 410 columns.</p>  <p>This gives same result (0.80804) as the benchmark:</p>  <p>model=linear_model.LogisticRegression(C=0.16penalty='l1' tol=0.001 fit_intercept=True)</p>
3370, <p>No Problem. If you win the competition you owe me 10% :P !</p>
3370, <p>we had a whole range from 0.8 to 0.9 (less than 0.9). I would say do not trust the leaderboard nor your cv score in this one. Trust whatever will not overfit in the end ;) .&nbsp;</p>
3370, <p>[quote=Giulio;49908]</p> <p>Are you suggesting I kill that Deep Learning model I had running for the past 3 days :-)</p> <p>[/quote]</p> <p>Add 30 more hidden layers please!</p>
3370, <p>I won't be surprised if we finish much lower (100++) than where we are at the moment. There are 2 sources of randomness.</p> <p>1) Small training set</p> <p>2) Small test set (e.g. even if you had a big training set scoring only 40 cases will always be subject to a great proportion of randomness)</p> <p>Congrats to the luckier&nbsp;;)</p>
3370, <p>Don't worry  at least your girlfriend does not moan about it!</p>
3370, <p>So our best model in private leader board (would finish top 10%-around 0.89) was to get the Pearson correlation coefficients for all variables and use it as normal coefficients (like in linear regression)- a simple sum-product!</p>
3370, <p>To the people that are going to write papers for this one...</p> <p>What really strikes me is the fact that stats fail really hard in this problem. I have a couple of 2-variable combinations that score around 0.87 on training set with logistic regression and a couple of 3-variable combinations with training AUC 0.9 (ish). All results were &quot;statistically significant&quot; at 0.001 (not even 0.01) . I have tried the same selections with SAS SPSS R and scikit (with regularization) . All results are consistent (and similar) with all packages yet again they scored around 0.5 (random) in public and private leaderboard. This makes me think about all the PhDs' thesis and medical science papers I've seen being carried out on mickey mouse sets  claiming statistical significance gives credibility to their findings ... Is machine learning more reliable than stats? I say if you can't predict it consistently on a hold out set then you got nothing whatever the tFChi-sq distributions say.</p>
3370, <p>[quote=Michael Jahrer;55202]</p> <p>We spend there hundreds of hours and they removed us without any reason.</p> <p>[/quote]</p> <p>Interesting (in a sad way :( ) I would think twice before I remove the Netflix winners from any competition...&nbsp;</p>
3370, <p>[quote=Michael Jahrer;55229]</p> <p>Is there a way to re-enable our team at higgs ? Sorry for the beginner mistake of one of our team member.</p> <p>[/quote]</p> <p>Hi Michael&nbsp;</p> <p>I am a big fan of your group's work and while (obviously) I am no admin I would state that&nbsp;rules are rules and it's good they are there  because the incentive to cheat is obvious when the given rewards are attractive (as it was in that case) . IMHO even if it was for one submission it still yields an advantage (not to waste&nbsp;the team's&quot;submissions).</p> <p>I am sure there will be many more competitions for your group to shine again in the meantime congrats for your solid results in this one :)</p>
3370, <p>I know about Pokemon...I used to play cards. I will (willingly)&nbsp;trade my Sandslash for your Blastoise if you post your solution :p</p>
3370, <p>[quote=Abhishek;52772]</p> <p>[quote=ACS69;52770]</p> <p>[quote=rcarson;52767]</p> <p>[quote=James King;52765]</p> <p>Great answer I nominate Triskelion for <em>de facto&nbsp;</em>competition admin. Also for best avatar...</p> <p>[/quote]</p> <p>Absolutely</p> <p>[/quote]</p> <p>Yes Triskelion can be the Competitor's Union Rep! He has my vote</p> <p>[/quote]</p> <p>+1</p> <p>[/quote]</p>  <p>+ 1.34387</p>
3370, <p>my cvs go from 0.29 to 0.44 (70-30)... whatever that tells you! Thank you for this by the way very informative :)</p>
3370, <p>This was a very messy dataset and although We made many submissions it was not until 3 days ago that we did some serious feature's analysis which revealed many inconsistencies in the sets. I guess what we&nbsp;learned from the competition is that we need to check the correlation matrix (attached) of the features before we start doing anything else!. For the correlation purposes we&nbsp;replaced all categorical variables to ranks based on the average loss in order to make them numeric and <strong>also&nbsp;replaced all missing values with -9999</strong>&nbsp;. Here is everything we've found:</p> <p>1) vars: Vars1-17 are ok all are different and were&nbsp;used.</p> <p>2) Var11 aka the <strong>weight has an inverse correlation with the target</strong> and I found it much better to use it as feature rather than as weight</p> <p>3) crime : <strong>only crime 247 were&nbsp;different .</strong> crime 135689 are the same (in terms of correlation given the aforementioned assumptions) and I picked only 1 of these.<br>4) geodem: <strong>all the geodem variables were&nbsp;perfectly correlated with each other</strong>. I picked only 1 of these<br>5) Weather <strong>!</strong><br> a) weather181-198 are all the same picked one<br> b) weather199-208 are the same picked one<br> c) weather 209-226 are the same picked one<br> d) weather227-236 are the same picked one<br> e)weather4 weather17 &gt;&gt; . picked one of the two<br> f) weather6 weather19  picked one<br> g ) weather41 weather54  same<br> h) weather43 weather56 &gt;&gt;<br> i) weather77 weather90&gt;&gt;<br> j) weather79 weather92 &gt; &gt;<br> k) weather113 weather125 &gt; &gt;<br> l) weather147 weather160 &gt;&gt;<br> m) weather149 weather162&gt;&gt;<br> n) Every 2 pairs from (e) to (m) there are strangely close correlations between the variables- almost like a pattern could be different days of measuring weather?<br> o) The rest of the weather variables were&nbsp;the same and I picked&nbsp;only 1!.namely:</p> <p>weather1weather2weather3weather5weather7weather8weather9weather10weather11weather12weather13weather14<br> weather15weather16weather18weather20weather21weather22weather23weather24weather25weather26weather27<br> weather28weather29weather30weather31weather32weather33weather34weather35weather36weather37weather38<br> weather39weather40weather42weather44weather45weather46weather47weather48weather49weather50weather51<br> weather52weather53weather55weather57weather58weather59weather60weather61weather62weather63weather64<br> weather65weather66weather67weather68weather69weather70weather71weather72weather73weather74<br> weather75weather89weather91weather93weather94weather95weather96weather97weather98weather99<br> weather100weather101weather102weather103weather104weather105weather106weather107weather108<br> weather109weather110weather111weather112weather114weather116weather117weather118weather119<br> weather120weather121weather122weather123weather124weather126weather127weather128weather129<br> weather130weather131weather132weather133weather134 weather135weather136weather137weather138<br> weather139weather140weather141weather142weather143weather144weather145weather146weather148<br> weather150weather151weather152weather153weather154weather155weather156weather157weather158<br> weather159weather161weather163weather164weather165weather166weather167weather168weather169<br> weather170weather171weather172weather173weather174weather175weather176weather177weather178<br> weather179weather180</p> <p>Our&nbsp;best submission includes a reduced set of features based on the redundancies as explained above and the following models:</p> <p>1) LamdaMart from Ranklib where each id was formed to be a different random set that had at least 70% of the total targets and 20k random 0's. we&nbsp;put a huge NDCG as well.</p> <p>2) XGBoost on Vars1-17 only (categories as ranks)</p> <p>3) XGBoost on 4 crime variables plus 1 geodem</p> <p>4) XGbbost on 20ish weather variables (as explained above)</p> <p>5) scikit GBM on all fetaures</p> <p>6) Ridge on Vars 1-17&nbsp;</p> <p>7) XGboost on vars1-17 with categorical features as dummies&nbsp;</p> <p>8) a ridge ensemble of various features.</p> <p>For the final blend we relied only on the performance of are cvs as weights.&nbsp;</p>
3370, <p>[quote=rcarson;53022]</p> <p>[quote=barisumog;53012]</p> <p>There's one little trick I used which I guess others have also done. Instead of predicting the losses directly I took the logarithm and predicted on that.</p> <p>[/quote]</p> <p>We also see logarithm helps but I don't really understand why.</p> <p>[/quote]</p>  <p>Strange. The log transformation did not work in my cvs never bothered submitting one because of that. We also tried binary models where the loss was higher than 0.38. It seemed to perform quite decently but not as well as normal counts (at least for me).</p>
3370, <p>[quote=David Thaler;53070]</p> <p>[quote=KazAnova;53016]</p> <p>3) crime : <strong>only crime 247 are different .</strong> crime 135689 are the same (in terms of correlation) and you should pick only 1 of these.</p> <p>4) geodem: <strong>all the geodem variables are perfectly correlated with each other</strong>. You should pick only 1 of these</p> <p>[/quote]</p> <p>Am I doing something wrong here? I'm not seeing this. I ran corr() on a Pandas data frame with 100K rows of train.csv and got a normal-looking correlation matrix with 1's on the diagonal and numbers in (-1 1) elsewhere.&nbsp;</p> <p>[/quote]</p>  <p>Actually I just made a very interesting discovery following your post and although my previous post can be misleading it was also the source for&nbsp;our good results I think!</p> <p>I forgot to mention 2 important steps...I r<strong>eplaced all missing values with -9999</strong> before I run this. I just re-run the results and my (Pearson) correlations are confirmed (with SAS). What I am (guessing) it happened is that the variables were correlated enough so that by replacing the missing values with a big value  it rounded all the very closely-correlated features to 1 leaving only these that had&nbsp;significant differences from each other.&nbsp;</p> <p>The second step is that these were absolute correlations and rounded to the third digit eg 0.999=1.&nbsp;</p>
3370, <p>[quote=ACS69;54429]</p> <p>Did you not see the major leaderboard moves after posting? some people went up 300 places?</p> <p>[/quote]</p> <p>(you 2) Your arguments never get old! I believe people overfit the leaderboard. I would not worry much about it.</p>
3370, <p>[quote=Michael Anuzis;54449]</p> <p>ACS69 I hope you won't&nbsp;leave Kaggle. &nbsp;</p> <p>[/quote]</p> <p>Noooo we won't let her go :)</p>
3370, <p>[quote=ACS69;56370]</p> <p>[quote=phunter;56369]</p> <p>Congratulations! And I want to know the magic from&nbsp;Charly B. * who managed jumping up 886 ranks in the private LB!</p> <p>[/quote]</p> <p>Those arrows aren't correct. We went up 90 places not down 3 and Charly went up about 480</p> <p>Congrats to the winners</p> <p>[/quote]</p>  <p>I think the arrows refer to the private leaderboard. E.g the fact that we have -3 means that at some point in the previous hours we were 2nd in the private leaderboard!</p>
3370, <p>[quote=rcarson;56380]</p> <p>Congrats to winners but also to&nbsp;UK calling Africa and&nbsp;Dmitry &amp; Abhishek. Picking two non-overfitting submissions out of 100+ candidates could be very challenging especially for competition like this.&nbsp;</p> <p>[/quote]</p> <p>Thanks :). I don't want to be cocky but I was expecting this kind of shake-up ( also mentioned here&nbsp;https://www.kaggle.com/c/afsis-soil-properties/forums/t/10351/beating-the-benchmark/54430#post54430 ) and although to finish that high (5th) when you are tested in 600 cases is always to a great extend luck we battled consciously against over-fitting . For us it was always about trusting our cvs and ignoring the leaderboard...and blending many different models. I cannot see but I suspect our best submission in the public leaderboard was also the best in private...but I'll have to check. I personally saw H2O not to perform better than linear models or linear with some transformations (e.g. svrs) and Abihshek's &nbsp;benchmark to significantly over perform on the public leaderboard &nbsp;than in my cvs (from 0.47++ to 0.436). However from my part I found Abhishek's benchmark to give the best single-model results (e.g. SVR with RBF from scikit) &nbsp; after&nbsp;&nbsp;tweaking the parameters differently for each CAPPHSANDSOC.</p>
3370, <p>This field is growing really fast and I strongly believe a benchmark in a live competition is more important for learning than one for a closed/finished competition.&nbsp;The newest competitions are the ones that will use more up-to-date software/ techniques etc ( as well as the old-school ones) so there is a great incentive posting benchmarks . A good example is the XGBoost in Bosson and H2O in this one. If you go back in Amazon you won't find these  but at the same time newer things are coming out and you don't want to be outdated therefore you join the trend. If someone posts an H2O benchmark for Amazon who is going to read it ? There are always active competitions...</p> <p>Also learning is good. Learning with a chance to win money (or glory) is even better. Assuming there are always active competitions that give money/points&nbsp;one will easily target these since they satisfy both.</p> <p>All in all&nbsp;&nbsp;you never win just with the benchmark code so keep posting :)</p>
3370, <p>[quote=ACS69;56467]</p> <p>For my part of UK Calling Africa:</p> <p>1. Use 3 data transformations: First derivative gap derivative and the SG. Remove CO2</p> <p>2. Run 2 datasets per transformation: with / without non-spectral</p> <p>3. For each dataset run BayesTree Bayesian Ridge and GBM</p> <p>4. Ensemble by straight averaging regardless of individual leaderboard results</p> <p>[/quote]</p> <p>And my part averaged as 50% - 50% with ACS69</p> <p>from Scikit</p> <p><br>1) 50 baggers (E.G. bootstrapping) of SVRs (with rbf kernel) with different parameters for each CA P...SOC etc<br>2) 50 baggers with SVRs (with poly kernel) on PCA-transformed set and with different parameters for each CA P...SOC etc<br>3) 50 baggers with ridge regressors with different regularization parameters for each CA P...SOC etc . Also trained on target = log (y + 7.5)<br>4) GBR (e.g. Gradient Boosting Regerssor) with base estimator svr (with rbf kernel) with different parameters for each CA P...SOC etc<br>5) GBR with with feature selection where I saw the relative strength of each feature for each target by first binning them in equal population and assessing r-qsuared on the transformed variables as you can see in the spreadsheet attached.&nbsp;<br>6) did stacking generalization of all these models and used predictions of all models to predict the rest. e.g. I used predictions of CA P PH and SAND to predict SOC. You can see that all of them are correlated with each other.</p> <p>My cvs were always 20 or &nbsp;50-folfd 50%-50%. All these where trained on all features (spectra and non-spectra).</p> <p>from Java</p> <p>1) Run SVR from libsvm with linear kernel with different parameters for each CA P...SOC etc and reduced set (picked every 20th column) and log(y + 7.5)<br>2) baggers of ridge regressions on same set trained with sgd and log(y + 7.5)<br>3) baggers of neural networks from encoge and log(y + 7.5)<br>4) Gradient boosting on the reduced set and log(y + 7.5)</p> <p>Thank you to my teammate for the great results and learning :)</p> <p>P.S I do not know if the winning submission is the one that contains the java-based models  but they did work in my cvs..not so much on public leaderboard</p>
3370, <p>[quote=Gilberto Titericz Junior;56591]</p> <p>My team didn't choose a good model for our final submission but I want to share some of our best models.</p> <p>Model 1 - LB Public: 0.42278 Private: 0.49602 - 68th place</p> <p>It's basically Abhishek's beat the benchmark but I used a subset of the spectrum. Subset means I use only odd features (1357911...3577) and SVR C=100000.</p> <p>Model 2 - LB Public: 0.43940 Private: 0.49115 - 32th place</p> <p>I used a subset of spectrum 10 times smaller (feats 1112131...3571) and ran 9 models crossvalidating 20 folds. The folds are alternating for ex. 1234...19201234... I don't care about pairs or geographics regions when doing cross validation. The 9 models are from sklearn for ex. SVR Ridge BayesianRidge KNeighborsRegressor GradientBoostingRegressor RandomForestRegressor LogisticRegression DecisionTreeRegressor PassiveAggressiveRegressor. Then I ensembled all using Nelder&#8211;Mead optimization technique. 20 fold CV: 0.4140.</p> <p>Model 3 - LB Public: 0.44298 Private: 0.49343 - 44th place</p> <p>It's basically Abhishek's beat the benchmark but I used 3 subsets of the spectrum one for each model. Model 1 uses the features 147... 3577. Model 2 use features 258...3575. Model 3 use features 369...3576. Then I did a simple mean of the 3 models.</p> <p>Model 4 - LB Public: 0.57362 Private: 0.47021 - 4th place</p> <p>That model is a bag of 10 Neuralnet models for each target. I just used a subset of the Spectrum 20 times smaller (features 1214151...3571) but each model the first feature of the subset is random from 1 to 20. The nn has only 1 hidden layer with 3 neuron. I used a early stoping criteria for speed and performance. Trainned used the 37 folds BreakfastPirate proposed. Local CV: 0.40.&nbsp;This model would have reached the fourth place if I had chosen :_-(&nbsp;&nbsp; .&nbsp; I didn't choose that model because his performance (0.57362) was not satisfactory and I believed that he was overfitting because of the early stop criteria I used.</p> <p>That model makes me think that NN are the BEST models for Africa competition... I didn't tryed to tune that model but I'm sure if I had tuned&nbsp; the performance would be better also.</p> <p>[/quote]</p>  <p>That is great stuff Gilberto what package did you use for your nns? I could not make them work better than my linear or semi-linear models in my cvs. On the other hand I used only encoge and H2O for deep learning.&nbsp;</p>
3370, <p>+1 for sharing when you were on the absolute &nbsp;top. No complains here!</p>
3370, <p>[quote=tinrtgu;55688]</p> <p>but it seems that KazAnova got his place right back.</p> <p>[/quote]</p> <p>That was a desperation submission. Spent the last glimpse of gas&nbsp;I had inside me to maintain my crown for a couple of minutes more . I am all out of tricks now!</p> <p>PS It is too early . Ranking does not matter until the last 2 weeks! Triskelion&nbsp;</p>
3370, <p>[quote=Phillip Chilton Adkins;55695]</p> <p>[quote=tinrtgu;55677]</p> <p>Other than modifying it to fit into a multilabel framework I also changed the adaptive learning rate. Unlike the <em>other</em> competition features in this data set is much more dense (they appear more often). Thus I changed it from a counting base adaptive learning rate to a sum of past gradient one.</p> <p>[/quote]</p> <p>I'm wondering what the basis for this per feature learning rate is; I've never seen it before.&nbsp; I do have an intuition as to why it's&nbsp; so much better than a global learning rate:&nbsp; It automatically regularizes by learning high-prevalence feature weights first and more independently of less-prevalent features ( since they are learned more slowly ) and then locks in-place these high-prevalence feature weights while the low-prevalence features only regress to the residuals left behind.&nbsp;</p> <p>It's an excellent learning strategy.&nbsp; Did you do this on purpose is it based on something you've seen before?&nbsp; Did you understand that it would have this automatic regularization effect?</p> <p>Also you just willy-nilly put in sqrt( sum of gradient updates) or sqrt( # of updates ) as the decay control.&nbsp; Is this just a kluge or is this derived from something?&nbsp;</p> <p>I like the sum of gradients because it automatically scales itself - the first update has size = 1.0 * alpha so you have complete control over the exact magnitude of the updates based on choice of alpha.&nbsp;</p> <p>[/quote]</p>  <p>It is a common trick in ml to use past-sum of gradients for updates. Vowpal Wabbit uses the same and I think they refer to this paper in their tutorial&nbsp;jmlr.org/papers/volume12/duchi11a/duchi11a.pdf . This is also a funny paper in the sense that writes 40 pages about something quite simple (as most papers)- e.g divide learning rate with the squared sum of previous updates for each feature. Give credit to&nbsp;<a href="http://www.ci.tuwien.ac.at/~alexis/About.html">Alexadros</a>&nbsp;that explained it to me so that I did not have to read it myself&nbsp;! &nbsp;The logic behind it (in plain terms) &nbsp;is that if a feature has already moved a lot (or descent a lot if you prefer)  it is likely to be moving too fast so taking into account previous steps makes certain that constrains its future updates.&nbsp;</p>
3370, <p>[quote=Pietro Marini;56588]</p> <p>Hi Tinrtgu</p> <p>thank you very much for sharing this code.</p> <p>I was wondering whether you can share some insight on how did you choose this adaptive learning rate scheme. Is there some article that justifies this choice?</p> <p>Thank you in advance</p> <p>Pietro</p> <p>[/quote]</p>  <p>see my answer before I paste it here as well:</p> <p>[quote=KazAnova;55697]</p> <p>It is a common trick in ml to use past-sum of gradients for updates. Vowpal Wabbit uses the same and I think they refer to this paper in their tutorial jmlr.org/papers/volume12/duchi11a/duchi11a.pdf . This is also a funny paper in the sense that writes 40 pages about something quite simple (as most papers)- e.g divide learning rate with the squared sum of previous updates for each feature. The logic behind it (in plain terms) is that if a feature has already moved a lot (or descent a lot if you prefer)  it is likely to be moving too fast so taking into account previous steps makes certain that constrains its future updates.</p> <p>[/quote]</p>
3370, <p>Hi</p>  <p>Nothing personal but I noticed that&nbsp;Chih-Ming does not fullfill the old criteria to be a master (as in kaggle).&nbsp;</p> <p>I can still see that to get the master status you need:</p> <p>Consistency: at least 2 Top 10% finishes in public competitions <br>Excellence: at least 1 of those finishes in the top 10 positions</p> <p>Did that change?</p>
3370, <p>It has to be something like 0.9996-9997</p>
3370, <p>Congrats to the rcarson and chen  you did scrutinize that set!</p> <p>Also well done to the people that took the online approach vs me(and others I guess)&nbsp;that loaded it all in memory !</p> <p>From my side I built a couple of models from&nbsp;3 different random samples (90%) of the data and saved the last 10% of each for meta.</p> <p>My best model was a random forest after converting all categorical variables to counts and using &nbsp;a multivariate label output (so trained it as a whole not each label y&nbsp;individually). that score 0.006</p> <p>I also run a knn with 150 neighbors in similar fashion.&nbsp;</p> <p>I did not do many interactions. but I did blend a couple of logistic (l1 l2) and svc models on exactly the same train/cv splits. simple average of these models was around 0.0055.&nbsp;</p> <p>The meta &quot;thing &quot; did most of the impact for me as i trained a model using exactly the same format as the submission where the number of the label is input and the rest of 3 (models) *10% &nbsp;validation models stacked were inputs too. Ironically even a model with only the labels as inputs (e.g 1.2.3..33) can score less than 0.01 in the leader board with an AUC more than 0.90 for these 10% samples. &nbsp;My &quot;meta&quot; model was trained with rfs and scored around 0.0047/0.0048 on pul/prl</p>
3370, <p>[quote=Jules;58383]</p> <p>I want to enter a challenge but as a sort of minimum insurance against exploitation I only read challenge descriptions with prize money of 25k or higher.</p> <p>[/quote]</p> <p>Man..do it for the joy consider the money a small plus! I can see progression (as in better results) through your kaggle life. This is more important than cash (IMHO) . Besides you get kaggle points that improve your ranking so that you can then go and&nbsp;brag about it to your friends and&nbsp;old school nemesis (you know the guy who was always more popular than you and got to be president and never played basketball because football was more like his&nbsp;taste etc..needless to say that all girls liked him!) !!!</p> <p>Since you ask I downvoted you for 2 reasons:</p> <p>1) Because &quot; you only read descriptions of competitions that pay big/enough money&quot;. This seems to be very against my beliefs You say this kind of thing in a place where -in principle -people have passion about problem solving (many of which come with a research background) or/and came to improve their skills .</p> <p>Of course there are some that came strictly for the money but I guess I personally do not want to be that person.</p> <p>Having said that I have no problem in requesting more money this is NOT&nbsp;why I downvoted you.</p> <p>2) Let me rephrase what you said. &quot;Listen guys I don't play in low-badget stuff (tres banal?)  if you want me to play better raise up the bar a little bit&quot;. E.g. kaggle should raise the money in this competition because generally YOU don't play unless the money is x. Just don't play.</p>
3370, <p>[quote=fchollet;58439]</p> <p>2) It's like video games but with algorithms.</p> <p>[/quote]</p>  <p>Haha very true!</p>
3370, <p>[quote=Jules;58442]</p> <p>&nbsp;I don&#8217;t think I have got a school nemesis (I used to get all the girls ...</p> <p>[/quote]</p> <p>I knew It was you...</p> <p>Well responded .&nbsp;&nbsp;</p>
3370, <p>Looks good but I am not ready to give up on Desktop Dungeons! (www.desktopdungeons.net)</p>
3370, <p>@Julian de Wit great explanation mate !</p> <p>I will talk for my part (which would probably have finished top 10)</p> <p>Based on some discussions I had recently with colleagues academics and other data science enthusiasts  I came into some sort of collision by claiming that on many different problems at present&nbsp;(and with the advances of machine learning) you can do really well without even inspecting&nbsp;&nbsp;the data - just by&nbsp;</p> <p>1) knowing the tools</p> <p>2) cross- validating properly when you make features' selections hyper parameter optimization etc</p> <p>3) ensembling many different models to capture different information</p> <p>In Light of this I did exactly what I tried in Tradeshift plus a couple of more models since we had more time. &nbsp;I only looked at the header of the&nbsp;data and once I understood the set is divided by days  I did a simple frequency count in that column. I also found &nbsp;the row that constitutes the beginning of day 30 (row&nbsp;36210029) and looked at that. That is the end of my data inspection.</p> <p>I did NOT divide my set in device or not did NOT look at what constitutes a &quot;user id&quot;(did not bother to understand that either) did not make features' selections (apart from setting interactions=true to Tingru's code or doing the equivalent for other techniques) . &nbsp;I made 17&nbsp;different models based on the given categorical variables :</p> <p>-(4) Tingru-based (thanks Tingru!) for l1 only l2 only l1 only + interactions l2 plus interactions cv at&nbsp;day 30</p> <p>-(7) sparse data with small samples and l1  l2 logits and LinearSVC &nbsp;with or without interactions ( 2 x ) &nbsp;and &nbsp;XGBoost in the same&nbsp;sparse data/ &nbsp;cv day 30</p> <p>-(3) converting categories to counts &nbsp;and running Extratrees XGboost Gradient Boosting with scikit&nbsp;&nbsp;/ cv day 30&nbsp;</p> <p>-(2) using days &lt;29 to calculate ratios of goods vs bads for each category which I applied on day 30 and 31. Then use gradient boosting of scikit or XGBoost to model these /&nbsp;internal random split 50-50 cv&nbsp;</p> <p>-(1) LibFM with 5 latent features and l2 for all features&nbsp;&nbsp;/ cv day 30</p> <p>I did all of these in a manner that I made predictions for days 30 and 31. Then I used Extra trees and XGBoost as meta-models to combine these and to get my final submission.&nbsp;</p>
3370, <p>[quote=Julian de Wit;63949]</p> <p>Hello Kazanova<br>What do you think.. Did&nbsp;any of the methods capture the historical features ?<br>Perhaps (3) ? Or do you think that some manual features would add to the score ?</p> <p>[/quote]</p> <p>Obviously the counts and the ratios of 1s&nbsp;and 0s&nbsp;allowed the trees to explore very deep interactions (in fact max_depth was higher than 20 ) historical-or-not while the linear models gave good baselines (averages) of the main categories. I think deeper relationships were captured by the meta-models too. A weighted average of my models (based on leaderboard feedback) was 0.388+&nbsp;but could get down to 0.3860 with the meta modelling with forests. Interestingly a &nbsp;logistic regression meta-model could come close to my single weighted average but nowhere as good as the meta-model with forests (which where quite deep too). It seemed to me like the most efficient logic IMHO was like &quot; hey when my probabilities of linear and non-linear models do not align look at the nonlinear models when they are quite close look at the linear ones &quot; and so on.</p> <p>I am fairly certain that having introduced a user-id idea (conceptually at least) would have added some good new information and maybe allowed my LibFM model score better ( albeit was quite poor). I guess some food for thought...<strong>I have not spent much time in this competition (in comparison to other comps) - my computer has !&nbsp;</strong>&nbsp;</p>
3370, <p>[quote=Daniel Yoo;63957]</p> <p>I've put up my factorization machine code on github.</p> <p>[/quote]</p> <p>+1 for not using regularization in the first epoch for the latent features</p>
3370, <p>[quote=laserwolf;64196]</p> <p>Am I understanding this correctly: you take the output click probabilities of a number of models and feed them to XGBoost to get a final probability?</p> <p>[/quote]</p> <p>So all my models were trained with the following logic:</p> <p>1) Choose a classifier</p> <p>2) Build a model with the selected classifier using data from 20-29 days</p> <p>3) make predictions (in probabilities) for day 30</p> <p>4) save the labels that correspond to day 30</p> <p>5) Build a model&nbsp;with the selected classifier using data from 20-30 days</p> <p>6)&nbsp; make predictions (in probabilities) for day 31</p> <p>7) once I have number of predictions for different classifiers I use as new training set my predictions for day 30 and as test set the predictions of the different models for day 31</p> <p>8) i used XGBOOST to make this meta-model (that uses as inputs the predictions of all the models)</p> <p>A weak point of this approach is that the models are combined based on predictions for day 30 which might be slightly different than day 31</p> <p>[quote=laserwolf;64196]</p> <p>I'm curious how many models and what improvement from your best single model you got.</p> <p>[/quote]</p> <p>I made 17 models</p> <p>my best single model was 0.392</p> <p>The&nbsp;weighted average of my models was around 0.388+</p> <p>The XGBoost ensemble took me down to 0.385+</p>
3370, <p>SEED=10</p> <p>param = {} <br> eta=0.04<br> max_depth=11<br> subs=1.0<br> thread=30<br> trees=600<br> param['objective'] = 'binary:logistic'<br> param['bst:eta'] = eta<br> param['bst:max_depth'] = max_depth<br> param['eval_metric'] = 'auc'<br> #param['bst:min_child_weight']=1<br> param['silent'] = 1<br> param['seed'] = SEED<br> param['nthread'] = thread<br> param['bst:subsample'] = subs</p>
3370, <p>I think with test data of 900000 rows it is very believable. IMHO the problem with getting your cv right in this one had to do with the fact that you were scoring on a new day in the future and the fact that this day (&quot;31&quot;) appeared to be quite different&nbsp;so that tiny improvements in cv were not being reflected- Still the results were consistent along that day hence the tiny shake-up</p>
3370, <p>[quote=David Thaler;63854]</p> <p>&nbsp;I don't think time was as important of a predictor here as most people think it was.</p> <p>[/quote]</p> <p>Interesting you say that. I recall removing day 30 from my training at some point and was getting much worse results on leaderboard...</p>
3370, <p><img src="http://sd.keepcalm-o-matic.co.uk/i/keep-calm-and-hate-cheaters-2.png" alt="Cheaters!" width="250" height="154"></p>
3370, <p>[quote=David Thaler;63984]</p> <p>Will&nbsp;</p> <p>It looks to me like Silogram got nailed. That surprises me. Are you sure that's right? I think he was on a team with Vladimir Nikulin.</p> <p>[/quote]</p> <p>Irrespective of whether it is true or notI am not surprised because of that:&nbsp;https://www.kaggle.com/c/yelp-recsys-2013/forums/t/5607/important-problem-of-invisible-cheating/29916#post29916 .</p>
3370, <p>[quote=rcarson;64120]</p> <p>Team merging is risky. Is it possible that kaggle starts cheater detection from the beginning of the contest so that we could get warnings from kaggle when we try to team up with someone who is suspicious to violate rules?</p> <p>[/quote]</p> <p>That is an excellent point - in fact that would promote collaboration even more. so far (and that is my current way thinking) I have avoided merging with people they have not a proven record of multiple competitions in their arsenal just because I am worried about cheating...I think it is a shame for a person-in-a-group to get penalized because another one has cheated - but difficult to track too. Maybe rc's point is to some extend the solution.&nbsp;</p>
3370, <p>[quote=inversion;64126]</p> <p>Yikes. Do team mates of cheaters get removed from Kaggle or just the contest?</p> <p>[/quote]</p> <p>I think just from the contest but the&nbsp;the stink&nbsp;remains forever...</p>
3370, <p>[quote=Bluefool;64152]</p> <p>It is scary teaming up. I remember offending Kazanova asking if he had ever cheated before we teamed up in the Africa one.</p> <p>[/quote]</p> <p>I was not offended .I felt I had to quickly destroy all the evidence (delete my extra kaggle accounts namely Triskelion Phill Culiton and Abhishek  hide plagiarized papers burn my 3 different passports do a plastic surgery etc &nbsp;)!&nbsp;</p>
3370, <p>:D</p>
3370, <p>[quote=superfan123;64227]</p> <p>ask her</p> <p>[/quote]</p> <p>I do have long hair now but I would still call it a &quot;him&quot; :) . True I am good at disposing.</p>
3370, <p>[quote=Mike Kim;64833]</p> <p>My approach was to overfit continuously until I dropped around 50 maybe 60 places. Then I would drink a beer.</p> <p>[/quote]</p> <p>Did you have the beer?</p>
3370, <p>[quote=Triskelion;64873]</p> <p>&nbsp;Luckily Marios convinced me&nbsp;</p> <p>[/quote]</p> <p>My true name is revealed! I 've been hiding it for the last 15 years!</p>
3370, <p>[quote=Wendy Kan;64915]</p> <p>Trent&nbsp;</p> <p>I agree they look suspicious but after cross referencing user logs and submission logs it's not obvious that those&nbsp;are cheaters (unlike the other ones that we removed).&nbsp;We think the GBM_2015_x groups&nbsp;might be a class of students using similar team names.&nbsp;</p> <p>[/quote]</p> <p>That looks really strange - even if they are a class and different people they have suspiciously pretty close scores - to the extend that some collaboration was unavoidable !</p>
3370, <p>[quote=Gilberto Titericz Junior;64928]</p> <p>I challenge each one of GBM_2015_x teams&nbsp;to describe their&nbsp;approach!!! What's your CV approach? What is your CV score? Preprocess? trainning method? Post-process?</p> <p>In a competition with a small dataset small number of instances in public LB small number of subjects thousand of features time dependente of variables and dissimilarity intersubject it's very hard &nbsp;to generalize well. I say that based in previous similar competitions.</p> <p>Well they are 7 teams under top 20 in their first Kaggle competition performing better than many Kaggle masters and with a very close score. &nbsp;It's at least suspicious...</p> <p>[/quote]</p>  <p>Well said. And it is not about the handful of extra kaggle points we are going to get from their removal but more like what is right&nbsp;do...</p>
3370, <p>I will be critical here not because certain action has or not been taken for a matter which I think many people here have shared good insight about but because the feedback from the admin has been inaccurate (e.g. when things will be ready) poor (e.g. no decent &nbsp;updates ) &nbsp;and out of place and time (as I see in another thread about &quot;great the competition will finalize soon&quot; it does not matter that so many people have concerns we&nbsp;will ignore them).&nbsp;</p> <p>Anyway I do not think an update would have been such a difficult thing to do even something like &quot;we are working on this right now have looked at the stuff you've posted and try to see whether there is any violation of the rules happening here&quot;.</p>
3370, <p>[quote=Giulio;65530]</p> <p>I agree on everything. I think Kaggle is currently just short staffed.</p> <p>http://www.wired.com/2015/02/data-science-darling-kaggle-cuts-one-third-staff/</p> <p>[/quote]</p> <p>Oh no no no! I would volunteer to help if I was considering skipping a competition (by not participating)  But I wont'!&nbsp;</p>
3370, <p>oh noooooo :( I just had time to looked at the dataset. What happened here??</p> <p>I hope we can restart this somehow even for less points/prizes etc</p> <p>Well done Kamil ! (you get no preset from me though).</p>
3370, <p>Hi</p> <p>I've never done image analysis / prediction before. I was wondering if anybody with some experience ( and patience!) is willing to join forces&nbsp;with me. I am really eager to try it out ! &nbsp;</p>
3370, <p>I would like to put that patience on the test :)</p>
3370, <p>[quote=Birchwood;60274]</p> <p>What is the best way to form a team?</p> <p>If you have a shot for prize is it better to develop your algorithm independently then ensemble with others with different method?</p> <p>[/quote]</p> <p>Personally I hope for people that have passion for what we do here and are fun to play with . Having said that  I specifically enjoyed my partnerships with (Triskelion and Phil C.) and Gert.</p> <p>If you go for the prize I guess the best strategy would be to solo it in the beginning and near the end try to find someone with a different approach that has done comparatively&nbsp;well and ensemble.</p> <p>If you go mostly for the knowledge and fun join up early to meet up people and share more code and ideas. I found in competitions that feature engineering and/or understanding the problem is important this approach to outperform the first one in tandem to&nbsp;leader board performance. &nbsp;</p>
3370, <p>Guys&nbsp;</p> <p>Thank you for your interest ( did not expect so much spare patience around here!) . We have already formed a team with 4 people . Lets us have each other in mind for the next competitions that seem to be coming like ants!</p>
3370, <p>This is one of the best benchmarks I have ever seen in my kaggle life (I wish I could give more than +1) ! I had no idea what I was doing before I read this! Simple- Well done :) (aka you will regret posting this :D)&nbsp;</p>
3370, <p>Who said &quot;climbing&quot; Everest was ever going to be easy!</p>  <p>I really enjoy the feedback guys (Will and Jeff) thanks :D</p>
3370, <p>[quote=Bluefool;67297]</p> <p>Come on Butler and UCLA! Make my score better</p> <p>[/quote]</p> <p>Yes! UCL Got that one!!!</p>
3370, <p>Hm I just found this:&nbsp;http://nba-stream.com/ . Just typed watch ncca online..don't know if it works</p>
3370, <p>[quote=MalNuggets;69258]</p> <p>For example how should I interpret the line below?</p> <p>00401000 56 8D 44 24 08 50 8B F1 E8 1C 1B 00 00 C7 06 08</p> <p>[/quote]</p> <p>I was taught by an alien tribe to read this and it means:</p> <p>&quot; There are 99 little bugs in my code take one down patch it around</p> <p>&nbsp; &nbsp;Now There are &nbsp;1124002124 little bugs in my code take one down patch it around!</p> <p>... Now There are 189114022934... &quot;</p> <p>Kidding aside I do not think you need to interpret this as something that has &quot;meaning&quot; but as a bag of words or something. For example does &quot;F1&quot; or &quot;8B&quot; appears in all Classes&nbsp;or mostly&nbsp;in class_1 virus?</p>
3370, <p>Try not to overfit? All scores in top 10 are quite close to each other and we are tested on only on 3 K cases. It may be that you are first in the private leaderboard and you just don't know it!</p>
3370, <p>[quote=Mikhail Trofimov;72221]</p> <p>@rcarson now you know what to do!</p> <p>@KazAnova you were right! =D</p> <p>[/quote]</p> <p>:D</p>
3370, <p>So this guy joined 2 hours ago... and is already 2nd in the leader board? Foolish move</p>
3370, <p>yeah...right :/ I don't think so!</p>
3370, <p>I challenge tgtg that joined kaggle for the first time 2 hours ago and downloaded this...data within 2 hours and had the time (with no collaboration of other players) to make a top 2 submission to reveal how he did it after the competition ends.&nbsp;</p> <p>In any case I will post the screen shot as it looks really spooky to me! Hopefully its validity will be investigated.&nbsp;</p>
3370, <p>[quote=Stergios;71679]</p> <p>Do you still bet on that one? :)</p> <p>[/quote]</p> <p>Abhishek would never do something like that :p . It is not in his mentality .&nbsp;</p>
3370, <p>[quote=Giulio;71717]</p> <p>Don't want to seem arrogant or anything especially because I'm far from the top. But if I had a model that allowed me to hone down the remainder logloss to a handful of observations I would hand label those for the sake of observing LB feedback. Would I choose one of those submissions as my final 2? Heck no. But would I use that information to improve my models if possible? Upvote this post if you want the answer (but you should know the answer :-) ).</p> <p>[/quote]</p> <p>For me that is cheating (aka I would never do that). Given the rather small size of the data set including 30-40 difficult-to-score hand-labelled observation can make some impact. Anyway I believe there will be shake-up due to this kind of over fitting- we'll see . &nbsp;</p> <p>I think there is a clear distinction in getting leaderboard feedback (even probing) and (-while having a relatively small training set-) to increase your set of known labels artificially like that.&nbsp;</p>
3370, <p>[quote=Abhishek;72140]</p> <p>with FFFFFFFF + 9000 features I think we are the ones who created most number of features ;)</p> <p>[/quote]</p> <p>I have lost count but  but in our ensemble we have a couple of models with more than 400 K features If I had to count I would say we should have generated over 1 Million features.</p> <p>Our 10-fold 50/50 cv</p> <p>Loglikelihood (fold 1/10):0.0038178<br>Loglikelihood (fold 2/10):0.004158945<br>Loglikelihood (fold 3/10):0.003210165<br>Loglikelihood (fold 4/10):0.005466825<br>Loglikelihood (fold 5/10):0.00578151<br>Loglikelihood (fold 6/10):0.003658095<br>Loglikelihood (fold 7/10):0.004555845<br>Loglikelihood (fold 8/10):0.005502735<br>Loglikelihood (fold 9/10):0.007954065<br>Loglikelihood (fold 10/10):0.00649593<br>Average M loglikelihood:0.005060475</p>
3370, <p>[quote=rcarson;72319]</p> <p>Our model is also largely based on Xgboost. Thank you very much Bing and Tianqi. :D</p> <p>[/quote]</p> <p>Ours too. Bing we love you.</p>
3370, <p>Congrats for winning again! Now we will have to replicate....</p>
3370, <p>[quote=Little Boat;72213]</p> <p>And congrats to KazAnova team and Mikhail team!&nbsp;</p> <p>[/quote]</p> <p>Thank you -the Force was with us what can I say. Well done again!</p>
3370, <p>[quote=Dmitry Ulyanov;72305]</p> <p>Congrats rcarson and KazAnova&nbsp;teams :) Eager&nbsp;for your solutions.</p> <p>[/quote]</p> <p>ours is briefly mentioned here too:</p> <p>www.kaggle.com/c/malware-classification/forums/t/13490/say-no-to-overfitting-approaches-sharing/72329#post72329</p>
3370, <p>Briefly our approach:</p> <p>Our cvs were 80-20 in training and 50-50 in meta.</p> <p>datasets:</p> <p>1) 1 gram bytes (256)</p> <p>2) 2 gram Bytes (65k)</p> <p>3) 3 gram &nbsp;Bytes (top 60 K most popular)</p> <p>4) 4 gram Bytes (top 60 K most popular)</p> <p>5) top 100K most popular per subject full-line bytes&nbsp;</p> <p>6) .zip  gzip ratios vs normal sizes along with other files's stats</p> <p>7) top 400 k most popular features in .asm (1 gram) exlduing words of length 2 and 8</p> <p>8) selection of features form .asm that made sense (based on section counts non-alphanumeric)</p> <p>all these modeled with 60% xgboost and 40% extratreesclassifier</p> <p>Generated a couple of datasets with different combos from all of these (e.g. 1grams bytes with .asm datasets or 2gram bytes with .asm datasets etc) around 20 in number.</p> <p>Then meta modelling to combine all again with xgboost and extra 50-50</p> <p>we only submitted if at least 9 out of 10 (50%-50%) folds improved our score</p> <p>our scores follow linearly the public leader board all along - We never over fitted.</p> <p>Stuff that did not work for us:</p> <p>-Linear models</p> <p>-clipping predictions (I tested internally different thresholds and all failed miserably - never bothered submitting to see what happens)</p> <p>-more than 5&gt; grams in bytes (while all other datasets are present)&nbsp;</p> <p>-2 grams or more in .asm</p> <p>I would presume you need a couple of days (if not weeks to fully reproduce our solution :/)</p> <p>One last:</p> <p>Big credit to my teammate Gert for creating the best data sets with&nbsp;exceptional feature generation all along. It was again an honor playing and exchanging ideas.</p>
3370, <p>[quote=Little Boat;72345]</p> <p>Great job! Does it require lots of computer power to reproduce what you did? :)</p> <p>[/quote]</p>  <p>It does not require huge power...However extra threads could heavily facilitate the process! Most feature engineering is online (printing in files) so it does not take much RAM. It takes time though..</p>
3370, <p>[quote=Michael George Hart;72408]</p> <p>&nbsp;I think I could have won this completion easily ....</p> <p>[/quote]</p> <p>There are many other kaggle competitions going on at the moment for you to <em>easily</em> win.</p>
3370, <p>[quote=Little Boat;72438]</p> <p>I don't know why some features work but some don't. We just kept trying and if it worked we try to understand it if it didn't work we move on to another idea.&nbsp;</p> <p>[/quote]</p> <p>I have also&nbsp;traditionally found this brute approach to be better than heavily spending time to understand why something works or not. I also&nbsp;believe this is the power of machine learning versus traditional analysis (aka we rely more on the machine algorithms) . I guess the latter is still useful when you try to sell your stuff to clients !</p>
3370, <p>Nice</p>
3370, <p>This is how all this sound to me :&nbsp;&#20013;&#22269;&#20284;&#20046;&#23545;&#25105;</p>
3370, <p>not many samples</p> <p>predefined set of features</p> <p>Classification</p> <p>All you need is a &quot;beat the benchmark code...&quot;</p> <p>Boom</p>
3370, <p>[quote=Abhishek;66762]</p> <p>and now you have the last one in the list ;)</p> <p>[/quote]</p> <p>Haha nice. My trick worked! I don't need to prepare a readcsv-and-model-and-make-predictions' code! :D&nbsp;</p>
3370, <p>[quote=Jordan Goblet;66973]</p> <p>&nbsp;Such a competition is &quot;artificial&quot; and its interest is very limited except if you&nbsp;want to benchmark different modelling techniques.</p> <p>[/quote]</p>  <p>If that is the case then everybody should have pretty much the same score which I will argue this will happen .There will be people scoring better than others(probably converging in the top positions as always - unless a really good beat-the-benchmark comes around! ). I do not think hardware will matter that much in this one since the data is quite small and almost everybody should be able to cope with it (thus making it more fair).</p> <p>The way I would put it is that there are different sort of competitions . I personally like this one because I presume prior knowledge of the field-matter is not that important (therefore we start at an equal basis) &nbsp;and it is up to the specific modeler-miner &nbsp;to extract/select/transform features in respect to the target variable  combine models validate etc to get the best&nbsp;scores.&nbsp;</p> <p>For me the difference between data mining and lets-say analysis is exactly that- the fact that you do not need to have very good prior knowledge of the subject&nbsp;to do generally well and you can let the &quot;machine&quot; find the features relationships notions of the data set that are important. &nbsp;</p>
3370, <p>[quote=Jesse Burstr&#246;m;67567]</p> <p>Could be informative to know if raw computer data power is behind the top LB or maybe something else. For us with less computer power that is...</p> <p>[/quote]</p>  <p>Extra cores do help but you can get my score with a 4GB RAM i3 (laptop) machine...in more time than I did!&nbsp;</p>
3370, <p>I will endanger a couple of of minus ones here  but I guess I can live&nbsp;with that:</p> <p>First of all I personally do not like people being beating-the-benchmark-supporters when they do it themselves and then go on and argue about it when somebody else does it. Be consistent . &quot;Rules such as I did not do it early and was not a top 10% at the time&quot; are written nowhere are not standard practice based on what has happened so far and therefore cannot be opposed&nbsp;unless there is some short of regulation or code of honor (which there is not) .</p> <p>Secondly &nbsp;I personally support any short of sharing- tune or not tuned  it helps to drive the scores upwards hence having better chance to solve the business problem. I can see the argument of directing a specific approach across everybody but my experience says  there is always more tuning to be done and/or incorporate different strategies including or modifying the current one. You can argue as much as you want about these benchmarks not helping but they have definitely helped me over the years ( such as Miros' Amazon Abhisek's stumble upon&nbsp;and Africa Tingru-many Triskelion-many that were top 10% at the time they came-some albeit late in the game). &nbsp;&nbsp;</p> <p>Third kaggle community has grown a lot&nbsp;because of sharing and because it is a place for people to learn (as well as compete and win). I guess the bar has been raised a little bit and some of you may feel they spent quite some time to be easily beaten by someone else who just copies-and-runs this benchmark but access to the tools is as important as anything really and there are still 50+ days left to build on that plus it is quite proven that there is room to get better than 0.456!&nbsp;</p>
3370, <p>[quote=Guido Tapia;68334]</p> <p>I have binaries compiled on x64 here:</p> <p>https://github.com/gatapia/py_ml_utils/tree/master/lib</p> <p>use at own risk and this is unofficial I have no relationship with xgb devs</p> <p>[/quote]</p> <p>Which I could give you more than +1. You have very collection of tools up there. I hope you don;t mind I get it :D ?</p>
3370, <p>[quote=Giulio;68753]</p> <p>My two cents: I do not understand why everybody is thinking leakage.&nbsp;</p> <p>[/quote]</p> <p>I agree. Time to look at the data. I will have to break the no-look-the-data oath I took.</p>
3370, <p>Shame! I was getting excited with that low score- made me think unconventionally. Now back to the normal stuff!&nbsp;</p>
3370, <p>S(h)ame old discussion...I am more surprised by seeing 40+posts in short time&nbsp;for something that it's been discussed over and over again rather than the 150 position drop.</p> <p>Even last minute code sharing is good and valuable (for me). I wish people had posted a 0.41+ code so that we had a chance to catch them. I&nbsp;hope they&nbsp;post it later anyways so that we can upskill ourselves. Besides a lot is about the tools.</p> <p>&nbsp;Now I am going to play Desktop Dungeons and pray that the god of overfitting has protected my team and screwed everybody else&nbsp;ahead of us (including you Phil...including you)!p.s.</p>  <p>p.s. and by the way losing 150 positions (about 4%) from the most popular competition ever by not playing&nbsp;for a week...it seems pretty&nbsp;logical drop to me - most people maximize the intensity at the end. &nbsp;</p>
3370, <p>[quote=phalaris;78908]</p> <p>How do you keep your Desktop Dungeons playing in check. I find once I start playing it or other more traditional roguelikes I end up losing a couple of weeks of my life:)</p> <p>[/quote]</p> <p>I don't know killing orcs/trolls/goblins brings me peace in a strange way! I've passed that stage. Now I just want to reach 100% (e.g. unclock all items finish each level with all classes etc). It is funny how video games are similar with what we do here...always something to reach.</p> <p>:D</p>
3370, <p>[quote=jmwoloso;78837]</p> <p>&nbsp;These competitions have binary outcomes. In the money/out of the money.</p> <p>[/quote]</p> <p>I gave -1 to that not as a personal attack but because it&nbsp;seems&nbsp;rather misleading and demotivating for some who may wish to start now. This is no royalty (where you know you 'll be king one day). You definitely cannot get it with the first try unless you keep trying (in kaggle or elsewhere)</p> <p>Actually I don't thing you can never hit the '1'  unless you treat this as a regression-continuous problem. The prizes here (as monetary remuneration)&nbsp;are almost meaningless. You can always get more though from the business world once you demonstrate a-good-and-solid&nbsp;problem-solving record. Here you can learn how to do that.&nbsp;</p>
3370, <p>Congrats for your victory Gilberto/Semenov it was a stellar performance.&nbsp;</p> <p>I have to stress though that I am a little bit concerned about some top 10 performances with very small number of submissions that also seem to share common past . 3 out of top 10 submissions seem to have come from people that attended the same Russian school?</p> <p>Anyway hope it is ok it would be interesting to see their solutions too and how they differ.</p>  <p>&nbsp;&nbsp;</p>
3370, <p>[quote=Dmitry Ulyanov;79228]</p> <p>We &nbsp;have a great ML and Kaggle-freaks community here in Moscow. The knowledge is shared in details after every competition. The high level ideas and tricks are known to everyone and I assume it is absolutely OK if no code shared. Don't be so suspicious about everything. &nbsp;</p> <p>Sorry but your posts are just frustrating for me because I know how much work these guys did.</p> <p>[/quote]</p> <p>First of all I do not doubt you have a great kaggle community and I am happy for the invitation if you really mean it. The fact that you have worked hard is a constant across most of us in top positions and it is irrespective of whether ideas may have been shared during the contest (not in public)&nbsp;. I am just posing some concerns especially because I have seen a correlated surge on the leader board from some participants in the last couple of days (with only but a few submissions) . It is up to others to check the validity - my concerns are valid. I do not care about the rank a few positions will not do me any good anyway. &nbsp;&nbsp;</p> <p>I do not see how comments like &quot;because I know how much work these guys did.&quot; are reassuring or more concerning.&nbsp;.&nbsp;</p>
3370, <p>[quote=Phil Culliton;79240]</p> <p>This sentence raised my antennae though: it is NOT absolutely OK as long as no code is shared. &nbsp;I don't talk about my solutions with my Kaggle friends during the competition unless they're on my team.</p> <p>[/quote]</p> <p>I second this. I can call up Phil and say  &quot;hey why don't you run a lasagne net put 3 hidden layers of 1024 512 256 dropouts  0.6..and so on &quot;.&nbsp;&quot; Now run xgboost and make certain you make it depth 10 and put eta=0.02&quot;...You cannot do that - you share data outside the teams  but enough said. I just raised some concerns I have nothing against the people here- they are all good scientists I have no doubt about that. Whether there has been misinterpretation of rules or me and Phil got it wrong or there just coincidences or we can call each other and discuss stuff like that I don't know but I am sure if that is the case our ranks will be much improved in the future .&nbsp;</p>
3370, <p>After every competition there is a reconciliation process where cheaters and duplicates accounts are being removed. Once this is done the leaderboard is finalized and your rank gets stabilized.&nbsp;</p>
3370, <p>Let the gamble begin!</p>
3370, <p>Don't worry everybody is over-fitting. In normal situations I would say trust your cv more than the leaderboard. In this scenario you cannot even trust that due to the small size of the training dataset. My advice to you would be to use something simple that makes sense and ignore the leaderboard.</p>
3370, <p>I would not worry too much :) This was a tiny dataset for both train and test.</p> <p>(I also speculate the test set was tiny- they just augmented it to avoid people handpick the cases.)</p> <p>My experience with this type of this-small-size type of competitions is that something really simple wins but still the element of luck is quite high. Also stats had failed me quite miserably in the past too (e.g. keeping only &quot;statistically significant&quot; variables etc) so my approach&nbsp;was to select only 5 variables that seem sensible and run a ridge model. My rank is not that good (900+) but at least did not spend more than 1 hr in this competition :D.</p>
3370, <p>We need the money to pay the IMF guys ! :D</p>
3370, <p>[quote=Bluefool;83129]</p> <p>lol http://www.independent.co.uk/news/business/news/greece-crisis-crowdfunding-campaign-crashes-indiegogo-raises-half-a-million-in-three-days-10357000.html</p> <p>UK are donating!</p> <p>[/quote]</p> <p>Come on al liitle bit more! +250 :D</p> <p>I like Tsipras. But do not want to make this into a political discussion bluefool  we can discuss through our usual means though!</p>
3370, <p>[quote=inversion;83138]</p> <p>Sorry the Greeks are suffering.</p> <p>[/quote]</p>  <p>At least we have kaggle right? :D</p>
3370, <p>[quote=barisumog;83150]</p> <p>Hope you guys see better days soon. Greetings and best wishes from your neighbors across the sea.</p> <p>=)</p> <p>[/quote]</p> <p>te&#351;ekk&#252;r ederim :)</p>
3370, <p>Fluxus I expect you to help the Greek economy! (well done)</p>
3370, <p>yup the usual suspects  linear models and xgboost.  NLP is quite straightforward. The key is in feature engineering and less about the model (where a simple linear model can do almost as well as the more  complex ones).</p>
3370, <p>Yes.</p>  <p>(As long as you manage to fit your features into your memory!)</p>  <p>Logistic regression is also more than enough for 0.98</p>
3370, <p>that depends on your input data. To my experience normalization quite often improves the scores. </p>
3370, <p>I cannot tell you which one works better  because it depends on your input data! I do not want to mislead you! If you do not have time for cv ...try the z-score one. but there is no guarantee that works better than the other...From my experience it seems to be working slightly better in general. </p>
3370, <p>May the Force be with you too my friend.</p>
3370, <p>haha thx. We are not that mad about it anymore!</p>  <p>This was a tough competition (given the reset and the big size of the data)  but a very interesting problem. </p>  <p>Well done to my teammates (Faron and Triskelion alphabetically) for their hard work. </p>  <p>Also big credit to mortehu (that also soloed this) and bibaze for their great finishes. </p>  <p>P.S. mortehu  I hope you don't hate us! Luck likes to play funny games it happened to favor us this time I hope next time to favor you :). Again well done on getting your master badge :)</p>  <p>Big thank you to the organizers for their hard work and the Dato-graphlab guys for sponsoring this. </p>
3370, <p>Our best single model was an xgboost trained on 3-grams  tf idf. it score 0.9873 by itself! I think it would make it top 10.</p>  <p>There was some pre-processing with removing stopwords and some clean-up of the files but generally we used all the content to create tf-idf.</p>  <p>We also created many other features sentiment-based count of urls dots and other non alphanumeric. I think our final solution includes around 50 models.</p>  <p>some models trained on normal input data (normally tf-idf matrices or svd componets) or meta models (e.g. models that used other model's predictions as input) . </p>  <p>I think we reached Level 4 meta-modelling in the end :)</p>
3370, <p>[quote=Artem;96192]</p>  <p>How much RAM it requires? </p>  <p>[/quote]</p>  <p>A lot!</p>  <p>128GB should be ok!</p>  <p>But to be honest we were reckless (as we had 256GB available). I cannot tell you what is the most optimum you can run it on. 64GB is possibly enough too.</p>
3370, <p>Actually  I just loaded each file as a big string (lets call it mline)  and then cleaned with this:</p>  <p>import re</p>  <p>mline=re.sub(&quot;[^a-zA-Z0-9]&quot;&quot; &quot; mline) # remove non-alphanumeric</p>  <p>then I re-Wrote this to a file. </p>  <p>The test is up to the to the tf-idf parameters!</p>  <p>tfv=TfidfVectorizer(min_df=2 max_features=None strip_accents='unicode'lowercase =True                         analyzer='word' dtype=np.float64 token_pattern=r'\w{2}' ngram_range=(1 3) use_idf=Truesmooth_idf=True      sublinear_tf=True stop_words = 'english')   </p>
3370, <p>[quote=Zach;96414]</p>  <p>How does that differ from other online algorithms e.g. vowpal wabbit?  Where can I get the software to try it out?</p>  <p>[/quote]</p>  <p>For me it is like an SGD with the difference that in each step the coefficients are being copied and converted to &quot;new values&quot; based on the regularization terms. </p>  <p>For l2 regul this should not have much difference with sgd but for l1 it makes quite a difference  because the coefficients will always remain zero (0.0) if they never pass the the C value (something not really achievable with simple sgd)  and will never be copied across . </p>  <p>In other words only if something becomes significant (after the updates's phase) will start being copied to further boost itself (hence the follow the leading features after regularization is applied!) </p>  <p>Also most of the times it uses passed gradients as a form of adaptive learning rate so generally as an algorithm is more independent. </p>  <p>At least that is my interpretation!</p>
3370, <p>haha you can change it to kazAnova. Although Mad professors (the Team name) would be more appropriate!</p>  <p>Thank you</p>
3370, <p>GERT. man that was awesome well done :). Only 19 submissions and very stable position in both public and private. </p>  <p>Well done (to the rest of the people too :D).</p>
3370, <p>First of all Let me start by congratulating my team mates Mathias (Faron) and Ning (Clobber) for their impressive work - I don't have the words to describe how awesome they are.  We put everything we had into this many sleepless nights we did not care about fever sickness or any other difficulties that we faced along the way &#8211; winning is not easy.</p>  <p>I would like to congratulate the Frenchies Team (and say thank you for not beating us!) for another great finish whose blazing breath we could feel along the way in the last week! thumbs up to New Model army for a great late run. </p>  <p>Also many congrats to Gilberto (my nemesis: P ha ha) Leustagos Stanislav for an unthinkable run it&#8217;s amazing how fast and quickly you moved up the ladder. If you had entered 3 days earlier we would have been toasted! I do have a wish to team up with you all one day. </p>  <p>Additionally I would like to congratulate the new masters especially Victor and Adam that monopolized the leader board early on and my friend Gert for solid finishes. &#8211; Inversion you did it! &#8230;And of course say thanks to my father who (came to visit me only to find me working for this competition 24/7! and) saved a piece of scrappy paper so as to keep monitoring how many submissions the other teams have made (to detect potential threats &#8211; his initiative!). </p>  <p>Last but not least I would like to thank kaggle (that exists! ) and Homesite for a great competition. <br> The last week was dramatic as we detected some over fitting issues and we had to re-invent the wheel in order to find it!  This song best describes how we felt trying to find what the issues were while everybody was reaching us! (<a href="https://www.youtube.com/watch?v=DQWI1kvmwRg">https://www.youtube.com/watch?v=DQWI1kvmwRg</a> )</p>  <p>(Luckily we found it 1 day before the end)</p>  <p>To the competition:</p>  <p>Our best single model was (Faron&#8217;s) XGBoost in the levels of 0.9694. We had to explore many interactions in the form of (x1-x2 or x1/x2) to get the xgb there. Feature engineering was utilized in many ways and Faron&#8217;s xgboost package XGBFI (<a href="https://github.com/Far0n/xgbfi">https://github.com/Far0n/xgbfi</a> ) was extremely useful in finding which interactions were useful. A lot of work here&#8230;</p>  <p>We cross-validated using strattifiedkFolder folds=5.</p>  <p>Our bets NN was around 0.967ish and best Logit around 0.965ish. </p>  <p>We built around 600+ models in various levels of:</p>  <p>1)  Xgbs Nns(lasagne and keras) Rfs Adaboosts svms  ETs Logits Libfms  vowpal (almost all of them were bagged and were both classifiers and regressions)  with</p>  <p>2)   Different input data (counts all numeric all categorical  mixed etc) </p>  <p>3)  With different feature selections (forward backwards mixed)</p>  <p>4)  And many different sub models for almost half the features (e.g. a model for each weekday each year etc)</p>  <p>Then we followed a similar meta modelling approach as with &#8216;stack net&#8217; (with 3 levels not 4) <a href="http://blog.kaggle.com/2015/12/03/dato-winners-interview-1st-place-mad-professors/">http://blog.kaggle.com/2015/12/03/dato-winners-interview-1st-place-mad-professors/</a> </p>  <p>Where at each stage we made different feature (model) selection from the huge pool of models that we had from the first round. Approximately 130 models were useful and made it in higher levels of the modelling process. It&#8217;s fair to say our approach has been very machine learning-ish where we tried to explore almost every possible combination of feature interactions feature selections model selections data-subsections etc. </p>  <p>We did have big machinery (and many cores) at our disposal &#8211; we admit it!</p>  <p>That&#8217;s it for now. We will regroup and come back. I am going to have a wine with my Father now (because I did not manage to spend much time with him)!</p>  <p>Disclaimer: No fathers have been tortured for the making of this top 1 finish!!</p>
3370, <p>[quote=Andrey Vykhodtsev;107347]</p>  <p>@&#924;&#945;&#961;&#953;&#959;&#962; &#924;&#953;&#967;&#945;&#951;&#955;&#953;&#948;&#951;&#962; KazAnova I have a specific question about sub-models if you don't mind (i.e model for specific year day of week etc.) Did you predict scores for each test record  with corresponding submodel or did you just predict for all records with all submodels and then average? </p>  <p>[/quote]</p>  <p>I don't mind :D</p>  <p>When I  made a model with 'Monday'  i predicted only 'Monday' for the test.  Tuesday for Tuesday and so on.  Does this answer your questions?</p>
3370, <p>[quote=Florian Laroumagne;107353]</p>  <p>@Faron almost same thing happened to us because of a leakage into some of our level_1 models... Great CV improve and a crappy LB score :(</p>  <p>By the way at the last stage of our ensembling model we did a rank average. Pretty neat way to mix models there :)</p>  <p>[/quote]</p>  <p>Yup! Geomean Rank average was also good :)  .</p>  <p>To give away how we found the mistakes.  We just replaced almost every model (we thought there is a chance to be leaky) with versions we were sure they were unoverfitted and kept re-running our ensemble.  We spotted a lot of problems there especially with sub models!</p>
3370, <p>[quote=Gilberto Titericz Junior;107362]</p>  <p>But special congratulations to Kazanova the new #1.  </p>  <p>we are focused in Genentech Competition and as it is a Master's Competition  it don't prize any Kaggle points.</p>  <p>[/quote]</p>  <p>Thank you you pushed me to the edge - haha!</p>  <p>Its a shame masters' competition do not give points</p>  <p>this is the reason I did not participate in them. </p>  <p>There are 6-7 active competitions that give both money and points and its tough to do them all ! </p>
3370, <p>[quote=Toby Cheese;107363]</p>  <p>My outmost respect for your knowledge stamina and the willpower to go through with this! I am scared to see that so much effort is needed nowadays to finish 1st at a kaggle contest though :-)</p>  <p>Can you share a bit on your insights about getting a reliable local validation? What did you find out about how the training/test split relates to public/private split how did you find out and how did you cope?</p>  <p>[/quote]</p>  <p>Thank you Toby</p>  <p>It does need a lot of a effort and a good team. we were lucky to have both :). You also need some luck too (e.g praying to the god of overfitting and plea to spare you sometimes help!)</p>  <p>Standard stratiffied Kfolding seemed to have worked extremely well for us. And we all had different seeds.</p>  <p>Until the very end (where we stumbled upon some overfitting issues)  the correlation between cv improvement and LB improvement was like 0.99!</p>  <p>All my validation was like :</p>  <p>number_of_folds=5 kfolder=StratifiedKFold(y n_folds=number_of_foldsshuffle=True random_state=15)</p>  <p>for train_index test_index in kfolder:</p>  <p>X_train X_cv = X[train_index] X[test_index]</p>  <p>y_train y_cv = y[train_index] y[test_index]</p>  <p>model.fit(X_trainy_train)</p>  <p>preds=model.predict_proba(X_cv) [:1]</p>
3370, <p>[quote=SM;107364]</p>  <p>@ &#924;&#945;&#961;&#953;&#959;&#962; &#924;&#953;&#967;&#945;&#951;&#955;&#953;&#948;&#951;&#962; KazAnova / Faron Could you guys please briefly share your features subsets selection strategy? </p>  <p>[/quote]</p>  <p>With feature subset  I trust you mean making models based on features values (like weekday - Monday Tuesday etc). </p>  <p>Sure</p>  <p>First we found the most useful features with xgboost (around 100).</p>  <p>Then we looked up individually each one of the features as categorical variables (e.g. we made bins feature 1 value from 0-12 1 bin 13-15 end bin etc)  where we  used optimized binning based on the target to find the optimal bins (for continues variables - for categorical we did not have to)  . This package is similar to how we did it (because ours was hard coded)</p>  <p><a href="http://www.r-bloggers.com/r-credit-scoring-woe-information-value-in-woe-package/">http://www.r-bloggers.com/r-credit-scoring-woe-information-value-in-woe-package/</a> </p>  <p>After that we subjectively grouped areas with enough population yet very different  average response areas and we built different models for these areas.</p>  <p>Does this makes sense ?- as I am really bad in explaining these things :/</p>
3370, <p>[quote=SkyLibrary;107426]</p>  <p>Congrats on being 1th on kaggle global ranking well deserved!</p>  <p>May I ask when you do stacking are you stacking probability or hard labels?</p>  <p>[/quote]</p>  <p>Thank you skylibrary .</p>  <p>I always use probability - it tells you more than just a hard label . However (not in this competition ) I have found situations that 0/1 predictions (labels) have added in my stacking  especially if they came from SVMs (like LInearSVC) . </p>  <p>In both situations (binary and multi class) I use probabilities. </p>
3370, <p>[quote=Justfor;107432]</p>  <p>Congratulations &#922;&#945;&#950;&#945;&#957;&#972;&#946;&#945;  Faron  and Clobber for number one in this competition and also to Kaggle#1. </p>  <p>[/quote]</p>  <p>Haha you found the Greek version of my nickname! Thank you!</p>  <p>I hope you get your Master status soon too because your profile seems very strong and consistently-good :)</p>
3370, <p>For those that may have missed it Faron has uploaded our best XGboost (with 4wise interractions)for this competition (top 15) here : <a href="https://github.com/Far0n/kaggle-homesite">https://github.com/Far0n/kaggle-homesite</a> </p>
3370, <p>[quote=Novice;107557]</p>  <p>I am just wondering why deep neural network results are not good comparing with XGBoost  As I know deep neural networks do not need a lot of features engineering and they suppose to perform better</p>  <p>[/quote]</p>  <p>My personal experience is that neural networks are more sensitive to the form of  the input data (and stuff like scalling  or log+1 transformations are often necessary). I have found Feature selection to be more important in nns rather than xgboost.  Xgboost is the magic pot where you throw in stones and gives you gold!</p>  <p>Another thing to consider is the metric logloss (that nns often optimize) may not be the right choice for AUC type of competitions. </p>  <p>I have seen nns to do much better without fs in competition where the metric is logloss (like Otto).</p>  <p>Anyway this is just my experience with that and may be wrong to generalize. </p>
3370, <p>[quote=clobber;107544]</p>  <p>Later I got a team merge invitation from Marios and then happily joined him and Mathias.  (I realized that I missed a team up invitation from Marios earlier and fortunately I did not miss the second one:))</p>  <p>[/quote]</p>  <p>I can get quite itchy when people don't pick up my calls ha ha! Kept calling until he responds!</p>
3370, <p>[quote=SkyLibrary;107621]</p>  <p>@&#924;&#945;&#961;&#953;&#959;&#962; &#924;&#953;&#967;&#945;&#951;&#955;&#953;&#948;&#951;&#962; KazAnova another question about your submodel. Since there are lots of category features you could potentially build submodel for all of them so how you figure out which features you sub-models to build upon is there any research paper/books behinds this or choose ones that give good validation scores?</p>  <p>[/quote]</p>  <p>We picked only the ones that were important for xgboost. If I remember well we did feature selection with xgboost and we were able to discard more than half of the features. Out of this shortlisted features we looked at each one individually and we decided which ones are useful (by means of having categories with significant frequency that yield much different averages of the target value) . I am not sure if there is relevant paper but we tried to group the distinct values of a single feature to low probability medium probability and high probability and make different models for the 3 of them.  I am sure it added value but could have been luck too!</p>
3370, <p>Many Congrats to the winners!</p>  <p>We had quite some over fitting impact - I think it may be due to  <a href="https://www.kaggle.com/realtwo/prudential-life-insurance-assessment/xgb-test/run/152209">that 0.67459  script</a>.</p>  <p>I made 5 models (XG ET RF Ridge NN) using the same input data as this script treating it as a regression problem. They did not score more than .6727.</p>  <p>5 more models (same selection and same params) and same data  but this time treating it as classification problem.</p>  <p>last  day I made 2 more models (nn and ridge)  using the same input data as <a href="https://www.kaggle.com/mariopasquato/prudential-life-insurance-assessment/linear-model">this 0.65 ridge LB script</a>. </p>  <p>I ensembled using ridge (e.g. regression)- it was doing much better than other models. </p>  <p>I used this cuttoff values to determine the class :</p>  <p>def from_probs_toclass(probs):</p>  <pre><code>classes=[] for i in range (0len(probs)):         if probs[i]&lt;=2.9:             classes.append(1)         elif  probs[i]&lt;=3.55:            classes.append(2)         elif  probs[i]&lt;=4.2:            classes.append(3)          elif  probs[i]&lt;=4.85 :            classes.append(4)         elif  probs[i]&lt;=5.6:            classes.append(5)          elif  probs[i]&lt;=6.2:            classes.append(6)         elif  probs[i]&lt;=6.9:            classes.append(7)                         else:            classes.append(8)                  return classes </code></pre>
3370, <p>[quote=Scirpus;108229]</p>  <p>did you ensemble using all data or a blind portion of the data?</p>  <p>[/quote]</p>  <p>I did k-folder and made predictions for all my training set (5 times until all is scored as validation). I use these predictions as meta features. Therefore I have used 100% of the training data.</p>
3370, <p>[quote=Wojciech Migda;108254]</p>  <p>Is there a quick way to retrieve private scores for all submissions made by yourself?</p>  <p>[/quote]</p>  <p>I think There is </p>  <p>I followed this thread :</p>  <p><a href="https://www.kaggle.com/c/rossmann-store-sales/forums/t/17900/private-score-for-submissions">https://www.kaggle.com/c/rossmann-store-sales/forums/t/17900/private-score-for-submissions</a></p>  <p>Or as Willie Liao states:</p>  <p>&quot; Or add your submission ID to the link below to bypass the calculations.</p>  <p><a href="https://www.kaggle.com/c/rossmann-store-sales/leaderboard?submissionId=">https://www.kaggle.com/c/rossmann-store-sales/leaderboard?submissionId=</a> &quot; so in your case it will be : </p>  <p><a href="https://www.kaggle.com/c/prudential-life-insurance-assessment/leaderboard?submissionId=">https://www.kaggle.com/c/prudential-life-insurance-assessment/leaderboard?submissionId=</a></p>  <p>You can get the submission ID after downloading your submission . The name of the file will be submissionId.zip so you can test . </p>  <p>I just tested but I am not sure I am getting public or private leaderboard scores!</p>
3370, <p>[quote=inversion;108261]</p>  <p>If you follow the steps using the following you'll be sure to get the private:</p>  <p><a href="https://www.kaggle.com/c/prudential-life-insurance-assessment/leaderboard/private?submissionId=">https://www.kaggle.com/c/prudential-life-insurance-assessment/leaderboard/private?submissionId=</a></p>  <p>[/quote]</p>  <p>Good catch !</p>
3370, <p>[quote=Nathaniel Shimoni;108310]</p>  <p>Fantastic ! so simple so elegant and apparently very robust solution!</p>  <p>Congrats  Bohdan!  very well done and surely deserved the master tier</p>  <p>[/quote]</p>  <p>I was also quite surprised by how well linear models were doing here in both normal and meta modelling . </p>  <p>A Well deserved Master status indeed :i)</p>
3370, <p>Well done on your first top 10%!</p>  <p>Normally there is a reconciliation process where they check for people that have violated the rules . From my experience it normally takes 1 or 2  days to standardise the results- but I have seen longer than that in some rare occasions  . </p>
3370, <p>[quote=William Cukierski;108463]</p>  <p>renegades</p>  <p>[/quote]</p>  <p>Love this word reminds me of mass effect!</p>  <p>The worst cheating performance ( in terms of proportion of cheaters) I had experienced was in this comp (<a href="https://www.kaggle.com/c/crowdflower-weather-twitter/leaderboard/public">https://www.kaggle.com/c/crowdflower-weather-twitter/leaderboard/public</a>)</p>  <p>where almost 30% of the competitors were removed. They were mostly from a school or something and they were cheating from each other!</p>  <p>(Still I hope they managed to somehow pass their modules in next semester!)</p>
3370, <p>In General I think that was a great competition . small/clean data  good prizes and nice participation.</p>  <p>In the spirit of improving this even further  I would suggest:</p>  <p>1) a no-rounded metric like RMSE or MAE </p>  <p>2) More test data (maybe to the size of the training data)</p>  <p>3) Allow for teams. they are more fun besides there is no guarantee that the top competitor gets employed (right?)  to be so strict with the solo mode.</p>  <p>On a slightly different note and I am not sure whether it applies to everyone or to only those that claim prizes  but this part of the rules seems a little-too harsh for me and does not seem much in line with the other competitions :</p>  <p>&quot;You are not eligible to participate in the Competition if you are (a) a resident of Brazil Italy or Quebec (b) a resident of Cuba Sudan Iran North Korea Syria or any other country that is the subject of trade restrictions or a sanctions program administered by the United States Treasury&#8217;s Office of Foreign Assets Control (&#8220;OFAC&#8221;) or (c) an individual listed on the OFAC Specially Designated Nationals and Blocked Persons list (see <a href="http://www.treasury.gov/resource-center/sanctions/SDN-List/Pages/default.aspx">http://www.treasury.gov/resource-center/sanctions/SDN-List/Pages/default.aspx</a> for additional information).&quot;</p>  <p>(So much more that some of the top competitors in kaggle come from these countries hence excluding them from prizes makes the competition less-competitive). </p>
3370, 
3370, <p>[quote=inversion;108354]</p>  <p>And unlike e.g. Homesite this contest did not require a super-team ensemble to optimize a score. It all came down to smart model selection (i.e. simple) and trusting local CV.</p>  <p>[/quote]</p>  <p>hm I believe teams could improve the top score  but not sure whether that would have added value for Prudential in this scenario. </p>  <p>[quote=inversion;108354]</p>  <p>No need to worry about team mergers jumping ahead of you.</p>  <p>[/quote]</p>  <p>Haha you definitely took your revenge in Homesite in regards to that one!</p>
3370, <p>Either share the account or ban those that copied and used it! It is so unfair!</p>  <p>I would like a proper and official response about how kaggle intends to handle this to adjust my strategy!</p>
3370, <p>[quote=DataGeek;103979]</p>  <p>I have just tweeted it. Click on my twitter link from my profile and get it.</p>  <p>[/quote]</p>  <p>Thank you !</p>
3370, <p>[quote=James Stewart;103982]</p>  <p>I'm also surprised quite frankly to see notable kagglers that I hold in high regard jump on this.</p>  <p>[/quote]</p>  <p>The madness of kaggle points!</p>  <p>I disapprove but if everyone else intends to use it (and kaggle to allow it)- I do not wanna fall behind...</p>  <p>Unfortunately It wont be the first time...</p>
3370, <p>For those that minus-one. I have not used this . I expect an official response from kaggle.  </p>
3370, <p>[quote=yilisg;103990]</p>  <p>That explains all the 12436585000.23400... so is this considered public information? I was firmly in top 10% and now got bumped.... !@%@#&amp;Y!@$@%</p>  <p>[/quote]</p>  <p>Same here...</p>
3370, <p>[quote=yilisg;103992]</p>  <p>Can we get a response from Kaggle admin please?</p>  <p>@&#924;&#945;&#961;&#953;&#959;&#962; btw you're doing very well on the homesite competition I am stuck on that one :)</p>  <p>[/quote]</p>  <p>Give credit to my team mates for that :) </p>  <p>try something called xgboost (haha!) </p>
3370, <p>[quote=Zach;103994]</p>  <p>The github repo in question is intended to be public.  It has 100 commits and 3 contributors and has been around since December 1st.  I think the repo owner is advertising an optimizer package.</p>  <p>I don't see any problem with sharing publicly in the forum as I guarantee you there are people who've been secretly using this code since early December.</p>  <p>DataGeek found it last night and I think he made the right call to share it on the forums.</p>  <p>[/quote]</p>  <p>I am generally fed up with these arguments in kaggle.</p>  <p>There have been many like that in the past where most of the times the unethical-yet justifiably legit solution gets to win - this is a the world we live in!</p>  <p>Hence I am quite blunt in my response too (e.g is this allowed yes or no?)</p>  <p>What happened here is that a  solution was copied by another person  whether he intended to share or not is in dispute .  This was found shared last moment so that people do not have much time to improve (puting the 1st-founders on top of that list)  - while hoping that the popularity of the response  (e.g. acceptance of the script in dispute) will tight kaggle in taking measures.</p>  <p>This is how I see it. </p>  <p>If kaggle intends to allow this . I will use it to take insight (not copy it bluntly)  But I do not agree with this nor I will stay behind since the kaggle points have some value (to me at least!)</p>  <p>End of story</p>  <p>i</p>
3370, <p>I have to stress that kaggle's message is not clear so far. </p>  <p>They have moderated the initial message without stating next steps.</p>  <p>In my opinion they have to either &quot;un-moderate&quot; the message (e.g.  open it up so that everybody can copy the script ) or  state any alternative measures/courses of actions.</p>
3370, <p>Given Geoffrey De Smet's answer  and the fact that kaggle has not made any statements so far I take it is legit to use this.</p>  <p>In any case I think your response (Geoffrey ) is well-written and it does seem you played by the rules and int the spirit of  sharing knowledge.</p>  <p>I guess is up to kaggle to legislate this  better in the future (if possible) .</p>
3370, <p>[quote=Charles Roberson;104041]</p>  <p>The only difference here is that it was shared in the forum in the last 11 hours of the competition. </p>  <p>[/quote]</p>  <p>...And that makes it legit. Unfortunately this is how it is. I hope this get fixed in the future.</p>
3370, <p>[quote=Charles Roberson;104044]</p>  <p>Your logic says it is ok to be illegal for 29.5 days just as long as you become legal in the last 11 hours?</p>  <p>[/quote]</p>  <p>I was talking for a person who wants/has to use this submission besides the team (like moi) . Since its made public - and-with-the-approval of the owner  it is legit to use and if many people are going to use is unfair for those who won't. </p>  <p>about the other you say...I think its tough to prove its not legit </p>  <p>It could have been an honest mistake too (e.g. They did not think that someone would look at their repository). </p>
3370, <p>[quote=DrNuke;104322]</p>  <p>Gamification is a double-edged sword. At the end of the day nobody really consider rank for data science excellence apart of top 10 positions from any given competition. [/quote]</p>  <p>I can guaranty that having a couple of top 10% or 25% kaggle performances in  your  CV does help!</p>  <p>EDIT: and if it does not help for some companies  you don't want to work for these ;)</p>
3370, <p>[quote=DrNuke;104539]</p>  <p>Hi Marios You are talented have the knowledge work hard and relentlessly here. Your results also show you would be a master even by my very strict suggestion of 5 top-10 finishes. You are the perfect role model I was thinking of while writing my post against gamification or the fuss with top 10% and top 25%.</p>  <p>[/quote]</p>  <p>Thank you for your kind words.  You give me far more credit that you should though!</p>  <p>It took many top 10%/top 25% (and less) to hit my first top 10  position  - and I did so heavily with the help from others (and it certainty did not happen overnight)  either through merging or reading what other people have done. </p>  <p>Every modelling experience counts (even if its done with Excel).</p>  <p>Having said that I would prefer to hire someone with some kaggle experience than not. It is definitely a good start that shows you are interested for data science.  You can easily detect if someone is really doing the competitions or just submitting scripts with a mini-test. </p>
3370, <p>[quote=Bluefool;116828]</p>  <p>Gender segregation is patronising - being called Women Power and then having the number 1 kaggler who is male helping out makes you look silly.</p>  <p>[/quote]</p>  <p>You just feel bad because you lost from a great team ! Next time you can join us!  haha</p>
3370, <p>[quote=Bluefool;116832]</p>  <p>says a man who helped out the little feeble women because he thought they were incapable of doing it on their own</p>  <p>[/quote]</p>  <p>That is your opinion and you are fully entitled to it. </p>
3370, <p>[quote=&#924;&#945;&#961;&#953;&#959;&#962; &#924;&#953;&#967;&#945;&#951;&#955;&#953;&#948;&#951;&#962; KazAnova;116835]</p>  <p>There was no need to call the team &quot;Women Power&quot;</p>  <p>[/quote]</p>  <p>There was no need for &quot;not to&quot;</p>
3370, <p>[quote=Bluefool;116838]</p>  <p>Well as a woman I will disagree with that</p>  <p>[/quote]</p>  <p>Our team did not mind. However  we can change it if that somehow offends you. </p>  <p>Edit: I have informed the team leader for a change in our name. </p>
3370, <p>[quote=Bluefool;116843]</p>  <p>hahahah bring it on mo-fos!! Marios (rank 1) needs to question why he went out of his way to team up with women (rank &gt; 800)! Positive discrimination does no-one any good</p>  <p>[/quote]</p>  <p>&quot;Marios&quot; does not need to question anything.  All people in my team are excellent data scientists and professionals with solid kaggle records and they have proved it once again. I never cared about ranks .  </p>
3370, <p>[quote=Bluefool;116870]</p>  <p>Well at least you got to be a Master from it heh?</p>  <p>[/quote]</p>  <p>Most of my good results are also product of teamwork . Daria should feel proud she helped us get a top 10 spot - we would not have done it without her in this one and I learnt a lot from her .  We changed the team name -if  there is anything else we can do to make you feel better @Bluefool  please let us know. I personally don't see much reason in  continuing this discussion though. </p>
3370, <p>[quote=beluga;115576]</p>  <p>Congratulations Guys. Impressive Feature Engineering and well deserved first place! This competition was a tough nut to crack. </p>  <p>[/quote]</p>  <p>Nice (late) LB surge by you too!</p>  <p>To dexter Lab:</p>  <p>Congrats . Awesome performance and great insights!</p>  <p>Interestingly we never figured out many of these things you mentioned (although we had various speculations that may have come close) and we used kind of brutal force transformations of the data to try and minimize the gap with you guys. </p>
3370, <p>Very Nice approach. Well-illustrated and well done for your great performance :)</p>
3370, <p>I think our solution may sound too plain compared to what the top 2 teams did :) .</p>  <p>We spent quite some time reading forum threads and applied many of things discussed with some success like :</p>  <p>1) rounding features to remove noise 2) finding the correct denominator of features to get them back to the initial integer state 3) converting categorical variables to likelihoods</p>  <p>They all helped  a little bit but the most important thing for us (and thanks to clobber for finding it! ) was interacting the categorical variables together  especially  v22 with all the rest categorical variables in 2way and even 3way manner. </p>  <p>Another thing that helped was rounding strongly ( 2 decimals) all the numerical features and creating interactions (as categorical &quot;numericalrounded_catgeorical&quot; with some of the categorical ones.) </p>  <p>Stans was kinda of our features'factory machine! - apart from likelihood and other target-based feature transformations on categorical data he did a lot of extra work in creating group-by type of aggregate features (like averages) of v50 and other variables (up to 3 way). Although we never understood exactly why this works stans was able to capture some very useful patterns here .</p>  <p>Our best single xgboost would have finished 4th (0.4242 in private) and was consisted of more than 7.5 K features (mostly interactions).</p>  <p>We also produced many different base level models without much Feature engineering just different input format types (like load all categorical variables as counts or as onehot encoding etc).</p>  <p>Our ensemble was consisted of 223 models. Faron did a lot of work in removing noise and discarding many of these in order to get to our bets score with a lvl2 ensemble of geomean weights between an ET   2NN and 2 Xgmodels. </p>  <p>We burnt many cores in case you might have questions about it!</p>
3370, <p>[quote=MadScientist;115676]</p>  <p>First of all congrats.  Well done!</p>  <p>[/quote]</p>  <p>Thank u!</p>  <p>[quote=MadScientist;115676]</p>  <p>Then of course your reward is to be peppered with questions! =)</p>  <p>[/quote]</p>  <p>Thank u x2!</p>  <p>[quote=MadScientist;115676]</p>  <p>I'm not sure I follow what you are saying--could you expand upon this?</p>  <p>[/quote]</p>  <p>for example max(v50)  mean(v50) min(v50) group by v22 We did that for many categorical variables.  and also group by 2 cats or even 3 cats. ps. when I say &quot;we&quot;  I mean Stans - haha.</p>  <p>[quote=MadScientist;115676]</p>  <p>.Am I understsanding that for 1 &quot;model&quot; you leave all category variables as encoded integers then for a second model you one hot encode. </p>  <p>[/quote]</p>  <p>Yes that's it . Also  trying different algorithm in the same data input is also considered a different model (at least in my slang!) </p>
3370, <p>[quote=Shubin;115688]</p>  <p>congrats! Could you elaborate more on nn structure? besides you only apply nn on the second level instead of the first?  if not what would be the strcuture look like for the first level nn?</p>  <p>[/quote]</p>  <p>Thank you :) . I dont feel I am a great nn-tunner-structure-master and there are many different structures that could work I guess. </p>  <p>I have tried nn in both 1st and end levels although in 2nd levels they were much more useful. </p>  <p>for second level I used  </p>  <pre><code>models = Sequential() models.add(Dense(100 input_dim=input_dim init='uniform' W_regularizer=l2(0.00001))) models.add(PReLU()) models.add(BatchNormalization()) models.add(Dropout(0.7)) models.add(Dense(output_dim init='uniform')) models.add(Activation('softmax')) opt = optimizers.Adagrad(lr=lrs) models.compile(loss='binary_crossentropy' optimizer=opt) </code></pre>  <p>epocs = 50</p>  <p>and for 1st level</p>  <pre><code>models = Sequential() models.add(Dense(150 input_dim=input_dim init='uniform' W_regularizer=l2(0.00001) activation='relu')) #models.add(PReLU()) models.add(BatchNormalization()) models.add(Dropout(0.8)) models.add(Dense(output_dim init='uniform')) models.add(Activation('softmax')) opt = optimizers.Adagrad(lr=lrs) models.compile(loss='binary_crossentropy' optimizer=opt) </code></pre>  <p>also epoc = 50.</p>  <p>Both in Keras.</p>  <p>Hope it helps. I think other people did much better job- as said I am not good with nns. </p>
3370, <p>I feel keen to mention that our team's name is inspired by:</p>  <p><a href="http://www.explodingkittens.com/">http://www.explodingkittens.com/</a></p>  <p>(A card game for those who like both kittens and explosions!)</p>  <p><img src="http://www.explodingkittens.com/img/defuse.png" alt="Exploding Kittens" title></p>
3370, <p>Hi Ian I was thinking to share some tips that have helped me over the past 70 + competitions to build a somehow reliable cv &#8211; undeniably 1 of the most important things in doing well in these types of  competitions . </p>  <p>1) Unless the data set you are being tested on (e.g the test set ) is in future period then <strong>most of the times use random stratified k-fold</strong> - it seems to be working well. However If the test data IS in the future (and time seems to be an important factor like in stock marker or store sales etc)  then you need to formulate your CV process to always train on past data and test on future data (e.g. do a time split).  This competition falls to the category that <strong>you have to use a time split</strong>. Maybe use season (+ previous tournaments) data and then predict the future tournaments. </p>  <p>2) <strong>Treat your CV like your test data</strong>. That means if there is something that you cannot know or do for your test data then you have to treat the validation data as if you don't know it too. Example lets say you run a neural net (that is easy overfit/underfit) and you use the validation data to know at what exact epoc/iteration to stop your net and you check the performance for the same validation data. Can you use the test data to know where to stop the nn??? NO YOU can&#8217;t because you don't know the labels for the test data therefore you don't know where to stop the training data hence it is unrealistic to use this for validation. (You could use a fixed epoc/iteration that works well on average for all your folds). From my perspective the best way to do that would be to split your training data into 3 parts when you do cv { e.g  1) to train the model  2) to validate where to stop the training and  3) finally to see the performance in a real holdout set}.</p>  <p>Generally always keep in mind that you need to treat your validation data as much as you can as your TEST data (e.g. like you don't know the labels) otherwise you may over fit. <strong>In this competition you should avoid the bias that may come from including ratings or other past data at the wrong timing</strong> when cving. E.g. some of the ratings you will receive will be up to BEFORE the tournament but for your training data you might have ratings that come out DURING or even AFTER the tournaments (since they are in the past you have access to this kind of information TODAY). You need to make your training data fair in respect to the validation data and not train on something that won&#8217;t be available in the same way for the test data.</p>  <p>3) Always test against the metric you are being tested on. If it is AUC then AUC. If it is RMSE then RMSE and so on. <strong>Here test on logloss.</strong></p>  <p>4 ) <strong>model type and size is quite important</strong> . It is more difficult to overfit with 500000 rows (hence you don&#8217;t need to be so strict with your cv -e.g. do whatever you want) versus with 10000 rows. If you have less than 500 you are kind of doomed. I have never found a reliable cv process with so little data (maybe you will :)). Likewise is more difficult to over fit with anything that involves bagging in it. Here although you have quite enough training data (if you use many  seasons and many tournaments) the test data will be small and a good proportion of the leader board will be chance too &#8211; not much you can do about it!</p>  <p>5) <strong>Sacrifice a couple of submissions</strong> to see whether you got it right. Once you formulate your CV test it! Bear in mind for anything less than 10000 rows big variation is expected (e.g. you might improve in CV but not on leader board public part). </p>  <p>6) <strong>don&#8217;t overtune your models</strong>. For example When you run logistic regression and you try to find best value for regularization and your C becomes 1.02412353563  you are definitely over-fitting! <strong>Do sizable increases/decreases of your hyper parameters.</strong> For example try C=1.2 then C=1.4 then 1.6 and so on. </p>  <p>7) Get intuition from your hyper parameters. Try to see if the change of the hyper parameters yields meaningful results. <strong>Most of the times the hyper parameters have a peak performance for a specific value</strong>. Let&#8217;s say the more you increase C in logistic regression the better the performance for your metric (logloss) becomes. However that is only until C reaches 2.0 (for example) . After that further increases in C decrease performance. If you don't encounter something like that or for some reason the best value of your hyper param has many peaks this most of the times indicates a problem for your CV. </p>  <p>8) You can always <strong>put more size in your validation</strong>. If there is too much volatility in your results increase you validation size. Bagging helps a lot too to stabilize your results (at the cost of more time). In this scenario you could try to stack many old seasons + tournaments together to form training data to predict many tournaments ahead as validation data. </p>  <p>Not sure if this is any helpful. But it has helped me on many occasions so maybe you will find something useful here :) Sorry for the typos I wrote that quickly :)  hopefully its not TOO bad!</p>
3370, <p>[quote=Bluefool;110603]</p>  <p>I attach Kazanova's picture of me in dedication of the event</p>  <p>[/quote]</p>  <p>Hahaha! I should have put you apologizing for no reason!</p>
3370, <p>[quote=MichaelWohlschlaeger;112413]</p>  <p>(Edit - I'll have to watch 2nd OT)</p>  <p>[/quote]</p>  <p>Texas A&amp;M it is. Probably the most impressive game I've watched in years. They were losing 10+ points 31 seconds before the end and they did not even have possession. but somehow they managed to win this!</p>
3370, <p>OMG YES! I Love this comp!</p>
3370, <p>[quote=Siddharth Chandrakant;107728]</p>  <p>Upvote if  you think Jeff Sonas is the best competition-admin ever!! :) [/quote]</p>  <p>Definitely one of the main reasons this competition is so great :) !</p>  <p>Edit:  William Cukierski seemed very passionate about it too! They make great combo!</p>
3370, <p>So I was thinking for fun( please don't judge my humor) to make a submission which is 1- my predictions (reverse ranking) . This should have made my LB score around 0.16+ (and I would have been first from the bottom which is a form of first place!)  however I did score 0.837. For verification I also submitted the normal preds and got the same score again.</p>  <p>Any feedback would be appreciated.</p>
3370, <p>Guys I have let you down!  I had never tried something like that and I did not know! I can still see a score below 0.5 I guess it must be with in that threshold...</p>
3370, <p>[quote=Triskelion;110340]</p>  <p>There can be only one <a href="https://www.kaggle.com/lastplacelarry">Last Place Larry</a>!</p>  <p>[/quote]</p>  <p>Does he do private lessons?</p>
3370, <p>So basically you guys/ladies are all trolls! You have tried that before!</p>
3370, <p>I also feel there is an honest mistake here. I suggest we try and resolve it internally. </p>
3370, <p>[quote=rcarson;116692]</p>  <p>The team leader can do merges without consent from all teammates. Is this right and should kaggle do something about this? To be honest I never realize this is a problem until today.</p>  <p>[/quote]</p>  <p>I agree mistakes can happen  maybe it would be good if kaggle could add a functionality where each player (in a team) has to approve a merger in order to happen?</p>
3370, <p>[quote=rcarson;116692]</p>  <p>Edit: this can not be solved internally. What if i don't want to team up with you guys? The merge is done.</p>  <p>[/quote]</p>  <p>But you have said &quot;It doesn't matter that I am gonna reply yes.&quot; </p>  <p>I understand that this was frustrating (not to w8 for your acceptance) but it seems it was an honest mistake . I personally apologise for that as I felt time was of the essence and I might have pushed things more than it should. We cannot undo the merger but we can find other means to make things even. </p>
3370, <p>[quote=Avishek;118428]</p>  <p>some members of the team have got an easy ride to top 3 and better Kaggle ranking points. </p>  <p>RIP Lucas!</p>  <p>[/quote]</p>  <p>Avishek I can assure you that was not the case in this one. In our best submission all people contributed and it was not the one with our highest public LB score. </p>
3370, <p>[quote=binga;118444]</p>  <p>are giba kazanova owen really humans? </p>  <p>[/quote]</p>  <p>@binga </p>  <p>Haha  . Thank you for your kind words. I feel the same way for the kagglers you mentioned.</p>  <p>As for me I think you give me too much credit as I have said many times most of my good results are product of team work and I have heavy machinery too :) </p>  <p>This whole situation attracted a little bit more attention than we would like to because we tried to do  things quickly and we made mistakes . We did not mean to cause trouble or discomfort to any of our co participants.</p>
3370, <p>[quote=Faron;118713]</p>  <p>We met an ambitious person who flooded our chat with the spirit of a fighter paired with an indomitable will to win this competition.</p>  <p>[/quote]</p>  <p>I wanted to share the very last message Lucas sent me 4 days before we learnt the news from his brother. He felt upset because he was not able to contribute up to this point and I told him not to feel stressed about it. You can see from his reply what kind of a man he was. Gutted and fighter to the very end.</p>  <p><img src="https://www.kaggle.com/blobs/download/forum-message-attachment-files/4179/I_want_to_win.jpg" alt="enter image description here" title></p>  <p>P.S. Leustagos had to win this one - we did what had to do towards that goal.</p>
3370, <p>Congrats to the Devil Team and specifically Stas for becoming the new kaggle number #1! Well deserved mate :)</p>  <p>Also congrats to ADAD to for a good late surge too.</p>  <p>A Little bit unfortunate about 8 + 9 = 11 that got pushed outside the prize money.</p>  <p>But most of all congrats to my team mates for a great result and big work on feature engineering and modelling . I think this will not be the first time you see them on the upper part of the leaderboard.</p>  <p>Happy to see the curse ending in #9 ;)</p>  <p>Many congrats to all!</p>
3370, <p>Hi&nbsp;</p> <p>&nbsp;</p> <p>Sorry to bother you but I accidentally made a submission with the wrong account . I am not having 2 accounts my friend is also a kaggle user and when I used her pc  it was already logged in with this name and I did not notice that... My kaggle name is kazAnova. You will notice that the score is almost identical to mine! &nbsp;I have 5-1(the wrong one)=4 submissions for today. I apologise for that.</p> <p>&nbsp;</p> <p>Regards</p>
3377, <p>Foxtrot -</p> <p>Thanks for the post. I am interested to see what comes next...</p>
3377, <p>I am curious to know if kaggle users would find it helpful to be able to search challenges by evaluation metric (AUC RMSE MAP)  and/or industry (retail healthcare advertising etc)? </p>  <p>Currently there are over 187 challenges (excluding in-class) <a href="https://www.kaggle.com/competitions">https://www.kaggle.com/competitions</a>. </p>  <p>This should prove useful as it brings more transparency to all the wonderful gems currently hidden under a simple competition name.</p>
3377, <blockquote> <p>Multiple but clearly distinct submissions per Entrant are allowed.</p> </blockquote> <p>Curios is it possible to have multiple submissions which could win differently from a single team?</p> <p>For example</p> <ol> <li>Team A - solution 1 (3rd place) </li><li>Team A - solution 2 (4th place) </li></ol> <p>or is it that a particular team may have a greater possibility of winning a <span style="text-decoration:underline"> single</span> place by having multiple submissions of distinct solutions?</p>
3377, <p>If you can post an example of what it is you are experiencing that would be helpful to the community...</p> <p>Although most likely it is due to the *.CSV file formatting. You should be able to select the column and perform &quot;Text to Columns&quot; and have it delimited based on space. This should get your data into the right columns...</p> <p>If that doesn't help post back...Kaggle has a great group of individuals ready to help where they can.</p>
3377, <p>Anybody have any updates on the Visualization challenge for the 24-hour hackathon?</p>
3377, <p>Keeler I don't think you have anything to worry about per se:</p> <p>&quot;...we are asking you in Stage 1 to predict something that you could easily look up and so if you want to look it up then you can submit a perfect prediction. There are no prizes for stage 1 because we have no way to control whether your methodology incorporates &quot;leakage&quot; of future information like this. However we are requesting that people be disciplined about this and develop methods that don't incorporate leakage since such methods would be worse than useless in Stage 2.&quot; ~Jeff Sonas</p> <p>Although honesty is always refreshing.</p>
3377, <p>To make using the scripts section more efficient for Kaggle users the below might prove useful:&nbsp;</p> <ul> <li>Search based on script keyword(s)</li> <li>have a category drop down (such as Hottest All Languages etc) for the script type (visualization prediction etc).</li> <li>also a bit quirky possibly would be to group scripts by where they were forked from - this would allow for user to compare scripts for new renditions&nbsp;</li> </ul>
3377, <p>Something else that may be interesting is to select two versions of a script under the versioning tab&nbsp;and have a way to compare the code highlighting in two colors only the differences in the code.</p> <p>I am currently looking through someone's NN script that has 45 versions. I currently look at the version tab then the log tab then look at the script and try and spot differences.</p>
3377, <p>Bingo!</p>
3384, ----
3385, ----
3386, <p>Hi Jeff </p>  <p>You should have a look at <a href="http://www.dataiku.com/dss/trynow/">Dataiku DSS</a></p>  <p>I think you could easily do what you describe:</p>  <p>Just load the data build a model (we integrate scikit learn models in a visual interface so you can directly train and compare basic algorithms)  that way you'll get the probabilities and score your products. You'll also see feature importance in that same interface.</p>  <p>Keep in touch if you have any questions.</p>  <p>Cheers</p>
3386, <p>Congrats to the winners&nbsp;<br> I wrote a blog post to give an insight of our approach too:&nbsp;<a href="http://dataiku.com/kaggle-contest-blue-book-for-bulldozers/"><br> http://dataiku.com/kaggle-contest-blue-book-for-bulldozers/<br> </a></p> <p>Hope you'll like it!&nbsp;<br> Anyway it was a good challenge and had a lot of fun :)</p>
3386, <p>Same for me.</p> <p>I tried to reconstruct the path in ordering the point but not knowing where is the first point and potentially making an error on all the path therefore is more expensive than simply put the mean for all the points.</p>
3386, 
3386, <p>We have a big trained model (~600M) to upload and we began to upload it five hours ago the website mentioned the percent to go it will be a lot of time. So we go to some rest to wait for its submission. But as we are too tired yesterday we just fell asleep when we wake up in the morning I find the upload is failed. I notice there is only one minute left for the deadline I just deleted one of the model file and tried to upload the model first but when we finish uploading it's already beyond the deadline. Is there any chances we can upload the model again?</p>
3386, <p>Hi Xavier</p> <p>I've sent the dropbox share link to you. Have you got it? Thanks.</p> <p>&nbsp;</p> <p>&nbsp;</p>
3403, <p>When I try to open the file it is asking for a password. Is there a password?</p>
3403, <p>Got it</p>
3403, <p>Unarchiver can handle 7z &#8212;&nbsp;http://wakaba.c3.cx/s/apps/unarchiver</p>
3412, ----
3428, ----
3438, ----
3439, ----
3445, <p>same question</p>
3445, <p>I think you just&nbsp; need to estimat ethe&nbsp; click though rate</p>
3445, <p>It's a good question but I don't think they will answer this question.</p>
3445, 
3445, <p>please delte this thanks</p>
3445, <p>the dropbox link seems working</p>
3445, <p>https://www.kaggle.com/c/the-icml-2013-whale-challenge-right-whale-redux/details/rules</p>
3445, <p>Saw this comment late. The code is forked from <a href="https://www.kaggle.com/zfturbo/avito-duplicate-ads-detection/python-xgboost-starter/comments">https://www.kaggle.com/zfturbo/avito-duplicate-ads-detection/python-xgboost-starter/comments</a> You comment is correct  especially if you use this script locally.</p>
3445, <p>I like the 1$ idea from Shea. <br> Maybe you could only ask 1$ to create/renew account. As people will have to give their credit card details it will prevent cheaters from creating or renewing accounts.</p> <p>Other idea: open your data and organize the 1rst &quot;fraud competition&quot;.</p>
3445, <p>I am very sad. I lost many competitions against Lucas won sometimes with him or against him. But I always learnt a lot from him. Thanks Lucas for all your sharing.</p>
3445, <p>I am not sure to have seen any rules concerning NAs (missing values) imputations in Kaggle's competitions. Can we train our algorithm on the combination of the training and test sets? or only the training set can be used?</p> <p>Thanks Xavier&nbsp;</p>
3445, <div>My big learning experience in this contest is not to trust fully the public leaderboard scores to rank models. I spent the last 16 days without any improvement in the public leaderboard while my submissions accuracy was improving against my cross validation  set (and&nbsp;the private test set!).</div> <div>I used an ensemble of 15 models including GBMs weighted GBMs Random Forest balanced Random Forest GAM weighted GAM (all with bernoulli/binomial error) SVM and bagged ensemble of SVMs.</div> <div>I haven't try to fine tune each models individually&nbsp;but looked for diversity of fits. &nbsp;</div> <div>My best score (0.89345 not in the private leaderboard as I haven't selected it in my final set) was an ensemble of 11 models which excluded the SVMs fits.</div>
3445, <p>I agree with Eu Jin on the importance of data cleaning but tend to disagree whilst fitting a GBM. </p> <p>GBM can do a lot of dirty work by itself. It accomodates missing values and outliers. It is also immune to monotone transformations.</p> <p>In this competition I chose to let GBM do the dirty work and focus on what GBM cannot do.</p> <p>I estimated the likelihood to be late more than 90 days (using a gbm) and I included the estimation as a predictor. The new predictor was by far the most important predictor and boosted the accuracy.</p> <p>My best GBM got a score of 0.86877 in the private set.</p>
3445, <p>Jason is right. It has been fun! </p> <p>Thanks to David and Peter who showed us how good we could be from the beginning to the end of the competition!</p> <p>Marcin and I took the 1rst place thanks to the average of 2 totally independent fits (an application of the Wisdom of Crowds in a small scale). </p> <p>Marcin's one was made in Poland using Weka and was based on Logit Boost mainly.<br> Mine was made in Singapore using R and was based on GAM RF GBM NN and GLMM with 3 sets of preprocessed data (one automatic 2 manual). </p> <p>I personally did a lot of blending (blend of blends!). I used GLM GAM RF GBM and NN for my blends based on 5-fold cross-validation predictions of my individual fits as predictors. </p> <p>In total I had 25 individuals models 20 blends and one blend of blends (using GLM). </p> <p>For the observations in the test set poorly represented in the training set (20% of the observations) my fits didn't use any location information. </p> <p>Congrats to everyone and Vladimir Tim and Stefan who performed very well individually !</p>
3445, <p>I agree with Tim and Zach. GBMs give the best individual performance. <br> I also agree with the poor performance of NNs noted by Raghu. But a poor fit can be informative and a good blend can take advantage of it by allocating negative weights!<br> As for the GAMs they worked well only as an offset of GLMMs (GLMMs on GAMs residuals).</p>
3445, <div>Hi Jeff</div> <div>I want to use&nbsp;http://federalgovernmentzipcodes.us/free-zipcode-database-Primary.csv (details on the data are available at&nbsp;http://federalgovernmentzipcodes.us/)</div> <div>Please advise us :</div> <div>- if it is OK to use the location names<br> - if other demographic features available in the data such as population wages can also be used.</div> <div>Thanks Xavier</div>
3445, <p>Hi Ben</p> <p>I have used the following:<br> - Academic word list<br> http://www.uefap.com/vocab/select/awl.htm<br> - active precise verbs<br> http://www.owlnet.rice.edu/~cainproj/writingtips/preciseverbs.html<br> - Transition words<br> http://www.smart-words.org/transition-words.html<br> - concatenation by Peter Norvig of several public domain books from Project Gutenberg and lists of most frequent words from Wiktionary and the British National Corpus <br> http://norvig.com/big.txt<br> - my own concatenation of books from Project Gutenberg<br> http://www.gutenberg.org/wiki/Main_Page<br> - did my own list </p> <p>Please let me know if it is ok.<br> Thanks!</p>
3445, <p>It looks like we can still submit new code in &quot;make a submission&quot;. Is it correct?</p>
3445, <p>I always look first at my own cross validation score before submitting as I try not to rely on the public leaderboard to take my modelling decision. This prevents me from overfitting it. In this case I got a good score with a single model but I sometimes  do a little more work before doing my first submission. </p>
3445, <p>Same as Peter. My cv score (only 5 folds) is in line with my score in the public leaderboard.</p>
3445, <p>Here are the key points of my solution:<br> - I converted the 12 months predictions pb into a single pb with the prediction month as a predictor.<br> - I fitted a GBM which gave me a very strong solution without any modeling effort. <br> That's what Jose called Black Magic!</p> <p>To maintain a good position in the leaderboard<br> - I improved the GBM fit with cubic splines (GAM) and incorporated other weaker individual fits in the GAM (RF...). This was harder work as I had to do some feature engineering.</p>
3445, <p>Hi Dmtry to incorporate fits from RF as a predictor in a GAM you use the RF CV-predictions (stacking).</p> <p>Note that the gain from blending was very small in this contest.</p>
3445, <p>Peter congrats on your win! I tried hard to steal you the 1rst place but your variable subsampling was too strong for me. <br> I would like to ask you 2 questions. <br> 1. have you compared the computing time of GBM in R vs sklearn?<br> 2. is the variable subsampling for GBM supported in sklearn or is it your home-brew solution? </p>
3445, <p>whilst opening train.tsv in Excel (or reading it in R) using mac I get two short answers (lines 3515 and 3516) without id essay set score1 and score2. Is it normal?</p> <p>the sentences are&nbsp;</p> <p>Pandas in China are similar to koalas in Australia by the way they both can eat only one thing. Panda eat bamboo while koalas eats eucalyptus this makes them diffrent from the pythons because pythons need a warm climate and smaller animals to live.</p> <p>Pandas in China are similar to Koalas in Australia because they both don't eat other animals they're like veggeterians. In the article it says&nbsp; China's panda which eats almost nothing but bamboo or Australia's Koala bear which eats eucalyptus leaves  almost exclusively.&quot;&quot; They both eat things that are grown from nature.&quot;</p> <p>Thanks</p>
3445, <p>My algorithms do quite well with all essay sets except for set 3. I had a look at the data and I find challenging to explain the scores given to specific essays for essay set 3.</p> <div>Please can you confirm that you are sure that the scores assigned are correct and that nobody got mixed up?&nbsp;</div> <div>Exples:&nbsp;</div> <div>&quot;Pandas in China are similar to koalas because they both can adapt to the climate change in areas rather than a python&quot; got a score of 2 by the 2 raters !!!!</div> <div>&quot;Pandas in China and koalas in Australia are similar in that their food sources (bamboo and eucalyptus leaves respectively) exists only in certain areas of the world and so those animals exist only where those food sources are. They are specialists and  are favored by stability. They are different from a python in the the python can eat a variety of food sources around the world so it can exist in many different places.&quot; got a score of 0 by the 2 raters!!&nbsp;</div> <div>Thanks!</div>
3445, <p>Thanks.<br> I have never tried the recursive filtering technique. I will definitely try it<br> First I want to make sure that the training set is correct before investing more time in this contest. <br> You are also right to complain against truncated essays. It is unfair to compare us against a human benchmark. </p>
3445, <p>I haven't seen anything wrong so far except that patients without any diagnosis (only 6 of them) have all diabetes.</p>
3445, <p>Some variables such as D<em>PX8PX88</em>.1 in set 2 have very different distributions in the training set and the testing set. </p> <blockquote> <p>table(training.E2$D<em>PX8PX88</em>.1)<br> 0 1 2 3 4 5 6 8 <br> 6586 237 1720 96 59 2 14 2 <br> table(test.E2$D<em>PX8PX88</em>.1)<br> 0 1 2 3 9 <br> 2550 106 232 18 1 <br> I guess this issue will be fixed with the new files.<br> Thanks Xavier</p> </blockquote>
3445, <p>Even after the updates I still find the data weird. There is a huge gap between my cv score and the public leaderboard score. The distributions of my predictions for the test sets look also suspect (particularly for Act 2 5 8 and 15).<br> Have you done any test yourself? My code is pretty simple with no risk of overfitting. Are you sure the test data are correct? Are the test samples generated randomly?</p>
3445, <p>I agree with dxyz that it would be nice to have the file as soon as possible or at least to have an idea when it will be supplied.</p>
3445, <p><span style="font-size:14px; line-height:1.4em">I'd prefer choice nr. 1. Thanks Xavier</span></p>
3445, <p>It seems that some data are missing for fbwindreports on the first day of Augmented_2: 3-01-2013. There are no reports for US before&nbsp;2013-01-04 02:41 !!!</p> <p>I chose not to use 3-1-2013 to train my models. But if this happens to the test set I will have a bug in my code and I will not be able to produce predictions.</p> <p>Ben can you confirm that it is a situation I have to handle (no fbwindreports available for a whole day) or it is a problem in your data that will not happen for the test set? For the PLB we had data for the 1st day (2012-11-26 09:19).</p> <p>Thanks Xavier</p>
3445, <p>Like Jacques we decided to discard our features generated from ATSCC data as we didn't find them useful at the early stage of the competition (too few events).</p> <p>Our solution is a blend of GBMs and RFs but a simpler solution would have provided us with similar accuracy: 7.987. This solution consists of a GBM to model the ERA error and a RF to model the taxi time (difference between runway and gate arrival times).  Our nb of training examples is similar to Jacques' nb.</p>
3445, <p>I can't reproduce the Public Leaderboard scores. For the basicPythonBenchmark I find 0.8170426. I used R functions mapk with k=10000. What about you?</p> <p></p>
3445, <p>I finally got it. My error was to remove duplicates. But does it make sense to keep duplicates?</p> <p></p>
3445, <p>It doesn't make sense to keep duplicated paperids. Can Kaggle modify its script and update our Public Leaderboard scores?</p>
3445, <p>I can't find the field PaperIds in test.csv!</p> <p>I read again the data description page. It says the following:</p> <p>&quot;P<span>apers that authors have &quot;confirmed&quot; (acknowledging they were the author) or deleted (meaning they were not the author) have been split into Train Validation and Test sets based on the author's Id. The Train.csv and Valid.csv sets are provided now  and the Test.csv set will be released later in the competition.</span></p> <table border="0" cellpadding="0" cellspacing="0"> <tbody> <tr> <td valign="top" width="200"> <p><strong>Name</strong></p> </td> <td valign="top" width="200"> <p><strong>Data Type</strong></p> </td> <td valign="top" width="200"> <p><strong>Comments</strong></p> </td> </tr> <tr> <td valign="top" width="200"> <p>AuthorId</p> </td> <td valign="top" width="200"> <p>&nbsp;Int</p> </td> <td valign="top" width="200"> <p>Id of the author</p> </td> </tr> <tr> <td valign="top" width="200"> <p>DeletedPaperIds&nbsp;</p> </td> <td valign="top" width="200"> <p>&nbsp;Nvarchar</p> </td> <td valign="top" width="200"> <p>Space-delimited set of deleted papers (Train only)</p> </td> </tr> <tr> <td valign="top" width="200"> <p>ConfirmedPaperIds</p> </td> <td valign="top" width="200"> <p>&nbsp;Nvarchar</p> </td> <td valign="top" width="200"> <p>Confirmed papers (Train only)</p> </td> </tr> <tr> <td valign="top" width="200"> <p>PaperIds</p> </td> <td valign="top" width="200"> <p>&nbsp;Nvarchar</p> </td> <td valign="top" width="200"> <p>PaperIds to rank from most likely to be deleted to least likely (Valid and Test only)</p> </td> </tr> </tbody> </table> <p>Have the rules changed?</p> <p></p>
3445, <p>I would also be happy to work on a new dataset.&nbsp;</p>
3445, <p>Most of the pb comes from the users' stars in tr.user. The pb is much more limited for bz_id as the information is partially censored: the average stars are rounded numbers (11.522.5...).</p> <p>I suggest you keep the same test set but discard all user_id for which the number of reviews is close to the number of records in the training set. If you choose as a the threshold a difference = 10 you will dramatically reduce the impact of reverse engineering and the test set will still have a reasonable size (around 17500 if I remember well).</p>
3445, <p>I fully agree with Lucas. It doesn't make sense to have released a 2nd test set if we let people use data derived from the Yelp website! If you allow this please let us know at the beginning of the contest and I won't enter it as I don't have any hacking skills.&nbsp;</p>
3445, <p>I agree with Lucas. At this stage of this competition I find really frustrating not to know the reasons for the very large gap between the top5 and the rest. Is it due to cheating use of external data or great models? </p> <p>Hope to see clearer soon and look forward to reading the winning solutions.</p>
3445, <p>my main model is a LibFM (thanks Steffen!)&nbsp; that did not use average stars by businesses and users but business_id and user_id.</p> <p>Features that added extra value are:</p> <p>- difference between the actual nb of reviews and a predicted value (using a gbm)</p> <p>- features measuring how much the users of a bz are changing location</p> <p>- features describing the neighborhood (&lt;0.3 miles): nb of business type of business average stars average nb of reviews...</p> <p>The rest looks quite similar to what was described by other competitors.</p> <p>&nbsp;</p>
3445, <p>I use 2 models. Each one can beat 0.5.</p>
3445, <p>Indeed you need to do a bit of feature engineering first. R with a lot of RAM - more than 16 Gb - can do the job. Sorry I can't tell you more at this stage!</p>
3445, <p>Congrats to STRAYA NCCU and adamaconguli! It was a fun but challenging competition. <br>Thanks to the competition host DonorsChoose.org for the dataset.</p> <p>Our best submission is a mixture of GBMs that took into account the time component and had a lot of historical features to describe what teachers did in the past. <br>It looks like that we should have been more aggressive in the time decay by adding additional decay. But we didn&#8217;t choose the right submission.</p> <p>One very good feature was the teachers donations history. Not what they received but what they gave. Teachers who donated to exciting projects have a higher likelihood to post exciting projects. Has anybody else observed this?</p> <p>Xavier</p>
3445, <p>[/quote]</p> <p>Hey dkay just to clarify when you say time decay do you mean some sort of feature/weights in your models or something like adjusting the output of the model (probabilities) with a decay factor? We have worked with the latter.</p> <p>[/quote]</p> <p>We tried both.</p> <p>1. We trained our GBMs on responses censored with time (0 if the project has not been funded yet at a given date and 1 if it was fully funded and exciting).</p> <p>2. Applied linear decay</p> <p>We chose to trust more the option 1 as it performed almost as well on the public leaderboard. So we&nbsp;discarded all our&nbsp;submissions with linear decays. Unfortunately!&nbsp;</p>
3445, <p>We made&nbsp;our code and the writeup to our approach available here</p> <p>http://www.datarobot.com/blog/datarobot-the-2014-kdd-cup/</p> <p>Like Yoon Peng and Black Magic we:</p> <p>- used GBMs (R and Python) <br>- improved our models thanks to history features and text mining <br>- and came up with a model strategy to account for the time bias due to the Mid May cut off in the test set.</p>
3446, <p>Perhaps the admins can release the results of the Test set as they did with the Validation set. Then at least we can compute our own scores. It is somewhat annoying that 1) the latest submission isn't selected by default and 2) team members cannot select  a submission. A point of improvement I would say.</p>
3446, <p>I retract my previous reply I see that all submissions have a public and private score. That's good enough for me.</p>
3446, <p>There are a number of conference id's in <em>Paper.csv</em> that cannot be matched to any conferences in <em>Conference.csv</em>. I suspect the same will be true for journal id's. Is this normal? It seems odd to have an id for a conference/journal for which there is no entry in the respective files. </p>
3446, 
3467, ----
3469, <p>my understanding is the store actualy sold the item is an extra information given</p>
3469, <p>I just have a look at the data it seems the selling item of the bookstore is always the number of stores&#43;-1(almost) so is part of the data generated?</p> <p></p>
3469, <p>Hi do the positions of the product mention within the text play a role in the evaluation of a submission ?</p> <p>For ex.</p> <p>If the correct entry for a test instance were</p> <pre class="x_prettyprint"><em><span class="x_lit">8c8e3756f1c2e33e2bdf17cff0c41344</span><span class="x_pun">:</span><span class="x_lit">529</span><span class="x_pun">-</span><span class="x_lit">530</span><span class="x_pun"></span><span class="x_typ">RgKfuLlgHtQ<br><br></span></em><span class="x_typ">and I were to submit</span><em><span class="x_typ"><br><br></span></em></pre> <pre class="x_prettyprint"><em><span class="x_lit">8c8e3756f1c2e33e2bdf17cff0c41344</span><span class="x_pun">:</span><span class="x_lit">525</span><span class="x_pun">-</span><span class="x_lit">527</span><span class="x_pun"></span><span class="x_typ">RgKfuLlgHtQ<br><br></span></em><span class="x_typ">would the second submission fetch the same score as the first one</span><span class="x_typ">?</span><em><span class="x_typ"><br></span></em></pre> <pre class="x_prettyprint"><em><span class="x_typ"><br></span></em></pre>
3469, <p>Hi</p> <p>We have found so far duplicates that have all the same values for a and b and choice is 0 and 1.</p> <p>Is this an error or there is an interpretation to this ?</p>
3471, ----
3477, <p>I am interested as well.</p>
3477, <p>RIP Lucas</p>
3477, <p>Hi bosie<br> Yeswe are in need of a programmer.Can you please tell me your skills?</p> <p>Vikram</p>
3477, <p>Hi Bosie<br> can you tell me your skype ID?<br> I need to talk to you.</p> <p>Vikram</p>
3477, <p>Hi Bosie<br> I cant find you on skype.Can you add me on skype?<br> My id is vikramjha89.</p> <p>vikram</p>
3477, <p>Hi DMK<br> Can you add me on skype?<br> My id is vikramjha89.</p> <p>vikram</p>
3477, <p>Dear Ian I did not use pylearn2 (only sklearn and opencv instead) but achieved better score than &quot;<strong>MLP mapping from bag of visual words to bag of words</strong>&quot; benchmark. Is it possible to share my method with you? Thanks!</p>
3477, <p>Thanks Ian!</p>
3477, <p>Dear Ian! Private test data is available but the Leaderboard and the score calculator is NOT updated.</p>
3477, <p>I thought so but...</p>
3477, <p>I also have the mentioned problem. And lost attempts to submit too...</p>
3477, <p>Dear William! Is it possible to add to data all the corresponding md5 check sums? Thanks in advance!</p>
3477, <p>Dear William! On Data descr. page one can see that there are 'samples from 385 users' but train.csv and question.csv have 387 corresponding unique values (DevId and QuizDev). Is that correct?</p>
3477, <p>[quote=Bruno Santos;37635]</p> <p>Hello everyone<br>I am facing a similar problem. What can I do to get a team together using kaggle?<br>thank you.<br>Regards<br>Bruno</p> <p>[/quote]</p> <p>Go to my team when you are competing in a competition. Click on &quot;My Team&quot;. Add a person to your team by his/her Kaggle email address.</p>
3477, <p>the multi-modal gesture recognition ended yesterday. first everyone got a rank 1 in that competition and points were awarded according to the rank(that was 1 for all). The ranking has been fixed but not the points. It seems to be some bug and I think Kaggle people are working on it. :)</p>
3477, <p>This post deserves a reply from a Kaggle Admin</p>
3477, <p>yeah. now it seems fixed :D</p>
3477, <p>same problem with me. works on my mac with chrome but not on my android&nbsp;</p>
3477, <p>are these knowledge competitions never going to end?</p>
3477, <p>please see the attached screenshot. this happens when I create a new topic in the forums. Forum name is not displayed properly</p>
3477, <p>All members are allowed to make submissions. The maximum number of submissions per day will be the maximum number of submissions for the whole team.</p>
3477, <p>IMO yes</p>
3477, <p>In the introductory e-mail text:</p> <p>To block all future contact attempts by all Kaggle users please check the &quot;Block all user contact emails&quot; preference in your user profile <strong>here</strong></p> <p>&nbsp;</p> <p><strong>'here'&nbsp;</strong>doesn't work (404 error)</p>
3477, <p>Please Read:&nbsp;https://www.kaggle.com/forums/t/8335/scoring-outage</p>
3477, <p>Master Status: Two top 10% one of which is also Top 10!</p>
3477, <p>Works in Germany!</p>
3477, <p>all work and no perks won't get many competitors ;)</p>
3477, <p>Yes. &nbsp;It helps you to find your post I think&nbsp;</p>
3477, <p>Quick solution: Build a time-machine. Go back in time and dont create a duplicate account to have more submissions per day!</p>
3477, <p>Can I redeem my earlier beating benchmark scripts for a hoodie? :D :D&nbsp;</p>
3477, <p>[quote=ACS69;56895]</p> <p>Also I don't want dates put on my profile because I'm supposed to be working!! ;)</p> <p>[/quote]</p> <p>haha.. yeah me too :D</p>
3477, <p>I started this competition&nbsp;http://inclass.kaggle.com/c/predict-movie-ratings for students of my university who are taking RecSys lecture. If you are interested in this give it a try as its open for all :)</p>
3477, <p>[quote=Vincent Firmansyah;57399]</p> <p>Much appreciated! Shall give this a shot&nbsp;</p> <p>[/quote]</p> <p>seems your shot was a big one :P</p>
3477, <p>Just one competition left to work on??? Is kaggle announcing any new competitions soon? If not I'll have to go out and play football in my free time :(</p>
3477, <p>[quote=ACS69;57767]</p> <p>[quote=Abhishek;57760]</p> <p>Just one competition left to work on??? Is kaggle announcing any new competitions soon? If not I'll have to go out and play football in my free time :(</p> <p>[/quote]</p> <p>It's awful isn't it? I'm going to have to decorate my spare room!</p> <p>[/quote]</p> <p>I should clean my one and only room :P&nbsp;</p>
3477, <p>[quote=beluga;57763]</p> <p>playing football sounds great :D</p> <p>Btw I am starting to see&nbsp;Xmas decoration&nbsp;in the shopping malls so probably a combinatorial optimization problem is also on the way...</p> <p>[/quote]</p> <p>in December I think...&nbsp;</p>
3477, <p>[quote=lnicalo;57762]</p> <p>I can guess that one of the next competitions will be http://neuro.embs.org/2015/bci-challenge/ :-)</p> <p>[/quote]</p> <p>I dont see Kaggle as a sponsor there...</p>
3477, <p>come on guys! I dont have kids :P and out of the three screens I use Ive reserved one for kaggle. So its kaggle during all breaks I take and full time in the evening :D</p>
3477, <p>[quote=Giulio;57809]</p> <p>I want a hackathon for Xmas (well not literally on XMas day...)</p> <p>[/quote]</p> <p>+2 for hackathon. but it would be nice to get an email about it 24hrs in advance. I already missed two hackathons and dont wanna miss any future ones.... :)</p>
3477, <p>&quot;NN (Neural Networks) will be rendered ineligible for a prize&quot; LOL!!</p>
3477, <p>there will be only one competition left after a few hours :( (sadmax)</p>
3477, <p>I'm so upset :(</p>
3477, <p>[quote=Trevor Stephens;57845]</p> <p>After being harassed on Twitter this morning by&nbsp;<a href="http://www.kaggle.com/users/5309/abhishek">Abhishek</a>&nbsp;to update the data...</p> <p>[/quote]</p>  <p>hahahaha.... so atleast you know now that people use it :P&nbsp;</p>
3477, <p>+1</p>
3477, <p>Check this out:&nbsp;https://www.kaggle.com/solutions/competitions</p> <p>Click on &quot;get in touch&quot; and they will let you know!</p>
3477, <p>+1</p>
3477, <p>random number generator challenge :P :P&nbsp;</p>
3477, <p>Yayyyy!!! :D :P</p>
3477, <p>[quote=inversion;69006]</p> <p>I'd like to ask the Admins to have this re-scored. &nbsp; :-)</p> <p>[/quote]</p> <p>use this cheat code:&nbsp;&#8593; &#8593; &#8595; &#8595; &#8592; &#8594; &#8592; &#8594;</p>
3477, <p>factorization machines.&nbsp;</p>
3477, <p>Awesome! Waiting for python support.... :)</p>
3477, <p>some problem with the scripts? my script is stuck waiting... tried lots of times....</p>
3477, <p>I think we need to wait. edX website says competition begins on 14th.</p>
3477, <p>[quote=Giulio;71394]</p> <p>BTW- why would a private entry competition issue standard Kaggle points and count towards achievements?</p> <p>[/quote]</p> <p>and thats why we are waiting ;)</p>
3477, <p>[quote=Giulio;71394]</p> <p>BTW- why would a private entry competition issue standard Kaggle points and count towards achievements?</p> <p>[/quote]</p> <p>Also the &quot;Masters Competitions&quot; (Im missing them) are private and award kaggle points and tiers.</p>
3477, <p>Ahh. So I lost my &quot;highest rank&quot; award (3rd)... Hmmm.. So much for nothing..</p>
3477, <p>[quote=Mike Kim;78232]</p> <p>Is it possible to keep the highest rank ever under the old system? I don't care about the current rank as it'll change.</p> <p>I just ask because if you were higher under the old system and you ever made a documented claim of being higher you might just look like a liar under this new one.</p> <p>[/quote]</p> <p>+1</p>
3477, <p>[quote=Leustagos;78338]</p> <p>Thumbs up!</p> <p>[/quote]</p> <p>Thats coz you benefited!&nbsp;</p>
3477, <p style="text-align: left">Can u please tell us?</p>
3477, <p style="text-align: left">Did anyone see rain too?</p>
3477, <p>[quote=inversion;78523]</p> <p>[quote=rcarson;78509]</p> <p>Did you see anything in the otto contest?&nbsp;</p> <p>[/quote]</p> <p>It was really quick but I could have sworn Abhishek was #11</p> <p>[/quote]</p> <p>Damn! Again? :-/</p>
3477, <p>I dont think its a good idea. It gives an advantage to lazy competitors.</p>
3477, <p>Now my scripts will never be on top except for first couple of weeks of a competition&nbsp;:(</p>
3477, <p>Sorry I missed this. However the wizard doesnt allow me to change the type. :(</p> <p>Screenshot attached.</p>
3477, <p>Is there any feature like that? Sort by votes replies views etc?</p>
3477, <p>They are always respected! Benchmarks never win ;)</p>
3477, 
3477, <p>Ohh... Maybe :D </p>
3477, <p>N I'm getting downvotes for all my posts recently.... I demand bonus votes :P</p>
3477, <p>[quote=Bluefool;86699]</p>  <p>What's good about 1337?</p>  <p>[/quote]</p>  <p>Leet</p>
3477, <p>Dunno y u use 369 when 42 is the answer to everything :D</p>
3477, <p>[quote=inversion;86709]</p>  <p>[quote=Bluefool;86706]</p>  <p>Tesla said:</p>  <p>&quot;If you only knew the magnificence of the 3 6 and 9 then you would have a key to the universe.&#8221;</p>  <p>[/quote]</p>  <p>Yeah but Tesla died with zero upvotes.</p>  <p>[/quote]</p>  <p>Lol</p>
3477, <p><img src="http://i0.wp.com/venturebeat.com/wp-content/uploads/2015/08/owen-zhang.jpg?fit=930%2C9999" alt="enter image description here" title></p>  <p><a href="http://venturebeat.com/2015/08/19/this-guy-is-the-superman-of-data-scientists/">http://venturebeat.com/2015/08/19/this-guy-is-the-superman-of-data-scientists/</a></p>  <p>:)</p>
3477, <p>I have used SVMs with great results in some of previous kaggle competitions</p>
3477, <p>DMatrix(np.array(dataframe))</p>
3477, <p>Takes 6 weeks. Sometimes more</p>
3477, <p>it seems everyone is asking these kinda questions... and i wanted to be first from Germany :D :P </p>  <p>So who is? ;)</p>
3477, <p>We are not working on Kaggle challenges at the moment (or are we?) :P</p>
3477, <p>Such a sad news :( ..... Rest in Peace Lucas.</p>
3477, <p>Oh yeah! She is gonna love it. You are gonna get some  . . . . . . .beating :P</p>
3477, <p>[quote=Ben Hamner;118518]</p>  <p>The goals of the ranking update will be to</p>  <ul> <li>adjust the competition ranking system as may be appropriate</li> <li>add a second ranking system that rewards collaboration (discussion and scripts)</li> </ul>  <p>[/quote]</p>  <p>Will the final Kaggle ranking (on the users page) be a combination of these two? </p>
3477, <p>BTW how old is the data being used in the script?</p>
3477, <p>The thread is abour revamping user profiles. Its not about adding new features. Please dont spam here..</p>
3477, <p>404 error</p>  <p>[quote=beluga;119002]</p>  <p>Maybe it would be useful to visualize your historic ranking instead of showing  the current/best only.</p>  <p><img src="https://www.kaggle.io/svf/228803/9837fdf605b9bc70eb212e564855b88e/__results___files/__results___4_0.png" alt="enter image description here" title></p>  <p>You could use <a href="https://www.kaggle.com/gaborfodor/d/kaggle/meta-kaggle/ranking-history">this</a> script to check your development.</p>  <p>[/quote]</p>
3477, <p>[quote=Bluefool;127100]</p>  <p>Anyone else Top 1% in all 3 tiers? lol I'm on a mission to rule!</p>  <p>[/quote]</p>  <p>I am.. :P</p>  <p>And my top forum post is StumbleUpon :P</p>
3477, <p>[quote=Bluefool;127154]</p>  <p>I voted Abhishek up this morning. Within 30 seconds someone must have downvoted him to get rid of my vote. Also I had 3 downvotes (and 1 friendly upvote) for my silly joke.</p>  <p>The forums have gone brutal</p>  <p>[/quote]</p>  <p>nobody wants me to become forum grandmaster :(</p>
3477, <p>@Myles why does my profile say &quot;unranked&quot; for Kernels? Have I not shared anything ? </p>
3477, <p>[quote=ololo;128191]</p>  <p>Yeah why not. It's a t-shirt after all. </p>  <p>[/quote]</p>  <p>I'd like this quote on tshirt...lol</p>
3477, <p>I wrote a post on approaching machine learning problems. I hope Kagglers will find it useful :)</p>  <p>&quot;Approaching (Almost) Any Machine Learning Problem&quot; <a href="https://lnkd.in/enkgmX7">https://lnkd.in/enkgmX7</a></p>
3477, <p>yes. thats right! I'm not using Windows :)</p>
3477, <p>has the winner published their technologies used and the code ?</p>
3477, <p>[quote=liubenyuan;28734]</p> <p>I think I won't be able to improve much above 0.75 I tried to dig more features out of the scatter data but currently I am using only one RF regressor. Could any one hint some light on improving further the results using some more **advanced** regressors ?&nbsp;</p> <p>And I am very keen to see the method that was used to reach Rank1!</p> <p>[/quote]</p> <p>&nbsp;</p> <p>did you also use the supplimentary data?</p>
3477, <p>[quote=liubenyuan;28738]</p> <p>Yes both SUP1 and SUP2 are used.&nbsp;</p> <p>I found that the CV test on SUP2 provided a good match to the final valid dataset.&nbsp;</p> <p>[/quote]</p> <p>&nbsp;</p> <p>what about SUP3? any specific reason for not using it? would you mind telling on how many features you are training your model?</p>
3477, <p>[quote=Eoin Lawless;28798]</p> <p>I fixed an asymmetry in my model and my score improved slightly. Still searching for the magic sauce that the people above 0.8 are using...</p> <p>&nbsp;</p> <p>Eoin</p> <p>[/quote]</p> <p>&nbsp;</p> <p>damn u took our position..</p>
3477, <p>[quote=Eoin Lawless;28800]</p> <p>I'm sure there'll be a lot of activity over the next few days... I don't expect to hold it for long...</p> <p>[/quote]</p> <p>&nbsp;</p> <p>yeah seems so. new teams coming up and huge increase in ranks and all...</p>
3477, <p>[quote=LucaToni;28827]</p> <p>Hi</p> <p>it is often very slow to upload the model via kaggle website.</p> <p>Do you have any alternative way?</p> <p>Thanks</p> <p>[/quote]</p> <p>&nbsp;</p> <p>works fine with me. what is the total size of your model? mine is around 12mb</p>
3477, <p>if the valid labels are released are we allowed to retrain the model on the new data?</p>
3477, <p>if you submit a model and go up the leaderboard once the test set is available they will verify it with the model you submitted. If you dont hamper the positions being held by top contenders (after the test set releases) you will keep your rank and thus the kaggle points without any prize. However according to the Kaggle rules your model can be challenged by any other competitor in that case (as this is a two stage competition).&nbsp;</p>
3477, <p>I had one submission left in the evening before the deadline but the training took unusually high amount of time. I have attached a post deadline submission and would like to ask the toppers/organizers/kagglers if this is even possible?</p>
3477, <p>this clearly seems like a bug to me</p>
3477, <p>[quote=kinnskogr;29076]</p> <p>Congratulations&nbsp;Abhishek it seems you have broken the rules of causality! ;)&nbsp;</p> <p>My experience from previous 2-stage competitions was that submissions after the public dataset was closed but before the private dataset was unveiled were not even accepted. After the private dataset was closed submissions to both the private and public datasets were once again possible to see what score they would get.</p> <p>[/quote]</p> <p>I dont think its breaking the rule. I just submitted an old file to see the leaderboard score. I think anyone who is a member of Kaggle can do that to see how would they have performed in case the competition was still open :D. I just think its a bug that didnt show me the exact place I would have got or maybe it has been done intentionally.</p>
3477, <p>thanks. noticed that a bit later...</p>
3477, <p>wasn't the key supposed to be released today?</p>
3477, <p>I dont see a point in pushing the final submission date as I dont think any model will take more than a day for processing (I may be wrong about this). Moreover it will give the teams more time to make changes to &quot;not win&quot; &amp; improve the kaggle rank.</p>
3477, <p>Shouldn't all the scores be 0.0 or 1.0(if submitted) like it used to happen in previous two-stage competitions? Otherwise its impossible to know who submitted or who didnt.</p>
3477, <p>hi</p> <p>Will the final results be announced just after the competition ends?</p>
3477, <p>ahh... okay so we cannot know how much others got... :P</p>
3477, <p>[quote=Domcastro;30083]</p> <p>Thanks Isabelle. I'm very very happy with our result :D</p> <p>[/quote]</p> <p>me too :D</p>
3477, <p>now all this made me think of something else. Will only 69 teams be counted towards the final Kaggle rankings? If yes it means to be in top 10% we should have a rank of less than 7?</p>
3477, <p>[quote=Isabelle;30094]</p> <p>The rest of the people did not submit test results and have a score of zero on the private leaderboard. I do not know the implications on the Kaggle ranking.</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Thanks Isabelle. Maybe Kaggle Admins can answer that. :)</p>
3477, <p>[quote=Jeff Moser;30559]</p> <p>The private leaderboard is now available.</p> <p>[/quote]</p> <p>So the final rankings and badges have been given by taking only 68(or 69) teams as total number of participants?&nbsp;</p>
3477, <p>[quote=Domcastro;30562]</p> <p>[quote=Abhishek;30560]</p> <p>[quote=Jeff Moser;30559]</p> <p>The private leaderboard is now available.</p> <p>[/quote]</p> <p>So the final rankings and badges have been given by taking only 68(or 69) teams as total number of participants?&nbsp;</p> <p>[/quote]</p> <p>&nbsp;</p> <p>They are all ranked at 69 - all the competitiors have been included so your rank is out of over 200</p> <p>[/quote]</p> <p>oh okay thanks. I was expecting a new badge but i cant see that in my profile.</p>
3477, <p>[quote=Domcastro;30565]</p> <p>Well done for new badge. I should get Master now but that hasn't been updated yet. What you up for?</p> <p>[/quote]</p> <p>yeah I think I should also get a master's badge but it seems to be the same for me. congrats to you too :)</p>
3477, <p>Lol :P</p>
3477, <p>[quote=Sitmo;30585]</p> <p>I have a hard time understand the &quot;master&quot; criteria. Can someone help me understand it?</p> <p>The description says &quot;2x top 10% 1x top 10&quot;.</p> <p>I ran my model on historical scores here and there seems to be a very strong causal relation between [top 10 -&gt; top 10%] (avgAUC = 0.97!). Can someone verify this with their model?</p> <p>If so what is the minimal number of competitions a master has participated in: 2 or 3?</p> <p>[/quote]</p> <p>2 top 10% one of which is in top 10</p>
3477, <p>Our features include five subsets:</p> <p>1. signal related features including the types of A types of B length #unique of A #unique of B pearson correlation and the absolute correlation value discrete entropy of A and B hsic features mutual information of A;B conditional entropy A|B and B|A normalize mutual information normalize variation information KL divergence. And further some linear/non-linear combination of these features. The initial guess was to extract some 'information' out of the pairs of A and B.</p> <p>2. IGCI features and its derivatives. Including the normalized entropy and the normalized integral and their linear/non-linear combinations. I also devided the numeric axis (A or B) into 20 bins calculate the mean and the variance of each bins and calculate their entropy. I convert the A-B plot into re-scaled (resize to 64x64 using sparse.m in MATLAB) images and also do the entropy calculation as before. This intuition was that the IGCI used to work well and also some of its derivatives.</p> <p>3. Standard solvers provided by the webmaster. Include LINGAM and GPI. The gpi code is rather slow and the improvement using this feature is not evident.</p> <p>4. Convert the A-B plot to a re-scaled image(resized to 64x64 using sparse.m in MATLAB) calculate the vertical line test on the resized image. I also do write a code to test whether the image is 'thin' on the A-axis or B-axis.</p> <p>5. calculate the Kolmogorov-Smirnov test and Chi2test.</p> <p>6. calculate the fit(xy) and compare the sum of its residules the fit algorithm are linear fit and RVM fit. I calculate the spectrum of sorted A and B and extract some features from it.</p> <p>In addition to these features we had a number of features: length types of samples different types of correlations different types of distances ANM and a number of percentile scores. All of them can be found in scipy.stats.</p> <p>The final model was an ensemble of six Random Forest Regressors and was optimized for AUC. Ensembling and optimization improved the score to a great extent approximately by 0.03 which was too high in this competition. We also tried gradient boosting but it did not improve the score in our case.</p> <p>The MATLAB part was fully done by Liu (https://www.kaggle.com/users/70859/liubenyuan) and I was responsible for feature extraction using scipy.stats sklearn metrics and the learning part</p>
3477, <p>same here. when will the submissions start.</p>
3477, <p>wasnt it 72hrs before the deadline?&nbsp;</p>
3477, <p>are submissions only allowed on the full data?</p> <p></p> <p>I get this error while submitting a model: &quot;<span>ERROR: Expected 7178 rows but only 3589 rows found&quot;</span></p>
3477, <p>is the leaderboard being updated in realtime?</p>
3477, <p>i also submitted twice using the new dataset but the leaderboard is empty for me. Previous submissions with error are understandable as they are not on the full dataset.</p>
3477, <p>hi</p> <p></p> <p>I wanted to know whether the probability values are either 0 or 1.?</p>
3477, <p>do we need to submit the model before the deadline?</p>
3477, <p>Can anyone tell me how is the private leaderboard being calculated as the model was not asked to submit? I'm confused. :D</p>
3477, <p>less than a minute</p>
3477, <p>yes im using python</p>
3477, <p>CV is cross validation. The number of times you do cross validation is called fold.</p> <p>more info: http://en.wikipedia.org/wiki/Cross-validation_(statistics)</p>
3477, <p>maybe you are overfitting the data?</p>
3477, <p>can you tell me how you made the cv_loop multiprocessing?&nbsp;</p>
3477, <p>the competition is intense :D</p>
3477, <p>hi whats B_test?</p>
3477, <p>[quote=ryank;26924]</p> <p>Here is the rough equivalent to Leustagos' code in python:</p> <p>def fopt_pred(pars data):<br> &nbsp; &nbsp; return np.dot(data pars)</p> <p>def fopt(pars):<br> &nbsp; &nbsp; fpr tpr thresholds = metrics.roc_curve(y_train fopt_pred(pars B_train))<br> &nbsp; &nbsp; return -metrics.auc(fpr tpr)</p> <p>x0 = np.ones((n_models 1)) / n_models<br> xopt = fmin(fopt x0)<br> preds = fopt_pred(xopt B_test)</p> <p>where n_models is the number of models in the blend B_train is a matrix of predictions from each model and y_train are the labels. Note that if you have scipy version &gt; 0.11 you should replace 'fmin' with 'minimize' and set the method to&nbsp;‘Nelder-Mead’.</p> <p>[/quote]</p> <p>To use it on the final test data what should B_train be taken as? the original labels of the test data?</p> <p></p>
3477, <p>pydev with eclipse. now planning to switch to sublime text2</p>
3477, <p>has anyone tried SMOTE (Synthetic minority oversampling technique) ?&nbsp;</p> <p>by using SMOTE I get a cross validation score of 0.97 but it seems that the data overfits now. I would like to know if someone else has had any experience with this algorithm..</p> <p></p> <p>The paper can be found here:&nbsp;http://arxiv.org/pdf/1106.1813v1.pdf</p>
3477, <p>@afroz I divide the dataset into 0.8 0.2. This has been done after applying SMOTE</p>
3477, <p>yeah I was planning to do that. However I wanted to know whether SMOTE is good for this scenario :)</p>
3477, <p>I tried SMOTE in the cross-validation loop as @Paul said. It doesnt improve AUC mine came around 0.6 after I applied it on the original features plus some of mine. (or maybe im doing something wrong :P)</p>
3477, <p>anybody knows how can this be done with sklearn in python?</p>
3477, <p>[quote=cacol89;27335]</p> <p>Hello guys</p> <p></p> <p>I'd like to open this thread for sharing the models we have found in scikit-learn that are compatible with sparse matrices. These are the ones I have spotted so far (no particular order):</p> <p></p> <li>linear_model.LogisticRegression() </li><li>svm.SVR() </li><li>svm.NuSVR() </li><li>linear_model.LinearRegression() </li><li>neighbors.KNeighborsRegressor() </li><li>naive_bayes.MultinomialNB() </li><li>naive_bayes.BernoulliNB() </li><li>linear_model.PassiveAggressiveRegressor() </li><li>linear_model.PassiveAggressiveClassifier() </li><li>linear_model.Perceptron() </li><li>linear_model.Ridge() </li><li>linear_model.Lasso() </li><li>linear_model.ElasticNet() <p></p> <p>[/quote]</p> <p></p> <p>did you get better results using SVR than Logistic Regression?</p> </li>
3477, <p>has anyone tried grouping data in more than 3 levels? 4/5/6 and then using greedy feature selection? if yes how were the results?</p>
3477, <p>till which level did u try? how much time did it take for greedy?</p>
3477, <p>[quote=Przemys&#322;aw Skibi&#324;ski;27750]</p> <p>My results from the leaderboard:</p> <p>1st+2nd+3rd degree features -&gt; 0.91521<br>1st+2nd+3rd+4th degree features&nbsp;-&gt; 0.91558<br>1st+2nd+3rd+4th+5th degree features -&gt; 0.91694</p> <p>&nbsp;</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Can you tell me what was the time required for greedy feature selection for 1234 and 12345? Also your system specification?</p>
3477, <p>[quote=Yiqun Hu;27817]</p> <p>Przemystaw Skibinski for 1st+2nd+3rd degree I only achineve 0.904. Can you provide some hints to get 0.915?</p> <p>[/quote]</p> <p>&nbsp;</p> <p>I was wondering about that too. Maybe he is using some extra &quot;goodies&quot; :D</p>
3477, <p>did u try the removal from first degree(original) too?</p>
3477, <p>by the way did you remove them or just used some common(new) category for them?</p>
3477, <p>[quote=Garret Vo;27776]</p> <p>Can anyone explain the MGR_ID attributes? Meanings roles etc.&nbsp;</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Isn't it too late for that now?</p>
3477, <p>hi</p> <p>&nbsp;</p> <p>Can you guys tell us your final cross validation score and the leaderboard scores please?</p>
3477, <p>cv fold: 0.9138</p> <p>leaderboard: 0.91884</p> <p>I have observed that as CV fold increases the leaderboard score becomes closer to it. Did anyone else observe that?</p>
3477, <p>Is there any way to compare new predictions with the old ones that I have already uploaded on Kaggle without uploading the new ones? (a vague comparison as only 4 uploads are left)</p>
3477, <p>[quote=sfin;27881]</p> <p>I have used very simple models. Namely I have done &quot;direct estimation&quot; of Pr(Action|Zi) where Zi is a i:th predictor variable in the data set. I have not done any categories selection thus always using all categories of a variable.</p> <p>My best ensemble is of type (below Z denotes all predictors):<br>Pr(Action|Z) = w1*Pr(Action|Role_Code) + w2*Pr(Action|Role_DEPTNAME)+ w3*Pr(Action|MGR_ID) + Pr(Action|Action|ROLE_FAMILY_DESC).</p> <p>Thus this ensemble is just approximation of joint distribution Pr(Action|Z) as linear combination of marginal distributions Pr(Action|Zi) where Zi is i:th predictor variable. Remark that I have used only 4 of predictors and not all of them.<br><br>I have tried 3 methods for calculating ensemble weights:</p> <p>1) average: w1=w2=w4=1/42) QP problem: w1+w2+w3+w4=1 wi &gt;= 0 for i=1...4<br>3) OLS estimation<br><br>Out of these the average corresponds to my best result but just of 0.83621. Thus far from leaderboard top.</p> <p>[/quote]</p> <p>&nbsp;</p> <p>you are overfitting when you are using these probabilities for action/role_code etc. and thus your score is less.</p>
3477, <p>IMO its Gradient Boosting...</p>
3477, <p>it was built on the last day. :P so keep trying :D</p>
3477, <p>I would also like to congratulate all the winners and thank their support to other participants. It would really be great to see the approach of the winners :)</p>
3477, <p>[quote=Leustagos;27949]</p> <p>I think one that also really deserves a special thanks is <strong>Miroslaw&nbsp;</strong>! His code shaped part of many winning solutions. With some modifications on it one could attain near 0.92 with a single model.</p> <p>Thank you Miroslaw!&nbsp;</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Yes <strong>Miroslaw</strong> deserves a special thanks. I learnt so much from him and his code was the base for me to achieve a good rank in this competition. I would also like to thank <strong>Nick Kridler</strong> who gave the idea of removal of infrequent data from the dataset and gave me a reason to come back and not give up in the competition when my rank was around 150 for a long time :)</p>
3477, <p>[quote=IzuiT;27970]</p> <p>Congrats to the winners!</p> <p>Miroslaw thank you for your python code! I think you should be awarded by some sort of special prize for this competition :) &nbsp;</p> <p>[/quote]</p> <p>&nbsp;</p> <p>+1</p>
3477, <p>My final CV score on last day was around 0.9124 which gave me a leaderboard score of 0.920 and 0.917 on private. There was a lot of variation between the min and max in the CV scores and I think I should have worked harder to increase the overall CV score more.</p>
3477, <p>I used some different types of models combined with Miroslaw's code.&nbsp;</p> <ul> <li>Grouped original data into 4 -&gt; greedy selection -&gt; logistic regression</li> <li>Manually added three new features -&gt; group into 4 -&gt; Greedy -&gt; infrequent feature removal -&gt; GBM (previously LR)</li> <li>Divide each feature into sub features -&gt; greedy -&gt; combine with the above features -&gt; Log Res</li> <li>Original data -&gt; remove/replace infrequent(1st 2nd and 3rd degree) [I included ACTION also and this enhanced my score] -&gt; Log Regression</li> </ul> <p>I found out on the last day that blending with GBM improved the AUC on the leaderboard. I didnt have much time to play with it though.</p>
3477, <p>https://www.kaggle.com/c/amazon-employee-access-challenge/forums/t/5283/winning-solution-code-and-methodology</p>
3477, <p>such an interesting competition comes to an end. good luck to everyone :) looking forward to know what has been used by the toppers&nbsp;</p>
3477, <p>Seems to be a great idea. I have a few questions.</p> <p>&nbsp;</p> <p>This is only for the competition winners or for everyone who participated? what will be the deadline for submission and where is it going to be published?</p>
3477, <p>[quote=Paul Duan;28590]</p> <p>I'm in.</p> <p>[/quote]</p> <p>&nbsp;</p> <p>+1</p>
3477, <p>One more question. Is there any format on how to write and what to include?&nbsp;</p>
3477, <p>any updates on this one?</p>
3477, <p>Do we have to predict for the blanks in the rec_labels_test_hidden.txt? What do the blanks represent?</p>
3477, <p>so the blanks have to be considered as negative samples. right?</p>
3477, <p>use&nbsp;</p> <p class="x_p1"><span class="x_s1">wavread from scikits audiolab</span></p>
3477, <p>[quote=Rafael;27879]</p> <p>Hi</p> <p>We have managed to use only the &quot;histogram of segments&quot; features provided with the dataset under python. Our best shot was .86337 on the public leaderboard. If someone has managed to use efficiently the multi-instance features of segments drop us a mail to see if we can form a team.</p> <p>Any idea on how to use the multi-instance under python is greatly appreciated</p> <p>[/quote]</p> <p>&nbsp;</p> <p>i have implemented MIML in python...</p>
3477, <p>the new parser says that we need to submit IDs and Probabilities. I have understood how IDs are being created. Can someone tell me what probabilities are? shouldn't this be predictions (eg 142.... )?&nbsp;</p> <p>&nbsp;</p>
3477, <p>can anyone tell me how to do cross validation on this kind of data? I have some feature vectors in the training set and it looks like:</p> <p>&nbsp;</p> <p>labels features</p> <p>2xxxx</p> <p>5yyyy</p> <p>13zzzz</p> <p>and so on...</p> <p>however the test dataset is different and im confused about both the cross validation and submission part....&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p>
3477, <p>hi thanks for the reply. I have gone through your code but I could not understand it well. Could you tell me how to use it in a cross validation loop??</p>
3477, <p>how is this different from using the different probability values that are predicted by sklearn (eg RF)? Cant you just use the probability value for the label in concern ?</p>
3477, <p>Im looking for a team member. If anyone is interestedsend me a message. I cide in python</p>
3477, <p>hi everyone</p> <p>I was wondering if some of you could tell me your CV scores and leaderboard scores. I am getting around 0.501 as my CV score and leaderboard is 0.7</p>
3477, <p>[quote=Konrad Banachewicz;28124]</p> <p>For the sake of experiment I did leave-one-out. The averaged score is now within ~ 0.03 of my leaderboard one but the error band around it is huge (~0.18).</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Can you please tell us how you are implementing the AUC calculation? I tried but all efforts went in vain :(</p>
3477, <p>[quote=Konrad Banachewicz;28197]</p> <p>Actually it seems to me the problem is not so much with calculating the AUC (just feed a 0/1 vector and a probs vector) but more about what to do when e.g. we have all 0's (a total blank row) as an argument.</p> <p>[/quote]</p> <p>I have the same question..</p>
3477, <p>Hi</p> <p>&nbsp;</p> <p>I have one more doubt with AUC. This time its about how the evaluation is being done for ranks in the leaderboard.</p> <p>Lets say I have probability values for every bird in a given audio file. Similarly I find probability values for all birds in all the files given. Now I want to know whether for scoring in the leaderboard the AUC is calculated on individual samples and then averaged or it is just the overall AUC?&nbsp;</p>
3477, <p>Also what to do for negative/blank labels? How should they be represented?</p>
3477, <p>[quote=William Cukierski;28371]</p> <p>Answered here (which I think you've seen already?)</p> <p>https://www.kaggle.com/c/mlsp-2013-birds/forums/t/4975/auc-evaluation-calculation</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Thanks I guess I overlooked your post. But still the second question remains unanswered :)</p>
3477, <p>[quote=Bojan Vujatovi&#263;;28375]</p> <p>Blank label simply means that there aren't any bird sounds in the recording. Therefore you should put 0 for all 19 species (for that specific recording) in the vector which your probability vector gets compared to.</p> <p>[/quote]</p> <p>&nbsp;</p> <p>So one hack can be to predict all the probabilities and then assign 0s to all the data (0 to 19) where there is no rectangle in the given segmentation data right? But this reduces the AUC rather than increasing it.</p>
3477, <p>is there a reference on how histogram of segment features are created from scratch? I have achieved some good noise reduction and would like to create my own features</p>
3477, <p>[quote=fb;28443]</p> <p>Nice job your noise reduction looks quite good.</p> <p>&nbsp;</p> <p>Histogram of segments is proposed in:</p> <p>&quot;Multi-Label Classifier Chains for Bird Sound&quot;</p> <p>http://arxiv.org/abs/1304.5862</p> <p>Section 5.2</p> <p>&nbsp;</p> <p>The histogram part is relatively simple. Computing the segment features is more complicated. That is described in:</p> <p>&quot;Acoustic classification of multiple simultaneous bird species: A multi-instance multi-label approach&quot;</p> <p>www.fsl.orst.edu/flel/pdfs/Briggs_2012_JASA.pdf&#8206;</p> <p>&nbsp;</p> <p>Sorry but I haven't made all of the code for this public yet. I intend to in the next few months but it is a big task to refactor/generalize/document it all so it is easy to use and I don't get flooded with tech support emails.</p> <p>[/quote]</p> <p>Thanks for the references.</p> <p>&nbsp;</p>
3477, <p>I am on the verge of finishing coding for MIML classifier in python. I have one question maybe people who have implemented it can answer. Do we need to have the same bag size of features for all samples?</p>
3477, <p>[quote=fb;28485]</p> <p>In general it is not a requirement for MIML that all bags have the same number of instances. All instances should be a feature vector with the same dimension.</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Thanks. I was able to implement MIML with k-NN and used it directly on the segmentation features. The IDs for which features were not available were left blank. The CV AUC that I achieved was around 0.50 which is much lower than what I used to get with other features and other classifiers. Is it a usual behaviour? Are the provided features not suitable for MIML?</p>
3477, <p>[quote=fb;28487]</p> <p>In &quot;Acoustic classification of multiple simultaneous bird species: a multi-instance multi-label approach&quot; MIML-kNN was applied to the same type of features (different dataset) and achieved the highest AUC compared to two other MIML classifiers (MIMLSVM and MIMLRBF). My guess is there is a bug in your code.</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Thanks for the comments fb. IMO the score is low not because of any bug (as i checked it on some other data and it works very well) but because of too many empty features in the data. It seems around 308 samples dont have any rectangular segmentation features and thus the score has to go down :)</p>
3477, <p>Thanks. will do. Right now im using MIML - kNN with different kinds of audio features. It seems slow :D</p>
3477, <p>[quote=fb;28526]</p> <p>I think that the behavior of MIML-kNN is not well-defined for bags with 0 instances. Remember it starts by looking for nearest neighbors in &quot;bag space&quot; using a distance measure between bags (i.e. Average Hausdorff distance). Such distances are not well defined (and might even result in a divide by 0) for bags with 0 instances.&nbsp;</p> <p>I suggest that you split the dataset into bags with 0 instances and bags with more than 0. Apply MIML-kNN to bags with some instances and do something different for bags with none.</p> <p>[/quote]</p> <p>With some changes in the MIML and using only the data with the segmentation data provided I was able to get a hamming loss of 0.08 which seems to be pretty good. However the cross validation AUC increased only to 0.66</p>
3477, <p>[quote=fb;28613]</p> <p>Are you generating scores in the range [01]?</p> <p>[/quote]</p> <p>&nbsp;</p> <p>if you are talking about cross validation AUC and hamming loss then yes.</p>
3477, <p>[quote=fb;28613]</p> <p>Are you generating scores in the range [01]?</p> <p>[/quote]</p> <p>&nbsp;</p> <p>The scores generated by MIML have no fixed range</p>
3477, <p>here are the implementations of MIML-kNN MIML-RBF and MIML-SVM</p> <p>&nbsp;</p> <p>http://cse.seu.edu.cn/people/zhangml/files/MIML-kNN.rar</p> <p>http://cse.seu.edu.cn/people/zhangml/files/MIMLRBF.rar</p> <p>http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/annex/MIMLBoost&amp;MIMLSVM.rar</p> <p>These implementations are in MATLAB.&nbsp;</p> <p>I have attached my translation of the MIML-kNN code from MATLAB to python.</p>
3477, <p>[quote=saraswathi;28634]</p> <p>Thanks Abhishek. I am trying to run the RBF version and my leaderboard score is around 0.77 (though the AUC on the train is around 0.93). What should I be looking at the improve the model?</p> <p>&nbsp;</p> <p>The MIML Segment Features dataset has the segments only for 154 test and 154 train rec_id's. I use the train set and predict the test set. For the remaining ones I set the probability to zero.&nbsp;</p> <p>&nbsp;</p> <p>[/quote]</p> <p>&nbsp;</p> <p>I'm also stuck with it so I dont think im the proper guy who can answer your questions.</p>
3477, <p>[quote=Domcastro;28867]</p> <p>Hi. Thanks for code. I'm not sure how to run it. What input files are you using? It looks like train10.mat but where did you get this file from?</p> <p>[/quote]</p> <p>&nbsp;</p> <p>which code are you talking about? the MATLAB one?</p>
3477, <p>the input is the training bags. So you can forget about train10. the bags can be in a list.&nbsp;</p> <p>So input = list containing 2d numpy arrays.&nbsp;</p> <p>the labels are arranged in a specific way. read the documentation of MATLAB code to know about it.</p>
3477, <p>Congratulations to all the winners.&nbsp;</p> <p>I could not do well in this competition even after trying a lot of stuff. I would really like to know from the winners and those above 0.90+ whether it was about the features or learning techniques.&nbsp;</p>
3477, <p>Are the winners going to open source their implementations?</p>
3477, <p>[quote=fb;26534]</p> <p>To review in a binary classification problem AUC is calculated as follows:</p> <p>1. The classifier outputs a score for each instance.</p> <p>2. The scores for all instances are compared to a threshold which is varied from low to high.</p> <p>3. At each different threshold value the score/threshold combination results in a specific predicted label for each instance. That label may be correct or incorrect and can also be described as a true/false positive/negative.</p> <p>3. At each different threshold the classifier achieves some level of sensitivity and specificity which are computed in terms of the number of true/false positives/negatives. Therefore each threshold gives a different point in the ROC curve (which is plotted in the axes of sensitivity vs. specificity).</p> <p>4. AUC is the area under the ROC curve which is a curve formed with one point corresponding to each different threshold.</p> <p>&nbsp;</p> <p>There are at least two variations on how AUC is computed in multi-label classification problems which are sometimes referred to micro and macro AUC (I forget which is which). Kaggle does one of the two which seems to the more commonly used one. As described by William above the way it works is that there is now a chance to make a true/false positive/negative on every class. In other words it is equivalent to computing AUC for a binary problem where there are C times as many instances where C is the number of classes. A single threshold is used across all classes.</p> <p>The other way (not used in the competition) is to compute AUC curves separately for each class then take the average of the per-class AUCs.</p> <p>[/quote]</p> <p>&nbsp;</p> <p>how is this threshold determined?</p>
3477, <p>Also till what rank?</p>
3477, <p>A paper would definitely be a great opportunity for students like me. I would definitely write one IF I achieve a good rank by the end of the competition. that's why the criteria is very important. You should also let us know how far down in rank will this go. Some people might have good feature extraction techniques for audio data but not good rank on leaderboard. :P</p>
3477, <p>do we have to submit results on validation data right now?</p>
3477, <p>does the MATLAB code work with the new data?&nbsp;</p> <p>The file it generates gives me an error while submitting</p>
3477, <p>ok. I have used the validation data as provided. and matlab code without any change but the submission parser wont accept my submission. first few lines of the submission file generated by the provided matlab code are as follows:</p> <p>&nbsp;</p> <p>IdSequence<br>0041017 <br>0041017 <br>004103 <br>0041017 <br>0041017 <br>0041020 <br>0041017 6 <br>004108 6 <br>004103 <br>0041117 8 <br>0041117 <br>0041117 8 <br>0041117</p> <p>.</p> <p>.</p> <p>.</p>
3477, <p>sent</p>
3477, <p>So Kaggle was down for a moment. and now everyone in this competition is ranked 1. I wish I had also submitted :P</p>
3477, <p>yeah my overall rank decreased by almost 40</p>
3477, <p>https://www.kaggle.com/Home/contact</p>
3477, <p>same here but not in this competition</p>
3477, <p>Can someone give a summary of the data? I'm able to view it using netcdf4 python but unable to understand it.</p> <p></p>
3477, <p>Thanks Alec. Can you tell me how train.csv is related to this big array of shape (5113115916) ?</p>
3477, <p>Did anyone achieve a &quot;good&quot; score using RandomForest?</p>
3477, <p>[quote=Herimanitra;33309]</p> <p>[quote=Abhishek;33304]</p> <p>Did anyone achieve a &quot;good&quot; score using RandomForest?</p> <p>[/quote]</p> <p>2036k</p> <p>[/quote]</p> <p>hmm... Im currently running RF regression its been on for last 12 hours lets see. btw this score was achieved after feature engineering or on the basic features?</p>
3477, <p>As many as 23 users in Kaggle with usernames MG133* and 10 in this competition. Can any MG133* explain if they are from a university or something? It looks so weird!</p>
3477, <p>i think they removed cheaters</p>
3477, <p>[quote=Sergey Yurgenson;29438]</p> <p>While we are waiting for description of October 25 event could you please answer one more question:<br>what percentage of test data is used to calculate public leaderboard? (probably it is mentioned somewhere I just cannot find it)</p> <p>[/quote]</p> <p>&nbsp;</p> <p>This is written just above the leaderboard:</p> <p>This leaderboard is calculated on approximately 30% of the test data.<br>The final results will be based on the other 70% so the final standings may be different.</p>
3477, <p>no.&nbsp;</p> <p>from the rules:</p> <p>Team Limits</p> <p>The maximum size of a team is 1 participant.</p>
3477, <p>haven't you tried Lasso in MATLAB?</p>
3477, <p>[quote=Black Magic;30638]</p> <p>funny that ML methods are not able to beat last seen value benchmark!</p> <p>[/quote]</p> <p>I was able to beat the last seen benchmark using ML methods. Though not by much.</p>
3477, <p>[quote=Miroslaw Horbal;30608]</p> <p>Does anyone have any tips on performing consistent cross validation on this competition?</p> <p>I am finding it difficult to find a method that is consistent with performance on the leaderboards. Most methods I try a decrease in my cross validation (using 10-fold cv) often times leads to a significant increase in my leaderboard score. Where as other models that perform weakly on local cross validations perform well on the leaderboard.</p> <p>[/quote]</p> <p>&nbsp;</p> <p>A 10 fold crossvalidation with a split of 90:10 gives me results which are very close to the leaderboard.</p>
3477, <p>it seems its down again :(</p>
3477, <p>translated slides of&nbsp;Alexander D'yakonov. I tried my best :P</p>
3477, <p>how can we take care of the NULL values for body/title/url ?&nbsp;</p>
3477, <p>[quote=Afroz Hussain;29283]</p> <p>yes Domcastro&nbsp;is correct.<br><br>For some understanding of ROC in relation with AUC you may check following :</p> <p><a href="https://www.kaggle.com/wiki/AreaUnderCurve">https://www.kaggle.com/wiki/AreaUnderCurve</a></p> <p>&nbsp;</p> <p>&nbsp;</p> <p>[/quote]</p> <p>&nbsp;</p> <p>I think he meant it should be called AUC and not ROC. AUC is just the area under ROC curve. A number corresponding to ROC doesn't make any sense.</p>
3477, <p>AFAIK in &quot;genfromtxt&quot; you need to specify the &quot;dtype&quot; for the text file. you cant have two different dtypes for one file. So you either read the text or the numbers at a time.</p>
3477, <p>can someone tell how are the raw files arranged? their numbering makes no sense</p>
3477, <p>thanks. I thought its serially arranged</p>
3477, <p>I have the same views as Giulio. Currently I'm using only the train.tsv. I will look into the raw data at a later stage in the competition as only train.tsv not gonna keep a good spot for me in the leaderboard :D</p>
3477, <p>whats your leaderboard score only for naive bayes ?</p>
3477, <p>the code given by BSMan works fine for me.&nbsp;</p>
3477, <p>I work on Mac OSX with python 2.7 and sklearn 0.14</p>
3477, <p>[quote=Kevin Hwang;29581]</p> <p>Yes title and/or url will improve your score.</p> <p>[/quote]</p> <p>&nbsp;</p> <p>title I can understand. How do you intend to use URL?</p>
3477, <p>are we allowed to crawl the websites provided in the database for more data?</p>
3477, <p>ohh :(</p>
3477, <p>since crawling is not allowed I assume that we are not allowed to use the page rank/alexa rank data also. Maybe the Admin can clarify on this.</p>
3477, <p>maybe we can have a competition for that :P</p>
3477, <p>IMO 0.90+ would be a very good score for this competition</p>
3477, <p>and metametametakaggle.com - contests about&nbsp;contests about contests about kaggle contests</p>
3477, <p>im not at all confident about my score. i know my score is going to drop. and to avoid that the only way is to ensemble different model ;)</p>
3477, <p>[quote=duni;32802]</p> <p>anyone wanna guess what rank the 0.87835 benchmark is gonna end up?</p> <p>[/quote]</p> <p>what's your guess? :D</p>
3477, <p>Can people write their CV Score and Leaderboard Score here? As Ive been seeing quite a lot of variation now.</p> <p>Starting with me</p> <p>CV : 0.877</p> <p>Leaderboard: 0.877</p> <p>&nbsp;</p> <p>Last Submissions</p> <p>CV: 0.885 | 0.882</p> <p>Leaderboard: 0.872 | 0.855</p>
3477, <p>my last cv score was 0.893 and leaderboard was 0.866 which was worse than my previous scores. Now I'm confused whether I should trust my CV scores..</p>
3477, <p>I dont think Im overfitting the data. Maybe the distribution is not same for the 20% test data we have and the training set...</p>
3477, <p>these are my new CV scores. Let's see how it performs after a few hours</p> <p>AUC (fold 1/10): 0.907955<br>AUC (fold 2/10): 0.901190<br>AUC (fold 3/10): 0.916381<br>AUC (fold 4/10): 0.913193<br>AUC (fold 5/10): 0.902309<br>AUC (fold 6/10): 0.900302<br>AUC (fold 7/10): 0.901089<br>AUC (fold 8/10): 0.907172<br>AUC (fold 9/10): 0.908583<br>AUC (fold 10/10): 0.902365<br>Mean : 0.906054009445</p>
3477, <p>[quote=Alec Radford;29663]</p> <p>It's a related question but doesn't anyone else experience a really weird spike where the validation performance over time slowly climbs from 0.5 to 0.65 then wildly oscillates around 0.65 to 0.8 then slowly climbs stably once you're above 0.8 again? </p> <p>[/quote]</p> <p>&nbsp;</p> <p>did you experience this during cross validation?</p>
3477, <p>[quote=Abhishek;29662]</p> <p>these are my new CV scores. Let's see how it performs after a few hours</p> <p>AUC (fold 1/10): 0.907955<br>AUC (fold 2/10): 0.901190<br>AUC (fold 3/10): 0.916381<br>AUC (fold 4/10): 0.913193<br>AUC (fold 5/10): 0.902309<br>AUC (fold 6/10): 0.900302<br>AUC (fold 7/10): 0.901089<br>AUC (fold 8/10): 0.907172<br>AUC (fold 9/10): 0.908583<br>AUC (fold 10/10): 0.902365<br>Mean : 0.906054009445</p> <p>[/quote]</p> <p>&nbsp;</p> <p>This gave me&nbsp;0.86768 on the leaderboard</p>
3477, <p>I did an extensive 200 fold cross validation on my earlier submission. The results are as follows:</p> <p>&nbsp;</p> <p>&gt;&gt; mean(auc)<br>0.908236329766<br>&gt;&gt;&gt; max(auc)<br>0.9308211761607238<br>&gt;&gt;&gt; min(auc)<br>0.8891315490791901</p> <p>The leaderboard was around 0.86 as mentioned earlier.</p>
3477, <p>Hi</p> <p>Is the test data relatively more noisy than the given training data? &nbsp;Are the number of missing values in test and training data same? Also I noticed that even for the values derived using Alchemy some categories have missing values even though those values can be generated by using Alchemy API for the same data. Is there any specific reason for this?&nbsp;</p>
3477, <p>thanks William. so it has nothing to do with Kaggle (ofcourse I knew that :D) . but still doesnt answer my second and third question.</p>
3477, <p>Thank you for your reply William. Sorry I couldn't make the question clear. &nbsp;The main problem was why the some of the feature data from test and train are missing when Alchemy can derive them( as I can see the data has been derived using alchemy api) . I wanted to know if this has been done intentionally. The first part of my previous un-understandable question has already been answered</p>
3477, <p>why not use the data that has been provided?</p>
3477, <p>I meant use the &quot;text&quot; data and Alchemy API to generate your own... :)</p>
3477, <p>fyi im not using it yet&nbsp;</p>
3477, <p>[quote=Domcastro;29632]</p> <p>I did. I haven't submitted any of the benchmarks so my score is the standard data with extra 2 columns for Alchemy. Improved my score by 0.02 which is a lot. You need to clean up the URL first (I used the Perl API so I used URI)</p> <p>[/quote]</p> <p>&nbsp;</p> <p>did you fill up the missing values or you created new features using Alchemy?</p>
3477, <p>how did you combine text features with numerical features?</p>
3477, <p>IMO the combination with sparse features will also be poop but a smaller one. :P</p>
3477, <p>[quote=Adam Daum;30100]</p> <p>Smaller poop is better I guess :-)</p> <p>So should I assume you're using something else Abhishek?&nbsp; Are you using the text only or some combination of text + other features? .. Or just the non-text features?</p> <p>&nbsp;</p> <p>[/quote]</p> <p>&nbsp;</p> <p>i would just say that I'm using the smaller poop with some of my extra goodies ;)</p>
3477, <p>I had the same problem. I switched to KFold now and the leaderboard score seems to comply with the CV score now ;)</p> <p>&nbsp;</p> <p>EDIT: It seems I was wrong. It doesnt comply</p>
3477, <p>a simple way to cross validate in python using sklearn is:</p> <p>print np.mean(cross_validation.cross_val_score(model X y cv=20 scoring='roc_auc'))</p>
3477, <p>[quote=Black Magic;30669]</p> <p>[quote=fchollet;30246]</p> <p>Using at least 10% of the training set as validation examples seems to correct the discrepancy.&nbsp;</p> <p>I'm getting essentially the same CV results with k folds and with random shuffling. At this scale I would suppose random shuffling yields test data that is representative enough of the set : )</p> <p>[/quote]</p> <p>&nbsp;</p> <p>fchollet:</p> <p>did'nt get you here. Am facing same problem as you had even with CV.</p> <p>Are you saying that if I randomly shuffle training set and then do a CV - it will help. Not sure how it would given that in k-fold cross-validation we allot the fold numbers to the rows randomly in any case....</p> <p>[/quote]</p> <p>&nbsp;</p> <p>I think its because of the noisy test data we have</p>
3477, <p>ya since last 12 hrs...&nbsp;</p>
3477, <p>Hi folks</p> <p>Ive taken part in a lot of competitions now and used the code provided by others a lot of times. Now I think its my turn to return the favors :D</p> <p>This benchmark will give you a leaderboard score of approximately <strong>0.878</strong>.&nbsp;</p> <p>It has been written in python and uses pandas sklearn and numpy.</p> <p>The basic idea is to use the boilerplate text from the training and test files do a TF-IDF transformation using TfidfVectorizer of sklearn and classify using Logistic Regression.&nbsp;</p> <p>Go nuts! (and don't forget to click &quot;thanks&quot;)</p> <p>&nbsp;</p> <p>&nbsp;</p>
3477, <p>why would you say that?</p>
3477, <p>The idea here is to learn. I learned from the people who kept posting &quot;beating the benchmark&quot; posts. So I thought I would give it a try this time. And if you see the code you will find out that nothing has been done actually there is no preprocessing or feature engineering involved. This is just to give an idea about the functions available and how to use them with the original data. If you were stuck on 0.86 with everything you tried and don't believe in competition and didnt try this basic stuff then I would say that you wrote it for yourself.</p> <p>Also from this competition's rules:</p> <p>No private sharing outside teams</p> <p>Privately sharing code or data outside of teams is not permitted. It's OK to share code or data if made available to all players such as on the forums.</p>
3477, <p>[quote=Domcastro;30549]</p> <p>lol so all the people who think &quot;beat the benchmarks&quot; are good for learning - this benchmark is based on bad practices!</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Its called semi-supervised learning and is not wrong in anyway!</p>
3477, <p>I just want to say one thing.&nbsp;</p> <p>&nbsp;</p> <p>Thanks to everyone for keeping this thread alive for such a long time</p> <p>&nbsp;</p> <p>Special thanks to Domcastro ;) :P</p>
3477, <p>[quote=eidonfiloi;31470]</p> <p>I have tried out both TruncatedSVD and SelectKBest (with different metrics) both with different number of features and I got 20 fold cross validation results over 0.90 but by submitting them got always around 0.86...</p> <p>[/quote]</p> <p>&nbsp;</p> <p>do the feature selection in a cross validation loop.&nbsp;</p>
3477, <p>[quote=Yevgeniy;31476]</p> <p>[quote=eidonfiloi;31470]</p> <p>I have tried out both TruncatedSVD and SelectKBest (with different metrics) both with different number of features and I got 20 fold cross validation results over 0.90 but by submitting them got always around 0.86...</p> <p>[/quote]</p> <p>TruncatedSVD: the result is worse than without it (both CV and leaderboard).</p> <p>SelectKBest: it tends to overfit badly and in CV feature selection loop I didn't see any improvements...&nbsp;</p> <p>[/quote]</p> <p>&nbsp;</p> <p>did you try chi2 feature selection?&nbsp;</p>
3477, <p>try a chi2 feature selection with 90 percentile features</p>
3477, <p>[quote=Jared Huling;31493]</p> <p>I tried incorporating latent dirichlet allocation to no gain</p> <p>[/quote]</p> <p>&nbsp;</p> <p>It did not work for me either.</p>
3477, <p>20 Fold CV Score:</p> <p>score: 0.861140<br>score: 0.856257<br>score: 0.871433<br>score: 0.862105<br>score: 0.881988<br>score: 0.884240<br>score: 0.859123<br>score: 0.876988<br>score: 0.904123<br>score: 0.887602<br>score: 0.890819<br>score: 0.869708<br>score: 0.878216<br>score: 0.867602<br>score: 0.879912<br>score: 0.881393<br>score: 0.857613<br>score: 0.898119<br>score: 0.875911<br>score: 0.895031<br><br>mean = 0.876966231618</p> <p>&nbsp;</p> <p>[quote=Godel;31889]</p> <p>[quote=Triskelion;31887]</p> <p>[quote=Godel;31886]</p> <p><em style="line-height: 1.4">Traceback (most recent call last):</em></p> <p><em> File &quot;&lt;stdin&gt;&quot; line 1 in &lt;module&gt;</em><br><em>TypeError: cross_val_score() got an unexpected keyword argument 'scoring'</em></p> <p><span style="line-height: 1.4">[/quote]</span></p> <p><span style="line-height: 1.4">What is your version of Scikit Learn? I think there were some updates in scoring and CV with last version that beat_bench.py relies on.</span></p> <p><code><span style="line-height: 1.4">import sklearn</span></code></p> <p><code></code><code><span style="line-height: 1.4">print sklearn.__version__</span></code></p> <p><span style="line-height: 1.4">gives:</span></p> <p><code><span style="line-height: 1.4">0.14.1</span></code></p> <p><span style="line-height: 1.4">and I was able to run beat_bench.py without any trouble.</span></p> <p><span style="line-height: 1.4">(If you are on Windows you can find easy installers for many tools at <a href="http://www.lfd.uci.edu/~gohlke/pythonlibs/#scikit-learn">http://www.lfd.uci.edu/~gohlke/pythonlibs/#scikit-learn</a> )</span></p> <p>[/quote]</p> <p>Thanks for the response.</p> <p>I am using version&nbsp;0.13.1</p> <p>What was the cross validation auc score you got on the training data using the beat_bench code?</p> <p>If it is same as what I am getting then there is no issue with sklearn per se.&nbsp;</p> <p>&nbsp;</p> <p>[/quote]</p>
3477, <p>It seems fine to me. What you can do is select a range of k starting from maybe 50 or 100 and then increase it. For every k select the features in the cv loop and then do a 10 fold cross validation. Then select the best k out of all the values for which you have tried cross validation.&nbsp;</p> <p>&nbsp;</p> <p>[quote=Kapil Dalwani;31959]</p> <p>[quote=Abhishek;31477]</p> <p>[quote=Yevgeniy;31476]</p> <p>[quote=eidonfiloi;31470]</p> <p>I have tried out both TruncatedSVD and SelectKBest (with different metrics) both with different number of features and I got 20 fold cross validation results over 0.90 but by submitting them got always around 0.86...</p> <p>[/quote]</p> <p>TruncatedSVD: the result is worse than without it (both CV and leaderboard).</p> <p>SelectKBest: it tends to overfit badly and in CV feature selection loop I didn't see any improvements...&nbsp;</p> <p>[/quote]</p> <p>&nbsp;</p> <p>did you try chi2 feature selection?&nbsp;</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Hi Abhishek wanted to check what do you mean by doing the&nbsp;SelectKBest in the cv loop.&nbsp;</p> <p>Here is the algo. of what &nbsp;I understood.&nbsp;</p> <p>&nbsp;</p> <p>[code]</p> <p><code>k = 10</code></p> <p>import&nbsp;cross_validation.train_test_split as cv_tt</p> <p><code>for i in range(K):<br>&nbsp; &nbsp; &nbsp; &nbsp;X_train X_cv y_train y_cv = cv_tt(X ytest_size=0.2)<br><span style="line-height: 1.4">&nbsp; &nbsp; &nbsp; &nbsp;ch2 = SelectKBest(chi2 k=1000)</span></code></p> <p><code>&nbsp; &nbsp; &nbsp; &nbsp;X_train = ch2.fit_transform(X_train y_train)</code></p> <p><code>&nbsp; &nbsp; &nbsp; &nbsp;X_test = ch2.transform(X_cv)<br>&nbsp; &nbsp; &nbsp; &nbsp;model.fit(X_train y_train)<br>&nbsp; &nbsp; &nbsp; &nbsp;preds = model.predict_proba(X_test)[:1]<br>&nbsp; &nbsp; &nbsp; &nbsp;auc = metrics.roc_auc_score(y_cv preds)<br>&nbsp; &nbsp; &nbsp; &nbsp;print &quot;AUC (fold %d/%d): %f&quot; % (i + 1 K auc)<br>&nbsp; &nbsp; &nbsp; &nbsp;mean_auc += auc<br> return mean_auc/K</code></p> <p>[/code]</p> <p>&nbsp;</p> <p>What I am not sure if how would I pick the best feature?&nbsp;</p> <p>[/quote]</p>
3477, <p>[quote=Kapil Dalwani;31966]</p> <p>In a way I do two loop first one for k and other one for cv.</p> <p>best_score = 0</p> <p>best_k = 0</p> <p>for j in k:</p> <p>&nbsp; &nbsp; for i in cv:</p> <p>&nbsp; &nbsp; &nbsp; &nbsp;score+=pred_score</p> <p>&nbsp; &nbsp;best_score = score</p> <p>&nbsp; &nbsp;best_k = j</p> <p>for the best score I chose the best k and use that to make prediction to my test data.</p> <p>If that seems right then my best score comes out at the k= max_no_features of X.&nbsp;</p> <p>Does that seem right?</p> <p>[/quote]</p> <p>&nbsp;</p> <p>yes</p>
3477, <p>[quote=Marian Dragt;81477]</p> <p>Thanks for sharing&nbsp;&nbsp;@Abhishek!!! I learned a lot thanks to your code.</p> <p>[/quote]</p> <p>Im glad that this code is helping people even after almost two years ;)</p>
3477, <p>there are no teams in this competition. are there any?</p> <p>EDIT: just saw a team. So Ive the same question now</p>
3477, <p>adding the preprocessing code to already existing beat_bench.py will surely improve your CV and LB score. maybe you are something wrong. adding numerical features will decrease your CV and LB and you need to figure out why. There exists a way to stack the numerical features with the text features.! and i think you should also look into feature selection</p>
3477, <p>Kapil your code looks fine. but I this stage I suspect if anyone is going to give you more hints!&nbsp;</p> <p>[quote=Kapil Dalwani;32244]</p> <p>Okie I have some code attached which does&nbsp;SelectPercentile along with&nbsp;lm.LogisticRegression. This is in a pipeline which does GridSearch to find out best parameters.&nbsp;</p> <p>&nbsp;</p> <p>It always end up with C =1 and percentile of 99 or 100.&nbsp;</p> <p>Does the code looks ok?&nbsp;</p> <p>[/quote]</p>
3477, <p>I also had the same problem with cross-validation score and leaderboard score. By the way my best Public score was 0.89447 which got 6th rank when the private data was revealed. I had 40+ submissions which would have got a Top 10 rank in the Private Leaderboard (best being 3rd).</p> <p>Anyways I tried to keep my model as simple as possible and there were only 3 classification models in my ensemble. My ensemble consisted of two Logistic Regression and a k-NN. I used python + sklearn throughout the competition.&nbsp;</p> <p>I divided the data into two parts :</p> <p>#1 Boilerplate: I used the preprocessing.py by Triseklion for preprocessing the boilerplate. In TFIDFVectorizer I used NLTK for stemming and tokenization. So it was basically the same as the beat_bench.py that I had posted except pre-processing and NLTK tokenizer.</p> <p>#2 Raw Data: I used my own data cleaner for cleaning and tokenization and HTML cleaner of NLTK.&nbsp;preprocessing.py by Triseklion was not used here as I had deployed my own pre-processing. I used the same TFIDFVectorizer as the one for Boilerplate data.&nbsp;</p> <p>The next step was SVD. The TF-IDF values obtained from both the data were passed through TruncatedSVD of scikit-learn. Both the SVDs used 120 components.&nbsp;</p> <p>SVD1 ---&gt; Logistic Regression</p> <p>SVD1 ---&gt; k-NN Classifier</p> <p>SVD2 ---&gt; Logistic Regression</p> <p>The final ensemble was a simple mean of these three models.</p> <p>&nbsp;</p> <p>Things that did not work for me (or gave a lower score) :&nbsp;</p> <p>#1 Rapid Automatic Keyword Extraction (RAKE) on both Boilerplate and Raw Data.</p> <p>#2 SVM (I thought it would but it didn't)</p> <p>#3 Naive Bayes worked to a certain extent the results were not satisfactory.</p> <p>#4 Use of Word Embeddings derived using neural network approach on Wikipedia Corpus.</p> <p>&nbsp;</p> <p>Overall it was a very interesting competition for me. Thanks to Kaggle Competition Admins and all the users who contributed their ideas in the forums.</p> <p>&nbsp;</p>
3477, <p>[quote=Yevgeniy;32998]</p> <p>Congratulations to all smart people who used the beating benchmark code. They ended up in top 25% after all. It is much better than people who worked hard on this competition. BTW what is a purpose of the public leader board? I mean your CV score goes up quite consistently with your public leader board score but at the same time your private score jumps randomly. That is all misleading... Mostly I tried to rely on my CV score but it could be my mistake. I had a situation where my CV scores were about the same they both got similar scores on public leader board but on the private leader board difference was very huge. I feel sorry for people who dropped more than 300 places. Domcastro was right after all. It is not that the code was very basic it is because lots of people used it. People who worked hard got nothing not even the top 25% mark.&nbsp;</p> <p>[/quote]</p> <p>&nbsp;</p> <p>maybe because of the benchmark people didnot work hard enough!</p>
3477, <p>also I would like to add one more thing. the basis of my being rank 8th and dropping only 4 ranks(although i was expecting to improve) was the benchmark I posted!</p>
3477, <p>[quote=William Cukierski;33036]</p> <p>FYI we just pushed a change to the way rankings are assigned (see <a href="https://www.kaggle.com/forums/t/6169/skewed-rankings-from-benchmarks">this thread</a>) when people tie. This was done to remove the false &quot;Top 25%&quot; awards from people that submit benchmarks and enter massive ties.</p> <p>[/quote]</p> <p>Hi William</p> <p>Its a really &nbsp;nice decision.!!</p> <p>i really appreciate that. but in imy opinion you cannot implement a new rule for the competitions that have already ended.! &nbsp;I know Kaggle can change the terms and conditions but its not written anywhere that you can change rank of someone just based on what he &quot;actually&quot; submitted!</p> <p>if you wanna change all these this should reflect in your &quot;terms and conditions&quot; and should NOT affect any previous competition!</p> <p>&nbsp;</p>
3477, <p>[quote=William Cukierski;33052]</p> <p>I don't want to argue the legal aspects of this but our thoughts are:</p> <ul> <li>There was greater harm in rewarding false historical achievements than in changing the face value of the rankings (and that's all we are changing).</li> <li>Ranks have never been set in stone. An old user deletes his/her account? You just moved up a rank. We found a ring of cheating? Goodbye old results.</li> <li>Once you have achieved Master status we never demote you (unless it was an error on our part). Everyone who was a Master before this change is still one now.</li> <li>This change only really affects a few competitions and only the relative bottom-ish part of the leaderboard. Yes it's the top 25% in some cases but the affected people can hardly claim they earned it compared to the effort to get top 25% in a non-massive-benchmark-tie competition.</li> </ul> <p>tl;dr - we're trying to do the right thing and sometimes the right thing is not always what makes the most people happy</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Great 4 points for the people who &quot;hate&quot; me here for posting the benchmark!&nbsp;</p> <p>my replies:</p> <p>1. &quot;No private sharing outside teams</p> <p>Privately sharing code or data outside of teams is not permitted. It's OK to share code if made available to all players on the forums.&quot; Your rules. &nbsp;Not mine not anyboy else's. We were allowed to post code and benchmarks and also we can post the zero benchmarks in all the competitions! its nowhere in &quot;rules&quot; that we cannot! And if you wanna change that you can do it for future competitions but not for the ones which have already finished or the ones which are on the verge of finishing!</p> <p>2. I dont really care about &nbsp;&quot;cheating&quot;. &nbsp;I performed well. and yeah i am new. I dont know the users who are ahead of me in this competition. Actually no one except one has replied to any threads here and really a guy with a submission on &quot;16th september&quot; has not replied yet. Then can I say its a false account!?</p> <p>3. Now this should be on your new &quot;rules&quot;</p> <p>4. If I submit some 0 benchmark i know that i have not earned it and even my employers will know that! but once again its the first thing u said. If u wanna change something you cannot do it for the competitions that have already ended and if you are gonna do something like that you should do it for all the competitions hosted by Kaggle till now!</p>
3477, <p>[quote=Domcastro;33056]</p> <p>Abhishek - many people love you for the benchmark. You are being oversensitive and this benchmark thing was started on another thread about the Cause and Effect competition and the Belkin one. xxxxxxxxxxxxxxxxxxxxxxxxx</p> <p>[/quote]</p> <p>Hi Domcastro</p> <p>Your post makes no sense according to Kaggle rules which has been changed just now. I think William can clarify!</p>
3477, <p>Congratualtions &quot;fchollet&quot;.&nbsp;</p> <p>I worked really hard on this competition and I ended up 8th. I would really like to know what you used for your best model and it would be great if you could post the code of your winning model for the Kaggle community :)</p>
3477, <p>has 'fchollet ' replied anywhere?</p> <p>&nbsp;</p>
3477, <p>the readme says it will take approx 5mins to run but on my Mac&nbsp;with 8GB of RAM and 4 cores at 2.7GHz it takes forever to &quot;predict&quot; however the training is quite fast :D</p>
3477, <p>can anybody throw some light on this topic?</p>
3477, <p>I've always wondered how Kaggle decides the timeline of a competition it is hosting. For example in this particular competition Jeff achieved a score of 0.96 just within two days. This score is very competitive and IMO the winning score will be around 0.98. When these kind of scores can be achieved within a short period of time then what's the use of running a particular competition for a period of 4 months? Won't the results be same even if the duration is halved?&nbsp;</p>
3477, <p>Great. I was working with nolearn convnet only but I didnt know that just 100 images can produce an accuracy of 94%. By the way I'm unable to find the example you mentioned.</p> <p>EDIT: nevermind found it</p>
3477, <p>remove that param from decaf/layers/Makefile and compile</p>
3477, <p>i tried decaf in galaxy zoo with random forest on top of it but couldnt get a score of less than 0.15</p>
3477, <p>tried all available ones using grid search</p>
3477, <p>Pip install pil</p>
3477, <p>the exact same code produces a score of 0.30 on the leaderboard. maybe Im using a wrong algorithm</p>
3477, <p>cross validation does give something like 0.17. I was talking about the Leaderboard.</p>
3477, <p>[quote=Giulio;32780]</p> <p>[quote=Abhishek;32454]</p> <p>cross validation does give something like 0.17. I was talking about the Leaderboard.</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Abhishek were you able to get your cv and LB scores closer? My CV is still around 0.17 with&nbsp;my&nbsp;leaderboard&nbsp;of 0.159. Even with TFIDF done within the CV loop...</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Same here... :)</p>
3477, <p>[quote=Giulio;33133]</p> <p>[quote=Abhishek;32781]</p> <p>[quote=Giulio;32780]</p> <p>[quote=Abhishek;32454]</p> <p>cross validation does give something like 0.17. I was talking about the Leaderboard.</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Abhishek were you able to get your cv and LB scores closer? My CV is still around 0.17 with&nbsp;my&nbsp;leaderboard&nbsp;of 0.159. Even with TFIDF done within the CV loop...</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Same here... :)</p> <p>[/quote]</p> <p>Fixed it :-)</p> <p>I hadn't thought through it :-)</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Wanna share how you fixed it? :D</p>
3477, <p>[quote=Giulio;33322]</p> <p>[quote=Abhishek;33321]</p> <p>Wanna share how you fixed it? :D</p> <p>[/quote]</p> <p>Well let me answer with a question. Are you predicting the original labels or a transformation of them?</p> <p>[/quote]</p> <p>Got it! Thanks :D</p>
3477, <p>apply this function to all the rows in your training and test data before you use any vectorizer:</p> <p>&nbsp;</p> <p>def clean_the_text(data):<br>&nbsp; &nbsp; alist = []<br>&nbsp; &nbsp; data = nltk.word_tokenize(data)<br>&nbsp; &nbsp; for j in data:<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; alist.append(j.rstrip('\n'))<br>&nbsp; &nbsp; alist = &quot; &quot;.join(alist)</p> <p>&nbsp; &nbsp; return alist</p> <p>&nbsp;</p>
3477, <p>[quote=LeastSquares;33425]</p> <p>@Abhishek the line:</p> <p>for j in data:</p> <p>should it not read</p> <p>for j in raw:</p> <p>?</p> <p>Cheers</p> <p>&nbsp;[/quote]</p> <p>updated. thanks</p>
3477, <p>[quote=Falconic;33421]</p> <p>How would I do that?</p> <p>[/quote]</p> <p>depends on how you are reading the data. If its a list:</p> <p>for i in range(len(alldata)):</p> <p>&nbsp; alldata[i] =&nbsp;clean_the_text(alldata[i])</p>
3477, <p>whats happening on the leaderboard. I should have logged into kaggle earlier.. :P</p>
3477, <p>[quote=David;33526]</p> <p>[quote=Jack Shih Tzu;33522]</p> <p>I think there's still a lot of improvement to be made at the top of the leaderboard.</p> <p>[/quote]</p> <p>Bah I don't know it probably isn't worth trying. At least not for another 21 days ;)</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Rome wasn't built in a day. It was built on the last day :P . so keep trying&nbsp;</p>
3477, <p>can you give link to SO question? I'll answer there</p>
3477, <p>n there goes my SO points... lol</p> <p>X=scipy.sparse.hstack([tfdif.fit_transform(Train['tweet'])cv.fit_transform(Train['state'])]).tocsr()</p>
3477, <p>but you also need&nbsp;<strong>.tocsr()&nbsp;</strong>:D</p>
3477, <p>[quote=Torgos;35469]</p> <p>Actually it seems to work without .tocsr(). &nbsp;This creates a new error message if used with SVR though:</p> <p>Buffer has wrong number of dimensions (expected 1 got 2)</p> <p>&nbsp;</p> <p>There doesn't seem to be a problem with other regression algorithms. &nbsp;Anyway thanks!</p> <p>[/quote]</p> <p>for SVR:&nbsp;y : array-like shape (n_samples)&nbsp;</p> <p>you can fit SVR only on 1D label</p>
3477, <p>[quote=oztalha;35502]</p> <p><img alt>Thanks. I will try more combinations then. I tried averaging some of them but ensemble CV scores also kind of averaged instead of getting better. BTW attached you can find bar graphs for each category showing class distributions.</p> <p>PS. I couldn't inserted it into the post. Insert/Edit Image button pops up a window but source field does not accept file path or URL?</p> <p>-Talha</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Nice plots. Might help some teams. Thanks</p> <p>&nbsp;</p>
3477, <p>[quote=prof_data;35520]</p> <p>I must be doing something wrong then. Since I am doing exactly that. Features-&gt;TFIDF-&gt; Breaking the label matrix into 24 label vectors-&gt;fit train on every label individually with Ridge -&gt; predict every label individually&nbsp;with Ridge and finally combining those results. But somehow I can't get better results than 0.16200-0.16300.</p> <p>[/quote]</p> <p>Do you scale your results after generating predictions?</p>
3477, <p>[quote=Chi;35590]</p> <p>In my case the gap of CV score and leader board score is high. I noticed there are scientific notation in my submissions is it causing the problem ? I had noticed this caused issue in one previous competition. I am using 'to_csv' from pandas. I am not sure how to handle it in Pandas. Please help how you solve this.</p> <p>[/quote]</p> <p>we are also using pandas .to_csv and our submissions also contain scientific notations. I dont think thats the problem</p>
3477, <p>[quote=David;35595]</p> <p>[quote=Abhishek;35594]</p> <p>[quote=Chi;35590]</p> <p>In my case the gap of CV score and leader board score is high. I noticed there are scientific notation in my submissions is it causing the problem ? I had noticed this caused issue in one previous competition. I am using 'to_csv' from pandas. I am not sure how to handle it in Pandas. Please help how you solve this.</p> <p>[/quote]</p> <p>we are also using pandas .to_csv and our submissions also contain scientific notations. I dont think thats the problem</p> <p>[/quote]</p> <p>Same here.</p> <p>&nbsp;</p> <p>On the CV side of things every single one of my submissions has been 0.006-0.007 worse on leaderboard compared to 10 k-fold cross validation and every submission with better CV has led to a increase in my leadboard score. Given the consistency I don't think this an issue of overfitting to training data and suspect that there just may be some underlying differences between the training and test sets or that leaderboard subsample is just an odd sample. Either way from what I've heard I think just about everyone is expierencing a pretty comparable gap (although based on the number of submissions some teams have I'm going to guess that a few are overfitting to the leaderboard).</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Damn! we have the highest number of submissions :P</p>
3477, <p>hi</p> <p>&nbsp;</p> <p>Im looking for a teammate for this competition. I program in python so someone with python skills would be great. If anyone is interested please message me</p>
3477, <p>Some sklearn models do have multiple columns as input labels eg Ridge Lasso etc</p>
3477, <p>#1 : You are doing something wrong if the fit_transform is not returning you what you need. Im using 0.15-git and its working fine.&nbsp;</p> <p>#2 : Softmax</p> <p>&nbsp;</p> <p>[quote=maoshouse;34857]</p> <p>I think your best bet (per the link) is to get the development version 0.15&nbsp;from git.</p> <p>&nbsp;</p> <p>I also have another question pertaining to the binarizer. I noticed that after running fit_transform(Y) on my data nothing is being transformed. If I print the binarized labels it's still the same as the untransformed one.</p> <p>Subsequently the model can now be fitted but if I try to predict I get a memory error</p> <p>&nbsp;</p> <p>Briggs I assume you're running a linear classifier with Scikit learn as well? How did you account for the weightings for the prediction such that they sum to 1?</p> <p>[/quote]</p>
3477, <p>Ridge from sklearn accepts multicolumn Y</p>
3477, <p>Its a classifier. use:</p> <p>clf.fit(x numpy.round(y) sample_weight=1.0)</p>
3477, <p>more RAM maybe</p>
3477, <p>The data is crowdsourced. Five different people classified one single tweet. Crowdflower then assigned scores to the classification by them. This resulted in predictions which were much closer to the reality. Obviously one user who is rating cannot see tweet#1056 and tweet#1134 simultaneous as you can and thus the discrepancy. IMO it is left for the competitors to decide which one is good and should be used.&nbsp;</p>
3477, <p>cheaters will be removed by Kaggle after the competition ends so no need to worry about it :)</p>
3477, <p>[quote=William Cukierski;35683]</p> <p>We're more or less finished. This is by far the most cheating we've ever seen in a competition.</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Will it affect the overall kaggle ranks and points?</p>
3477, <p>s1-s5 should add up to 1. but your submission will be valid even if they dont.</p>
3477, <p>Partly Sunny With a Chance of #Hashtags</p> <p><strong>Approach for the team (no_name):</strong></p> <p>For classification we treated S W and K separately and created different models for each of them. The dataset was also preprocessed separately for the 3 variables.</p> <p><br><strong>Feature engineering</strong>:<br><strong>Sanitization function</strong> - Each tweet was sanitized prior to vectorization. The sanitization part converted all tweets to lower-case and replaced &#8220;cloudy&#8221; with &#8220;cloud&#8221; &#8220;rainy&#8221; with &#8220;rain&#8221; and so on.<br><strong>Sentiment dictionary</strong> - A list of words for different sentiments and emoticons constituted the sentiment dictionary. <br><strong>Sentiment scoring</strong> - we provided a score to each tweet if the tweet consisted of any words found in the sentiment dictionary.<br><strong>Tense detection</strong> - A tense detector was implemented based on regular expressions and it provided a score for &#8220;past&#8221; &#8220;present&#8221; &#8220;future&#8221; and &#8220;not known&#8221; to every tweet in the dataset.<br><strong>Frequent language detection</strong> - This function removed tweets for infrequent languages was (languages with 10 or less occurences were removed).<br><strong>Tokenization</strong> - A custom tokenization function for tweets was implemented using NLTK.<br><strong>Stopwords</strong> - Stopwords like 'RT''@''#''link''google''facebook''yahoo''rt'  etc. were removed from the dataset.<br><strong>Replace two or more</strong> - Repetitions of characters in a word were removed. Eg. &#8220;hottttt&#8221; was replaced with &#8220;hot&#8221;.<br><strong>Spelling correction</strong> - Spelling correction was implemented based on Levenshtein Distance.<br><strong>Weather vocabulary</strong> - A weather vocabulary was made by crawling a few weather sites which scored the tweets as related to weather or not.<br><strong>Category OneHot</strong> - The categorical variables like state and location were one hot encoded using this function.</p> <p><strong>Types of Data Used</strong>:<br>All tweets<br>Count Vectorization<br>TFIDF Vectorization<br>Word ngrams (12)<br>Char ngrams (16)<br>LDA on the data<br>Predicted values of S W and K using Linear Regression and Ridge Regression</p> <p><br><strong>Classifiers Used</strong>:<br>Ridge Regression<br>Logistic Regression<br>SGD</p> <p><strong>Model</strong>: <br>The different types of data were trained with all the classifiers. The ensemble was created from the different predictions. <br>We used approximately 10 different model-data combinations for creating the final ensemble.<br>The predictions for S and W were normalized between 0 and 1 in the end.</p> <p>We also used the extra data for &#8220;S&#8221; available at : https://sites.google.com/site/crowdscale2013/shared-task/sentiment-analysis-judgment-data</p> <p>Our model scored 0.1469 on the leaderboard.</p> <p>In the end we did an average with <strong>Jack</strong> and ranked 2nd on the public leaderboard and 4th on the final leaderboard.</p> <p><strong>Things that didn't work</strong>:<br>- Building a hand-crafted tense detection using keywords (similar to sentiment detection)</p> <p>Things we should have tried:<br>- Build more diverse models and use ensembling/averaging (similar to what Maarten Boosma did in stumbleupon)<br>- Stacking (e.g. pipeing the predictions of ridge/sgd into a tree estimator)<br><br></p> <p><strong>Things we noted</strong>:<br>- The model for W (when) was performing the worst (RMSE of about 0.19-0.20) whereas S (sentiment) and K (kind) were 0.13 and 0.1 respectively<br>- Most predictions in W related to the current weather situation predictions for &quot;I can't tell&quot; were very difficult</p> <p><strong>Tools used</strong>:<br>- sklearn<br>- nltk<br>- langid<br>- NodeBox:Linguistic</p>
3477, <p>[quote=Matt;36022]</p> <p>[quote=David McGarry;36017]</p> <p><span style="line-height: 1.4">unless you switched your S and K scores (above)</span></p> <p>[/quote]</p> <p>Hi David you're right S and K are mixed up (my bad).</p> <p>[/quote]</p> <p>Yeah S and K are mixed up. Fixed now</p>
3477, <p>I got a score of&nbsp;0.34309 on the LB. when I sort the same submission by IDs and submit the score is 0.68</p>
3477, <p>[quote=William Cukierski;33637]</p> <p>[quote=Abhishek;33635]</p> <p>I got a score of&nbsp;0.34309 on the LB. when I sort the same submission by IDs and submit the score is 0.68</p> <p>[/quote]</p> <p>Check your submission for scientific notation.</p> <p>[/quote]</p> <p>a total of 3 values have scientific notation</p>
3477, <p>seems there will be big changes in the top 50 of Kaggle :P</p>
3477, <p>Congrats to all winners :).&nbsp;</p>
3477, <p>are there usual kaggle points for this competition?</p>
3477, <p>You can use the evaluation version that is valid for 30 days</p>
3477, <p>[quote=Bright future;36296]</p> <p>I'm currious whether Abhishek&nbsp;will beat the&nbsp;1.612.061 score or not :)</p> <p>[/quote]</p> <p>Isnt that impossible?! ;)</p>
3477, <p>[quote=Rudi Kruger;36301]</p> <p>C'mon Abhishek master of the sleigh give us just a teeny-tiny tip ;]</p> <p>[/quote]</p> <p>At this point I better stay quiet ;) I think Ive hit the plateau</p>
3477, <p>[quote=Abdallah Sayyed-Ahmad;36330]</p> <p>1559482 !!! is impossible. There has to be overlap or a bug in the evaluation script.&nbsp;</p> <p>[/quote]</p> <p>the sleigh is magic!</p>
3477, <p>[quote=Victor;36077]</p> <p>[quote=MathWorks;35947]</p> <p>&nbsp;The MATLAB prize is available to MATLAB users..</p> <p>[/quote]</p> <p>I don't understand how is it possible to check whether somebody uses only MatLab since any program may be translated to matlab. So one can initially use C and later translate code to matlab.</p> <p>[/quote]</p> <p>Thats why they are not giving extra time to upload model</p>
3477, <p>[quote=MathWorks;35947]</p> <p>We'd like to see as many competitors as possible so the competition is open to any solution! The MATLAB prize is available to MATLAB users but the Leaderboard and Rudolph prizes are open to any solution (including MATLAB)!</p> <p>[/quote]</p> <p>&nbsp;</p> <p>are we allowed to used MATLAB mex subroutines for C/C++ if competing for MATLAB prize?</p>
3477, <p>[quote=Ben Barsdell;35987]</p> <p>Hi all</p> <p>For those who are impatient like me I've put together some C++ code that uses the data-parallel Thrust library for speed. It's available here: https://github.com/benbarsdell/KaggleSantaPacking</p> <p>On my outdated laptop it takes around half a second to validate a solution and around a tenth of a second to compute its score.</p> <p>Please let me know if you find any bugs or have any questions or suggestions.</p> <p>[/quote]</p> <p>&nbsp;</p> <p>I get the following errors while compiling:</p> <p>In file included from /tmp/tmpxft_000043d0_00000000-1_check_solution.cudafe1.stub.c:1:<br>/tmp/tmpxft_000043d0_00000000-1_check_solution.cudafe1.stub.c:85: error: macro &quot;__cudaRegisterEntry&quot; requires 4 arguments but only 3 given<br>/tmp/tmpxft_000043d0_00000000-1_check_solution.cudafe1.stub.c:85: error: macro &quot;__cudaRegisterEntry&quot; requires 4 arguments but only 3 given<br>/tmp/tmpxft_000043d0_00000000-1_check_solution.cudafe1.stub.c:85: error: macro &quot;__cudaRegisterEntry&quot; requires 4 arguments but only 3 given<br>/tmp/tmpxft_000043d0_00000000-1_check_solution.cudafe1.stub.c:85: error: macro &quot;__cudaRegisterEntry&quot; requires 4 arguments but only 3 given<br>/tmp/tmpxft_000043d0_00000000-1_check_solution.cudafe1.stub.c:85: error: macro &quot;__cudaRegisterEntry&quot; requires 4 arguments but only 3 given<br>/tmp/tmpxft_000043d0_00000000-1_check_solution.cudafe1.stub.c:85: error: macro &quot;__cudaRegisterEntry&quot; requires 4 arguments but only 3 given<br>/tmp/tmpxft_000043d0_00000000-1_check_solution.cudafe1.stub.c:85: error: macro &quot;__cudaRegisterEntry&quot; requires 4 arguments but only 3 given<br>/tmp/tmpxft_000043d0_00000000-1_check_solution.cudafe1.stub.c:85: error: macro &quot;__cudaRegisterEntry&quot; requires 4 arguments but only 3 given<br>/tmp/tmpxft_000043d0_00000000-1_check_solution.cudafe1.stub.c:85: error: macro &quot;__cudaRegisterEntry&quot; requires 4 arguments but only 3 given</p> <p>&nbsp;</p> <p>any idea?</p>
3477, <p>if anybody needs:</p> <p>http://beatingthebenchmark.blogspot.de/2013/12/packing-santas-sleigh-python-code-for.html</p>
3477, <p><span style="text-decoration: line-through">I'm getting a submission exception: present not inside the sleigh although the values look good to me and MATLAB metric calculation gives me the following:</span></p> <p><span style="text-decoration: line-through">&nbsp;</span></p> <p><span style="text-decoration: line-through">&quot;&quot;&quot;Dimensions check PASSED: Input and submission dimensions match</span><br><span style="text-decoration: line-through">Packages fit within sleigh check PASSED: No packages out of sleigh&quot;&quot;&quot;</span><br><br></p> <p><span style="text-decoration: line-through">Can anyone tell me what could be wrong?</span></p> <p>&nbsp;EDIT: Nevermind missed the v2 of MATLAB metric</p>
3477, <p>10 mins. In pure python</p>
3477, <p>Nice visualizations.&nbsp;</p> <p>What did you use?</p>
3477, <p>[quote=razapor;36204]</p> <p>Gravity compaction of an H=1040510 solution reduced H by only 1112. My layer lower surfaces are very random. The Gravity code is small and quick.</p> <p>[/quote]</p> <p>wont gravity compaction affect the order ?</p>
3477, <p>[quote=Sergey Yurgenson;36222]</p> <p><span style="line-height: 1.4">[quote]</span></p> <p><span style="line-height: 1.4">Now you have to tell us what did you do ;)</span></p> <p>[/quote]</p> <p>hehe.. let me enjoy the first place for a few days :P lol</p>
3477, <p>[quote=Hebrew Hammer;36226]</p> <p>I was just about to ask a question about gravity when I saw this discussion. So I'll go ahead and ask it here. Are solutions in which some presents are floating (ie with no support under them) like in some of the examples above valid? Or do presents have to have another present immediately below them in at least one unit square (like in Tetris)?</p> <p>[/quote]</p> <p>Presents can float. There is no gravity</p>
3477, <p>What is the best score that you have attained with one error being zero?</p> <p>Mine is&nbsp;2678476 with ordering error = 0</p>
3477, <p>[quote=Ants;36332]</p> <p>Oh...hmm.... :-/</p> <p>Now as the theoretical limit is broken it would be a good time to fix the official benchmark and reevaluate all :)</p> <p>[/quote]</p> <p>the sleigh is magic!</p>
3477, <p>im being a bit lazy and would like to ask if someone has written any code for metric calculation in c++ and would like to share&nbsp;</p>
3477, <p>[quote=WBTtheFROG;36337]</p> <p>[quote=WBTtheFROG;36316] it may even be possible to beat 1612062 and that would be pretty interesting.&nbsp; After the conversation in this thread I'd be impressed by someone who figures out a way to do that and see that as within the spirit of the competition.</p> <p>[/quote]</p> <p>Congrats Abhishek.&nbsp; The sleigh is magic :-)</p> <p>[/quote]</p> <p>Yep. but I dont see why you are congratulating for I havent won...</p>
3477, <p>origin is (111)</p>
3477, <p>[quote=Kyle Willett;37068]</p> <p>Hi everyone</p> <p>Quick update - I've been working on the dataset much of this evening. It's not 100% fixed yet but there are some bugs in the data that violate the constraints laid out in the decision tree. I've found the source for some of them (a very small percentage of raw data classifications didn't match numbers due to our weighting scheme) but the normalized values for your data still aren't agreeing perfectly. I will continue working on this tomorrow.</p> <p>If we can isolate the cause of the troublesome galaxies we'll either fix or remove them from the data. We'll keep you updated about any changes as soon as we've made a decision. In the meantime if you're thinking about solutions I would assume that the ultimate dataset will obey the constraints in the decision tree that have already been described.</p> <p>[/quote]</p> <p>Does it mean the leaderboard will be re-evaluated?</p>
3477, <p>imo these violations should be eliminated from the data and new data should be given. It wont be helpful for the organization to get predictions and model on a flawed data.</p>
3477, <p>@david have you read &quot;the galaxy zoo decision tree&quot; page?</p>
3477, <p>[quote=Black Magic;37237]</p> <p>I am totally confused. I had some submissions before and now I see leaderboard has only 8 entries.</p> <p>What happened? is there a minimum number of submissions per week. I am re-entering competition after a week</p> <p>[/quote]</p> <p>This happened:&nbsp;https://www.kaggle.com/c/galaxy-zoo-the-galaxy-challenge/forums/t/6794/data-constraints-and-competition-reboot</p>
3477, <p>Since I've been ridiculed previously for the beating the benchmark posts this time I wont post the code but a method which would enable you to beat the central pixel benchmark and would take only 20mins.</p> <p>&nbsp;</p> <p>Step 1 : for all images in train and test resize to 50x50 and vectorize to 1-D array.</p> <p>Step 2: Run the RandomForestRegression from scikit-learn on the train and test set with 10 estimators.</p> <p>Step 3: Submit Results</p> <p>Step 4: You have beaten the benchmark</p> <p>Step 5: Click thanks if this post helped you ;)</p>
3477, <p>aint that true :P&nbsp;</p> <p>[quote=Giulio;37245]</p> <p>[quote=Abhishek;37233]</p> <p>Since I've been ridiculed previously for the beating the benchmark posts&nbsp;</p> <p>[/quote]</p> <p>LOL.</p> <p>[/quote]</p>
3477, <p>[quote=Frank Schilder;37329]</p> <p><span style="line-height: 1.4">The question is whether it is worthwhile to invest the extra time to install cv2. OpenCV seems to offer quite a lot of useful resources for image processing though. Anybody out there who was able to run cv2/OpenCV on a Mac?</span></p> <p>[/quote]</p> <p>&nbsp;</p> <p>I work on a mac and installed opencv using homebrew.&nbsp;</p>
3477, <p>Im using OSX mavericks. I didnt face any conflicts with scipy and all. As I dont have a GPU on my mac I cannot use theano/pylearn2 for convolutional neural networks. However I can use neural networks which do not require GPU and thus theano/pylearn2 are working fine for me. I had a hard time installing opencv but I finally managed to install it. (https://github.com/Homebrew/homebrew-science/issues/402)</p> <p>[quote=Domcastro;37337]</p> <p>Abhishek - what MacOS are you using? Out of interest have you manged to get Theano and Pylearn2 working with GPU? I had to install Anaconda Python to get Python on my Mac. I'm on Mac OS X Lion 10.7.5 (11G63) - there was a conflict with scipy (xcodes/gcc/lvcc ?&nbsp; etc)</p> <p>[/quote]</p>
3477, <p>@domcastro did you install an external GPU for your Mac?&nbsp;</p> <p>[quote=Domcastro;37337]</p> <p>Abhishek - what MacOS are you using? Out of interest have you manged to get Theano and Pylearn2 working with GPU? I had to install Anaconda Python to get Python on my Mac. I'm on Mac OS X Lion 10.7.5 (11G63) - there was a conflict with scipy (xcodes/gcc/lvcc ?&nbsp; etc) when I tried to install packages individually</p> <p>[/quote]</p>
3477, <p>[quote=George Oblapenko;41393]</p> <p>If I understood the decision tree correctly it goes as follows (I'll denote the probability of a class by P(class)):</p> <p>P(1.1) + P(1.2) + P(1.3) = 1.0<br>P(7.1) + P(7.2) + P(7.3) = 1.0 * P(1.1)</p> <p>And so on. Unless explicitly stated that the probabilities should sum to 1 the probabilities of a class should sum to the probability of the previous node in the decision tree.</p> <p>[/quote]</p>  <p>What does the last statement mean? Can you elaborate further with one more example please?</p>
3477, <p>[quote=Triskelion;36633]</p> <p>Have there been done benchmarks on similar (or the same) data sets?</p> <p>What is the state-of-the-art RMSE to beat?</p> <p>[/quote]</p> <p>Yes some papers would be great</p>
3477, <p>I dont know what's the fuss is all about when the same algorithm you used previously can give you similar results. Isnt it so? or am i missing something here?</p>
3477, <p>what just happened?</p>
3477, <p>[quote=Wen K Luo;38951]</p> <p><span style="line-height: 1.4">I assume you haven't actually looked at the data yet or are using some extremely blackbox methods given that you've already submitted but don't know this potential to get a perfect score?</span></p> <p>[/quote]</p> <p>&nbsp;</p> <p>I havent started with this competition yet</p>
3477, <p>I have just made the benchmark submission and tried some small approaches. Not started as in a full-fledged way :)</p>
3477, <p>Has it changed now since the competition is no more a knowledge competition?&nbsp;</p>
3477, <p>So only the last 3 are categorical ones?</p>
3477, <p>[quote=Parthiban Gowthaman;37626]</p> <p>Any one facing this problem&nbsp;</p> <p>In train data</p> <p>Even after filling NA by 0 in scikit using&nbsp;</p> <p>df.fillna(0)</p> <p>I am getting error like</p> <p>ValueError: Array contains NaN or infinity.</p> <p>&nbsp;</p> <p>[/quote]</p> <p>Use X = numpy.nan_to_num(X)</p>
3477, <p>https://www.kaggle.com/c/loan-default-prediction/details/evaluation</p>
3477, <p>I use pandas and it takes me a few seconds to read data and I have no problems with memory. My configuration is same as yours.</p>
3477, <p>Can anyone tell me if cross-validation on the training set is giving them similar results on the leaderboard?&nbsp;</p>
3477, <p>[quote=Triskelion;38064]</p> <p>A problem I find with this approach is that if you predict that nobody will default your plain accuracy is still very high. You'll miss about 9% of defaulters so your accuracy is about 91%.</p> <p><span style="line-height: 1.4">[/quote]</span></p> <p><span style="line-height: 1.4">this dataset is highly skewed and using a metric like accuracy measure wont give you proper results. If you are building a defaulter/non-defaulter system first you should be using area under the ROC curve to evaluate your model.</span></p>
3477, <p>Hi all</p> <p>It seems to be pretty difficult to beat the zero benchmark in this competition. So I wrote a quick and dirty script to achieve that. ;)</p> <p>The main idea is feature selection and a two model approach. One for predicting the loan deafult and another for predicting the LGD.</p> <p>The code is located at my blog:&nbsp;http://beatingthebenchmark.blogspot.de/ and one copy can be downloaded from here. The code doesnot have any comments so feel free to ask about any part which is un-understandable.</p> <p>This version will give you&nbsp;~0.83265 on the leaderboard (Thanks Triskelion for verifying)</p> <p>Dont forget to click &quot;thanks&quot; if this post helped you in any way :)</p>
3477, <p>I havent yet checked how the code performs on the leaderboard. Can someone tell please?</p>
3477, <p>[quote=Triskelion;38231]</p> <p>[quote=Abhishek;38229]</p> <p>I havent yet checked how the code performs on the leaderboard. Can someone tell please?</p> <p>[/quote]</p> <p>~0.83265.</p> <p>An incredibly high score. You are at&nbsp;~0.83261. Took about 30 minutes to run and needs ALL the memory. Latest version of everything (with Scikit-learn this seems to matter).</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Great. So its expected. Btw LinearSVC will select different features on every run so its better to find an alternative ;)</p>
3477, <p>Created a model for predicting default using logistic regression. The features for this model were selected using LinearSVC. The defaulters were kept for further processesing (Logistic regression and all the available features) and the non-defaulters were assigned a prediction of zero. It should be noted that I have only used the numerical features in the code and thus there is a lot of room for improvement.&nbsp;</p> <p>@Triskelion what is your current AUC if i may ask?</p> <p>&nbsp;</p>
3477, <p>my current AUC is around 0.71. After some feature selection I've already achieved 0.74 (results not on LB yet)</p>
3477, <p>One more hint: If you remove the feature selection using LinearSVM the results would be stable and would still beat the benchmark ;)</p>
3477, <p>[quote=Giulio;38242]</p> <p>Thanks Abhishek this is really interesting!</p> <p>I've a question: do you guys believe that this type of approach (which is what I have been trying as well) will lead to substantial improvement if refined further? I just feel that unlike other Kaggle competitions where you keep improving little by little by building on an initial approach it will be hard to refine the approach we have all been trying to move into low 0.7 territory.</p> <p>Any thoughts?</p> <p>[/quote]</p> <p>This approach wont win the competition but wont overfit ;)</p>
3477, <p>[quote=John Galt;38302]</p> <p>What is the peak memory usage of this python code? I can't seem to run on 8GB RAM.</p> <p>[/quote]</p> <p>I run it on my mac with 8gb ram with some other apps open and it works perfectly :)</p>
3477, <p>[quote=Karthik Ramarathnam;40172]</p> <p>Can someone please explain what the column headings stand for ? I am quite confused as all the headings are with &quot;f&quot; and it does not make any business sense to me ..... without an understanding of the business context it will not be possible to proceed with the analysis.</p> <p>Thanks in advance for the help.</p> <p>[/quote]</p> <p>Do you guys never read the other forum posts at all?</p>
3477, <p>did you try L2?</p>
3477, <p>[quote=barisumog;39069]</p> <p>I talked to a friend who works in banking. I explained the premise and told that 0.91 auc can be achieved with just 2 features. He laughed and asked if the features were &quot;probably gonna default&quot; and &quot;most probably gonna default&quot;.</p> <p>I currently have 0.95 auc on the train set with local cv. I told him so and he replied &quot;Nah you'd already be kidnapped by now.&quot;</p> <p>So this is my friendly warning to those at the top of the leaderboard. Lock your doors and windows.</p> <p>=)</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>[/quote]</p> <p>So you took help of an outsider &nbsp;does that mean you broke the rules?&nbsp;</p>
3477, <p>That's normal. But a 1000 &nbsp;places is not :P</p> <p>[quote=&#9757;;39204]</p> <p>I've been seeing this under my username on the leaderboard while the submission is scoring. After it's done scoring it shows the normal message. &quot;Your submission scored ... You moved up 1000 places...&quot;</p> <p>[/quote]</p>
3477, <p>[quote=DataGeek;39208]</p> <p>[quote=William Cukierski;39207]</p> <p>Can you guys clarify what's going on for me? I pulled the stats and the mean scoring duration for this comp is &lt; 2 seconds. The longest ever took 30 seconds. You are saying your submission score reads NULL and then changes to its rightful value?</p> <p>[/quote]</p> <p>Yes William. It says your submission scored NULL and then shows me my current leader board score. It happened with me one time only (last submission).&nbsp;</p> <p>[/quote]</p> <p>&nbsp;</p> <p>The LB always shows &quot;NULL&quot; when its scoring your submission. Its nothing new.</p>
3477, <p>[quote=Giulio;39414]</p> <p>[quote]</p> <p>&nbsp;classification scores 0.99 AUC / 0.95 F-score on validation</p> <p>[/quote]</p> <p>BTW... How many features does your 0.99 AUC model tend to include?</p> <p>[/quote]</p> <p>0.99 AUC here too. 2 features</p>
3477, <p>[quote=yr;39423]</p> <p>[quote=Abhishek;39420]</p> <p>[quote=Giulio;39414]</p> <p>[quote]</p> <p>&nbsp;classification scores 0.99 AUC / 0.95 F-score on validation</p> <p>[/quote]</p> <p>BTW... How many features does your 0.99 AUC model tend to include?</p> <p>[/quote]</p> <p>0.99 AUC here too. 2 features</p> <p>[/quote]</p> <p>The same&nbsp;0.95 F-score? I used 5 features AUC aroud 0.985 and F1-score around 0.88. Guess I might spend some effort to find these golden features</p> <p>Regards</p> <p>[/quote]</p> <p>Obviously not. &nbsp;See my rank :P</p>
3477, <p>[quote=3pletdad;39517]</p> <p>[quote=yr;39508]</p> <p>I see what you mean. I tried a few (50 times) different random splits of train/test while glm may not converge sometimes it does a fair job with my current selected features resulting in average auc around 0.9846 &nbsp;(sd=0.001) and f1-score around 0.8963 (sd=0.0036) on the test set.</p> <p>Given that there are about 100k samples and a few features (about&nbsp;10) I assume that a simple classifier like lr/glm is less prone to overfitting and regularization may do a little help.</p> <p>Regards</p> <p>[/quote]</p> <p>I wonder if I'm doing something wrong. Everyone seems to be getting 0.9846 AUC with ~10 features and I was able to get there with 4. Is there ever a time less feature is bad?</p> <p>[/quote]</p> <p>I have only 3-4 for 0.99</p>
3477, <p>my F1 is 0.91 for AUC = 0.99&nbsp;</p>
3477, <p>[quote=Neil Summers;39353]</p> <p>I got a MSE of 0.47.</p> <p>[/quote]</p> <p>MAE or MSE?</p>
3477, <p>Kaggle usually removes all the cheaters after the competition finishes.</p>
3477, <p>It's just 20% data. &nbsp;You should trust cv</p>
3477, <p>[quote=paparator;40336]</p> <p>i add 250 (can be tuned) zero loss instance to non-zero loss set.</p> <p>[/quote]</p> <p>what does this mean?</p>
3477, <p>[quote=David McGarry;38112]</p> <p><span style="line-height: 1.4">Well I guess the leakage variables had a very little impact on my score but i'm not loving the leadboard going from ~200 to 25.</span></p> <p>[/quote]</p> <p>They will be back. And more people wil join...</p>
3477, <p>In my opinion most people are using the so called leakage variables just to get good ranks and thus the Kaggle points. The problem is for other people who are trying hard to win or get the masters badge without utilizing the leakage as no one is going to verify your model unless u r a winner. I think Kaggle should have changed the whole dataset or shuffled the samples and variables when the leakage was found instead of deleting the old variables</p>
3477, <p>any updates on the python sample code?</p>
3477, <p>I have attached the python code to &quot;Last Quoted Plan Benchmark&quot;. It uses pandas.</p> <p>Click thanks if it helped ;)</p>
3477, <p>if it has errors its not counted towards your daily submission limit and you can submit a new one without errors.</p>
3477, <p>[quote=Luca Massaron;41316]</p> <p>By the way will points be awarded (...) or is the competition really just for fun?</p> <p>[/quote]</p>  <p>Hey u r winning :P</p>
3477, <p>What's the score? &nbsp;;)&nbsp;</p>
3477, <p>tbh 89 variables is nothing</p>
3477, <p>man. I want this competition on main page!</p>
3477, <p>Hi</p> <p>My latest submission shows pending. Are there any competition admins for inclass competitions?</p>
3477, <p>Well the new one got scored but the previous one is still pending. Do I get a submission refund? :P</p>
3477, 
3477, <p>[quote=Frans Slothouber;49941]</p> <p><strong>If the three winners can send me their postal address ( email me at frans.slothouber (at) gmail.com ) I will send them some 'loot' as a reward for their achievements.</strong></p> <p>[/quote]</p>  <p>What's the loot? :D</p>
3477, <p>these variables are the same right?&nbsp;</p>  <p>repeaters = repeattrips &gt; 0 ?</p>
3477, <p>So its reapeat to the shop/brand/category?</p>
3477, <p>[quote=David;44469]</p> <p>It's possible to use repeattrips to train the model but not use it explicitly as a feature.&nbsp;</p> <p>[/quote]</p> <p>Do let us know if you find out how</p>
3477, <p>A negative value in productquantity and purchaseamount indicates a return</p>
3477, <p>Are all test users in testhistory present in transactions.csv?</p>
3477, <p>Yeah. got that. There was something wrong with what I was doing and was getting less number ;) . thanks anyways</p>
3477, <p>Use&nbsp;awk -F '$4 = 706' transactions &gt; trans_cat.csv &nbsp;and so on to avoid junk rows</p>
3477, <p>dict is always faster than the list in python (comparable for small data)</p>
3477, <p>Leave AUC aloneee....</p>
3477, <p>[quote=Art_Ghazaryan;42047]</p> <p>Triskelion</p> <p>Your thoughts of creating new features from the transaction data are absolutely right. One note though. Brand is associated with category and company So if a shopper buys a brand he/she buys the category and the company as well. So you can combine these three into one feature. To be more accurate I think category and company could be left out because the offers are made on brands not categories or companies. I think a new binary feature that indicates if a shopper has purchased the brand prior to the offer would be sufficient. In other words a shopper who has prior exposure to the brand is more likely to respond to the offer than the shopper who has not had the exposure.&nbsp;</p> <p>[/quote]</p> <p>No.</p> <p>Brand is not associated with category (with company?). A sparkling water bottle is a category. A company can make many brands but all brands do not belong to one company. Brand is a trademark of a given company. If you buy kinley still mineral water you are buying from department = water category = still water brand = kinley company = coca cola</p>
3477, <p>statsmodels</p>
3477, <p>0.59104 is also good. You might wanna improve on that.</p>
3477, <p>I'm on a mac too and my score with VW in the begining was also 0.59410</p>
3477, <p>AFAIK Triskelion is using Windows.</p>
3477, <p>split -l 200000&nbsp;transactions.csv&nbsp;part_</p>
3477, <p>Sample submission contains all zeros. The 0.5 score on the leaderboard is Area under the ROC curve (AUC). If you want to learn in detail how AUC is calculated:&nbsp;http://en.wikipedia.org/wiki/Receiver_operating_characteristic</p>
3477, <p>They used latest version of vowpal-wabbit</p>
3477, <p>See. I told you :P</p>
3477, <p>&lt;10 mins. &lt;2gb ram</p>
3477, <p>[quote=Brenton Mallen;44673]</p> <p>Where is the &quot;official Kaggle metric code&quot; located? &nbsp;The evaluation page just points to Wikipedia.</p> <p>[/quote]</p> <p>here:&nbsp;https://www.kaggle.com/wiki/AreaUnderCurve</p>
3477, <p>The sponsor is anonymous in this competition.</p>
3477, <p>[quote=ACS69;50485]</p> <p>The worrying thing is that some of the dummy accounts are currently being deleted. Havingfun at number 13 exists no more - does this mean someone in the Top 12 is a cheat?</p> <p>[/quote]</p>  <p>Yes!</p>
3477, <p>[quote=saikumar allaka;49598]</p> <p>Total number of rows in transactions dataset is 349655789 so i was thinking how to pre-process and extract features.</p> <p>Answer is hadoop and Hive Partitioning and bucketing..</p> <p>Soon i hope to be in top 10.</p> <p>[/quote]</p>  <p>takes me 20-30 mins to extract ~300 features from the&nbsp;349655789 rows transaction data with less than 2 gb ram use.&nbsp;</p>
3477, <p>[quote=Chitrasen;50062]</p> <p>[quote=Abhishek;49617]</p> <p>takes me 20-30 mins to extract ~300 features from the&nbsp;349655789 rows transaction data with less than 2 gb ram use.&nbsp;</p> <p>[/quote]</p> <p>Abhishek how many cores are in use ? Do you split the file for multi-processing ?</p> <p>Only streaming the 22gb file takes me more than 1 hour.</p> <p>[/quote]</p>  <p>without splitting and no multiprocessing. Yes i have an SSD and that makes all the difference ;)</p>
3477, <p>hahaha :D</p>
3477, <p>DELETED</p>
3477, <p>Very high benchmark! Thanks anyways ;)</p>
3477, <p>Hi</p> <p>Will the final rankings be the &quot;real&quot; rankings or would they be like the last competition that we had?</p>  <p>Thank you</p>
3477, <p>Score will be &gt;3.8 :D</p>
3477, <p>Is the distribution of signal and background same in public and private leaderboards?</p>
3477, <p>I was waiting for someone to post something like this about xgboost. This confirms that its not just me.&nbsp;</p>
3477, <p>I'm also trying to test XgBoost with some dummy data and I know AUC cannot be 0.5 as for the same dataset using sklearn I get AUC &gt; 0.9</p>
3477, <p>I'm doing something like:</p>  <p># xgmat = xgb.DMatrix(X_train label=y_train)<br> # watchlist = [ (xgmat'train') ]<br> # num_round = 150<br># bst = xgb.train(plst xgmat num_round watchlist)<br> # bst.save_model('xg.model')</p> <p><br> # xgmat = xgb.DMatrix(X_test)<br> # bst = xgb.Booster({'nthread':8})<br> # bst.load_model('xg.model')<br> # y_pred = bst.predict( xgmat )</p>
3477, <p>Fixes for me too. Thank you!</p>
3477, <p>It seems XgBoost overfits way too much :)</p>
3477, <p>[quote=Peter Williams;47385]</p> <p>Sklearn's&nbsp;<a href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html">GradientBoostingClassifier</a>&nbsp;doesn't appear to have</p> <ul> <li>handling of missing values</li> <li>class weighting&nbsp;</li> <li>an auc target (I don't know if this is important)</li> </ul> <p>I can see at least one <a href="http://www.kaggle.com/users/1955/gilles-louppe">sklearn person</a>&nbsp;competing here. Can the sklearn people tell us how to configure&nbsp; GradientBoostingClassifier for missing values uneven class weights and an auc target?&nbsp;</p> <p>[/quote]</p> <p>- missing values can be handled by using preprocessing.Imputer()</p> <p>- check this for class weighting:&nbsp;https://github.com/scikit-learn/scikit-learn/pull/3224&nbsp;</p> <p>- I dont know what you mean.</p>  <p>By the way there are two sklearn persons that I can see on the LB :D</p>
3477, <p>IMO he means something that can beat the benchmark</p>
3477, <p>IMO its directed towards other competitors for hints and/or discussion about handling missing data.</p>
3477, <p>Whats the leaderboard AMS for this?</p>
3477, <p>@Bing Xu they already listed it!</p>
3477, <p>use:</p> <p>import pandas as pd</p> <p>data = pd.read_csv('file_name')</p>
3477, <p>[quote=Lubo&#353; Motl;48988]</p> <p>Thanks Balazs!</p> <p>I just found a stunningly brutal error in my code which didn't prevent it from getting to #3 at some point. Holy cow it's like treating the numbers in training.csv as having units of meters and test.csv as having feet to recall a NASA (?) incident. My error is really more devastating than that. And this error spoiled most of the features of each test.csv event. How could it work at all? ;-)</p> <p>Everybody: What do you think will happen when I submit a result from the corrected code tomorrow? ;-)</p> <p>[/quote]</p>  <p>Why wait till tomorrow?</p>
3477, <p>Can we see some of these analysis now?&nbsp;</p>
3477, <p>LB score?</p>
3477, <p>Why is this such a big deal?</p>
3477, <p>Kaggle is now verifying accounts and duplicate accounts are deleted at the end of competition anyways. I was saying two people with the same name is not a big deal!</p>
3477, <p>Miroslaw is back.... &nbsp;\m/</p>
3477, <p>[quote=Algoasaurus;53810]</p> <p>[quote=Giulio;53806]</p> <p>&nbsp;... you're putting many folks in a place where they need to decide whether staying true to months worth of own work is ethically worthwhile while tens of people are jumping ahead of you thanks to somebody else's work.</p> <p>[/quote]</p> <p>Exactly Giulio. Thank you for speaking so eloquently to these points.</p> <p>[/quote]</p> <p>I also agree! Sharing is fine but not at the last minute. Unfortunately this is a new Kaggle trend!</p>
3477, <p>All your submissions will be ranked on the leaderboard. But you should remember that the leaderboard we see right now is Public Leaderboard which evaluates and scores only 18% of your submission. The Private Leaderboard (i.e the other 82%) will be revealed as soon as the competition finishes. Thats why you have the choice of choosing two submissions which you think will perform best on the overall dataset and not just the 18% that you are seeing currently</p>
3477, <p>The set seems proper to me. If you want to use pandas to read it ive posted a benchmark code in another thread.</p>
3477, <p>Well this is my favorite forum topic it seems. :D Whats your cross validation score? Mine is very high compared to the leadearboard score!</p>
3477, <p>whats the difference for you guys? mine is approx 0.2!!</p>
3477, <p>Damn!</p>
3477, <p>I wrote a quick script to beat the benchmarks in this competition using only the essay data. The code can be grabbed here:&nbsp;http://beatingthebenchmark.blogspot.de/</p> <p>or downloaded from this post.&nbsp;</p> <p>Dont forget to click &quot;Thanks&quot; if it helped you in any way. :)</p> <p>If you dont understand anything feel free to ask.</p>
3477, <p>The evaluation metric is AUC so you can submit any real values. Why dont you try with essay lengths as responses? You have 5 submissions a day ;)</p>
3477, <p>wow... just noticed it.. great job(?) by the team&nbsp;</p>
3477, <p>still no news from anywhere? (wondering)</p>
3477, <p>Who is that?</p>
3477, <p>Read:&nbsp;https://www.kaggle.com/wiki/AreaUnderCurve</p>
3477, <p>You have to submit probabilities only.</p>
3477, <p>Submit anything you want</p>
3477, <p>This is simply amazing! Is there something wrong or is it genuine?</p>
3477, <p>It seems they fixed it now :D</p>
3477, <p>Take a look here:&nbsp;http://stackoverflow.com/questions/1871524/convert-from-json-to-csv-using-python</p>
3477, <p>Hi</p>  <p>Are the labels in the train data ordered in decreasing order of their significance?</p>
3477, <p>I just submitted a csv with labels as floats eg. 1.0 2.0. Instead of raising an error the system evaluated my submission to 0.0000 and I lost one&nbsp;submission.&nbsp;</p>
3477, <p>Thanks</p>
3477, <p>Whats the LB score for this?</p>
3477, <p>Change to naive bayes to get .42-.43 ;)</p>
3477, <p>Is it for real or some kind of mistake happened here? :D</p>
3477, <p>[quote=Ulo Gulo;48981]</p> <p>Is anybody able to reproduce the benchmark?</p> <p>[/quote]</p>  <p>17 &#8595;3 wenxin zhao <br>0.80804<br>1 Sat 07 Jun 2014 19:52:23<br>18 &#8595;3 Vivant Shen Team <br>0.80804<br>5 Wed 11 Jun 2014 06:33:23 (-3.4d)<br>19 &#8595;2 Ali Ziat <br>0.80804<br>5 Wed 11 Jun 2014 00:00:30 (-23.5h)<br>20 &#8595;2 optimizer</p> <p>0.80804</p>
3477, <p>[quote=Ulo Gulo;48994]</p> <p>Thanks a lot KazAnova I used&nbsp;your approach to find a bug in my data flow. It seems to work now.</p> <p>[/quote]</p>  <p>Could you tell us what the bug was?</p>
3477, <p>Now that you have beaten the benchmark are your CV scores close to LB now?</p>
3477, <p>My CV is off by some extent. Cant say the exact value right now. The problem is my last 3 models scored the same on the LB :D and had different CV. I have a model with CV AUC of 0.97 but I havent tried it on the LB yet as I'm afraid it might be overfitting :)</p>
3477, <p>[quote=Kevin Hu;50127]</p> <p>@Upul I also have this problem before and inspired by David Thaler the reason why is I put the test data to do feature selection the result is very optimistic I don't know if you to do feature selection as I am.&nbsp;usually It must be the test data already&nbsp;exposure before test.</p> <p>[/quote]</p>  <p>how did you use test data for feature selection?</p>
3477, <p>Well I tried the first method long back. Got some good features and CV was 0.97 however LB score was 0.78 ;)</p>
3477, <p>No</p>
3477, <p>I would like to know the views of fellow competitors in this regard. As we know the test data has been inflated heavily what do you think is the actual test dataset size we are dealing with?</p>  <p>I say ~200</p>
3477, <p>:D</p>
3477, <p>after the competition ends all cheaters will be removed from the leaderboard. So dont worry :D</p>
3477, <p>As most of my submissions have the same score I was wondering if I don't choose any submissions which two submissions will be chosen by Kaggle?&nbsp;</p>
3477, <p>I think so too but I think a confirmation from one of the Admins would be nice</p>
3477, <p>[quote=Rogers F Silva;50790]</p> <p>Also make sure to upload all the code (including your feature selection strategies) required to replicate your selected submission models.</p> <p>[/quote]</p> <p>Do we need to upload our model before the timeline? I don't see any links for uploading the model.</p>
3477, <p>Thank you...</p>
3477, <p>and so many cheaters in this one. so dont worry about the ranks :P</p>
3477, <p>24mins to go... and we will know the luckiest person :P&nbsp;</p>
3477, <p>lolllll... my biggest fall...lol</p>
3477, <p>pretty amazing that a person who joined a couple of weeks back won with only one submission. must be a f*cking stable model! would really like to know about it :P</p>
3477, <p>[quote=Giulio;50961]</p> <p>I'm really glad I only spent&nbsp;maybe 6&nbsp;hours&nbsp;on this competition :-)</p> <p>[/quote]</p> <p>me too... per day :P</p>
3477, <p>this is my best:</p> <p>public: 0.59375 &nbsp; &nbsp;private: 0.85641</p> <p>:D im just laughing out loud....&nbsp;</p>
3477, <p>time to concentrate on large datasets.... :D</p>
3477, <p>[quote=Sandro;50977]</p> <p>I would have won if I would had chosen my worst submission according to the public leaderboard. Public: 0.62946 Private: 0.94359 &nbsp;:P</p> <p>[/quote]</p> <p>what model was that?</p>
3477, <p>model for which abs(public-private) &nbsp;~ 0 (or very less)</p>
3477, <p>[quote=Giulio;50972]</p> <p>BTW- congratulation David! Curious to know more about your model.</p> <p>[/quote]</p> <p>Really. The only model I would like to know about in this competition! Congrats David!</p>
3477, <p>[quote=Giulio;51031]</p> <p>No cheaters removed yet?</p> <p>[/quote]</p> <p>weird</p>
3477, <p>use the split command in terminal.</p>  <p>Also you have to use fit first then you have to do partial_fit with classes and then you can use partial_fit</p>
3477, <p>Fit on the first chunk of data that you have and partial_fit on the subsequent chunks</p>
3477, <p>[quote=G Alexander;52052]</p> <p>With current leader values around 0.44xx I don't see how this much better than tossing a coin. &nbsp;This advertising (click vs no-click) is only ~23% successful. I wonder if advertisers know this? &nbsp;</p> <p>This indicates to&nbsp;me that either the algorithms being used really aren't that good or the data is dirty i.e. noisy sampled or categorized poorly.</p> <p>[/quote]</p> <p>23% for click/no-click? I dont think so!&nbsp;</p>
3477, <p>whats the LB score?</p>
3477, <p>It seems the test data set provided is corrupt. Train expands without any problems</p>
3477, <p>Good old auc is like 0.99 already from the sample benchmark. &nbsp;</p>
3477, <p>bummer :P</p>
3477, <p>well it works for me..</p>
3477, <p>omg! you have two train sets!</p>
3477, <p>You stole my copyrighted topic name :P lol</p>
3477, <p>Thats a very high benchmark! &nbsp;:(</p>
3477, <p>Amazing!</p>
3477, <p>Amazing!</p>
3477, <p>^^sarcasm? Right?&nbsp;</p>
3477, <p>Congrats guys! You deserved it!!!</p> <p>Special congrats to Barisumog for becoming a Master!!!</p> <p>Looking forward to reading about your approach!</p>
3477, <p>Hello again!</p> <p>For all the beginners out there I'm providing you with a benchmark which will get you around ~0.29112 on the public leaderboard. The benchmark uses only 7 variables and runs in a couple of minutes. Go nuts!</p> <p>Dont forget to click &quot;Thanks&quot; as usual ;)</p>
3477, <p>8 gb</p>
3477, <p>[quote=ACS69;52770]</p> <p>[quote=rcarson;52767]</p> <p>[quote=James King;52765]</p> <p>Great answer I nominate Triskelion for <em>de facto&nbsp;</em>competition admin. Also for best avatar...</p> <p>[/quote]</p> <p>Absolutely</p> <p>[/quote]</p> <p>Yes Triskelion can be the Competitor's Union Rep! He has my vote</p> <p>[/quote]</p>  <p>+1</p>
3477, <p>a couple of mins after the competition ends!</p>
3477, <p>We still have anything can happen tuesday :P</p>
3477, <p>What about Stumbleupon? :P</p>
3477, <p>Cheaters were removed!&nbsp;</p>
3477, <p>[quote=Max Christ;53261]</p> <p>Based on what do you assume that I or Ben are&nbsp;cheaters? I can only assume that it happened&nbsp;because we uploaded the submissions from the same IP environment. (from work) I assume that there are even more people from our company who are participating in waggle competitions.&nbsp;</p> <p>But we used different servers and wrote different code.</p> <p>I feel bad because I spend a lot of time improving my models and then I get deleted without any notification. Maybe next time I score in the top 10 and I get deleted again. Whats the point in participating if in the end you can get arbitrarily&nbsp;deleted?</p> <p>[/quote]</p>  <p>I never said you cheated. Kaggle removes cheaters after every competition. If you think its a mistake follow ACS69's advice and write to them asap</p>
3477, <p>[quote=Soheil Hooshdaran;57401]</p> <p>What is R?</p> <p>[/quote]</p> <p>Question of the century!</p>
3477, <p>http://www.r-project.org/</p>
3477, <p>[quote=Steven Du;56286]</p> <p>Finally I&nbsp;manage to&nbsp;download these&nbsp;data.&nbsp;</p> <p>[/quote]</p> <p>how?</p>
3477, <p>Just starting this competition... ;)</p> <p>interictal = 0</p> <p>preictal = 1</p>  <p>am i correct?</p>
3477, <p>lol i dont know why i have negative vote in this one. if you know whats correct you can reply to this thread&nbsp;</p>
3477, <p>thanks... whats smb btw? :P</p>
3477, <p>somebody gonna get hurt real bad :P :P</p>
3477, <p>yes</p>
3477, <p>Yeah. &nbsp;It will happen if you don't read the rules properly!&nbsp;</p>
3477, <p>I think that is against the rules...&nbsp;</p>
3477, <p>Ive hit the wall. Cannot improve anymore no matter what I try.</p>
3477, <p>[quote=Rainman;52916]</p> <p>10 folds and standard deviations are:</p> <p>Ca: 0.099</p> <p>P: 0.3211</p> <p>pH: 0.0387</p> <p>SOC: 0.0676</p> <p>Sand: 0.0495</p> <p>(Yes I can't do P either :P)</p> <p>[/quote]</p> <p>this seems weird n incorrect</p>
3477, <p>Ahan I thought they are errors :D. Sorry about that!</p>
3477, <p>Let my rank drop &nbsp;:P</p>
3477, <p>the wait is over! :P</p>
3477, <p>Im afraid this is going to be similar to MLSP</p>
3477, <p>Hi Everyone!</p> <p>I'm back with a simple python script to beat the benchmark. The script is attached and is self explainatory!</p> <p>Let me know if you have any further questions.</p> <p>And please dont forget to &quot;vote-up&quot;!</p>  <p>LB score: 0.43621</p>
3477, 
3477, <p>[quote=AngryTomato;54146]</p> <p>the svm you used here is&nbsp;Multi-class classification? &nbsp;and besides train and test have 3594 cols why you use&nbsp;xtrain xtest = np.array(train)[::3578] np.array(test)[::3578] &nbsp;but no&nbsp;xtrain xtest = np.array(train)[::3594] np.array(test)[::3594]</p> <p>[/quote]</p>  <p>AngryTomato please don't be angry. I have included only the spectral features in the benchmark code ;)</p>
3477, <p>Look at the data indices I'm using</p>
3477, <p>Because its a benchmark code! I didnt want to post something which gives a top 10 rank. Try and include the spatial features and share your views...&nbsp;</p>
3477, <p>[quote=ACS69;54426]</p> <p>out of interest why do you feel the need to post high performance benchmark code? You pretty much have ruined Kaggle for me. thanks</p> <p>[/quote]</p>  <p>Why do you think its a high performing benchmark?</p>
3477, <p>As far as I remember the first person who used the benchmark was around 39-40</p>
3477, <p>[quote=ACS69;54437]</p> <p>And look at the Avito forum - you'll see Abhishek complaining about benchmark code</p> <p>[/quote]</p>  <p>Just because it was posted a week before the competition deadline.</p>
3477, <p><strong>Beating the Benchmark Version 2.0</strong> : If you create the dataset as specified in the data page i.e by removing the CO2 columns you will get a much higher score with the same old benchmark script.&nbsp;</p>
3477, <p>Great stuff. Anyone tried this yet?</p>
3477, <p>Hello all</p> <p>What do you guys think this competition's results are gonna look like? Is this going to be another MLSP?&nbsp;</p> <p>Any guesses on the winning score?</p>
3477, <p>You mean above the baseline? No improvements? That's kind of impossible</p>
3477, <p>Great stuff. &nbsp;What is the leaderboard score?&nbsp;</p>
3477, <p>I'll choose both. &nbsp;You are allowed to choose two submissions ;)&nbsp;</p>
3477, <p>Im looking for a team (individual) to merge with in order to share ideas and combine models. If anyone is interested please mail me :P</p>
3477, <p>&lt;0.40 in &lt;3mins</p>
3477, <p>Congrats to Yasser!!!!&nbsp;</p> <p><span style="line-height: 1.4">Damn &nbsp; yet again I'm not in top 3 :P</span></p>
3477, <p>Sorry guys &nbsp;as long as rules don't change about public sharing &nbsp;I'll keep on posting benchmarks... ;)&nbsp;</p>
3477, <p>[quote=ACS69;56465]</p> <p>[quote=Abhishek;56453]</p> <p>Sorry guys &nbsp;as long as rules don't change about public sharing &nbsp;I'll keep on posting benchmarks... ;)&nbsp;</p> <p>[/quote]</p> <p>Always the bridesmaid never the bride</p> <p>[/quote]</p>  <p>&nbsp;Haha.... &nbsp;Without you Kaggle and benchmarks are no fun! :D</p>
3477, <p>[quote=ACS69;56486]</p> <p>[quote=Abhishek;56482]</p> <p>[quote=ACS69;56465]</p> <p>[quote=Abhishek;56453]</p> <p>Sorry guys &nbsp;as long as rules don't change about public sharing &nbsp;I'll keep on posting benchmarks... ;)&nbsp;</p> <p>[/quote]</p> <p>Always the bridesmaid never the bride</p> <p>[/quote]</p> <p>&nbsp;Haha.... &nbsp;Without you Kaggle and benchmarks are no fun! :D</p> <p>[/quote]</p> <p>lol - same ;) I said to Kazanova that I wanted you to win this one as you deserved a prizewinner badge. But in the end glad I beat ya :P</p> <p>[/quote]</p>  <p>Im also glad that you beat me... :P :P you know why ...haha</p>
3477, <p>Can we see the individual private scores now?&nbsp;</p>
3477, <p>[quote=joycenv;56512]</p> <p>The leaderboard has now been verified. You should see both private and public leaderboard scores on your &quot;My Submissions&quot; page.</p> <p>[/quote]</p>  <p>Thank you. Is there something wrong with the points calculation?&nbsp;</p>
3477, <p>Any info on who is leading currently on private lb? :)&nbsp;</p>
3477, <p>Yep. &nbsp;That's my favorite one. &nbsp;I was in top 10 for the whole time line and then u can see wht happened! Lol</p>
3477, <p>Damn. I forgot the deadline and wanted to post a better model at the end. Anyways it was a good competition for me. I'll upload my model within a couple of days. Do winners get something ? ;)&nbsp;</p>
3477, <p>Great Stuff!</p>
3477, <p>[quote=James King;55838]</p> <p>[quote=Triskelion;55685]</p> <p>Not only is your online learning script able to beat the benchmark. It is able to get to #1!</p> <p>[/quote]</p> <p>Just to confirm - when adding the 46 hash interaction features to the algorithm I get a leader board score of&nbsp;0.0088416 can anyone confirm?</p> <p>[/quote]</p>  <p>Yes&nbsp;</p>
3477, <p>One more addition. &nbsp;Bits =24 with the 46 interaction features will give you my current score ;)&nbsp;</p>
3477, <p>whats the score?</p>
3477, <p>which part takes the most amount of time?</p>
3477, <p>Strange. It took me a few mins before the meta classifier.&nbsp;</p> <p>What score did 300 trees give you?</p>
3477, <p>lol</p>
3477, <p>Which sparse features did you create? Can you share the parameters of your SGDClassifier?</p>
3477, <p>Thats very common in advertising. People sleep at night and thats why there is no click</p>
3477, <p>Inspired by @tinrtgu (http://www.kaggle.com/users/185835/tinrtgu) I present to you a modified version of his script used in tradeshift challenge.</p> <p>LB Score: ~0.0885</p> <p>pypy takes around 5mins on my system.</p> <p>Vote up if this helped :D</p>
3477, <p>[quote=tinrtgu;56735]</p> <p>Do I need to vote up too?</p> <p>[/quote]</p> <p>No we will do that for you :D</p>
3477, <p>[quote=Eugene Nizhibitsky;56737]</p> <p>Do you actually have a cron task which monitors new competitions tests some methods and submits &quot;Beating ...&quot; posts? :)</p> <p>[/quote]</p> <p>Nopes... But thats a nice idea :P</p>
3477, 
3477, <p>It seems the revised dataset is available now! When will submissions open? :D</p>
3477, <p>[quote=Saptarshi Ray;57595]</p> <p>I have not found any click variable in test_rev2.csv.Do you guys also face the same thing</p> <p>[/quote]</p> <p>what will u predict if you have everything?</p>
3477, <p>v3. without change: LB:&nbsp;0.6931472</p>
3477, <p>[quote=ACS69;58335]</p> <p>strange - are you sure it's not writing the submission file wrong? as it's the sample submission score</p> <p>[/quote]</p> <p>sure. all predictions generated by v3 are 0.5</p>
3477, <p>[quote=Herimanitra;59523]</p> <p>If 'train.csv' is not comma separated It raises an error. Any hints?</p> <p>[quote=Yannick Martel;59216]</p> <p>@all</p> <p>....</p> <p>[ like in: python fast_solution_plus.py train --train train.csv -o first.model.gz ]</p> <p>...</p> <p>Yannick</p> <p>[/quote]</p> <p>[/quote]</p>  <p>Use:&nbsp;csv.DictReader(f delimiter='\t' quoting=csv.QUOTE_NONE)</p>
3477, <p>does it have any option to keep header intact?</p> <p>EDIT: found it&nbsp;</p>
3477, <p>shuffling did not give me a better score...</p>
3477, <p>[quote=lewis ml;58781]</p> <p>Just out of curiosity what's XBG? Or do you mean XGB?</p> <p>[/quote]</p> <p>xgboost.&nbsp;https://github.com/tqchen/xgboost</p>
3477, <p>More than half of the LB is based on same benchmark code. If people try new ideas the score should improve....</p>
3477, <p>I went to the store &nbsp;sat on Santa's lap &nbsp;asked him to bring Domcastro all kinds of crap :P</p>
3477, <p>For my current model it takes less than 30mins. I'm currently using a very simple model.</p> <p>EDIT: no feature selection and no subsampling.</p>
3477, <p>Use this:&nbsp;http://patternsonascreen.net/cuSVM.html</p>
3477, <p>just discussing and sharing everything offline...</p>
3477, <p>get 200gigs RAM use SGD from sklearn n u will get below 0.40 easily ;)</p>
3477, <p>There is something wrong with your test.csv</p>
3477, <p>do you have the dev version of&nbsp;libgfortran3?</p>
3477, <p>When will leaderboard be final?</p>
3477, <p>[quote=KazAnova;64170]</p> <p>[quote=Bluefool;64152]</p> <p>It is scary teaming up. I remember offending Kazanova asking if he had ever cheated before we teamed up in the Africa one.</p> <p>[/quote]</p> <p>I was not offended .I felt I had to quickly destroy all the evidence (delete my extra kaggle accounts namely Triskelion Phill Culiton and Abhishek  hide plagiarized papers burn my 3 different passports do a plastic surgery etc &nbsp;)!&nbsp;</p> <p>[/quote]</p> <p>hhahahaa :D :D</p>
3477, <p>Dataset has been updated due to some problems in the rating values</p>
3477, <p>[quote=Yogi Dhiman;58177]</p> <p>Trying this from past 4 days and I guess I am lost somewhere in half way.</p> <p>Some one can give me a Hint if I am using matrix factorization method !!! &nbsp;Abhishek :)</p> <p>[/quote]</p>  <p>http://sifter.org/~simon/journal/20061211.html this might help!</p>
3477, <p>Make it a regression task by one hot encoding the user and movie columns</p>
3477, <p>Thank you for participating. We will share methods soon.</p>
3477, <p>Version 2 is available now:&nbsp;https://inclass.kaggle.com/c/predict-movie-ratings-v2</p>
3477, <p>im confused. can anyone tell me how are the trainlabels related to training files?</p>
3477, <p>I dont think they will. its a pretty simple benchmark.</p>
3477, <p>or use this:&nbsp;http://www.kaggle.com/c/inria-bci-challenge/forums/t/11009/beating-the-benchmark :)</p>
3477, <p>Hello All</p> <p>I'm back with a quick script to beat the Random Forest benchmark using Random Forests :)&nbsp;</p> <p>The script is attached. If you dont understand anything feel free to ask.&nbsp;</p> <p>VOTE UP if it helped you in any way :)&nbsp;</p> <p>Thanks!</p> <p>comment with what score you get on LB</p>
3477, <p>updated and fixed some typos&nbsp;</p>
3477, <p>To the one who gave a - 1 please explain why!&nbsp;</p>
3477, <p>Sorry im not fond of R. :(</p>
3477, <p>or do this: sudo pip install numexpr</p>
3477, <p>that will just increase the chances of overfitting . the dataset is very small...</p>
3477, <p>They remove the cheaters after competition ends. So dont worry!</p>
3477, <p>i think this will be like africa challenge or MLSP challenge.&nbsp;</p>
3477, <p>Or maybe something wrong with the evaluation system like last year's Santa problem ;)</p>
3477, <p>This is crap</p>
3477, <p>I think rank 1 should post his code and submission so that we can verify if its genuine.</p>
3477, <p>[quote=Giulio;58910]</p> <p>[quote=Abhishek;58909]</p> <p>I think rank 1 should post his code and submission so that we can verify if its genuine.</p> <p>[/quote]</p> <p>Yes yes agreed. Just do it around 8PM Pacific Standard Time when most of Europe is asleep and the East Coast is going to bed.</p> <p>[/quote]</p> <p>Who cares. He already won the competition. So he can post the code. Also if its genuine Kaggle should &nbsp;close the competition as they already have optimal solution .</p>
3477, <p>just an advice dont try to get a lower score i already did it last year :P :P&nbsp;</p>
3477, <p>lol one more optimal solution after just one submission ... hahaha</p>
3477, 
3477, <p>There is no private dataset as this is an optimization problem.&nbsp;</p> <p><span style="line-height: 1.4">PP.S. I didn't down vote</span></p>
3477, <p>Always been always will be :D</p>
3477, <p>[quote=Alexander Ryzhkov;58980]</p> <p>[quote=William Cukierski;58961]</p> <p>Our users gave to us the answers without any fuss :-/</p> <p>[/quote]</p> <p>In my opinion it will be great to start another competition (Helping Santa's Helpers Part 2 for example) with corrected&nbsp;parameters and&nbsp;objective function and closed this competition as solved (even without money prize - ranking points and tiers are better present :) ). In this case all people in Kaggle community will be happy - contestants who solved this task get the ranks equal to their positions in the leaderboard and others can do the same&nbsp;in the Part 2 contest.</p> <p>Hope it will be so</p> <p>Alex</p> <p>[/quote]</p>  <p>Its better to restart this competition. And as William said Santa will talk to Kamil. I dont think the competition &nbsp;ran for enough time in order to award points and tiers.</p>
3477, <p>[quote=Alexander Larko;58991]</p> <p>Kamil must receive a cash prize!&nbsp;It is very fast (sparkling) solution !!!</p> <p>[/quote]</p> <p>I never said Kamil shouldn't receive any prize for the hardwork he has done. I meant starting a new competition and awarding ranks and points for this one makes no sense.</p>
3477, <p>[quote=Giulio;59066]</p> <p>How is Kaggle handling points and badges both for the first attempt and the re-start?</p> <p>[/quote]</p>  <p>i think its like other competitions which restart and get revised data. so everything should remain the same (hopefully!)</p>
3477, <p>Thanks ;)</p>
3477, <p>[quote=Skabed;66345]</p> <p>There is one thing I don't quite understand with the given code here. Why is it using the test sentences in the model pipeline? If I understand it correctly the input to the tfidf vectorizer are both the training sentences and the test sentences. I'm specifically refering to:</p> <p><em>tfv = TfidfVectorizer(...)</em></p> <p><em>X_all = traindata + testdata</em></p> <p><em>...</em></p> <p><em>tfv.fit(X_all)</em></p> <p>It kind of feels wrong to me tbh.&nbsp;</p> <p>[/quote]</p>  <p>Why?</p>
3477, <p>can anyone upload the full file for submission (just being lazy) :P</p>
3477, <p>Anyone offering ec2 with Tesla for the whole course of competition? :P :P</p>
3477, <p>Is there any difference between using opencv imread and skimage.io.imread? It seems im unable to beat even 5.0 if i use opencv with a validation score of 4.17</p>
3477, <p>Nevermind. i=0 ruined my model</p>
3477, <p>from the rules: &quot;Semi-supervised learning is permitted.&quot;</p>
3477, <p>Ahh... I wish I had GPU :-/</p>
3477, <p>[quote=gregl;61632]</p> <p>[quote=clustifier;61601]</p> <p>thanks.</p> <p>but why our md5's are different?</p> <p>[/quote]</p> <p>I built im2bin with blas not the default mlk.</p> <p>Also upstream the md5sum of train.lst is:</p> <p>399ab27e818cabcb5bd2158c15451a80 train.lst</p> <p>with the first 5 lines:</p> <p>3406 10 data/train/chaetognath_non_sagitta/23677.jpg<br>22212 90 data/train/radiolarian_chain/157296.jpg<br>19772 83 data/train/protist_fuzzy_olive/147898.jpg<br>23435 98 data/train/siphonophore_calycophoran_rocketship_young/36262.jpg<br>18710 72 data/train/hydromedusae_solmaris/47760.jpg</p> <p>[/quote]</p> <p>how did you build im2bin with blas when mshadow needs mkl?</p>  <p>EDIT: Nevermind</p>
3477, <p>is it possible to compile CXXNET without OpenCV?</p>
3477, <p>[quote=emolson;62784]</p> <p>using the software is fine but using pre-trained caffe networks violates the &quot;no external data&quot; rule.</p> <p>[/quote]</p>  <p>I dont think it does. Maybe some admin can clarify this..</p>
3477, <p>which version are of scikit-learn are you using? In 0.15.2 they improved RF and it doesnt copy the dataset. try 0.15.2 or 0.16-git</p>
3477, <p>Generally submissions&nbsp;are open after final deadline&nbsp;and public/private scores can both be seen for them.&nbsp;</p>
3477, <p>[quote=mike1886;65246]</p> <p>I am using a lasagne + nolearn setup you can check it out here:</p> <p>https://github.com/msegala/Kaggle-National_Data_Science_Bowl</p> <p>[/quote]</p> <p>is this your code? what is the logloss do you get with this?</p>
3477, <p>congrats to Sander et. al.</p> <p>http://benanne.github.io/2015/03/17/plankton.html</p>
3477, <p>n i just need a kaggle mug :-/</p>
3477, <p>[quote=Giulio;66403]</p> <p>[quote=Ben S;66401]</p> <p>they might</p> <p>[/quote]</p> <p>I noticed many folks&nbsp;are&nbsp;referring to Cardal as &quot;they&quot;. He/she is not a team on the LB and would be (I think) against the rule to have multiple people work together under one Kaggle id...</p> <p>Anything I'm missing?</p> <p>[/quote]</p>  <p>Then i think ISFA team should be removed as one member consists of many persons &quot;ISFA_students&quot;</p>
3477, <p>yes</p>
3477, <p>April Fool in advance :P :P&nbsp;</p>  <p><img src="http://d.ibtimes.co.uk/en/full/1371422/april-fools-day-meme.jpg" alt width="700" height="536"></p>
3477, <p>nobody likes jokes *sadmax* :(</p>
3477, <p>Welcome to v2.0 of the competition for University of Paderborn students. The dataset has now more samples and includes a timestamp too. Obviously the competition is open for all.!</p> <p>Happy Competition!</p>
3477, <p>Hi Mary</p> <p>The competition was closed due to some problems with data files. It will reopen as soon as the problems are resolved.</p>
3477, <p>A movie's rating wont remain the same over time. eg. In IMDB movies start with a rating of 8-9 and then obtain their optimum value over a course of time. How you can use this data is upto you!! ;)</p>
3477, <p>Great Stuff! Thanks :)</p>
3477, <p>whats the error?</p>
3477, <p>there is no problem with downloading of any files.</p>
3477, <p>[quote=Rachana Bagde;62874]</p> <p>@Leo Buettiker</p> <p>nrow shows correct values...but on submitting I get this</p> <p>ERROR: Could not parse 'NA' into expected type of Double (Line 1882 Column 9) ERROR: Could not parse 'NA' into expected type of Double (Line 2403 Column 9) ERROR: Could not parse 'NA' into expected type of Double (Line 5280 Column 8)...</p> <p>[/quote]</p> <p>There should be rating for every ID in test set. You cannot submit NA as any rating!</p>
3477, <p>You still have NA in the submission file.</p>
3477, <p>One submission per day is too much... Can we change it to 1 per month?</p>
3477, <p>I'd also like to know the same...</p>
3477, <p>gimme a week ;)&nbsp;</p>
3477, <p style="text-align: left">No link to the paper?</p>
3477, <p style="text-align: left">Thanks everyone and congrats to phalaris srk and Marios.&nbsp;</p> <p style="text-align: left">We are going to keep our approach a secret ;)</p>
3477, <p style="text-align: left">Why does it take so much time these days to finalize leaderboard enable private scores of all submissions and awarding of points?</p> <p style="text-align: left">Cheater removal takes some time for LB and points but private scores should be available as soon as competition ends right?</p>
3477, <p>I just wanted to see my private scores without submitting individual files again :)</p>
3477, <p>If you submit the brackets on his website he might be able to use it on kaggle and may get a better rank than yours.. so beware!</p>
3477, <p>download the cookies of kaggle and use the following:</p> <p>wget -x --load-cookies cookie_file.txt <em>url_to_dataset</em></p>
3477, <p>Well Im not using lynx. I also have google login to kaggle and the above command works perfectly fine for me.</p>
3477, <p>did you try this:&nbsp;</p>  <p>wget -x --load-cookies cookies.txt --no-check-certificate&nbsp;https://www.kaggle.com/c/malware-classification/download/train.7z</p>  <p>or simply:</p> <p>wget -x --load-cookies cookies.txt http://www.kaggle.com/c/malware-classification/download/train.7z</p>
3477, <p>after some processing my training and test dataset size is approx 50mb. Im still trying to figure out how 0.02 logloss is even possible... :D</p>
3477, <p>[quote=WWW BIG - Cup Committee;63696]</p> <p>Yes it is possible :) and can even be improved.</p> <p>Did not expect it to happen so early but this is how sports is.</p> <p>Good luck to all!</p> <p>[/quote]</p> <p>Now I believe you! :D</p>
3477, <p>@jay: any success with FFT features yet?</p>
3477, <p>What does ?? represent in hexdump data?</p>
3477, <p>why not!</p>
3477, <p>Its same for others too...!</p>
3477, <p>nice!</p>
3477, <p>Also you can take a look at spot instances in AWS. there is a risk of losing data (you can always backup) but prices are way cheaper...</p>
3477, <p>[quote=Robert Fontaine;63953]</p> <p>Am I asking the right questions?</p> <p>...&nbsp;</p> <p>[/quote]</p> <p>nop!</p>
3477, <p>you dont have to load any data to dbms or anything. create a script that mines information from individual files and writes it to a separate file. getting a logloss of less than 0.1 will take you only a couple of hours using R or python.</p> <p>P.S. I dont think kaggle will go out of business anytime soon :)</p>
3477, <p>I dont know if you can do that with 7z. I had enough space to decompress the dataset.&nbsp;</p>
3477, <p>Good Job. Did you try not using &quot;??&quot; ?</p>
3477, <p>;)</p> <p>P.S. SVNIT rocks! :P&nbsp;</p>
3477, <p>[quote=UD1989;65025]</p> <p>hey guys how much time does the data consolidation script take to run ? Its been on for 3 hours now.....wondering if the changes i made to the scripts are causing the problems.....</p> <p>[/quote]</p> <p>if you read the very first post carefully you must know that it takes around 6 hours! Come on please read the post instead of blindly using the benchmark scripts!!!</p>
3477, <p>its a reserved space</p>
3477, <p>[quote=eNVy;64166]</p> <p>Hi guys even I m a learner I saw in datasample.7z that we were provided .asm and .bytes but in training.7z there are no .bytes files so now do we have to use &quot;hexdump&quot; to convert the .asm to .bytes?</p> <p>[/quote]</p> <p>there are both asm and bytes files in training.7z. Either your extraction failed or there was some download error.</p>
3477, <p>[quote=Shanky Sharma;64165]</p> <p style="text-align: justify">Hi</p> <p style="text-align: justify">I am new to data science and machine learning a complete novice. I took part in this competition to learn practical applications of ML. When I went through the training data I found it is made up of &quot;.asm&quot; files any ideas on how I can extract features from these files. I am a newbie in the field.</p> <p style="text-align: justify">&nbsp;</p> <p style="text-align: justify">Also is this the right approach?</p> <p style="text-align: justify">Thanks in advance. :)</p> <p style="text-align: justify">&nbsp;</p> <p style="text-align: justify">P.S.: Good-luck everyone.</p> <p>[/quote]</p>  <p>Take a look at the benchmark post in the forums. Although it deals with the bytes files it will give you some idea.</p>
3477, <p>Any guesses?&nbsp;</p>
3477, <p>i meant the score.. :D</p>
3477, <p>already 0.007 :D</p>
3477, <p>given the amount of data in this competition its too easy to overfit...&nbsp;</p>
3477, <p>58kxhXouHzFd4g3rmInB is empty... =&gt; ?????</p>
3477, <p>wont you interpret &quot;??&quot; bytes as only gaps?</p>
3477, <p style="text-align: left">They are one of those 10k files</p>
3477, <p>i meant some of them. I havent checked which ones (havent used this info in my model yet).</p>
3477, <p>http://www.winmd5.com/</p>
3477, <p>[quote=David Tran;64789]</p> <p>Interesting ideas but I would start with something much simpler. How about byte frequencies? There are only 256 different bytes so for every file you can simply record how many times each byte appears. Ignore any location information. That way you can reduce the training/test set from half a terabyte to a few MB. Even with such a drastically reduced data set you can achieve a log loss of less than 0.2 I believe.</p> <p>[/quote]</p> <p>&lt; 0.02!</p>
3477, <p>[quote=LLMSI;65273]</p> <p>1- Is the order of the test files in the submitted probabilities file important? Or can they have any order as long as the Id column is specified?</p> <p>2- Should the first column having the file name always have &quot;quotations&quot;?</p> <p>Thanks in advance</p> <p>[/quote]</p> <p>1-No</p> <p>2-No</p>
3477, <p>None of them are blank.</p>
3477, <p>[quote=Vinh Nguyen;67680]</p> <p>Just wondering whether anyone has tried this idea with generating 255M n-grams :-)</p> <p>[/quote]</p> <p>yes!</p>
3477, <p style="text-align: left">Will this be published somewhere?</p>
3477, <p>How do I cite this dataset?</p>
3477, <p>[quote=WWW BIG - Cup Committee;73140]</p> <p>You can cite BIG at WWW together with a link to the kaggle dataset.</p> <p>Are you doing any work on this dataset that will be interesting fro presentation at the conference?</p> <p>[/quote]</p> <p>I am still doing some work on this competition and writing a paper. I would be interested in presenting at WWW BIG but I dont think my private rank will allow me to do so.... or is it possible? :)</p>
3477, <p>Great!</p>
3477, <p>Is re-scoring on this one over?</p>
3477, <p style="text-align: left">Hand labelling I bet!!!</p>
3477, <p>what about the 2 after top2 ? :P</p>
3477, <p>Lol</p>
3477, <p>[quote=Stergios;71679]</p> <p>[quote=Abhishek;70092]</p> <p style="text-align: left">Hand labelling I bet!!!</p> <p>[/quote]</p> <p>Do you still bet on that one? :)</p> <p>[/quote]</p> <p>$10 bucks says its handlabeling for all teams above us.</p>
3477, <p>I think everyone in top 10 already achieved 99.9% accuracy.</p>
3477, <p style="text-align: left">Its not about 0.00. We never used 0.0. It was done only to show that its possible. There were a lot of other reasons for the drop. And we will release everything soom</p>
3477, <p>yes yes yes....!</p> <p>if selected are flights covered? ;)</p>
3477, <p style="text-align: left">You cannot make a submission to this competition anymore as the first submission deadline is over.</p>
3477, <p>Nothing!</p> <p>It was clearly mentioned in the Rules that you accepted before downloading the data. It is also on top of every page of this competition.</p>
3477, <p>move it to recycle bin</p>
3477, <p>with FFFFFFFF + 9000 features I think we are the ones who created most number of features ;)</p>
3477, <p>Thanks Bing! xgboost helps a lot. Dont know what the private LB will bring but our is solely based on xgboost with a lot of feature engineering and feature selection :D</p>
3477, <p>btw we dont think there is a data leak (or maybe we couldn't find one). Its all about features.!</p>
3477, <p>[quote=Bing Xu;72186]</p> <p>Awesome! I should suggest Tianqi to write a formal publication about XGBoost!</p> <p>[/quote]</p> <p>That would be nice. Since we are using it in one of our research and writing a paper but dont know how to reference it.</p>
3477, <p>We have submissions that could have got a very good rank. I think in the end it was just because of one malware that fucked up our results. Anyways we will post the best result on private set with all the documentation about features soon.... Maybe it will help others...</p>
3477, <p>wait for them to finalize the LB and then u have both public and private scores.</p>
3477, <p>[quote=&#924;&#945;&#961;&#953;&#959;&#962; &#924;&#953;&#967;&#945;&#951;&#955;&#953;&#948;&#951;&#962; KazAnova;72454]</p> <p>[quote=Michael George Hart;72408]</p> <p>&nbsp;I think I could have won this completion easily ....</p> <p>[/quote]</p> <p>There are many other kaggle competitions going on at the moment for you to <em>easily</em> win.</p> <p>[/quote]</p> <p>If only it was so easy. ;)</p>
3477, <p>When will we know all the private scores without submitting each and every file? When will the leaderboard be finalized?</p>
3477, <p>Will write about it soon ;)</p>
3477, <p>Also is it possible to provide md5 of the datasets?</p>
3477, <p>We dont need predictions for all the files in the test set?</p>
3477, <p>as usual the mistake is on my part. Extraction of files got terminated and i was confused why sample submission has less rows....pff!&nbsp;</p> <p>Thanks!</p>
3477, <p>its already on data page.</p>
3477, <p>[quote=Jaco Cronje;64472]</p> <p>Can someone maybe center crop and resize all the images to something like 256x256 or 512x512 and save them as raw .png files please. I really want to have a go at this competition but the download size is just way to large to download. It will take a couple of days or weeks for me to download all the original files.</p> <p>I'm sure the data can be reduced to one file that is less than 8GB.</p> <p>[/quote]</p> <p>Do you also need someone to write a getting-started code with some image transformations and convolutional neural nets?</p>
3477, <p>have you seen the license of scikit-learn? Which rule says we cannot use it?</p>
3477, <p>[RESERVED]</p>
3477, <p>umm? I havent checked how much the code scores yet... :-/</p>
3477, 
3477, <p>Hi All</p>  <p>Attaching a fast image resize script (in case anyone needs it :))</p> <p>Takes 40mins to resize all images (train+test) to 96x96 on 32 cores.</p> <p>Vote up if it helped ;)</p>
3477, <p>[quote=&#924;&#945;&#961;&#953;&#959;&#962; &#924;&#953;&#967;&#945;&#951;&#955;&#953;&#948;&#951;&#962; KazAnova;66737]</p> <p>not many samples</p> <p>predefined set of features</p> <p>Classification</p> <p>All you need is a &quot;beat the benchmark code...&quot;</p> <p>Boom</p> <p>[/quote]</p> <p>and now you have the last one in the list ;)</p>
3477, <p>[quote=&#924;&#945;&#961;&#953;&#959;&#962; &#924;&#953;&#967;&#945;&#951;&#955;&#953;&#948;&#951;&#962; KazAnova;66829]</p> <p>[quote=Abhishek;66762]</p> <p>and now you have the last one in the list ;)</p> <p>[/quote]</p> <p>Haha nice. My trick worked! I don't need to prepare a readcsv-and-model-and-make-predictions' code! :D&nbsp;</p> <p>[/quote]</p> <p>lol</p>
3477, <p>Hi Folks</p> <p>As usual I present to you the beating the benchmark script for this competition. The script is in python and I have commented most of the stuff. Its pretty simple and should give a score of 0.60 on the leaderboard. With some optimization you can achieve my current score.</p> <p>Feel free to ask questions in case of any doubts in the script.</p> <p>Don't forget to <strong>vote-up</strong> in case this script helped you in any way.</p>
3477, <p>Well this code can give you top 3! :D</p>
3477, <p>yes yes i forgot to remove it :D</p>
3477, <p>I did not crossvalidate. The model takes about 10mins on 8 cores.</p>
3477, <p>[quote=Foxtrot;67116]</p> <p>Hey! People! This is Abhishek's BTB thread. One more post about R and I'm gonna enter the competition! Consider yourself warned.</p> <p>[/quote]</p> <p>haha...</p>
3477, <p>[quote=Colin;67114]</p> <p>In text mining idf is used for down-weighting those words that appear too frequently overall like 'the' 'a' etc. I guess here the organizer maybe already did sort of feature filtering as the size of features is not that big. Using idf may not be the best choice to weight the features.</p> <p>[/quote]</p> <p>Could be. I didnt check! I wanted to add something extra to the code except a simple RF on the provided features. ;)</p>
3477, <p>[quote=Foxtrot;67219]</p> <p>[quote=Triskelion;67123]</p> <p>I found this slideshow&nbsp;by&nbsp;<a href="https://www.kaggle.com/users/17379/xavier-conort">Xavier Conort&nbsp;</a>about <a href="http://fr.slideshare.net/DataRobot/final-10-r-xc-36610234">10 R packages to win Kaggle competitions</a>. I didn't know where to put it so I figured this place would be as good as any.</p> <p><sub>And thank you for the benchmark!</sub></p> <p>[/quote]</p> <p>OK that was it.</p> <p>[/quote]</p> <p>aRRRRRRRRRR...........Damn!</p>
3477, <p>Then I think I should write one more :D&nbsp;</p>
3477, <p>[quote=Dipanjan Paul;67737]</p> <p>I am new to Python. Is there anything specific that has to be coded in Python to use multiple cores (when multiple cores are available available).. like using the multiprocessing package. Or does the Python RandomForestClassifier package does it automatically (using multiple cores) while training.</p> <p>[/quote]</p>  <p>use n_jobs in&nbsp;RandomForestClassifier to specify number of cores. n_jobs=-1 means all cores will be used.</p>
3477, <p>[quote=Neil Slater;70121]</p> <p>I don't think the TFIDF transformation adds much&nbsp;- if anything - for this data. See what happens when you comment out those lines . . .</p> <p>[/quote]</p>  <p>As I already mentioned I added tfidf since it was counts data and i wanted to add something extra to the code rather than just putting an RF in there...</p>
3477, <p>okay. I couldnt stop myself after this post. I can get 0.47 with NN</p>
3477, <p>[quote=Daia Alexandru;67197]</p> <p>Hello&nbsp;Abhishek and&nbsp;mproust could &nbsp;you &nbsp;give us some &nbsp;clues &nbsp;about your ann &nbsp;topologies?Tnks</p> <p>[/quote]</p> <p>I cannot give you the exact architecture right now (maybe at a later stage) since im still tuning the network....</p>
3477, <p>[quote=mproust;67210]</p> <p>[quote=Daia Alexandru;67197]</p> <p>Hello&nbsp;Abhishek and&nbsp;mproust could &nbsp;you &nbsp;give us some &nbsp;clues &nbsp;about your ann &nbsp;topologies?Tnks</p> <p>[/quote]</p> <p>Input/Hidden/Output : 93/200/9</p> <p>Activation : ReLU</p> <p>Regularization : L2 with 0.001</p> <p>[/quote]</p> <p>I think a simple RF can give you better results than this one.</p>
3477, <p>[quote=mproust;67213]</p> <p>[quote=Abhishek;67211]</p> <p>[quote=mproust;67210]</p> <p>[quote=Daia Alexandru;67197]</p> <p>Hello&nbsp;Abhishek and&nbsp;mproust could &nbsp;you &nbsp;give us some &nbsp;clues &nbsp;about your ann &nbsp;topologies?Tnks</p> <p>[/quote]</p> <p>Input/Hidden/Output : 93/200/9</p> <p>Activation : ReLU</p> <p>Regularization : L2 with 0.001</p> <p>[/quote]</p> <p>I think a simple RF can give you better results than this one.</p> <p>[/quote]</p> <p>Yeah I think so. Btw I don't have much experience on this area I was just toying around with basic models :)</p> <p>[/quote]</p> <p>Me too. I started NN yesterday.... No good results though !</p>
3477, <p>I dont understand why would it take 2 hours to train a simple model. You must be doing something wrong!</p>
3477, <p>This metric is being used to penalize heavily in case you are doing a wrong prediction.&nbsp;</p> <p>[quote=Bluefool;67058]</p> <p>[quote=Dean McKee;67055]</p> <p>Are you using R (I'm guessing so because of GLM) -</p> <p>You can use my (or anyone else's code) from this post to calculate log-loss.</p> <p>http://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/12895/compute-score-before-submission</p> <p>[/quote]</p> <p>So this problem has to be dealt with as multinomial because of the metric?</p> <p>[/quote]</p>
3477, <p>Im using a OVA approach</p>
3477, <p>Have you seen the class distribution? Its pretty messed up ;)&nbsp;</p>
3477, <p>my submission is stuck :(</p>
3477, <p>now i think i should post 0.45 :P</p>
3477, <p style="text-align: left">Then you are overfitting or doing wrong cross validation. Use stratified cv. The cv and lb scores in this competition are quite close</p>
3477, <p>So now we are giving out all the parameters too? Great!</p>
3477, <p>deja vu (i miss domcastro :P)&nbsp;</p>
3477, <p>not RBM but i tried some black box approaches  (neural networks!)</p>
3477, <p>no good results though . im happy its not image data... beat me CNN! &nbsp;lol</p>
3477, <p>read both the documentations again. xgboost(tree vs. linear). sklearn gbc (init loss) and you will find the answer.&nbsp;</p> <p>also What are your corresponding sklearn gbc params?&nbsp;</p>
3477, <p>found this somewhere (might be useful):</p> <p>gbm ------ xgboost<br>n.trees&nbsp;------&gt; nrounds<br>interaction.depth ------&gt; max.depth<br>bag.fraction ------&gt; subsample<br>distribution ------&gt; objective<br>n.cores ------&gt; nthread<br>n.minobsinnode ------&gt; min_child_weight?</p>
3477, <p>[quote= valerio orfano;71904]</p> <p>Sorry guys which metric are you using to evaluate the best algorithm?</p> <p>[/quote]</p> <p>We are using multiclass-logloss. The metric which is used to evaluate entries on the leaderboard.</p>
3477, <p>golden features? reminds me of loan default prediction....</p>
3477, <p>Good. i can resume now... pff!</p>
3477, <p>from main page: 2000+ players 14000+ submissions</p>
3477, <p>http://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/13102/wow/68989#post68989</p>
3477, <p>I can assure you that no one from our team uses multiple kaggle accounts...</p>
3477, <p>[quote=rakhlin;77703]</p> <p>[quote=skwalas;77699]</p> <p>I also imagine that a large number of the top 1000 are overfitting to the leaderboard</p> <p>[/quote]</p> <p>Impossible - just look at number of submissions half of them is&nbsp;single digits and the other half is&nbsp;low too.</p> <p>[/quote]</p> <p>too hard to overfit in this competition :P :P&nbsp;</p>
3477, <p>too good solution at the end of competition!</p>
3477, <p>Who told you to be away and miss everything?!</p>
3477, <p>[quote=Nicholas Guttenberg;78844]</p> <p>Good luck!&nbsp;Is there a traditional Kaggle celebratory ritual for when a contest is completed?</p> <p>[/quote]</p> <p>All participants do a jungle dance at home and upload it on YouTube after competitions are over :P</p>
3477, <p>which one is the private lb?</p>
3477, <p>@josef what was the inhouse bechmark?</p>
3477, <p>[quote=&#924;&#945;&#961;&#953;&#959;&#962; &#924;&#953;&#967;&#945;&#951;&#955;&#953;&#948;&#951;&#962; KazAnova;79217]</p> <p>Congrats for your victory Gilberto/Semenov it was a stellar performance.&nbsp;</p> <p>I have to stress though that I am a little bit concerned about some top 10 performances with very small number of submissions that also seem to share common past . 3 out of top 10 submissions seem to have come from people that attended the same Russian school?</p> <p>Anyway hope it is ok it would be interesting to see their solutions too and how they differ.</p> <p>[/quote]</p> <p>I agree with you many people have spent so much time in this competition it is important to get it right and shred any glimpse of concern...</p>
3477, <p>Thanks Mike!</p>
3477, <p>[quote=inversion;79593]</p> <p>There was one person in the top 20 I wasn't surprised got removed. Otto was this person's&nbsp;first and only contest. While that's possible of course it also reeks of cheating.</p> <p>[/quote]</p> <p>Who?</p>
3477, <p>[quote=William Cukierski;79612]</p> <p>Cheaters removal is done!</p> <p>[/quote]</p>  <p>Points coming soon?</p>
3477, <p>[quote=C&#233;dric Gouy-Pailler;79774]</p> <p>Tiny bug: Public leaderboard rankings are displayed in profiles</p> <p>[/quote]</p> <p>I confirm that</p>
3477, <p>[quote=inversion;67065]</p> <p>[quote=Jerry Lin;67054]</p> <p>Hey I am not so clear about the features you use to describe each product. Can you provide more explanations about feature?</p> <p>Thanks!</p> <p>[/quote]</p> <p>Would your model be any different if Feature_1 was the number of 5-star reviews or say the number of times&nbsp;someone left a review with the word &quot;banana&quot; in it?</p> <p>[/quote]</p> <p>lol.... mine would be :D</p>
3477, <p>So I think you should trust your CV? ;)</p>
3477, <p>[quote=barisumog;68312]</p> <p>[quote=Abhishek;68306]</p> <p>So I think you should trust your CV? ;)</p> <p>[/quote]</p> <p>That's all we can hold on to. But if the &quot;real&quot; test set is as small as the train set then a single restaurant with 30M revenue will turn&nbsp;the whole competition into a lottery.</p> <p>[/quote]</p> <p>Well in this case I think a transformation of the labels might help (combined with a trusted cross-validation)?</p>
3477, <p style="text-align: left">Are you not allowed to transform twice? Wait for my btb to know more ;)</p>
3477, <p>If you find it please let me know :D</p>
3477, <p>how are you doing CV with such small data? I dont think a 70/30 or 80/20 cv score will correlate to leaderboard score at all.&nbsp;</p>
3477, <p>Kaggle points will always be awarded unless you are kicked out of the LB after the competition finishes. You can choose whether you want to accept the prize or not.&nbsp;</p>
3477, <p>Hi All</p> <p>After Otto challenge this is my benchmark model for ECML/PKDD 2015.&nbsp;</p> <p>The script uses first and last but one latitude and longitude values. I know I have mixed up the names while creating the training set but they are corrected for submission file.</p> <p>The script is in python and I have commented several blocks of code. So it should be easy to understand. In case of any doubts feel free to ask here.</p> <p><strong>LB Score:</strong> <strong>&lt; 3.17</strong> (Better than my current score) [Unverified]</p> <p><strong>Vote Up</strong> if this helped you in any way!&nbsp;</p> <p>Enjoy!</p>
3477, <p>If anyone uses this as it is it would be nice if he/she could confirm the LB score and save me a submission ;)</p>
3477, <p>Why dont you just convert this code :D</p>
3477, <p>[quote=Paul H;73025]</p> <p>Thanks Abhishek.&nbsp; Your benchmark scored 3.22084.&nbsp; Some people may have problems running it as it wanted to use 17 GB RAM...</p> <p>[/quote]</p> <p>Hmm its weird that it scores 3.22 since i used only 10 estimators for my score. maybe random seed problem. It doesnt take 17 gb it takes around 13-14. but yea im working on optimizing it! :)</p>
3477, <p>[quote=NxGTR;73103]</p> <p>I tried as well but is asking 16GB of RAM (I only got 14.5GB free) I managed to run it by sub-sampling the training set and it scored 3.5XXXX</p> <p>[/quote]</p>  <p>Its all because of the stringified list of lists. Must look for a workaround.</p>
3477, <p>Great! Did you see any difference if using asm.literal_eval instead of json.loads. Do you mind if i add this to my script ?</p>
3477, <p>also what happens when u use the full list?</p>
3477, <p>[quote=DEBASHISH ROY;77455]</p> <p>hi abhishek can you plz try Haversine formula instead of Random Forest Regressor ?&nbsp;</p> <p>[/quote]</p> <p>I have no idea what that means....</p>
3477, 
3477, 
3477, 
3477, 
3477, <p>No more benchmarks from me :)</p>
3477, <p>@ben okay :D but let that script be there. I'll modify it soon such that some useful results are produced for the Kaggle community :)</p>
3477, <p>Hi All</p> <p>I present to you a very simple Script to beat the all the current benchmarks and score <strong>0.71+</strong></p> <p>I have commented the script so its easy to understand.</p> <p>In case of any questions feel free to ask.</p> <p><strong>Script</strong>:&nbsp;http://bit.ly/BeatTheBench</p> <p>Thanks!</p>
3477, <p>[quote=Foxtrot;78038]</p> <p>No... Again?</p> <p>[/quote]</p> <p>Couldn't stop myself ;)</p>
3477, <p>[quote=aldente;79708]</p> <p>Edit: One more question why Abhishek is at the 329th position of LB? Does he give up? Thanks.</p> <p>[/quote]</p>  <p>Im taking some rest. Will resume next week :)</p>
3477, <p>yes. u can make submissions even after the deadline.</p>
3477, <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>Are there any?? :D</p>
3477, <p>seems like people dont like jokes :P&nbsp;</p>
3477, <p>[quote=Gert;82015]</p> <p>[quote=Wendy Kan;82013]</p> <p>since this is now available to anyone you can now exploit this data (and this data only) before the end of the competition.&nbsp;</p> <p>[/quote]</p> <p>I can't believe that... it implies that&nbsp;the rules change one day before the competition is over?</p> <p>[/quote]</p> <p>They dont have any other option</p>
3477, <p>[quote=Bluefool;82026]</p> <p>The only reason you don't care is because you've already been using this data</p> <p>[/quote]</p> <p>+1</p>
3477, <p>Great! Its good and works only for the top 3 (since models are evaluated after the competition closes). What can be done about other participants who just care about &quot;good&quot; rank?</p>
3477, <p style="text-align: left">U stole my trademark topic name :P</p>
3477, <p>Hi All</p> <p>Some of you might remember this script of mine.</p> <p>This gives a Public LB Score of <span style="text-decoration: line-through">0.466&nbsp;</span> 0.54</p> <p>In case of any questions just ask! :)</p> <p>Thanks all!</p> <p>Oh yea the script is located here: https://www.kaggle.com/users/5309/abhishek/crowdflower-search-relevance/beating-the-benchmark</p> <p>EDIT: add new version. more preprocessing feature selection scaling and replace model</p> <p>EDIT-2: Add grid search for parameter selection for SVD and SVM. Optimize for quadratic weighted kappa.</p>
3477, <p>[quote=NxGTR;77984]</p> <p>What happened with the original post? next challenge: &quot;Find the probability to lose a post&quot; :D</p> <p>[/quote]</p> <p>If only you were watching the forum :D</p>
3477, <p>[quote=David Tran;77985]</p> <p>If asking for upvotes is forbidden maybe you can ask for right swipes on Tinder instead.</p> <p>[/quote]</p> <p>hahaha... :D :D&nbsp;</p>
3477, <p>A little birdie told me that it had never seen a SVM benchmark for Kaggle Competitions (I dont know how true is that). But I took it seriously and decided to take it to neext level.</p> <p>The newer version of script uses feature selection from sparse data preprocessing and a better model (SVM). Enjoy!</p> <p>In case of any questions feel free to ask &nbsp;( Triskelion will always be there to answer :P lol )&nbsp;</p> <p>;)</p>
3477, <p>[quote=Bluefool;78753]</p> <p>It's not a good idea really. However these benchmarks always seem to be lucky for the leaderboard. Bad practice but good leaderboard score. Happens all the time.</p> <p>[/quote]</p> <p>It is a benchmark code and its supposed to be dirty. You should be the one to clean it and use it. If that's a problem I'll post a much CLEANER version of the code with much better score both in cross validation and leaderboard.!</p>
3477, <p>UPDATE: I have updated my script with grid search for parameter selection. It is only 2 folds since kaggle doesnt offer&nbsp;enough resources for scripts (for obvious reasons). It directly optimizes for quadratic_weighted_kappa.</p> <p>Fitting 2 folds for each of 4 candidates totalling 8 fits<br>[CV] svm__C=1.0 svd__n_components=120 ...............................<br>[CV] ...... svm__C=1.0 svd__n_components=120 score=0.095201 - 14.0s<br>[CV] svm__C=1.0 svd__n_components=120 ...............................<br>[CV] ...... svm__C=1.0 svd__n_components=120 score=0.109611 - 13.9s<br>[CV] svm__C=10 svd__n_components=120 ................................<br>[CV] ....... svm__C=10 svd__n_components=120 score=0.466751 - 14.3s<br>[CV] svm__C=10 svd__n_components=120 ................................<br>[CV] ....... svm__C=10 svd__n_components=120 score=0.459042 - 14.4s<br>[CV] svm__C=1.0 svd__n_components=140 ...............................<br>[CV] ...... svm__C=1.0 svd__n_components=140 score=0.137393 - 15.9s<br>[CV] svm__C=1.0 svd__n_components=140 ...............................<br>[CV] ...... svm__C=1.0 svd__n_components=140 score=0.138725 - 16.1s<br>[CV] svm__C=10 svd__n_components=140 ................................<br>[CV] ....... svm__C=10 svd__n_components=140 score=0.480929 - 16.6s<br>[CV] svm__C=10 svd__n_components=140 ................................<br>[CV] ....... svm__C=10 svd__n_components=140 score=0.459656 - 16.5s<br>Best score: 0.470<br>Best parameters set:<br> svd__n_components: 140<br> svm__C: 10</p>  <p>best score output will be less than LB because of only 2folds. The Leaderboard score should not change. 0.54....!</p>
3477, <p>[quote=phalaris;78958]</p> <p>Thanks!&nbsp; I was trying to make a kappa scorer for sklearn but hadn't gotten around to finding the correct documentation.</p> <p>[/quote]</p> <p>I faced the same problem when I started :D</p>
3477, <p>[quote=portia brat;79309]</p> <p>Is there any intuition for using tfidf and PCA? &nbsp;My first thought was to just one-hot-encode all the queries and add a few other features.</p> <p>[/quote]</p> <p>Who said I'm using pca.? Anyways u have a better rank than mine so u know better. Did the benchmark help? ;)</p>
3477, <p>[quote=gand;79846]</p> <p>[quote=Abhishek;77983]</p> <p>Hi All</p> <p>Some of you might remember this script of mine.</p> <p>This gives a Public LB Score of <span style="text-decoration: line-through">0.466&nbsp;</span> 0.54</p> <p>In case of any questions just ask! :)</p> <p>Thanks all!</p> <p>Oh yea the script is located here: https://www.kaggle.com/users/5309/abhishek/crowdflower-search-relevance/beating-the-benchmark</p> <p>EDIT: add new version. more preprocessing feature selection scaling and replace model</p> <p>EDIT-2: Add grid search for parameter selection for SVD and SVM. Optimize for quadratic weighted kappa.</p> <p>[/quote]</p> <p>Hi&nbsp;Abhishek</p> <p>starting with your benchmark and stressing it a little bit further I am now 8th in the leaderboard so thanks :)&nbsp;</p> <p>[/quote]</p>  <p>Thanks! Feel free to share your findings in this thread ;)</p>
3477, <p>[quote=Aleix Gimenez;80027]</p> <p>Hi!</p> <p>A small question if refit=True in the GridSearchCV then you don't need to fit the model again with the best parameters isn't it? Thanks for the benchmark btw! :)</p> <p>[/quote]</p>  <p>Yes! That's correct...</p>
3477, <p>[quote=Elena Cuoco;80021]</p> <p>Thanks Abhishek! your Beating the Benchmark code is always a good starting point!</p> <p>[/quote]</p> <p>Glad you liked it...!</p>
3477, <p>I have updated the benchmark for you guys!</p>
3477, <p>[quote=yr;80049]</p> <p>For the current version what's the LB?</p> <p>[/quote]</p> <p>It says 0.57+ but I dont think u need it ;)</p>
3477, <p>[quote=Vectors;82987]</p> <p>Why is Kappa scorer necessary for this ? Just trying to get some idea.</p> <p>[/quote]</p> <p>since kappa is the evaluation metric i use kappa scorer to find optimal hyperparameters which maximize kappa..</p>
3477, <p>let me update my script with how to cross-validate...</p>
3477, <p>done! plesase check the script for cross-validated parameter selection</p>
3477, <p>I updated the code. so it should be fine now...</p>
3477, <p>[quote=Triskelion;80811]</p> <p>Created a script to show some peculiarities with the Quadratic Weighted Kappa metric in comparison to Mean Absolute Error and Classification Accuracy.</p> <p>https://www.kaggle.com/triskelion/crowdflower-search-relevance/kappa-intuition</p> <p><code>ONE EXTREME ERROR<br>Ground truth: [1 2 3 1 4 4 4 4 4 4]<br>Predicted : [4 2 3 1 4 4 4 4 4 4]<br>MAE : 0.3<br>Accuracy : 0.9<br>Kappa : 0.6564885496183206</code></p> <p><code>FIVE SMALL ERRORS<br>Ground truth: [1 2 3 1 4 4 4 4 4 4]<br>Predicted : [1 2 3 1 4 3 3 3 3 2]<br>MAE : 0.6<br>Accuracy : 0.5<br>Kappa : 0.7037037037037037</code></p> <p><code>KAPPA CHANGES WHEN DISTRIBUTION CHANGES<br>Ground truth: [1 1 3 1 4 4 4 4 4 4]<br>Predicted : [1 1 3 1 4 3 3 3 3 2]<br>MAE : 0.6<br>Accuracy : 0.5<br>Kappa : 0.75</code></p> <p>[/quote]</p>  <p>U just wanna snatch my kaggle swag. Dont you? :P</p>
3477, <p>what is the error message?</p>
3477, <p>Ha ha... I don't know what u r doing with the open source stuff.... But I'd love it if a script wins wins one (or beats mine in private)</p>
3477, <p>[quote=Bluefool;82930]</p> <p>Have you won yet? :P</p> <p>[/quote]</p> <p>I'd quit making benchmarks :P</p>
3477, <p>any updates from admins on this one?</p>
3477, <p style="text-align: left">This rule is understood.... U r just hogging the leaderboard....!!</p>
3477, <p>[quote=Mendrika Ramarlina;83092]</p> <p>[quote=Chenglong Chen;82666]</p> <p>In my experience 0.67~0.68 is achievable using single model with careful text cleaning and feature engineering. But that requires a lot of tuning. Ensemble ease the work and some other not very well performed models can also be used to improve the score. It worth a try (even simple averaging).</p> <p>[/quote]</p> <p>I was finally able to get 0.65+ with a single SVM model but it beats me how you guys get 0.68 with one model. It will be interesting to read your write-ups after the competition ends.</p> <p>[/quote]</p>  <p>come on man... I believe some top guys already have 0.69-0.70 using a single model with google/bing and amazon search results... lol..&nbsp;</p>
3477, <p>Have Fun!</p>
3477, <p>Sorry I dont know R. But it should be very easy and similar to the python code. Maybe someone else can share an R code :)</p>
3477, <p>I hope your issue is resolved?</p>
3477, <p>Ids are strings. Try 1.0 instead of 1.</p>
3477, <p>can you post some code snippet so that I better understand your problem?</p>
3477, <p style="text-align: left">Try &nbsp;1.0 instead of 1&nbsp;</p>
3477, <p>It will be the best private score out of those three..... Choose wisely ;)</p>
3477, <p>Make ids like 1.02.03.0......</p>
3477, <p>Could you please add .0 after each prediction? Eg: change 1 -&gt; 1.0 2 -&gt; 2.0 and so on ?&nbsp;</p> <p>Let me know if it works!</p>
3477, <p>give it sometime the competition just started. why dont you create one?</p> <p>anyways its coming soon... ;)</p>
3477, 
3477, <p>yes:&nbsp;https://www.kaggle.com/abhishek/avito-context-ad-clicks/beating-the-benchmark</p>
3477, <p>Based on tinrtgu's code:</p> <p>https://www.kaggle.com/abhishek/avito-context-ad-clicks/beating-the-benchmark</p> <p>All credits to&nbsp;<strong>tinrtgu </strong>!</p>
3477, <p>[quote=Pavitrakumar;80897]</p> <p>is that tinrtgu's code from Avazu&nbsp;competition? :P</p> <p>[/quote]</p> <p>yes</p>
3477, <p>[quote=Leustagos;85524]</p>  <p>It is worth mentioning that in this benchmark bits is defined wrongly. instead of using <strong>bits = 20 we should use bit = 2**20 or 2**24</strong> should be a better starting point. The its defined right now won't do much better than defining a single average.</p>  <p>[/quote]</p>  <p>Its not a bug and nothing wrong there. It was left for the users... ;)</p>
3477, <p>Hi All</p> <p>Attaching a beating the sample benchmark here. Python code. Code will be available here and on scripts (if enabled for this competition). Code is simple and well commented.&nbsp;</p> <p>Ask me questions if you have any!</p> <p>Don't forget to swipe right on Tinder if this post helped you :P&nbsp;</p> <p>Score &lt; 0.40</p>  <p><strong>MOVED TO</strong> :&nbsp;https://www.kaggle.com/abhishek/caterpillar-tube-pricing/beating-the-benchmark-v1-0</p>
3477, <p>Ahh I forgot to write... its only for girls... :P</p>
3477, <p>[quote=inversion;82920]</p> <p>@Abhishek - You are a beast!!</p> <p>[/quote]</p> <p>lol ... :D</p>
3477, <p>Because its a regression problem. Isnt it?</p>
3477, <p>[quote=NxGTR;82979]</p> <p>There is only one way to find out ;)</p> <p>[/quote]</p> <p>Yes! You have to walk 20miles everyday non-stop!&nbsp;</p>
3477, <p>how much does it give on the leaderboard?</p>
3477, <p>downvoter please explain the down-vote..</p>
3477, <p>Next time please choose the topic of your post carefully... I''d suggest you change it to &quot;newbie question&quot; or &quot;blah blah blah i dont know shit..&quot;</p>  <p>[quote=Artificial Brilliance;83823]</p>  <p>I played around with the data sets and ran them through Random Forest Regression and Neural Nets when it dawned on me &quot;we are missing vital data.&quot;</p>  <p>When attempting to predict anything in the real estate market you need more data points. The prime information missing is geographical location. Other vital factors are things like the age of the home size of the home the lot is it 2 stories and the home value. Even the average value of nearby homes is important. All this is readily available on the net via API.</p>  <p>The main reason these are important is home hazards are highly affected by weather. Plus the home value and neighborhood are good indicators of maintenance and up-keep habits.</p>  <p>That being said it seems like the important part of this contest is not what can be predicted from this dataset but what the dataset should include in the first place. 90% of the battle in data science is won or lost in deciding which data points to include and exclude before feeding it to algorithms. In this case the data points are not as relevant to the goal as assumed.</p>  <p>[/quote]</p>
3477, <p>Yep. Thanks!</p>
3477, <p>[quote=Nim J;84003]</p>  <p>He (Abhishek) has already got negative 14 votes on his comment. I guess that's enough.</p>  <p>[/quote]</p>  <p>29!</p>
3477, <p>What do you think?</p>
3477, <p>Did knn work for anyone in this competition?</p>
3477, <p>cold start item recommendation ;)</p>
3477, <p>this is also useful: <a href="https://www.kaggle.com/c/avito-prohibited-content/forums/t/9584/average-precision-at-k-ap-k">https://www.kaggle.com/c/avito-prohibited-content/forums/t/9584/average-precision-at-k-ap-k</a></p>
3477, <p>[quote=deluXe;88344]</p>  <p>@Abhishek I am the one that contacted your boss last week. ;-) </p>  <p>[/quote]</p>  <p>My boss??? I'm the boss! :P :P</p>
3477, <p>[quote=cash_FEG;88527]</p>  <p>@Admin <br> Could you please show us what is the 30% in the public leaderboard? <br> 30% of the users? coupons? or something else?? <br> Thank you.</p>  <p>[/quote]</p>  <p>Users</p>
3477, <p>Great Idea! Lets write a script for that :P</p>
3477, <p>LB score?</p>
3477, <p>Just a guess: maybe because he works for CERN...</p>
3477, <p>This competition is a joke</p>
3477, <p>[quote=Bluefool;88326]</p>  <p>As Gilles hasn't been removed from the leaderboard I take it that he is still eligible for the Kaggle ranking points?</p>  <p>[/quote]</p>  <p>There will be people in the end using the agreement test dataset just to gain Kaggle points. How do you plan to remove them? ;)</p>
3477, <p>Pretty impressive @NxGTR!</p>  <p>Did anyone check the dato benchmark??</p>
3477, <p>[quote=Jiming Ye;88944]</p>  <p>Still struggling with pre-processing</p>  <p>[/quote]</p>  <p>It seems you are using the provided script :P</p>
3477, <p>[quote=khyh;88950]</p>  <p>@Abhishek</p>  <p>Are you going to post the &quot;Beating the Benchmark ;)&quot; script?</p>  <p>[/quote]</p>  <p>Should I ?? :)</p>
3477, <p>Im getting 67536... thats the total number of HTML files?</p>  <p>EDIT: this issue is fixed.</p>
3477, <p>For example:</p>  <p>2851560_raw_html.txt 2309400_raw_html.txt 1650252_raw_html.txt .....</p>  <p>are empty...</p>
3477, <p>96 empty files in 1.zip:</p>  <pre><code>1934233_raw_html.txt 585865_raw_html.txt 1774087_raw_html.txt 3781567_raw_html.txt 2053747_raw_html.txt 2488771_raw_html.txt 2672551_raw_html.txt 1881199_raw_html.txt 3549997_raw_html.txt 1319761_raw_html.txt 3070561_raw_html.txt 1872079_raw_html.txt 2492173_raw_html.txt 3121537_raw_html.txt 449833_raw_html.txt 3314623_raw_html.txt 4011301_raw_html.txt 3737173_raw_html.txt 3965065_raw_html.txt 2265499_raw_html.txt 2639557_raw_html.txt 2886973_raw_html.txt 1352533_raw_html.txt 3200785_raw_html.txt 1282591_raw_html.txt 3914965_raw_html.txt 1198687_raw_html.txt 2933077_raw_html.txt 1432495_raw_html.txt 2796325_raw_html.txt 2165431_raw_html.txt 2962015_raw_html.txt 3458569_raw_html.txt 885133_raw_html.txt 2962183_raw_html.txt 2690749_raw_html.txt 3742045_raw_html.txt 3954739_raw_html.txt 1725697_raw_html.txt 2944663_raw_html.txt 631099_raw_html.txt 1735039_raw_html.txt 3517681_raw_html.txt 2119489_raw_html.txt 1191715_raw_html.txt 1804219_raw_html.txt 1595011_raw_html.txt 1826827_raw_html.txt 581011_raw_html.txt 1754875_raw_html.txt 1467841_raw_html.txt 2281339_raw_html.txt 834403_raw_html.txt 1259353_raw_html.txt 951001_raw_html.txt 3704143_raw_html.txt 50041_raw_html.txt 3744931_raw_html.txt 419689_raw_html.txt 2800789_raw_html.txt 1759495_raw_html.txt 1808281_raw_html.txt 1638835_raw_html.txt 2883499_raw_html.txt 2786701_raw_html.txt 2784949_raw_html.txt 2695483_raw_html.txt 3272413_raw_html.txt 2198791_raw_html.txt 1118797_raw_html.txt 3706777_raw_html.txt 1478647_raw_html.txt 245077_raw_html.txt 1380547_raw_html.txt 4009819_raw_html.txt 573613_raw_html.txt 2254621_raw_html.txt 3905005_raw_html.txt 1140229_raw_html.txt 849271_raw_html.txt 1471291_raw_html.txt 1700191_raw_html.txt 2424883_raw_html.txt 876931_raw_html.txt 892453_raw_html.txt 3572479_raw_html.txt 886957_raw_html.txt 630547_raw_html.txt 553219_raw_html.txt 1549573_raw_html.txt 2740135_raw_html.txt 876217_raw_html.txt 3290887_raw_html.txt 1381189_raw_html.txt 1696759_raw_html.txt 2281003_raw_html.txt </code></pre>
3477, <p>empty files in 0.zip:</p>  <pre><code>2851560_raw_html.txt 2309400_raw_html.txt 1650252_raw_html.txt 3709986_raw_html.txt 2193258_raw_html.txt 629352_raw_html.txt 2579274_raw_html.txt 3938196_raw_html.txt 3637380_raw_html.txt 2521542_raw_html.txt 2357040_raw_html.txt 1080840_raw_html.txt 496788_raw_html.txt 3047736_raw_html.txt 3441636_raw_html.txt 396888_raw_html.txt 1519344_raw_html.txt 1298244_raw_html.txt 1844358_raw_html.txt 551286_raw_html.txt 3159600_raw_html.txt 870672_raw_html.txt 3404412_raw_html.txt 2905926_raw_html.txt 244770_raw_html.txt 1625952_raw_html.txt 12504_raw_html.txt 2183268_raw_html.txt 3946368_raw_html.txt 729306_raw_html.txt 1437642_raw_html.txt 3922974_raw_html.txt 3316254_raw_html.txt 2574126_raw_html.txt 3437664_raw_html.txt 408294_raw_html.txt 716688_raw_html.txt 4047252_raw_html.txt 68640_raw_html.txt 3136470_raw_html.txt 1438740_raw_html.txt 2672976_raw_html.txt 26580_raw_html.txt 733914_raw_html.txt 2919048_raw_html.txt 3176580_raw_html.txt 1872594_raw_html.txt 2976594_raw_html.txt 3588804_raw_html.txt 276450_raw_html.txt 3594540_raw_html.txt 1190082_raw_html.txt 261072_raw_html.txt 1472454_raw_html.txt 450888_raw_html.txt 1607202_raw_html.txt 2683236_raw_html.txt 3570552_raw_html.txt 3904662_raw_html.txt 2285244_raw_html.txt 2621496_raw_html.txt 3328878_raw_html.txt 696324_raw_html.txt 794556_raw_html.txt 1595004_raw_html.txt 1570944_raw_html.txt 134184_raw_html.txt 1289382_raw_html.txt 865632_raw_html.txt 1230408_raw_html.txt 594246_raw_html.txt 1424226_raw_html.txt 2473368_raw_html.txt 258654_raw_html.txt 2749170_raw_html.txt 2810700_raw_html.txt 2953890_raw_html.txt 3282468_raw_html.txt 319926_raw_html.txt 186528_raw_html.txt 3882006_raw_html.txt 252102_raw_html.txt 3852990_raw_html.txt 1564932_raw_html.txt 2922582_raw_html.txt 20910_raw_html.txt 1129050_raw_html.txt 1419432_raw_html.txt 873318_raw_html.txt 2346348_raw_html.txt 995898_raw_html.txt 1990926_raw_html.txt 412794_raw_html.txt 1113630_raw_html.txt 3780390_raw_html.txt 3157794_raw_html.txt 2662128_raw_html.txt 3458538_raw_html.txt 2274462_raw_html.txt 3713004_raw_html.txt 2786064_raw_html.txt 3142446_raw_html.txt 149832_raw_html.txt 3470868_raw_html.txt 390192_raw_html.txt 2295708_raw_html.txt 754176_raw_html.txt 3878472_raw_html.txt 3796110_raw_html.txt 902370_raw_html.txt 2762526_raw_html.txt 3088254_raw_html.txt 3697968_raw_html.txt 2053038_raw_html.txt 3157950_raw_html.txt 3811938_raw_html.txt 732036_raw_html.txt 4001136_raw_html.txt </code></pre>
3477, <p>empty in 2.zip:</p>  <pre><code>1135808_raw_html.txt 971090_raw_html.txt 593474_raw_html.txt 1536974_raw_html.txt 3919184_raw_html.txt 418964_raw_html.txt 3866228_raw_html.txt 1919576_raw_html.txt 2372534_raw_html.txt 769556_raw_html.txt 2328566_raw_html.txt 1724234_raw_html.txt 338654_raw_html.txt 1931306_raw_html.txt 482468_raw_html.txt 1444604_raw_html.txt 3531890_raw_html.txt 1235402_raw_html.txt 3213554_raw_html.txt 3146378_raw_html.txt 3908918_raw_html.txt 1420586_raw_html.txt 2051378_raw_html.txt 3111548_raw_html.txt 1863536_raw_html.txt 1011704_raw_html.txt 2684630_raw_html.txt 634040_raw_html.txt 2249138_raw_html.txt 3750008_raw_html.txt 1199606_raw_html.txt 1676102_raw_html.txt 2883194_raw_html.txt 1004576_raw_html.txt 2487500_raw_html.txt 3360458_raw_html.txt 2272268_raw_html.txt 975344_raw_html.txt 1254494_raw_html.txt 2365064_raw_html.txt 1190630_raw_html.txt 504872_raw_html.txt 3273860_raw_html.txt 55874_raw_html.txt 1152872_raw_html.txt 3871568_raw_html.txt 3441854_raw_html.txt 1142888_raw_html.txt 2572298_raw_html.txt 1483070_raw_html.txt 2143814_raw_html.txt 1315472_raw_html.txt 1330952_raw_html.txt 2243462_raw_html.txt 622610_raw_html.txt 3125108_raw_html.txt 3905306_raw_html.txt 2267090_raw_html.txt 2389874_raw_html.txt 707972_raw_html.txt 1176530_raw_html.txt 3688682_raw_html.txt 1031906_raw_html.txt 498602_raw_html.txt 3462518_raw_html.txt 2350994_raw_html.txt 1960256_raw_html.txt 1189454_raw_html.txt 1134260_raw_html.txt 4039796_raw_html.txt 1912436_raw_html.txt 592346_raw_html.txt 3463868_raw_html.txt 2980340_raw_html.txt 3502502_raw_html.txt 3731624_raw_html.txt 1652_raw_html.txt 3000908_raw_html.txt 3177398_raw_html.txt 3290306_raw_html.txt 3637838_raw_html.txt 1043636_raw_html.txt 2886698_raw_html.txt 2559476_raw_html.txt 3845804_raw_html.txt 3194270_raw_html.txt 1531100_raw_html.txt 971060_raw_html.txt 3223658_raw_html.txt 1879442_raw_html.txt 1869170_raw_html.txt 2117150_raw_html.txt 3812798_raw_html.txt 3705698_raw_html.txt 267722_raw_html.txt </code></pre>
3477, <p>empty in 3.zip:</p>  <pre><code>3373617_raw_html.txt 3599277_raw_html.txt 2849721_raw_html.txt 2599323_raw_html.txt 2084283_raw_html.txt 106173_raw_html.txt 1468653_raw_html.txt 3829563_raw_html.txt 1823073_raw_html.txt 3195081_raw_html.txt 319695_raw_html.txt 145821_raw_html.txt 1602909_raw_html.txt 3733905_raw_html.txt 860805_raw_html.txt 3017865_raw_html.txt 2449773_raw_html.txt 1959999_raw_html.txt 1808253_raw_html.txt 597969_raw_html.txt 699753_raw_html.txt 86241_raw_html.txt 1601775_raw_html.txt 251037_raw_html.txt 1709181_raw_html.txt 3023175_raw_html.txt 484533_raw_html.txt 2518041_raw_html.txt 1063857_raw_html.txt 2878683_raw_html.txt 669309_raw_html.txt 2995881_raw_html.txt 3880113_raw_html.txt 3150987_raw_html.txt 887205_raw_html.txt 3198789_raw_html.txt 252945_raw_html.txt 4046673_raw_html.txt 1884489_raw_html.txt 3265869_raw_html.txt 995511_raw_html.txt 1311273_raw_html.txt 1510245_raw_html.txt 2184789_raw_html.txt 3184719_raw_html.txt 371031_raw_html.txt 389781_raw_html.txt 2120817_raw_html.txt 1315383_raw_html.txt 3219405_raw_html.txt 375633_raw_html.txt 89403_raw_html.txt 2160411_raw_html.txt 2193243_raw_html.txt 963735_raw_html.txt 2648607_raw_html.txt 3626187_raw_html.txt 1179681_raw_html.txt 2298003_raw_html.txt 414825_raw_html.txt 1349829_raw_html.txt 1000047_raw_html.txt 3329469_raw_html.txt 684483_raw_html.txt 2215407_raw_html.txt 2687025_raw_html.txt 3969465_raw_html.txt 3058011_raw_html.txt 707871_raw_html.txt 3442419_raw_html.txt 46641_raw_html.txt 3033825_raw_html.txt 2015091_raw_html.txt 1436799_raw_html.txt 1941681_raw_html.txt 2959365_raw_html.txt 341907_raw_html.txt 3050145_raw_html.txt 3116811_raw_html.txt 359925_raw_html.txt 2977413_raw_html.txt 1360287_raw_html.txt 1100505_raw_html.txt 2686575_raw_html.txt 2509917_raw_html.txt 2407401_raw_html.txt 997689_raw_html.txt 577197_raw_html.txt 3626415_raw_html.txt 1343067_raw_html.txt 3748611_raw_html.txt 2725995_raw_html.txt 678369_raw_html.txt 1619829_raw_html.txt 72417_raw_html.txt 2083197_raw_html.txt 853101_raw_html.txt </code></pre>
3477, <p>empty in 4.zip:</p>  <pre><code>1126840_raw_html.txt 3024136_raw_html.txt 790588_raw_html.txt 1351750_raw_html.txt 3209824_raw_html.txt 1912192_raw_html.txt 241600_raw_html.txt 2153536_raw_html.txt 901858_raw_html.txt 3400534_raw_html.txt 2508124_raw_html.txt 3992716_raw_html.txt 747478_raw_html.txt 1263676_raw_html.txt 243094_raw_html.txt 3017062_raw_html.txt 3807814_raw_html.txt 439438_raw_html.txt 2603704_raw_html.txt 3760312_raw_html.txt 2197408_raw_html.txt 2722138_raw_html.txt 455116_raw_html.txt 1506166_raw_html.txt 746056_raw_html.txt 1575448_raw_html.txt 3329590_raw_html.txt 2224000_raw_html.txt 2879884_raw_html.txt 58366_raw_html.txt 3505192_raw_html.txt 2449936_raw_html.txt 562534_raw_html.txt 3353590_raw_html.txt 2910988_raw_html.txt 3810382_raw_html.txt 791650_raw_html.txt 2720854_raw_html.txt 286492_raw_html.txt 2165734_raw_html.txt 1436752_raw_html.txt 2447362_raw_html.txt 3517036_raw_html.txt 3130546_raw_html.txt 896836_raw_html.txt 2570482_raw_html.txt 2890210_raw_html.txt 2484256_raw_html.txt 3547798_raw_html.txt 2217586_raw_html.txt 769186_raw_html.txt 990490_raw_html.txt 1909270_raw_html.txt 671806_raw_html.txt 521038_raw_html.txt 2942452_raw_html.txt 3684478_raw_html.txt 348406_raw_html.txt 3343180_raw_html.txt 1983634_raw_html.txt 918214_raw_html.txt 259804_raw_html.txt 5818_raw_html.txt 893776_raw_html.txt 2245744_raw_html.txt 3381838_raw_html.txt 36562_raw_html.txt 1815646_raw_html.txt 1202260_raw_html.txt 2340232_raw_html.txt 1130086_raw_html.txt 2633086_raw_html.txt 3570712_raw_html.txt 696508_raw_html.txt 3725908_raw_html.txt 1153672_raw_html.txt 914788_raw_html.txt 669550_raw_html.txt 116104_raw_html.txt 1496524_raw_html.txt 1231480_raw_html.txt 1800586_raw_html.txt 3791008_raw_html.txt 1212622_raw_html.txt 605380_raw_html.txt 784564_raw_html.txt 136492_raw_html.txt 2559496_raw_html.txt 1932502_raw_html.txt 1223236_raw_html.txt 1147198_raw_html.txt 698998_raw_html.txt 2196196_raw_html.txt 2244694_raw_html.txt 246268_raw_html.txt 3438844_raw_html.txt 492826_raw_html.txt 995890_raw_html.txt 1506088_raw_html.txt </code></pre>
3477, <p>Takes me 1 GB!</p>
3477, <p>Lol</p>
3477, <p>Garbage collector. <a href="https://docs.python.org/2/library/gc.html">https://docs.python.org/2/library/gc.html</a></p>  <p>[quote=Glenn Blasius;89834]</p>  <p>[quote=RockBottom;89745]</p>  <p>gc() might be useful. </p>  <p>[/quote]</p>  <p>What is gc()?</p>  <p>[/quote]</p>
3477, <h1>Mine:</h1>  <h1>Kaggler:</h1>  <ul> <li>8gb ram / i5</li> <li>6 competitions for top 10% : Amazon challenge</li> </ul>  <h1>Master:</h1>  <ul> <li>8gb ram / i5</li> <li>9th competition top10: Cause Effect Challenge </li> </ul>
3477, <p>old macbook. its pretty slow now as im used to SSDs now. :D</p>  <p>[quote=inversion;89932]</p>  <p>I obviously need to get one of those 8gb i5 machines. Looks like they perform well in kaggle competitions!</p>  <p>[quote=Abhishek;89910]</p>  <h3>Kaggler:</h3>  <p>8gb ram / i5 6 competitions for top 10% : Amazon challenge</p>  <h3>Master:</h3>  <ul> <li>8gb ram / i5</li> <li>9th competition top10: Cause Effect Challenge  [/quote]</li> </ul>  <p>[/quote]</p>
3477, <p>No. Read the data page. It wont be provided.</p>
3477, <p>ur score is FTRL?</p>
3477, <p>[quote=Glenn Blasius;89938]</p>  <p>Guys there's a big hint in the competition overview: </p>  <p><em>You are challenged to construct new meta-variables and employ feature-selection methods to approach this dauntingly wide dataset.</em></p>  <p>I'm thinking PCA as it is good at deriving orthogonal features from a large number of features....</p>  <p>[/quote]</p>  <p>Yes! so u think if its not written u dont have to do it?</p>
3477, <p>Yes! u shouldn't participate in this competition! Its all black box! lol :P</p>
3477, <p>r u sure its a CSV file?</p>
3477, <p>Since I'm the overfitting expert I would like to hear from others how much they are overfitting with a single model? :P</p>  <p>mine right now: 0.79909</p>
3477, <p>[quote=Kuber@IITB;96405]</p>  <p>Can someone  who is not taking part now buc can submit lend me their credentials? Please..</p>  <p>[/quote]</p>  <p>I dont think its allowed. Its against the rules.</p>
3477, <p>I got a lot of requests during the competition for a benchmark script. So as promised here it is.... After all its all about learning :D</p>  <p>The script will easily give you 0.80+ in  Public LB. In Private LB you can easily get a top 10% rank using this script. With some modifications this script gives top 30 rank out of 2200+ competitors.... not bad ha?! ;)</p>
3477, <p>No. You are not allowed.  From the rules: &quot;External data is not allowed in this competition&quot;</p>
3477, <p>[quote=Sudeep Juvekar;100988]</p>  <p>Any plans of supporting older GPUs/GPUs with compute capabilities &lt; 5.0 on Neon? </p>  <p>Most of us can only get hold of grid K520 on EC2 for example. </p>  <p>[/quote]</p>  <p>I have the same question</p>
3477, <p>Cool.</p>  <p>Did you also get this error:</p>  <blockquote>   <p>2015-12-15 04:04:03589 - neon.data.imageloader - ERROR - Unable to   load loader.so. Ensure that this file has been compiled Traceback   (most recent call last):   File &quot;./localizer.py&quot; line 47 in        point_num=point_num)   File &quot;/home/ubuntu/whale/whale-2015/localizer_loader.py&quot; line 31 in   <strong>init</strong>       subset_pct nlabels macro dtype)   File &quot;/usr/local/lib/python2.7/dist-packages/neon-1.1.3-py2.7.egg/neon/data/imageloader.py&quot;   line 73 in <strong>init</strong>       self.start()   File &quot;/usr/local/lib/python2.7/dist-packages/neon-1.1.3-py2.7.egg/neon/data/imageloader.py&quot;   line 192 in start       self.loader = self.loaderlib.start(ct.c_int(self.img_size) AttributeError: 'LocalizerLoader' object has no attribute 'loaderlib'</p> </blockquote>
3477, <p>[quote=Sudeep Juvekar;101368]</p>  <p>That shouldn't happen. Did your 'make' run successfully? It should create .so in neon/data/loader/. Did you activate venv in neon?</p>  <p>Problem with me is that the cuda kernel fails to compile with Kepler in localize.py</p>  <p>It seems like they used to have a support for older GPUs in an earlier version (0.9.0) but it was deprecated. Try running 'make -e GPU=cudanet' (I doubt it will work with latest commits though). Just in case it succeeds please report back :D</p>  <p>[/quote]</p>  <p>No. It doesnt. On CPU it will take 15 hours lol  ( for one epoch :( )</p>
3477, <p>I think make got interrupted at some point. I have solved this issue :) Thanks!</p>
3477, <p>[quote=Christin B. Khan;103908]</p>  <p>Fingers and toes crossed over here!</p>  <p>[/quote]</p>  <p>Why? I thought Admins can see both public and private LB :D</p>
3477, <p>hahaha.... 5309 :D</p>
3477, <p>You can submit it right after the competition ends and see your score.</p>
3477, <p>I can confirm this for Android!</p>
3477, <ul> <li><span style="line-height: 1.4">Is there a way to delete our own script?&nbsp;</span></li> </ul>  <ul> <li><span style="line-height: 1.4">Is it possible to add a new tab : &quot;Scripts&quot; in user profile page which lists all the scripts created by that user (alongwith the number of votes)? (Just like Results tab lists all the competitions)</span></li> </ul>
3477, <p>So downvotes are allowed now?</p>
3477, <p>[quote=Ben Hamner;78934]</p> <p>[quote=Abhishek;78931]</p> <p>So downvotes are allowed now?</p> <p>[/quote]On scripts? No why do you think that?</p> <p>Do you think they should be?</p> <p>[/quote]</p> <p>clicking up arrow again (after you have upvoted) downvotes it. I just lost a vote &nbsp;# [not that i care ;) ]</p>
3477, <p>ahan! so votes can be taken back! Cool...thanks!</p>
3477, <p>What about a list of users who submitted submission file using a script?&nbsp;</p>
3477, <p>[quote=WhizWilde;80647]</p> <p>I want to ask something related to kaggle scripts. Hope it is the right place.</p> <p>I submitted accidentaly a script in the WNV competition while I just wanted to download the dataset (damn smartphones) and while it helped me reach a better position on the LB I don't want my best submission to be the fruit of someone else work.</p> <p>I still can't beat it even if I am not that far from it from my last submissions but were people I would point on my profile I don't want to be regarded as claimed positions and ability because of someone else work.</p> <p>So it is possible to erase this submission from my profile? (I don't want to refill my submission count it is not the problem).</p> <p>Thanks by advance.</p> <p>[/quote]</p> <p>there is a hide button that you can use....&nbsp;</p>
3477, <p>Need: Down-Votes! :)</p>
3477, <p>lot of sales? </p>
3477, <p>I'm also wondering why we have to submit the model now. The model can be improved a lot in the last week. </p>
3477, <p>Ahh such an interesting competition and i didnt have anytime (also infrastructure) to do it :(</p>
3477, <p>Why do we need spark for this competition? :)</p>
3477, <p>[quote=KSrinidhi;103769]</p>  <p>First timer in the competition please tolerate...</p>  <p>Is there a way to compute scores offline before submission? Thanks</p>  <p>[/quote]</p>  <p>You cannot know the exact score you will get but u can split ur training data into training and validation sets train on training set and calculate error on validation. Its called cross-validation: <a href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)">https://en.wikipedia.org/wiki/Cross-validation_(statistics)</a></p>
3477, <p>Congrats to all the winners! My special congrats to &quot;A FEW with No Clue&quot;. 9 New masters coming up from a single team :) </p>
3477, <p>Very imp variable </p>
3477, <p>[quote=raddar;100056]</p>  <p>Your estimations are way off when you look at LB :)</p>  <p>[/quote]</p>  <p>because there is some problem with the evaluation ;)</p>
3477, <p>Are we allowed to use pretrained networks with open-source license?</p>
3477, <p>Also there is something hidden in that script. Include that and you will be rank 1.</p>
3477, <p>Yet again data was about finding a &quot;leak&quot;. Disappointed.</p>
3477, <p>hahahaha :D</p>
3477, <p>Bluefool is back! \m/</p>
3477, <p>Ahh Marios. Didn't expect this from you :P Kaggle subtracts ur auc from 1 if it is below a certain threshold  I think 0.5 so that no one can trick the leaderboard and come out on top out of no where in the end :D :D</p>
3477, <p>The  public private split is always given at the top of the leaderboard and is decided by the organizers.</p>
3477, <p>hahahaaa... i love it! These kind of posts are very common when a competition is about to end :D maybe we can train a model to generate a post like this in the last week of the competition :P :P </p>
3477, <p>[quote=BreakfastPirate;116955]</p>  <p>[quote=Abhishek;116954]</p>  <p>hahahaaa... i love it! These kind of posts are very common when a competition is about to end :D maybe we can train a model to generate a post like this in the last week of the competition :P :P </p>  <p>[/quote]</p>  <p>Speaking of common posts during the last week I'm still waiting for the typical post that says &quot;Hey! Nobody told me there was a first submission deadline!&quot;</p>  <p>[/quote]</p>  <p>here you go: <a href="https://www.kaggle.com/c/santander-customer-satisfaction/forums/t/20464/cannot-submit-anymore">https://www.kaggle.com/c/santander-customer-satisfaction/forums/t/20464/cannot-submit-anymore</a></p>
3477, <p>I'm so frustated I downloaded data on 2nd March and I've been working 10hours a day on this dataset and have an awesome model that will beat 0.85 and now I cannot submit my model anymore even with 6+ days to go ... so disappointed! :P</p>
3477, <p>i have 0.999</p>
3477, <p>[quote=narsil;115660]</p>  <p>What shall I do to avoid this?</p>  <p>[/quote]</p>  <p>dont use the scientific notation?</p>
3477, <p>So is it a leak?</p>
3477, <p>Someone please upvote me 1000 times to ask about the leak :P</p>
3477, <p>Since now a days all competitions need neural nets and deep learning why not have a competition in which you provide a Tesla gpu to top10? ;)</p>
3477, <p>either overfitting or a leak :)</p>
3477, <p>My validation score is 0.90+ but my LB is my current score 0.77. Is anyone else having the same problem?</p>
3477, <p>Wow... its like when i was applying for jobs first time... 15 years of python experience is something you are missing.... hehehehe :D</p>
3477, <p>[quote=NxGTR;119040]</p>  <p>Hi! I am looking for someone to team up in this competition right now I am just overfitting the LB as in the Santander competition but hopefully someone can help me avoid that.</p>  <p>I do have some requirements:</p>  <p>1) You must be a Kaggle Master</p>  <p>2) You must be ranked above me</p>  <p>3) You must be a Kaggle Early Adopter</p>  <p>4) You should have at least 1 Prize Winner achievement</p>  <p>5) You must have at least 35 (Top 10% + Top 25% combined) achievements</p>  <p>6) You should have at least 1000 forum upvotes</p>  <p>Yes I am the same guy that dropped over 1000 places in Santander yet I am very picky :D</p>  <p>So if you are out there and comply with these requirements I would love to team up! let me know <a href="https://www.kaggle.com/domcastro">Bluefool</a>!</p>  <p>[/quote]</p>  <p>Damn im not an early adopter :(</p>
3477, <p>I can confirm that its idprobability </p>
3477, <p>Since external data is allowed please share it here before you use them.. :)</p>
3477, <p>[quote=DataGeek;118959]</p>  <p>I will be using pre-trained models from this thread <br> <a href="https://www.kaggle.com/c/state-farm-distracted-driver-detection/forums/t/20141/official-pre-trained-models-and-external-data-thread">https://www.kaggle.com/c/state-farm-distracted-driver-detection/forums/t/20141/official-pre-trained-models-and-external-data-thread</a></p>  <p>[/quote]</p>  <p>I think that is too general. One could say I'll use the pretrained data accessible to everyone on www.google.com</p>
3493, <p>Hi</p> <p>Really new to here and to chess game.I hava a question I am not clear: For those players whose rating are not in the initial rating list how can we determine their rating? Can we just assume their initial ratings are 0?</p> <p>&nbsp;</p> <p>Thanks!</p>
3493, Thanks Jeff.  I do have another question: for the WhiteExpect column of the submission file are the values supposed to be one of 0 0.5 and 1 or a probability?
3495, ----
3497, ----
3504, <p>dear adam</p> <p>Does data.zip contains both train.csv and test.csv?&nbsp;</p> <p>&nbsp;</p> <p>thanking you!</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p>
3504, <p>Dear friends</p> <p>&nbsp;</p> <p>is there a correlation between&nbsp;Normallized Discounted Cumulative Gain and classification accuracy! Kindly clarify how to infer NDCG via precision?</p> <p>&nbsp;</p> <p>thanks so much friends</p>
3504, <p>Dear Contest Admin Could you kindly tell where you have made available a sample submission ? Kindly help with a Sample CSV File along with what shall be the contents of the Description that you require for Kaggle Expedia Personalization Challenge!</p>
3504, <p>&#8226; 5 - The user purchased a room at this hotel<br>&#8226; 1 - The user clicked through to see more information on this hotel<br>&#8226; 0 - The user neither clicked on this hotel nor purchased a room at this hotel</p> <p>&nbsp;</p> <p>Dear Contest Admin</p> <p>Please kindly give a Hint on a Feature/Features which indicates the User Query &nbsp;has resulted in Purchase of a Room or Other way!&nbsp;</p> <p>Kindly throw light on feature &nbsp;indicators in the Dataset which helps with deciding the Relevance Score is 5 or 1 or 0!</p> <p>is the Feature/Features Explicit in Dataset or We have to infer !</p> <p>&nbsp;</p> <p>Thanking you!&nbsp;</p>
3504, <p>yes OWEN ! thanks so much!&nbsp;</p>
3504, <p>Thanks so much Forbin! I wanna Merge with you as a team member &nbsp;if possible Mate!&nbsp;</p> <p>&nbsp;</p> <p>Thanks so much Forbin!</p>
3504, <p>How could i interpret Relevance in terms of Correlation Coefficient! Your Kind Help in this regard will be much appreciated mates!</p> <p>&nbsp;</p>
3504, <p>I have posted to expedia the following 3 files:</p> <p>&nbsp;</p> <p>1) Expedia Datasets Reduced using my Custom IR-AR Filter</p> <p>2) Classifier Model with Training Run over the Target Class Book-bool</p> <p>3) Classifier Output Result File</p> <p>&nbsp;</p> <p>Does this helps? Iam sorry i have no clue on submission as per your format!</p>
3504, <p>Thanks so much Forbin! Very Kind of You! Wish you all the very best in all your endeavours in Kaggle!</p> <p>&nbsp;</p> <p>With Best Regards</p> <p>Dr.Senthamarai Kannan Subamanian</p> <p>drsskannan.hpage.com</p> <p>&nbsp;</p>
3504, <p>I have felt the thrill and excitement with the Expedia Optimization Challenge through applying my own Instance Reduction-Attribute Reduction Filters!</p> <p>Now It is time for me to invite the Score Leaders in the Expedia Optimization to partner with our team SURECOMMENDER so that we can Mutually Lift each other!&nbsp;</p> <p>&nbsp;</p>
3504, <p>Participation in &nbsp; Expedia challenge is a Great Pride in itself and a Great Learning Model in particular this Forum! I love to share with the Entire Classic Research Community!<br>I Sincerely Convey my congrats and feel my deepest thanks to the people at kaggle for hosting such a wonderful challenge!</p> <p>Further I have registered my team as GOOGLER &nbsp;for the another great challenge on Search Results Personalization! Hope it is sponsored by Yandex!</p> <p>I have got some ideas! Let us Explore together by joining my team and feel the joy of sharing our collective knowledge! i firmly believe we have to be inspired by nature!&nbsp;</p> <p><span style="line-height: 1.4">Keep Winning!&nbsp;</span></p> <p>With Warm Regards</p> <p>Senthamarai Kannan Subramanian</p> <p>drsskannan.hpage.com</p>
3504, <p>&nbsp;</p> <p>Can any one of the following could be indicators of Relevance Score in the context of Expedia Hotel Ranking!<span style="line-height: 1.4">&nbsp;</span></p> <p>Mean absolute error&nbsp;<br>Root mean squared error&nbsp;<br>Relative absolute error&nbsp;<br>Root relative squared error&nbsp;<br>&nbsp;</p> <p>Kindly help! Thanks so Much</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p>
3504, <p>SURE Mate!</p>
3504, <p>Dear Admin</p> <p>Your Kind help required in understanding the Ranking of &nbsp;Leaders in the Score Board!</p> <p>As there are Two Scores I would like to understand Both!</p> <p>One Score is Explicit where as the other score Pops up when i move the cursor over the Explicit Score!&nbsp;</p> <p>Kindly Throw Light on the Implicit Scoring Method!</p> <p>&nbsp;</p> <p>Thanks so Much!</p> <p>&nbsp;</p>
3504, <p style="text-align: center">Correctly Classified Instances 2550 99.2604 %<br>Incorrectly Classified Instances 19 0.7396 %<br>Kappa statistic 0.9228<br>Mean absolute error 0.0078<br>Root mean squared error 0.0645<br>Relative absolute error 12.9201 %<br>Root relative squared error 37.2864 %<br>Total Number of Instances 2569 </p> <p style="text-align: center">Am I Making Progress ! Does Root mean Square Error is the Real Indicator of Classifier Error!</p> <p style="text-align: center">Please help friends!</p>
3504, <p>Dear Admin</p> <p>I have the Add Files and Start Upload Facility in your kaggle expedia submission!</p> <p>Is it possible for me to upload my top results for example: segmentation or Classifier Design separately using this facility!</p> <p>Does it helps Expedia!</p>
3504, <p>Dear Admin</p> <p>I would like to know the right option for &nbsp; Expedia classifier Design!</p> <p>I have Training Data only!&nbsp;</p> <p>i.e. I have Both classes(0 and 1) for Book_bool as well as Click_bool</p> <p>Do i need a separate Test.CSV file for &nbsp;testing my classifier?</p> <p>I have not much time left out with preparing separate test.csv file?</p> <p>Thanks so much!</p>
3504, <p>Really! I submitted only top 9 hotels!&nbsp;</p>
3504, <p>I Used only&nbsp;</p> <p>Property_Location_Score2!</p> <p>I hope Iam Right! How about Promotion_Flag? Is Promo Significant With respect to Expedia Booking!</p> <p>&nbsp;</p> <p>Thanks!</p> <p>&nbsp;</p>
3504, <p>Dear Friends!</p> <p>I have applied my local search ranker which uses Feature Quality Information to Focus the Top Features! But Again I applied &nbsp;my custom algorithms and got my feature pool as just a one single feature! Ex:- Promotion_Flag!</p> <p>But I was Skeptical and Preferred a List of Features over a Single Feature!</p> <p>How shall i submit my Feature Modeling for Expedia as a research poster! Please Guide Admin!</p> <p>With Best Regards</p> <p>Kannan</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p>
3504, <p>Thanks so much Mate!</p>
3504, <p>Please do the needful Gert! Very Kind of You! Thanks Mate!</p>
3504, <p>I Express my Congrats to the Team Commendo for the great win!&nbsp;</p>
3504, <p>fox! why u r angry on me?</p>
3504, <p>i understand now fox!&nbsp;</p>
3504, <p>Just a Guy! yes i used one trick! As I always click when there is a Promotion! I represented the Promo_flag in place of Click_Bool!</p> <p>But Still I have a Point to Ponder! What Variables Influences Booking? Is it a Single Variable like Property_Score or any other variable! Since with my algorithm &nbsp;I have obtained only one feature !</p> <p>I invite Top 3 Scorers on the Leader Board to clarify on It is a SIngle Feature or More than One which influences Booking!</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p>
3504, <p>Yes David! I did not read all threads as i was busy and left out with little time! I participated the Expedia challenge as an opportunity for the sake of designing a better Feature Model and still iam wondering what are the best features as i have to vaildate my feature selection experiments!&nbsp;</p> <p>But Now after reading so many posts on the approach i could understand it is not a feature optimization problem in it's entirety!&nbsp;</p> <p>&nbsp;</p>
3504, <p>Dear&nbsp;Haobo SU</p> <p>You could read my paper on ranking ! it is published in elsevier knowledge based systems! It is the Premier Journal for Data Mining and Knowledge Discovery in Databases!&nbsp;</p> <p>As far my Expedia Experience is concerned I was not able to score since i submitted only 9 srch-ids and hotel-ids! So I could not &nbsp;score! I came to know this post the competition so i could not win!</p> <p>Anyway!&nbsp;</p> <p>I Used Local Search procedure &nbsp;based on Symmetrical Uncertainty for Ranking the Best Hotels as per the search query!</p> <p>Again Do you want to team with me! mail me stanfordssk@gmail.com</p> <p>With Best regards!</p> <p>Dr.Kannan</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p>
3504, <p>Dear Adam</p> <p>please post details of expedia poster workshop at Dallas!&nbsp;</p> <p>&nbsp;</p> <p>with warm regards</p> <p>kannan</p> <p>&nbsp;</p>
3504, <p>Abhishek! Shall We Team!</p> <p>With Best Regards</p> <p>Kannan</p>
3504, <p>Iam Using Nature Inspired Paradigms for Large Search Problems ! I am a Strong Believer of the saying &quot; Imagination is More Important than Knowledge&quot;!</p> <p>Any one with the same frequency shall pair up with me!</p> <p>&nbsp;</p>
3504, <p>Dear Prof_Data Did you managed to open Train.CSV or Test.CSV?</p> <p>How are you going to predict the values ( 0 or 1) for all the 24 Labels Since Invariably all 24 have either a 0 or 1.&nbsp;</p> <p>Do we have to prepare the Tweets for Training (OR) Shall we use Test.CSV for Cross validation?</p> <p>David will you please explain !</p> <p>Does Infering the Sentiment is more important (or) Inferring What Variables causes Bad Weather or Storm is Important!</p> <p>Why the Sample Submission format contains only ZEROS and not ONE for all Variables!</p> <p><span style="line-height: 1.4">Could you Please Explain Admin!</span></p> <p>&nbsp;</p> <p>&nbsp;</p>
3504, <p>Dear Mates!</p> <p>Can I Selectively reduce the Rows using R! Do i need any big data tools like Hadoop for this Challenge! Or R/Rattle Miner is enough! please clarify</p>
3507, ----
3509, ----
3517, <p>I use Anaconda for Windows and Mac</p>
3517, <p>Hi</p> <p>Just letting you know that the Cause-Effect competition pages are down</p>
3517, <p>I don't understand?</p>
3517, <p>ah ok. thanks</p>
3517, <p>Cancer is a very complicated disease. It would be very difficult for a Computer Scientist to make sense of it without collaboration from biologists and chemists. Also there is a move towards &quot;Personalised Medicine&quot; - each individual's genetic profile will be analysed and they will be put on the appropriate treatment depending on their personal DNA profile. 10 people may have breast cancer and all could be on totally different treatments. This makes large genetic studies where individual profiles get lost amongst the &quot;majority&quot; profiles a bit redundant. Some good data can be found here http://www.cbioportal.org/public-portal/ This contains profiles of matched cancer/normal samples. EG. A patient's tumour sample and the same patient's normal sample. By looking at this type of data a personal profile may be created. This will be the future of cancer data analysis</p>
3517, <p>Must have - I lost about 8000 points (must be 2 year loss)</p>
3517, <p>I wonder why I lost so many points though. My rank went down about 40 places and I lost about 7-8000 points :( Could be the 2 year loss for me I suppose</p>
3517, <p>I think maybe it's the score pre_Amazon competition?</p>
3517, <p>Did this get sorted? or did I lose 8000 for 2 year loss?</p>
3517, <p>I think it's been fixed. Only lost 1 place in the ranking. Still lost 3000 points but this would be about right as I had a lull using Kaggle when I started new job. Need to grab those points back!</p>
3517, <p>I have an annoying problem. I have set all the appropriate seeds and can&nbsp; reproduce exact results after training each run. However I saved the RF model to file and everytime I load model I get different (but very similar) predictions. I reset the seed after loading the model but still gives me different predictions each time. As the competitions require exact reproducibility from saved models I'm having a problem. Any ideas? thanks in advance</p>
3517, <p>Anyone else getting fed up of trying hard and then having someone post a &quot;beat the benchmark&quot; code - so pretty much your one opponent turns into 200. What happened to original thought. There's one thing &quot;learning&quot; and but this has gone too far. Each time I am FORCED to run all these benchmarks just to get back into. I would prefer to compete against intelligent people rather than a bunch of people who just submit other people's code.</p> <p>&nbsp;</p> <p>So really are you a genuine competitor or do you just hang around using other people's code and change the seed? Enter the Knowledge competitions if you have such a need to help others</p>
3517, <p>I just get stroppy because you might as well wait until the end of the competition before entering. I usually don't mind the &quot;beat the benchmarks&quot; but the one that annoyed me puts people about 5th out of 230 ish - that's high. I feel like I've wasted a couple of weeks. I've learnt loads too but there still should be an element of innovation and competition. Rather than everyone tweaking the same models - slight changes by random seed changes! So yes sour grapes but what has been the point of me making submissions for 2 weeks? Waste of my time and disappointing</p>
3517, <p>Lol Triskelion - this is your first ever Kaggle competition!!!</p>
3517, <p>Even the Amazon code was nowhere near as damaging as this.</p> <p>So this is your first competition</p> <p>this is the first time someone has posted such a high ranked leaderboard score</p> <p>you went up over 60 places on the leaderboard</p> <p>you are now 11th</p> <p>This seems to have given you the impression that Kaggle is easy.</p> <p>&nbsp;</p> <p>As for outranking me - no YOU don't outrank me at all. The poster of the code is out ranking me. You are actually 60 or so places behind me by your own hand.</p> <p>&nbsp;EDIT: Shocking that you genuinely think YOU are outranking me. </p>
3517, <p>true but when employers are now asking what one's Kaggle rank is - something has to change. I have a lower overall rank than Invincible Guy who's only ever submitted benchmarks. If we applied for the same data analysis job he would have a higher a priori probability than getting it than me. However he hasn't done any data analysis. Companies are putting focus on Kaggle rank and therefore it has to be a fairer system where innovation is rewarded</p>
3517, 
3517, <p>So why are teams disallowed in the Stumbleupon competition but posting public code is allowed? So basically you're versus a team of 100. It's these inconsistencies that are annoying. I've forgotten that I used to find this fun - it's more stressful than my stressful job. Maybe I should avoid the forums and just plod along like I used to</p>
3517, <p>I'm over it all now.</p>
3517, <p>I haven't been able to install scikit-learn on my mac. I have to use my Windows machine for Python. It's some clash between LCC compilers and Xcode so you need to remove Xcode etc etc but the that knocks on to other tools. I even took my mac to IT services and they couldn't do it either</p>
3517, <p>I agree with everything you say but as a digression you haven't taken into account the competition difficulty. Eg the competitions with harder preprocessing (eg cause and effect solar image signal sound) always have a lower number of competitors and harder benchmarks to beat. The more competitiors the &quot;easier&quot; the competition - for example some competitions you can run algorithms on the provided data as is.</p> <p>2nd digression. The Cause and Effect competition was also a two-stage competition as was the KDDs and Bluebook so if you're not doing well there's no point in spending time preprocessing the test data and submitting predictions. It's not always about playing the system - these 2 examples have lots of ties because of the fact they were 2 stagers. Though usually it is to play the system however some of your examples are misleading.</p>
3517, <p>oh no - don't give up the cause. I totally agree it's bad.</p>
3517, <p>and makes the final selections for the private leader board</p>
3517, <p>Hi As there seems to be a general disrespect and deliberate breaking of the rules I would like to know how legally binding these rules are? Has Kaggle appointed lawyers to determine the legal position of these rules?</p>
3517, <p>There was a wind forecasting competition https://www.kaggle.com/c/GEF2012-wind-forecasting. The forums and winner's code may be useful to you.</p>
3517, <p>Hi</p> <p>Just a user feature request. It would be nice to have one page with all the leaderboards on it&nbsp; - so a little box for each competition where the top 10 or so are showing then the user can scroll down each competition.</p> <p>thanks</p>
3517, <p>I've done a basic spreadsheet for estimating your Kaggle points for a competition. Please correct me if I've boobed.</p>
3517, <p>I've been asked which of the NoSql technologies (Hadoop MongoDB JsonNeo4j ) would be suitable for biological and chemical data for cancer.</p> <p>Large amounts of Expression RNAi DNA sequences clinical data and chemical compound / drug data.</p> <p>We have funding for &quot;Novel Storage&quot;.</p> <p>I'm going to go through all the tutorials but which one sounds most appropriate? ie which one should I start with?</p>
3517, <p>Hi thanks. I'll let you know when the jobs are posted :)</p>
3517, <p>I have received no email from Experfy.</p> <p>I'm in the UK and I am on LinkedIn. I use the same email adress on Kaggle and Linked In.</p> <p>They must have got it elsewhere - have you googled your email address?</p>
3517, <p>I'd try your tool if you made one. I start off organised in these competitions and by the end I have lost all control</p>
3517, <p>Depends on competition. Usually I do A. For the shoppers competition we did B using 4 * 50:50 and only using 50% to predict on test (and then averaged the 4)</p>
3517, <p>All works from the UK.</p> <p>Remove leading 0</p> <p>Add +44</p>
3517, <p>You could have gone for voice verification like other people in India..............</p> <p>https://www.kaggle.com/forums/t/9883/sms-account-verification</p>
3517, <p>Bump. Now Kaggle has the Git site maybe this could now happen? We all want a mug</p>
3517, <p>I want to flag this post but for humour's sake I won't!</p>
3517, <p>It's only an issue with small data or non-random split data. Africa competition was both of these. In most competitions you are safe to pick your top 2 leaderboard submissions.</p>
3517, <p>Help me out! Let's get flagging the spam. Highlight over the user or at bottom of message and you see a FLAG option</p>
3517, <p>Isn't the Highest Rank based on many performances? There would be no date. It's cumulative. Also I don't want dates put on my profile because I'm supposed to be working!! ;)</p>
3517, <p>[quote=Rohan Rao;56897]</p> <p>You would get your Highest Rank after the end of some competition (when the points are recalculated). It is cumulative over your past performances but you 'achieve' it on the date of the end of a competition.</p> <p>[/quote]</p>  <p>yes but you could &quot;achieve&quot; it by doing rubbish in the last competition! So for example if I was 50th and just submitted the benchmark in a competition and came 300th and got a few points.At the same time the person who's 49th didn't enter - I could then become 49th. So I achieved it by coming 300th in a competition!</p>
3517, <p>[quote=Abhishek;57760]</p> <p>Just one competition left to work on??? Is kaggle announcing any new competitions soon? If not I'll have to go out and play football in my free time :(</p> <p>[/quote]</p> <p>It's awful isn't it? I'm going to have to decorate my spare room!</p>
3517, <p>Ha! Abhishek doesn't want to play football (https://www.kaggle.com/forums/t/10898/im-bored/57840#post57840)</p>
3517, <p>The forum overview page hasn't been working for a few days. I'm not being impatient but I suddenly thought that maybe Kaggle doesn't know?</p>
3517, <p>[quote=William Cukierski;57915]</p> <p>We're looking into the cause of the slowness.</p> <p>[/quote]</p> <p>DOS attack?</p>
3517, <p>Hi It's still not working for me? I've closed browser and reopened</p>
3517, <p>Hi</p> <p>I've attached some screenshots. The first is the overview page. You can see that for The Epilepsy and bike forums the last posts are a few days ago. But when you go into the individual forums there are more recent poststhanks</p> <p>This is pretty much the pattern for all the forums (for me)</p> <p>edit: It picks up edited versions of old posts but no new posts</p>
3517, <p>I don't know if it helps but every now and then I get &quot;502 Bad Gateway<br> The server was acting as a gateway or proxy and received an invalid response from the upstream server.&quot; when trying to access the forums</p>
3517, <p>Any update on when this will be fixed?</p>
3517, <p>I agree that the particular one is too long as it's changed format of the leaderboard but I like all the names changing (I'm guilty) - it adds a level of amusement for the leaderboard addicts</p>
3517, <p>https://www.kaggle.com/forums/t/9883/sms-account-verification</p>  <p>There's a lot about this here. There is also a solution for you here</p>
3517, <p>I enjoyed the online ones. That was new to me and I loved tnrtgu's code</p>
3517, <p>I work in bioinformatics and overfitting is a real problem. The number of times a biologist comes to me with a 0.98 AUC thinking they've solved some biological problem. No CV no hold out test set etc. What's even worse is that other biologists review their papers and they get accepted into top biology journals! lol I could cry sometimes!</p>
3517, <p>[quote=yeswell;66952]</p> <p>[quote=Bluefool;66820]</p> <p>I work in bioinformatics and overfitting is a real problem. The number of times a biologist comes to me with a 0.98 AUC thinking they've solved some biological problem. No CV no hold out test set etc. What's even worse is that other biologists review their papers and they get accepted into top biology journals! lol I could cry sometimes!</p> <p>[/quote]</p> <p>I worked in bioinformatics too. I heard about such stories a lot but thankfully I have never witnessed one.</p> <p>I read mostly Bioinformatics and BMC Bioinformatics... Low-impact journals do not have overfitted analyses while the high-impacts do??? We can cry together T_T</p> <p>[/quote]</p>  <p>I've seen some in Nature Biotechnology!!! BMC Bioinformatics PLOS Comp Bio and Bioinformatics are usually fine</p>
3517, <p>errr the 2015 competition is still going on. Cheeky!</p>
3517, <p>Eh? didn't know you had to be over 18. Just enter I doubt you'll win a prize in the next year anyways. Good luck - just go for it.</p>  <p>EDIT: and welcome to the world of Data Geek</p>
3517, <p>Have you informed Anonymous? If I was you I would contact Anonymous via Twitter. At the least you'd get public awareness of the situation</p>
3517, <p>Can someone please tell me if the forum posts are looking odd? ie no voting and unformatted or is it my browser problem?</p>
3517, <p>Congrats Abhi</p>
3517, <p>Perl would be a good one</p>
3517, <p>That looks better more reflective of consistency</p>
3517, <p>oh no - I bet I won't like the new change as much when the decay kicks in! ;)</p>
3517, <p>Careers are overrated - stop over-worrying</p>
3517, <p>oh no - did anyone see the West Nile one?</p>
3517, <p>Will you tell us? otherwise the people who saw have a great advantage!</p>
3517, <p>Thanks. Could have been worse for me so happy with that</p>
3517, <p>I'm quite pleased because I got despondent with the West Nile one because it's one of those &quot;overfitting&quot; competitions. At least I know I'm overfitting but not crazily!</p>
3517, <p>[quote=Jose M.;80239]</p> <p>I find surprising that the debate has been focused on the team-size factor.&nbsp; This is what I find most controversial:</p> <p>[quote=William Cukierski;78218]</p> <p>Winning a 100 person competition is skill. Winning a 1000 person competition is skill and luck.</p> <p>[/quote]</p> <p>As a consequence of this belief the corresponding weight is given by log(1+log(N)) instead of just log(N).</p> <p>I couldn't disagree more with William's sentence. I also find disappointing that Kaggle admins credit luck so much.&nbsp; To win a 100 person competition you need to defeat 99. To win a 1000 person competition you have to defeat 999. That is more challenging than the &quot;log of the log&quot; factor reflects. Not only you need luck: you need to work harder to for example build a cross validation procedure that better approaches the LB score. In other words if I make 100 submission to a 100 person competition and 100 submissions to a 1000 person competitions it looks like it takes the same amount of work but my experience is that it doesn't.</p> <p>[/quote]</p>  <p>I disagree. If 1000 people enter it means it was easy. It means you downloaded the data and ran an algorithm through it. If 100 people enter it means it's complicated and a real data munging competition. Plus you get more people enter when there's all the freebie &quot;Beat the Benchmarks&quot; so have you really beaten 999 people or have you beaten 20 and 979 benchmark tuners? The smaller the number of competitors the harder the competition - should be rewarded</p>
3517, <p>[quote=Jose M.;80246]</p> <p>Ok I admit that winning a 1000-competition is not 10-times harder than winning a 100-competition as Bluefool pointed out but I think it takes more than a 26.2% of increase in work/skill.</p> <p>[/quote]</p> <p>I still disagree. lol. I just came 194th in a &gt;3000 competition by making no effort what so ever!</p>
3517, <p>you'll get better each competition -just stick with it and learn how the code works. Aim for a 25% badge then in a couple of competitions aim for a 10%. You'll see yourself climb. @Inversion is a good example who started out and stuck with it and saw himself steadily improve each competition</p>
3517, <p>I remember seeing this and thought of data science and food predictions http://www.washingtonpost.com/blogs/wonkblog/wp/2015/03/03/a-scientific-explanation-of-what-makes-indian-food-so-delicious/</p>
3517, <p>Just close page and reopen - the scoring system sometimes gets stuck. As long as it has scored your submission then everything is ok and has no further effect on you.</p>
3517, <p>hahaha the trauma when Kaggle breaks your heart</p>
3517, <p>I get a &quot;oops&quot; error when I tried to comment</p>
3517, <p>Motion in West Nile Virus</p>
3517, <p>Also from the Forum page West Nile Virus if you click on Ben Hamner's name it gives you a 404 error rather than taking to last comment as it does on standard forum posts</p>
3517, <p>I can see your comment on that post</p>
3517, <p>Now you even have special adverts of benchmark codes for the lazy. Maybe one day the hard work people put in for weeks will be respected</p>
3517, <p>No but they get tuned and they also give out ideas. It's getting divisive. </p>
3517, <p>Not sure if this is a bug - I accidentally voted someone down whilst scrolling. I went to cancel my downvote but it then gives them an Up vote - so it goes from -1 to +1 rather than -1 to 0</p>
3517, <p>ah ok thanks - yes that works.</p>
3517, <p>[quote=the1owl;81788]</p> <p>All is fair in love war and kaggle competitions...</p> <p>* I really wanted that hoodie :(</p> <p>[/quote]</p> <p>And it's that attitude that has caused the problems in the first place.</p>  <p>Edit:&nbsp; and you've only been a member for a month.&nbsp; Go take your lack of morals somewhere else</p>
3517, <p>You so remind me of a young Triskelion!</p>
3517, <p>ah - it's a compliment to both</p>
3517, <p>William it's going to take a lot more than a blog post to sort these problems out. In the last week of West Nile  we've had many benchmark codes - one of which was Top 5% - the Leaderboard is now full of this one submission. Today we have someone who has given out an External Data link explaining how people can improve their score by cheating!! lol</p> <p>10% and 25% badges are seemingly now becoming worthless. There's nothing to aim for and people are demoralised. There are several Noobies who are shocked and upset by how Kaggle operates with the benchmarks.</p> <p>I know you would like Kaggle to be self-policing but like communism this will only work if people have the same ethical and moral framework - which obviously they don't.</p>
3517, <p>We have a proverb in England -&quot;He who smelt it Dealt it&quot;</p>
3517, <p>[quote=James King;82371]</p> <p>I've lost interest in kaggle recently because I do not enjoy working on a problem for weeks and then seeing a top ten &quot;benchmark&quot; posted that blows my efforts out of the water. And I'm then in the position of improving someone else's solution rather than developing&nbsp;my own if I want to get high on the leaderboard which is less enjoyable to me.</p> <p>I'm trying out a new way of using the site which is to start competitions once they have ended. I don't have to worry about how well I place (since it doesn't show up on the leaderboard and I don't get any kaggle points). I won't be leapfrogged by two dozen &quot;competitors&quot; overnight. And I can ignore the forum to start with and refer to it only when I get stuck. I can play with the datasets as much or as little as I want without worrying about how many top 10 finishes I have or if I'm ever going to win a prize.</p> <p>[/quote]</p> <p>+1 it has become a bit boring. I've become lazy and sometimes don't bother to try things because I know it will take me a couple of days coding and then someone might post benchmark with higher score. I prefer working on my own code than tuning others too.</p>
3517, <p>I've adapted now. My new work flow will be to :</p>  <ol> <li><p>Wait a couple of weeks see what everyone's doing on scripts</p></li> <li><p>Try and think of something different - different data different features.</p></li> <li><p>Carry on in my normal way but keep eye on Scripts</p></li> <li><p>Then in the last week start ensembling</p></li> <li><p>Finish day before end and save all last-day submissions until last hour - just in case there's a late Script</p></li> </ol>
3517, <p>[quote=James King;85613]</p>  <p>[quote=Bluefool;85606]</p>  <p>I've adapted now. My new work flow will be to :</p>  <ol> <li><p>Wait a couple of weeks see what everyone's doing on scripts</p></li> <li><p>Try and think of something different - different data different features.</p></li> <li><p>Carry on in my normal way but keep eye on Scripts</p></li> <li><p>Then in the last week start ensembling</p></li> <li><p>Finish day before end and save all last-day submissions until last hour - just in case there's a late Script</p></li> </ol>  <p>[/quote]</p>  <p>Is that fun? It doesn't sound like fun.</p>  <p>[/quote]</p>  <p>I'll let you know in a couple of months! </p>  <p>EDIT: Also a lot of the scripts aren't good practice so may calm down?</p>
3517, <p>I think it's just a case of &quot;time heals&quot;. My whole thinking has changed from 5 weeks ago. I'm quite enjoying it all now and my routine has changed. I also now stand up and admit &quot;I am a click and submitter&quot;</p>
3517, <p>[quote=WhizWilde;86904]</p>  <p>What changed your mind? I guess your &quot;moral compass&quot; did not change so quickly so what made you walk this way? It's it because you enjoy taking the most of other ideas?  [/quote]</p>  <p>I changed as</p>  <p>I have now ran my first Lasagne network and cranked up the benchmark script by +0.3</p>  <p>I have now learnt how to early stop in xgboost</p>  <p>I have now learnt how to write a bespoke scorer for Xgboost in R</p>  <p>I am currently attempting to learn some signal processing</p>  <p>If it wasn't for the Scripts I wouldn't have been able to enter the Physics or Grasp competitions. I still understand Barismog's complaint but I'm now going with the flow</p>
3517, <p>I hate this!! Quoting is harder and you now have a preview. Why do we need a preview on the same page? It's really confusing FFS you've even made the forum crap. Top 10% benchmarks in last 4 hours of a competition and now this EDIT: Now I've calmed down - the problem is the filtering out the Preview from the message. I don't have the ability to do this. I see my stuff being written twice and it confuses my brain. I cannot match up the Quote ends with each other because I see the Quotes in the Preview</p>
3517, <p>Why don't you just ban non-&quot;normal&quot; people from Kaggle? Kaggle used to be for geeks and everything was about rules and structure. Now it's about anarchy and low ethics. I blame the Norms - they're every where and even infiltrating Kaggle EDIT: Meltdown over more Xavier than Magneto again</p>
3517, <p>I know it just takes me a while but I updated my ITunes today and now all my radio plays have gone disordered too. It was a crazy morning where I wanted to move to a wood with no computers!</p>
3517, <p>Anyone know how you get a  character? My posts look ok but then when I post all the  disappear and everything looks messy</p>
3517, <p>[quote=Jeff Moser;85436]</p>  <p>[quote=Bluefool;84435]</p>  <p>Anyone now how you get a <code>&lt;newline&gt;</code> character? My posts look ok but then when I post all the <code>&lt;newlines&gt;</code> disappear and everything looks messy</p>  <p>[/quote]</p>  <p>Not sure I follow. Newlines would just be enter (or double enter for a new paragraph). Most things that look like an HTML tags will get removed.</p>  <p>[/quote]</p>  <p>I'll try a double return - I'm going to press return now now</p>  <p>now</p>  <p>EDIT: OK 1 return gets removed but 2 actually works</p>
3517, <p>I think it would be nice to have an Annual &quot;Make a Master&quot; competition on top of the usual Christmas Competition. 10% and 25% badges aren't so prestigious as they used to be because of the new Scripts (not moaning!) </p>  <ul> <li>Only Kagglers and Novices can enter</li> <li>No prizes but places 1 to 3 (if they already have a 10% badge) will automatically get their Masters</li> <li>Places 7 to 10 get a 10% badge</li> <li>No Scripts!</li> <li>No teams</li> <li>No Forum over-sharing</li> <li>Run like a Recruitment competition</li> </ul>  <p>Kagglers on an even territory going for Gold!</p>
3517, <p>Bless ya Triski. We'll team up soon</p>
3517, <p>The &quot;no teams&quot; was more a numbers thing - If the top 3 teams have 5 people each in them  that's 15 Masters created!</p>
3517, <p>How many submissions were made on the Tuesday? If it's more than 5 between the accounts then you might as well put it down to experience and move to another competition. </p>
3517, <p>Of course you'll have the opportunity! Just learn from it. You'll just get removed from the leaderboard - nothing else.</p>
3517, <p>Rubbish . </p>  <p>EDIT: removed because I could get in trouble!</p>
3517, <p>They have had a long time to practice their cheating then.</p>
3517, <p>What's good about 1337?</p>
3517, <p>ah ok - I did google that but still couldn't understand why Leet was good either! I love numbers I always use 369 as my seed and even have it tattooed on my arm lol</p>  <p>EDIT: you got 2 bonus votes for answering me</p>
3517, <p>I like 42 too but Tesla said:</p>  <p>&quot;If you only knew the magnificence of the 3 6 and 9 then you would have a key to the universe.&#8221;</p>  <p>so I got it tattooed with an x in front.</p>
3517, <p>[quote=inversion;86709]</p>  <p>[quote=Bluefool;86706]</p>  <p>Tesla said:</p>  <p>&quot;If you only knew the magnificence of the 3 6 and 9 then you would have a key to the universe.&#8221;</p>  <p>[/quote]</p>  <p>Yeah but Tesla died with zero upvotes.</p>  <p>[/quote]</p>  <p>hahahhahahahha true!</p>
3517, <p>As I've worked in the Public Sector and charities for years I can only say from experience that those who encourage others to be altruistic usually end up rich get a promotion or use it to receive funding. Always self-interest somewhere long the line</p>
3517, <p>[quote=udibr;88873]</p>  <p>at least the reddit gave some <a href="https://www.reddit.com/r/MachineLearning/comments/3g3t28/kaggle_allows_admins_to_change_rules_during/ctuzr65">response</a> from Kaggle team</p>  <p>[/quote]</p>  <p>I'd claim my money back from Kaggle if I was you</p>
3517, <p>In previous competitions there is a Winners section. This was good because you could compare the Winners with the Leaderboard and then see who the true prize winners were. It doesn't seem to be around for a while.</p>
3517, <p>They're not actually supposed to submit benchmarks in last week <a href="http://blog.kaggle.com/2015/07/27/a-rising-tide-lifts-all-scripts/">http://blog.kaggle.com/2015/07/27/a-rising-tide-lifts-all-scripts/</a></p>  <p>I think the problem is that the owner of the script still gets the button to submit. Maybe this should be disabled then people wouldn't know if it was a script or not from looking at leaderboard</p>
3517, <p>The British BBC have been running a series on Robots and AI <a href="http://www.bbc.co.uk/news/technology-33978561">http://www.bbc.co.uk/news/technology-33978561</a></p>  <p>They also have a &quot;how much is your job at risk from robots?&quot; question</p>  <ol> <li>Who thinks Data Science can be automated?</li> <li>Who is scared that their data science job could be at risk from robots?</li> </ol>
3517, <p>I think some aspects can be automated - especially the algorithm side. With regards to jobs it's a difficult one. I worked on the Robot Scientist project <a href="https://en.wikipedia.org/wiki/Robot_Scientist">https://en.wikipedia.org/wiki/Robot_Scientist</a> and this caused controversy amongst lab workers. There may be job reshuffling but  new technology usually leads to new jobs. Automation allows humans to spend their time being creative rather than on repetitive tasks.</p>
3517, <p>[quote=the1owl;93467]</p>  <p>Did someone here <a href="http://www.computerworld.com/article/2985100/robotics/instead-of-robots-taking-jobs-ai-may-help-humans-do-their-jobs-better.html">co-author this</a>?</p>  <p>[/quote]</p>  <p>Don't think so. The article makes sense though</p>
3517, <p>There's only 1 competition that you can't enter. I'm not sure what AVMs are?</p>
3517, <p>I sometimes use fuzzy clustering - like k-means but an instance can be in several clusters (% membership)</p>
3517, <p>I downloaded these for Dream 5 - 9 (I think it's 10 now?) I never took part because they always chose the most inappropriate evaluation metric. So many people used to complain that they may have changed it now. They used to pretty much always use Spearman's Rank even for normally distributed responses - ie most of the predictions are around 0 so 0.0030020.001 are all pretty much 0 but for a rank metric the exact order is really important. The winning model scored 0.21 correlation - which isn't really correlated at all! When you suggest that MAE MSE etc may be better they got all techy and defensive. I think it was biologists who used to set the challenges - hopefully it's now computer people.</p>
3517, <p>I think I'll probably enter the Astra-Zeneca one as it's what i did as a job. Unfortunately they have still not got their act together on the evaluation metric and have still not decided how to evaluate - even though the competition has been going for a month!</p>  <p>Thomas Yu 17 days ago We are still in the midst of discussing how the subchallenges will be scored. The prediction and scoring metrics wikipage will be updated when we determine the method of scoring. Thank you for your patience.</p>  <p>Thomas Yu Sage Bionetworks </p>
3517, <p>Be at peace Leustagos. You are truly a Kaggle Master</p>
3517, <p>Calling UKers</p>  <p>December 16th</p>  <p>Join us for the launch of our London based ODSC Meetup. Our speaker this month will be Amanda Schierz (Bluefool) from DataRobot speaking about open-source bioinformatics for data scientists. Schedule to be up this week so please book your place soon.</p>
3517, <p>DataRobot beats me all the time</p>
3517, <p>Excellent - love it. Nice one</p>
3517, <p>He was amazing. He was truly a Master. I was devastated at this news. The world became a worse place. Here's to Lucas.</p>
3517, <p>Congratulations Inversion. That's amazing. 25 years and I hope there's lots more to come. I think you're on to a loser with the gift though - but after 25 years I doubt she'll be surprised :P</p>
3517, <p>You would never get dessert again!</p>
3517, <p>Congrats fakeplastictrees. Strangely it is also my friends 25th too!! She got 25 yellow roses - she had never heard of a gpu when I asked if she would have preferred that instead from hubby</p>
3517, <p>Please revamp it in any way that puts be higher than Abhishek! :D</p>
3517, <p>[quote=beluga;118664]</p>  <p>[quote=Bluefool;118657]</p>  <p>Please revamp it in any way that puts be higher than Abhishek! :D</p>  <p>[/quote]</p>  <p>Sure thing <a href="https://www.kaggle.com/gaborfodor/d/kaggle/meta-kaggle/calculating-the-kaggle-user-rankings/notebook">https://www.kaggle.com/gaborfodor/d/kaggle/meta-kaggle/calculating-the-kaggle-user-rankings/notebook</a></p>  <p>[/quote]</p>  <p>hahah awesome!</p>  <p>@inversion - you'd win that. I'd win the Marmite Award <a href="https://www.amazon.co.uk/Pants-Fire-Games-Ltd-PFG006A/dp/B00NU0Q772">https://www.amazon.co.uk/Pants-Fire-Games-Ltd-PFG006A/dp/B00NU0Q772</a></p>
3517, <p>I must admit Kaggle is less interesting for me since Scripts - I think it's because I've been doing it so long that I've become lazy - I can't be bothered spending 3 days coding stuff anymore - I just wait for someone else to do the work!</p>  <p>Also people like NxGTR should be Masters. Top 10 out of 2000 is a lot harder than in the old days. And now with the formation of large teams - people can get their Masters from piggy-backing someone else (like Wei-wu said on the previous page)</p>  <p>I still think there should be a team size limit of 3. Some of the team sizes recently have been stupid. </p>
3517, <p>I think it's the cached Santander LB?</p>
3517, <p>What's that new strange black icon at bottom left of screen?</p>
3517, <p>I still prefer R for munging </p>
3517, <p>Out of the mouths of babes</p>
3517, <p>[quote=TM;120495]</p>  <p>My subsequent submissions were between 83.4 - 84.0% atually. So I would have picked any one of those instead of 60%. It still would not have put me in the top but it would have at least shown a correct number in my public profile. Now the entire time I spent on the competition is a waste as I cannot even showcase it.</p>  <p>[/quote]</p>  <p>You're going to have to put it down to experience and move on. It's unfortunate but at least you won't make the same error in the future. Good luck</p>
3517, <p>I like it but I'm a little worried (guilty?) of our activity been shown. lol</p>
3517, <p>Actually I have a criticism. I think the &quot;Masters&quot; category is a little wide for Competitions. But other than that (and guilt) I like it</p>
3517, <p>Inversion is quite unique so I doubt others will be able to reach GM in Discussions. </p>
3517, <p>Is anyone GM in all 3? I might come out of retirement and write some babies</p>
3517, <p>[quote=Ben Hamner;126991]</p>  <p>[quote=Bluefool;126985]</p>  <p>Is anyone GM in all 3? I might come out of retirement and write some babies</p>  <p>[/quote]No one's even a master in all three (the highest script ranking we have is Seniors excluding Kaggle employees). Inversion is the sole discussion grandmaster.</p>  <p>[/quote]</p>  <p>Awesome. I'm near GM in discussions so just need to write some good early-on scripts. I'm going for it!</p>
3517, <p>Anyone else Top 1% in all 3 tiers? lol I'm on a mission to rule!</p>
3517, <p>hahhaha git!</p>
3517, <p>Suggestion: Be able to sort the User Ranking table by number of Golds</p>
3517, <p>Suggestion: You need to run some more Kernel only competitions other wise it will become a &quot;first come gets the votes&quot; for the kernels</p>
3517, <p>Also I got some award thing for my Hello World kernel but it only gets a silver :( It was script only competition. Only benchmark scripts get loads of votes</p>
3517, <p>Oh no  you've all ruined my ego trip!</p>
3517, <p>Hi Admins There is deliberate malicious down-voting happening. Please name and shame</p>
3517, <p>I don't want to play a fecking game. I want to down vote and up vote the way I have always done. I don't want to stop writing on the forums because people don't want me to be Discussion GM. And Abhishek - that's at least 2 votes of mine have been taken from him for no reason.</p>
3517, <p>lol stop it!</p>
3517, <p>[quote=inversion;127209]</p>  <p>I just found out there is a keyboard that can track and display how many times I up-vote Bluefool.</p>  <p><a href="https://www.kickstarter.com/projects/1229573443/das-keyboard-5q-the-cloud-connected-keyboard">https://www.kickstarter.com/projects/1229573443/das-keyboard-5q-the-cloud-connected-keyboard</a></p>  <p>[/quote]</p>  <p>hahha you always make me laugh!</p>
3517, <p>[quote=&#924;&#945;&#961;&#953;&#959;&#962; &#924;&#953;&#967;&#945;&#951;&#955;&#953;&#948;&#951;&#962; KazAnova;127187]</p>  <p>[quote=dune_dweller;127131]</p>  <p>So am I! Maybe Kaggle should start putting numbers like Top 0.1% there =).</p>  <p>One more thing I like is that I used to be one of those pesky new masters from overly large teams and with the new system I can be a master even with only my solo results. I'm fine being ways away from grandmaster status for now =).</p>  <p>[/quote]</p>  <p>[quote=Bluefool;127134]</p>  <p>Oh no  you've all ruined my ego trip!</p>  <p>[/quote]</p>  <p>revenge is a dish best served cold - haha!</p>  <p>[/quote]</p>  <p>My Team Women Power comments were more directed at the men in the team. I will always blame the &quot;wolf in sheep clothing&quot; men - I was just cross with Dune Dweller for allowing it to happen. She should have metaphorically punched you (in my world). Dune Dweller will succeed on her own and doesn't need &quot;pity help&quot;</p>
3517, <p>[quote=&#924;&#945;&#961;&#953;&#959;&#962; &#924;&#953;&#967;&#945;&#951;&#955;&#953;&#948;&#951;&#962; KazAnova;127243]</p>  <p>[quote=Bluefool;127241]</p>  <p>My Team Women Power comments were more directed at the men in the team. I will always blame the &quot;wolf in sheep clothing&quot; men - I was just cross with Dune Dweller for allowing it to happen. She should have metaphorically punched you (in my world). Dune Dweller will succeed on her own and doesn't need &quot;pity help&quot;</p>  <p>[/quote]</p>  <p>Now I am tempted to give you -1  because you are rehearsing the same things . I did not help her  we helped each other . Our team was fuelled by Women power! </p>  <p>[/quote]</p>  <p>The team was 75% men? stop being cliche</p>
3517, <p>[quote=anokas;127249]</p>  <p>[quote=Bluefool;127245]</p>  <p>[quote=&#924;&#945;&#961;&#953;&#959;&#962; &#924;&#953;&#967;&#945;&#951;&#955;&#953;&#948;&#951;&#962; KazAnova;127243]</p>  <p>[quote=Bluefool;127241]</p>  <p>My Team Women Power comments were more directed at the men in the team. I will always blame the &quot;wolf in sheep clothing&quot; men - I was just cross with Dune Dweller for allowing it to happen. She should have metaphorically punched you (in my world). Dune Dweller will succeed on her own and doesn't need &quot;pity help&quot;</p>  <p>[/quote]</p>  <p>Now I am tempted to give you -1  because you are rehearsing the same things . I did not help her  we helped each other . Our team was fuelled by Women power! </p>  <p>[/quote]</p>  <p>The team was 75% men? stop being cliche</p>  <p>[/quote]</p>  <p>Guys I am not interested in kaggle drama and don't want to take a side - but this is a thread about the new Kaggle progression system not about complaints with past competitions. I am getting sufficient drama from Brexit already!</p>  <p>[/quote]</p>  <p>lol sorry I'm backing off now. Went on my soap box with all the down-votes I received. ha</p>
3517, <p>[quote=&#924;&#945;&#961;&#953;&#959;&#962; &#924;&#953;&#967;&#945;&#951;&#955;&#953;&#948;&#951;&#962; KazAnova;127248]</p>  <p>Also big thank you to the people that started downvoting my top ranked posts ( I will be 472 back in no time!) and started upvoting Bluefool and inversion!</p>  <p>[/quote]</p>  <p>Someone has actively downvoted my posts today as well. I'm just up-voting like I have done for the last 6 years (check my votes given stats if we still have them)</p>
3517, <p>lol not sure how I feel about that either haha. good to know though. Yes my work mate told me that someone was actively down-voting me today. I feel like a 5 year old hahaha</p>
3517, <p>how?</p>
3517, <p>ha don't even start me off on Brexit .............</p>
3517, <p>Zach!</p>
3517, <p>I think Pokemon Go will replace my geocaching!</p>
3517, <p>I voted Abhishek up this morning. Within 30 seconds someone must have downvoted him to get rid of my vote. Also I had 3 downvotes (and 1 friendly upvote) for my silly joke.</p>  <p>The forums have gone brutal</p>
3517, <p>I was really annoyed lol</p>
3517, <p>and I don't want you to be a forum  GM haha</p>
3517, <p>I bet you end up voted down. lol.</p>
3517, <p>and you didn't upvote my post!</p>
3517, <p>lol someone's out to get me.</p>
3517, <p>Thanks for demoting me in both Kernels and Discussions. So someone actively down votes me which made people actively upvote me and now I'm the fucking loser? thanks</p>
3517, <p>fuck the forum.</p>
3517, <p>Reward loyalty my fucking arse. Livid and betrayed</p>
3517, <p>[quote=Myles O'Neill;127438]</p>  <p>[quote=Bluefool;127435]</p>  <p>Thanks for demoting me in both Kernels and Discussions. So someone actively down votes me which made people actively upvote me and now I'm the fucking loser? thanks</p>  <p>[/quote]</p>  <p>Both upvotes and downvotes have the same conditions applied to them so both upvoters and downvoters systematically going through your profile will have been cancelled out. In general the changes mean that less medals are awarded to everyone across the board. You are currently a single silver medal away from hitting Master in discussion again though.</p>  <p>[/quote]</p>  <p>You deliberately reduced my tier in Discussions and you removed my only Kernel Silver. You demoted me in both categories. Fuck your forum . last message from me</p>
3517, <p>So if you write a kernel that gives all the lazy people a high score on the Leaderboard - you get a GOLD</p>  <p>If you write a kernel that wins a Kernel competition and is given the Kaggler's choice award you get a measly fucking bronze</p>  <p>fuck kernels</p>
3517, <p>oh and my Discussion medals are worse now too.  Fuck this</p>
3517, <p>I have 27 votes for a script but only get a bronze. I thought &gt;20 was a silver?</p>
3517, <p>That script won a script only competition. Bit rubbish it only gets a bronze especially as the nature of the competition didn't promote forking. Once again there's only incentive to get people up the leaderboard. People who write scripts for all the new datasets aren't going to get any recognition. But someone can fork (copy and paste and create new script) a Competition script change &quot;<em>.789&quot; to &quot;</em>.80&quot; and get a Gold medal</p>
3517, <p>You should remove medals from Competitions Kernels. They should be for &quot;Datasets&quot; only</p>
3517, <p>What did you look like as a teenager?</p>
3517, <p>You just need to be creative in the way you train and subsample. Apart from Solar Energy and the recent Avito my results have come from maximum 16GB Macbook pro. And even Avito I could get 0.92 on my Mac</p>
3517, <p>I'm not sure they are? In the competition I entered with Kazanova my name is ahead of his but I'm lower ranked. Don't worry about the down-voter the forums became brutal since the change!</p>  <p>EDIT: Alphabetic within tier?</p>
3517, Hi Have you more info on this concept? Not sure how you have categorised the players? ie. How does it work with non-web problems? Why is white IN and Black OUT? Got any papers you couls send me :) thanks
3517, Hi - subject says it all really. Are the ID# the player ranks?<BR>thanks
3517, <p>I think it would have been cheaper for Heritage to spend their money on a data remediation / cleaning project for past data and then putting validation / verification checks on new data entries. Data analysis would become a lot easier then. It's not like  they can't contact the members. Not saying I'm complaining about the competition but being a data management person I can't stand dirty data and the fact everyone just accepts it as the norm- where's input validation and verification? Aren't they training  the staff properly? If it is was my company I'd spend 3 million on hiring people to phone the members and then get in house data analysts to analyse the updated data and software engineers to prevent this happening again. There are no excuses for dirty data.</p>
3517, <p>Hi</p> <p>Before Kaggle was involved in the competition I preregistered for the event. When I read the rules it says that entrants would be expected to pay a "modest registration fee". I can't see this mentioned anymore - has this changed? I'm a bit worried that in 2012 at the final stage we will be charged for entering?</p> <p>thanks</p>
3517, <p>I'm from the UK and we are very lucky to have a free for all health system. From just reading someone's post just now it seems that time in hospital is linked to the insurance company and not a decision made by doctors. Could someone confirm this? Are Doctor's decisions overruled by insurance companies? Very surprised by this. The UK Health system RULES!</p>
3517, <p>Hi</p> <p>I'm getting confused by the laws so I am going to ask questions that are relevant to me that will just require YES or NO answers</p> <p>&nbsp;</p> <p>1. Can I use R?</p> <p>2. Can I use Weka?</p> <p>3. Can I use Excel?</p> <p>4. If I organise the data in a novel way and just use a standard processing algorithm such as Naive Bayes is this OK?</p> <p>&nbsp;</p> <p>Many thanks this is all I need to know</p>
3517, <p>[quote=Information Man;3430]</p> <p>[quote=Karan Sarao;3429]</p> <p>Out of curiosity then how did the benchmark become .4  why pull .4 out of the box?</p> <p>[/quote]</p> <p>Because they want to spend 500k rather than 3m?</p> <p>[/quote]</p> <p>My Thanks was more of a Facebook Like as it made me laugh!</p> <p>&nbsp;</p>
3517, What's &quot;Ambulance&quot;? Is this a 1 day Urgent Care? Thanks
3517, <p>Hi</p> <p>No - keep going. I just put the data into MS Access and been fiddling - can import and export on a 4gig ram Windows PC</p>
3517, <p>Hi</p> <p>I would prefer the CSVs please</p> <p>thanks</p>
3517, <p>Hi I don't think there is a customer 40 in the training set</p>
3517, <p>I really enjoyed this competition even though I wasn't very successful. Just saying thanks</p>
3517, <p>Hi</p> <p>Sorry for simple question but having trouble finding a complete reference. What do these numbers mean in the R script</p> <p>[-c(12712)]</p> <p>I know it's a data frame object and I know there are 12 columns but can't figure out the rest. I want to either delete a column or add a column to the data so I'm assuming these numbers will change.</p> <p>&nbsp;EDIT: not using these columns? Think I've sussed it</p> <p>thanks - new to R</p>
3517, <p>Hi</p> <p>Sorry if I've missed the information but how are the predictions evaluated please?</p> <p>&nbsp;</p> <p>EDIT: OK - seen AUC on leaderboard</p>
3517, <p>I've been using ROC in the R package Verification <a href="http://rss.acs.unt.edu/Rdoc/library/verification/html/roc.area.html"> http://rss.acs.unt.edu/Rdoc/library/verification/html/roc.area.html</a></p> <p>&nbsp;</p>
3517, <p>Is AUC the best way of evaluating the models? I would have thought sacrificing FPs for TPs would be more realistic in the credit industry. My training model with 99% TNs 18%TPs has a better AUC than my balanced model of 88% TN 66% TPs. (unless I have calculated  the AUC incorrect) I think if I was a banker I'd prefer the second model.</p>
3517, <p>I'd prefer a maximum submission limit. Sometimes I can work on it all day and sometimes I don't have time at all. I lose my 2 submissions on my busy days</p>
3517, <p>Hi Excel is having trouble with the purchase date. Please can you confirm that all dates are in US format mm/dd/yyyy?</p>
3517, <p>I predominantly enter the competitions for fun. I can get bogged down with teaching and therefore the competitions are a ready made challenge for me where I don't need to create a problem find data publish a paper etc. I enjoy these competitions very much.<br> I would also like to say that Kaggle is hosting a student competition for me. My students are really enjoying it. There are 37 of them and they have made about 1100 entries between them. I am very grateful for this. There are many Universities who have hosted  student competitions (see Kaggle in Class) - this is all free-of-charge and Jeff Moser manages the forum and the evaluation / validation programs.</p> <p>As for jobs entering these competitions is a good way of learning about data analysis. I recommend that my PhD students enter. Each new competition adds new learning - either the evaluation criteria data organisation etc I learn something new each time  and I am now an R convert because of Kaggle (and the lovely forum people) - learning these new skills makes one more employable.</p> <p>Just my view - I would love to win but if I don't I'll just enter the next competition! </p>
3517, <p>Is this competition normalised gini? or doesn't it make a difference?</p>
3517, <p>Thanks - that's what I thought (the other competition had Normalised Gini on the leaderboard). I used the function above and thought I had some serious overfitting going on! Ok now though</p>
3517, <p>[quote=JKARP;5946]</p> <p>I am so confused...</p> <p>Is it true or not that gini = 2*AUC-1</p> <p>When I calculate AUC for this project and run the model on a holdout sample I get gini's much higher than what is being reported on the leaderboard.&nbsp; I am gettimg AUCs of 0.75 which based on the equation above would produce a gini of 0.5.&nbsp;</p> <p>What am I missing?</p> <p>[/quote]</p> <p>I can't answer your question but I can say that if you use the inner function of Alex's code above then the score you get on training is about the same as the leaderboard (for me anyway)</p>
3517, <p>real number [01]</p>
3517, <p>Bit confused. I used the zip files and yes those ids aren't there but I don't get submission errors when I submit. I would assume I would have the wrong number of rows?</p> <p>&nbsp;</p> <p>EDIT: My number of rows is the same as requested in the &quot;make submission&quot; section. I assume people using the .csv not from the zip would get submission errors?</p>
3517, <p>You've written above that &quot;the probability of the user getting a question correct is simply logit(ability - difficulty). These abilities and difficulties are estimated using the lmer function from the lme4 package&quot; but in the R benchmark code the prediction  seems to be the sum of the constant and the random effects for user (ability) and question (difficulty) </p> <p>predictions[row<em>id] = logit(sum(c(model</em>info[[&quot;constant&quot;]] model<em>info[[&quot;question</em>est&quot;]][as.character(question<em>id)] model</em>info[[&quot;user<em>est&quot;]][as.character(user</em>id)]) na.rm=TRUE))</p> <p>Am I missing something here - I've not use IRT before.</p>
3517, <p>Hi I know there's an introductory post about this but I think the Forum is broken and is not allowing me to go to the 2nd page of topics.</p> <p>Anyway I'm having trouble understanding IRT and reverse engineering the code. Could someone in simple terms explain to me how the user ability and question difficulty is worked out.</p> <p>ta</p> <p>&nbsp;EDIT: Are these just the &quot;random effects&quot; output from LME?</p>
3517, <p>I think new data has come out - read the other post that's just come out</p>
3517, <p>Yes - a lot of people use it. The software you use only becomes an issue if you are in the top 3.</p>
3517, <p>The best plan is to learn the subject on your own enter some knowledge competitions then enter a main competition on your own. Then when you've shown that you can contribute then invite people to join you.</p>
3517, <p>There's tutorials with code in the Knowledge competitions</p>
3517, <p>Tristan have a go at some of the Knowledge competitions and research competitions. When you feel you know the area then send someone an email near the same rank as yourself to see if they want to team up. You need to be careful as well because some people cheat so you need to check that everyone on your team knows the rules. Kazanova https://www.kaggle.com/users/111640/kazanova and myself are in the UK and we've had a UK team before (&quot;UK calling Africa&quot;) so when you're ready  we can always have another UK team</p>
3517, <p>I have a 16gig mac at the moment. Apart from the Solar competition all my results have been on an 8gig mac. Sometimes I want more memory but 16gig not too bad</p> <p>EDIT: and I have a dual-boot 16gig windows/ubuntu machine too</p> <p>EDIT: and I have a good graphics card for GPU stuff</p>
3517, <p>If I was you I'd upgrade to 32gig and play with some competitions. When you're confident enough then start thinking about further hardware / AWS</p>
3517, <p>I won a Music Genre Competition in 2011. Report of competition here www.mimuw.edu.pl/~mwojnars/papers/ismis-2011-contest.pdf . Other people used SVMs - we didn't. Our paper is here : http://link.springer.com/chapter/10.1007%2F978-3-642-21916-0_76 but not sure if you need subscription. I don't have copy of paper as I'm a rubbish academic!</p>
3517, <p>yes - some algorithms can be tuned to a metric. So in R for glmnet you can evaluate the CV by AUC  MAE or MSE. In R for GBM you can use MSE or MAE etc. Basically you need to evaluate your CV based on the metric used by Kaggle - it just gives you a more consistent score between CV and LB</p>
3517, <p>Interesting. My answer is &quot;probably yes&quot; but I'm a creature of habit and always run (with the correct family/distribution/cv metric set to the Kaggle metric) GBM GLMNET Random Forest and nowadays XGBOOST and if Big Data online code in Python (Tinrtgu's code)</p>
3517, <p>[quote=the1owl;83332]</p> <p>Now submit your fist prediction&nbsp;by forking your favorite&nbsp;script and clicking on the CSV submit.</p> <p>[/quote]</p> <p>lol you were doing so well until then Owlie!</p>
3517, <p>I have 4 Usual Suspects:</p>  <p>GBM</p>  <p>RF</p>  <p>GLMNET</p>  <p>XGBOOST</p>  <p>I have a validation script that runs all 4 against a holdout set - this is how I tune the parameters.</p>  <p>But remember GIGO Garbage In Garbage Out. You need to make your data suitable first (create features)</p>
3517, <p>Have a look at the Knowledge competitions. Can do your project on the &quot;Titanic&quot; (though not BigData)</p>
3517, <p>R for munging and mining</p>  <p>Python for mining</p>  <p>Perl for munging</p>
3517, <p>Anyone noticed that the year made in the appendix has mistakes. EG in Train machine</p> <table border="0" cellspacing="0" cellpadding="0" style="border-collapse:collapse; width:65px"> <colgroup><col width="65" style="width:65pt"></colgroup> <tbody> <tr style="height:15.0pt"> <td align="right" width="65" height="15" style="height:15.0pt; width:65pt">1080989 &nbsp;</td> </tr> </tbody> </table> <p>was made in 1984 and sold in 1989. In the machine appendix it was made in 2010 which means it was -11 at sale rather than 5 years. Not sure which year made to use?</p> <p>&nbsp;</p> <p>EDIT: OK saw earlier post - just bad data</p>
3517, <p>Hi</p> <p>Thought I'd install Python on my Mac to run the benchmark code. I'm new to Python. When I run the code I get the error</p> <p>&nbsp;</p> <p>File &quot;random_forest_benchmark.py&quot; line 8<br> SyntaxError: Non-ASCII character '\xc2' in file random_forest_benchmark.py on line 8 but no encoding declared; see http://www.python.org/peps/pep-0263.html for details</p> <p>Not sure if I need to change something here?</p> <p>&nbsp;</p> <p>many thanks</p> <p>&nbsp;</p> <p>ps. just checking I have the paths right</p>
3517, <p>No I haven't. Thank you</p>
3517, <p>What does <strong>Serialize the trained model to disk</strong><span>. actually mean please?</span></p>
3517, <p>Bit confused as well. Doesn't Altitude determine temperature A &lt;- B in the slide rather than temperature determines altitude? Also as with the commenter above doesn't Age determine wages? so A -&gt; B</p> <p>&nbsp;</p> <p>Both of these slides are opposite to what is expected?</p>
3517, <p>How can you test for non-inveribility? I'm getting lost in maths.</p>
3517, <p>Blimey - just read the other rules! Very harsh to expect people to write a 6 page paper just to be considered for the prizes! Don't mind submitting a model / software but won't be writing a paper unless I'm in top 3 (which we won't know until the conference)</p>
3517, <p>Hi</p> <p>What packages are people using for the AUC stats? I've tried packages &quot;verification&quot; and &quot;ROCR&quot; but they are for binary classes.</p> <p>thanks</p>
3517, <p>Thanks but the language is too formal for me to understand.</p> <p>Do you mean you remove all -1 predictions and just do auc{01) then remove &#43;1 and do AUC{0-1) then take the average. </p> <p>&nbsp;</p> <p>&nbsp;</p>
3517, <p>I'm not a statistician so having trouble understanding invertibility. I have understood about inverse of matrices but to see if a matrix has an inverse it has to be a square matrix. Is it something simple like the data types?</p>
3517, <p>thanks&nbsp; - I now have something to Google - vertical line test.</p> <p>Isabelle - none of those papers are useful to me because they are for experts and I can't try out the software because it's all in Matlab.</p>
3517, <p>Ah - not me!</p>
3517, <p>Is it for time-series as well? like Granger?</p> <p>&nbsp;</p> <p>&quot;Our alternative approach convergent crossmapping<br> (CCM) tests for causation by measuring the<br> extent to which the historical record of Yvalues can<br> reliably estimate states of X. This happens only if X<br> is causally influencing Y. Inmoredetail CCMlooks<br> for the signature of X in Y’s time series by seeing<br> whether there is a correspondence....&quot;</p>
3517, <p>&quot;<span style="font-family:verdanasans-serif"><span>The papers will need to be accepted to qualify for prizes.&quot;</span></span></p> <p>&nbsp;</p> <p><span style="font-family:verdanasans-serif"><span>It will be difficult for a non-expert in the domain to get a paper accepted in the domain</span></span></p> <p>&nbsp;</p>
3517, <p>Hi I have a strange problem so I thought I'd share. I had written an R script with a GBM a GLM and a Random Forest. Random Forest always produced the best results 88.406 on leaderboard. I thought I'd best tidy up my code so I removed the GBM and GLM from  the script. The random seed has been set the same for both scripts. However when I now run the script with just the Random Forest my score is 88.385. I've looked at the variables set for GBM and GLM (ntree minobs depth lambda) and I can't see a clash  in the wording with anything from Random Forest. I know it's not a big change and for the competition it doesn't really matter - but I was wondering what it could be. Any ideas?</p> <p>&nbsp;</p> <p>EDIT: And for the first training tree the MSE difference is 2%</p>
3517, <p>yes - that's what I thought. It must reset the seed the same way though as I can reproduce the results for each individual script. It must leave the seed different - haven't come across this before as usually averages work the best.</p>
3517, <p>Cruncher - this hasn't been a stereotypical competition. It's probably not the best one to have as your first! Go and have a look at the Amazon competition.&nbsp; The data will be changing for this competition so I wouldn't waste your time at the moment. (Look  at the leaderboard -noone's entering anymore) Have a look at one of the knowledge competitions.</p>
3517, <p>You shouldn't really change the test data class distributions - the valid test set would have been selected based on these distributions. This means how a model performs on the valid set will now be different from the test set! Not really fair as you only  have one shot on the test data (and can't see results).</p> <p>&nbsp;</p> <p>&nbsp;</p>
3517, <p>I'm a bit put out that the test data has been changed half way through this competition. I assume the valid test set was a random sample taken from the data - but now the rest of the data will be changed which means that the valid set is not a reflection  of the test set anymore. Considering you only have one blind shot at the test data - this is very frustrating especially when you have to write a 6 page academic paper just to take part!</p>
3517, <p>Also how did you find out about the &quot;artefacts&quot;? By looking at people's valid predictions?</p> <p>&nbsp;</p> <p>I think you will need to change the train / valid and test data if you are now going to muck about with the test data</p>
3517, <p>&nbsp;Will you disqualify someone with a high score because it just happens their algorithm picks up the artificial data? Will you disqualify someone who follows the benchmark and uses unique values? I understand the need to publish but in another competition  I only had to publish after I had been told I had won. Here you have to write the paper before with the risk you may be disqualified because of artefacts you weren't aware of!</p>
3517, <p>You could write another paper on the perils of creating artificial data. Domain knowledge has now increased as you now know how not to create artificial data. So it's all good. Science is incremental. People should publish failures more often as it will  reduce duplication of effort (and failure!)</p>
3517, <p>If the final evaluation is real data then the training and valid data should also only be real</p>
3517, <p>Ok - don't know if this helps as I don't know if I'm someone who has predicted the artificial data. I have just made 2 submissions (9a and 9b) 9a included number of unique values and the max values. 9b includes unique / total ratio only. 9a scored about  88.248 and 9b scored about 87.883. If submission 9b is better at predicting artificial data then there are more artefacts than just unique values and max value.</p>
3517, <p>My 2 &quot;purest&quot; features according to the Random Forest is Mean of A and&nbsp; Mean of B. I removed them from the dataset and ran 9a (above) The score is 0.88205!</p>
3517, <p>But all of these are standard variables - they would also be needed to classify the real data? Anyway have you noticed the jumps in the leaderboard since the artefacts have been revealed?</p>
3517, <p>mmmm so what's the actual plan then?&nbsp; Is there a point in giving more artificial data? I'm very annoyed about the amount of time I've wasted on this competition</p> <p></p> <p>Do we get rid of the old training data? You haven't explained yourself very well</p> <p></p> <p></p> <p>Sounds like you are giving more artifical data in training and only have real data in testing.&nbsp; lol!!! The validation set is supposed to be a reflection of the test set so people can estimate their final score. Now I've 88 on validation and might end up  bottom!!!</p>
3517, <p>Also you're pretty much choosing who will win the competition : You've gone through peoples validation predictions found people with good real predictions and now finding a way to make them win</p>
3517, <p>you haven't released new validation data so have you just given us the answers?</p>
3517, <p>Moody comment</p>
3517, <p>And what is the point of rebuilding the models when in a couple of weeks the data will change again to suit her favourite validation predictions? I doubt this will be the last data change. We now have some answers to a validation set a validation set based  on discarded training data and as yet a test set that does not exist but will exist based on validation predictions.</p>
3517, <p>Hi - I haven't bothered to rebuild them. I'm very busy which is why I put so much effort in at the beginning. Also I don't want to put new effort in because it's likely that the data / validation / *everything* will change again. I'm going to wait a few  weeks and then give it a go.</p> <p></p> <p>Though I think I will go down because my model didn't deal with negative numbers! so I need to rethink the whole thing</p>
3517, <p>35% is a huge drop! I ran my preprocessing tool but it crashed with the negatives so I need to start again but it's not a good time for me to write &quot;fun&quot; tools</p>
3517, <p>[quote=Isabelle;24676]</p> <p>The results on the validation set are shown on the public leaderboard. The final test set (which has not been released yet) will be used to produce the results of the final ranking that will show on the private leaderboard.</p> <p>[/quote]</p> <p></p> <p>Isabelle</p> <p></p> <p>1. The public validation leaderboard is now useless - it will not be a reflection of the score one would obtain on the final test set</p> <p>2. In the rules it say when the test set is released the answers to the validation set will be given</p> <p>However the validation set is in the old format and therefore the &quot;artificial data artefacts&quot; will be present. Therefore is there any point in using the validation set for predicting the test set?</p> <p>3. You need to make a decision on the final test set and should really stop looking at peoples validation predictions - this is introduing bias into the competition</p> <p>I appreciate the data changes are best for your field but I don't think you're appreciating data analysis protocols - the validation set is vital to data miners but the one that is provided is totally different from the new training sets and is totally  different from the test set. This is data analysis sacrilege!</p> <p></p>
3517, <p>it's any real number so your predictions are -inf&nbsp; to &#43;inf</p> <p>So having predictions -1.34 -.74 -0.0980 .32.681.34&nbsp;&nbsp; are all valid</p>
3517, <p>Any update?</p> <p></p>
3517, <p>I'll start with a sad disappointing result:</p> <p>Original training data : 85% training;&nbsp; Applied to leaderboard validation set: 88%</p> <p></p> <p>Sup2 only</p> <p>Training : 69%; Applied to leaderboard validation set in original form : 63%</p> <p>oops</p> <p></p> <p></p> <p></p>
3517, <p>It's just a bad evaluation metric. So just carry on - the competition is ruined once again. This was an observation that didn't need to be shared</p>
3517, <p>It's not a bug - it's bad choice of evaluation metric</p>
3517, <p>Are you sure? because I get the exact same in training</p> <p></p> <p>I get 86 AUC one way A-&gt;B and 82 AUC B-&gt;A in training so the score I got I was expecting</p>
3517, <p>[quote=Sitmo;27017]</p> <p>I think it's great that James shared it. It's a flaw that needs fixing otherwise we are not doing science</p> <p></p> <p>[/quote]</p> <p></p> <p>I didn't realise we were doing science I thought it was a Kaggle competition where many competitions have been won by de-anonymising and data leakage (take the recent KDD competition) The science of this whole academic field has been put to the test. The  way that artificial data is created is flawed the metric is deifnitely flawed (going by my training results the leaderboard reflects this score) and there's no current &quot;state-of-the-art&quot; benchmark for the new data. I gave up on the science ages ago.</p> <p></p>
3517, <p>[quote=Sitmo;27025]</p> <p>The evaluation metric could be chosen to be anything it's part of the challenge. &nbsp;However the implementation of the described metric has indeed a bug: it does not adhere to the specs it behaves different.</p> <p></p> <p>A lot of people must have been building symmetrical model probably tweaking them accidentally into a direction that was preferring asymmetry. It would be really helpful if *all* historical submissions were re-evaluated with the fixed metric rather than  resetting us all again to below 0.50. Do you think that's possible Isabella? It would provide us with much better feedback compared to a reset.</p> <p>[/quote]</p> <p></p> <p>Well it must be an R bug then because as I've said I get over 80% AUC both ways in training (and none of my predictions are &lt; 0)</p>
3517, <p>I think it depends if you use the ROCR package or the VERIFICATION package (making -1s to 1s). On one I get 30 AUC and the other 80. It's how it deals with the -1. Sorry James - I just couldn't understand what you were saying because of the symmetry I was  getting following the same model. </p>
3517, <p>lol you just made me laugh. First time I've laughed about it since the &quot;troubles&quot;. lol Been a Kaggle member for years and this is the first time I've turned into an insane cow. Cause and Effect!</p>
3517, <p>Hi Are your sure the new ones are correct? I was expecting about 68. (OK -mine haven't been rescored yet)</p>
3517, <p>All my submissions are still &quot;Pending&quot;. Do I have to resubmit all previous models or are you doing the rescoring by punch cards?</p>
3517, <p>Hi Isabelle - yes the problem persists - none of my entries have been rescored. I think it will be quicker if I re-run them all. New submissions are being scored correctly.</p>
3517, <p>Thanks - all fixed now</p>
3517, <p>Hi We're not going to know if it's incorrect or not. Same with your other post. If you think they're incorrect so be it but it makes no difference to us as competitors. There's some level of noise in the data so expect some labels to be incorrect</p>
3517, <p>The labels are nothing to do with Kaggle - they are provded by the &quot;customer&quot; ie the experts in the areas.</p>
3517, <p>The data is not time-series or ordered.</p>
3517, <p>I've uploaded another model with a new readme file</p>
3517, <p>I think it's just like other Kaggle double-hit competitions. The important thing is to submit test predictions even if you haven't submitted a model (this confused me in the KDD competition)</p>
3517, <p>No - have a look at the KDD leaderboards. Everyone who doesn't submit test predictions is given the same rank . In this competition I reckon everyone even the bottom placed competitor will get in the top 25% (as in many people share last place so 200 people could end up with rank 40). All the code stuff and rules is only really important if you come in the top 3 or want to submit a subsequent paper at the workshop. For example if I finish in the top 3 of the leaderboard then I will fulfill my obligations of tidying code open sourcing paper writing but if I don't then I won't be touching anything again! It won't effect my Kaggle rank / points. I have a basic readme file with my uploaded model and they can recreate the reults but the rest can wait until I see my final score.</p>
3517, <p>Create attributes (columns / features) from the data points and then run a standard regression-based classifier on them. All competitions could have manual tweaking but it would be embarrassing for a competitior if they were found out!</p>
3517, <p>Will the Valid labels be released?</p>
3517, <p>[quote=Sitmo;29031]</p> <p>What is the purpose of the test data submission task&nbsp;next week? I was under the impression that the leaderboard score was based on a subset of our submission and that the final ranking would be done today on the full evaluation of selected 3 best submissions?</p> <p>[/quote]</p> <p>&nbsp;</p> <p>No it's a 2 stage competition. You will now have to make submissions on the test set using your model&nbsp; (if you uploaded one) I assume we retrain using the exact model when we have the valid labels</p>
3517, <p>Hi Isabelle</p> <p>thanks for your email threatening to disqualify me. I have read the rules:</p> <p>Reproducibility: To qualify for prizes the participants must submit their software prior to the deadline (see the updated schedule) and cooperate with the organizers to reproduce their entries. This will include filling out a fact sheet about their methods. The winners will be required to make their code publicly available under a popular OSI-approved license if they accept their prize within a week of the deadline for submitting the final results.</p> <p>&nbsp;</p> <p>This is what we have done. Our results can be reproduced. You have stated that we will be disqualified if we don't :</p> <p><strong>&quot;I will run it to produce predictions on the test data BEFORE releasing the </strong><br><strong>decryption key.</strong> If I cannot do that you will be disqualified.Hence please provide tomorrow:- the trained model(s) that you will use to make the final predictions- scripts to perform feature extraction and prediction using the model(s)- good documentation&quot;</p> <p>Having an executable that new data could plug into was not a requirement of the rules. Originally you said that the validation labels would be released which meant our models would be retrained so why the change. I'll only be changing the filenames of the scripts you have  ie VALID to TEST so not sure why you feel the need to threaten people with disqualification.</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p>
3517, <p>Hi Do you have the split files for the test set? My code uses the splits</p>
3517, <p>Do we have to resubmit if we successfully submitted the first time?</p>
3517, <p>sorry mistake</p>
3517, <p>Our score is just very slightly less from leaderboard score. I think the surprises might come in from people who didn't get model validated but get high score?</p>
3517, <p>Thanks Isabelle. I'm very very happy with our result :D</p>
3517, <p>Maybe didn't submit test results and this competition may be different in the way it deals with non-submissions? (or something simple like only first few pages snapshot?)</p>
3517, <p>It's probably the first few pages only maybe it shows down to the people with 00000</p> <p>&nbsp;</p> <p>Edit: Didn't see Isabelle's post when I posted this</p>
3517, <p>Usually they will be all be ranked at 70th. Your rank will be out of everyone</p>
3517, <p>I think they were reproducing all our test results to see if they matched the leaderboard - probably takes time.</p>
3517, <p>That's only the provisional scoreboard. I'm hoping the Kaggle ranks will get changed according to the test set submissions that have been reproduced. Also Kaggle will try and remove the teams with multiple accounts - so the provisional board posted won't be the final board.</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p>
3517, <p>crap question?</p>
3517, <p>[quote=Abhishek;30560]</p> <p>[quote=Jeff Moser;30559]</p> <p>The private leaderboard is now available.</p> <p>[/quote]</p> <p>So the final rankings and badges have been given by taking only 68(or 69) teams as total number of participants?&nbsp;</p> <p>[/quote]</p> <p>&nbsp;</p> <p>They are all ranked at 69 - all the competitiors have been included so your rank is out of over 200</p>
3517, <p>Well done for new badge. I should get Master now but that hasn't been updated yet. What you up for?</p>
3517, <p>[quote=Sitmo;30566]</p> <p>Thanks for efforts And feedback!</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Cheeky boy!!! but TBH Kaggle points rule ;)</p>
3517, <p>Yay well done We can have Master-scale arguments now</p>
3517, <p>congrats - I feel strangely satisfied about it. God knows why as it has no effect on my life!</p>
3517, <p>Our method was an ensemble of 3 models. 2 were data intensive and Sayani's was data analysis intensive.</p> <p>Model 1 and 2 preprocessing:</p> <p>Created a total of 186 features. 43 were from discretizing the numerical data and putting them into 10 equal width bins; 59 were raster-image based (divided scatterplot into 25 quadrants) and the rest treated all data as numeric. Lots of curves were generated (up to polynomial 5 and logs) and some checking for invertible functions (duplicates of fitted points)</p> <p>Model 1: Ran Random Forest 500 trees on all this data</p> <p>Model 2: Ran basic Best First Greedy Search by data type on this data and ran a Random Forest per data type.</p> <p>Model 3 : Independent model of ensembles of GBMs and Random Forest on different types of attributes.</p> <p>&nbsp;</p> <p>Final model : A ratio of each the predictions from the 3 models above</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p>
3517, <p>Hey Dee5 - who were the other two members (novices) in your team - You only have given the one name when I was expecting the whole team to be named.</p>
3517, <p>Also I assume the 3rd place team didn't submit code does this mean their Kaggle 3rd place can be challenged?</p>
3517, <p>Congrats for winning x</p>
3517, <p>The challenge for this competition isn't the lack of information about the data but just the lack of training data.&nbsp; Not sure why it's called the &quot;black box learning challenge&quot; - would be more deserved to be called &quot;Lack of training data&quot; challenge.</p> <p>&nbsp;</p> <p>The winners would have proved that they are the best at working with minimal training data. Lack of data knowledge seems to be irrelevant (which defeats the whole object of the competition)</p>
3517, <p>I don't consider unlabelled data as training data. Most of the competitions on Kaggle don't require an expert so I'm not sure why this one is any different. (apart from the lack of training data) So the challenge is semi-supervised learning rather than black  box</p> <p>EDIT: Sorry - just was excited about not having to spend intellectual effort on this competition (ie just data mining) but now realise I have to spend more intellectual effort learning semi-supervised methods. No time for this effort :( </p>
3517, <p>I tried using nnet (though I'm not used to working with MLPs) and it kept saying too many weights so couldn't run.</p>
3517, <p>ah yes - silly me. :)</p>
3517, <p>I reckon it's image data</p> <p>&nbsp;</p> <p>EDIT: and to be really specific I started thinking too much on it and thought could be 75*25 Facebook cover photos!!! </p>
3517, <p>Hi I get a message under &quot;make a submission&quot; saying</p> <div class="x_information-message">This competition has completed. You can still submit an entry to see what your score would have been.</div> <div class="x_information-message"></div> <div class="x_information-message">When can we submit test predictions. thanks</div>
3517, <p>Hi Some new entries with no submitted models have made it on the private leaderboard. Could you check these please?</p> <p></p> <p>There's at least 5 entries where the test predictions were submitted but they have no models. I'm assuming they shouldn't be on leaderboard</p>
3517, <p>Private Leaderboard entries 39 42 46 50 52 58 61 63&nbsp; shouldn't be there at should be at 000000</p>
3517, <p>Ah sorry I misunderstood</p>
3517, <p>I don't think it is allowed because this person http://www.kaggle.com/c/yelp-recsys-2013/forums/t/5380/please-ignore-my-second-submit-the-top-one</p> <p>performed a web crawl and was 0.86 on leaderboard. He possibly subsequently deleted his account because his score has now gone. It's put me off competition because people can reverse engineer back to a model.</p>
3517, <p>I ran an experimental model where I only used data that covered train and test (reviewcounts in dataset longitute latitude etc - no average stars) The CV result was still far off from the leaderboard score</p>
3517, <p>Are the review IDs in the correct order? I just made a usbmission and it was ok</p> <p>&nbsp;</p> <p>EDIT: I also made a SUBmission and that worked too ;)</p>
3517, <p>Problems began when you introduced a ranking system It plays on the competitive nature of humans. Having a high Kaggle rank is good for the CV but this has unleashed the worse in the competitors&nbsp; - for example I don't know why you're worrying about the &quot;other competitors&quot; because I'm such a cynic I have assumed that they are just the second accounts of the people who have entered! In the Cause and Effect competition I am currently 7th and have had my model validated however I probably won't end up in the top 10 because people who haven't abided by the rules (uploaded model had model validated used external data) will pip me at the post. If they don't finish in the Top 3 then their methods will never be verified - they could have manually made predictions for all I know - but they will still get more Kaggle points than me who did stick to the rules. For a couple of competitions I have started to also &quot;play the system&quot; by just submitting the benchmarks - this is because this person https://www.kaggle.com/users/16491/vikram-jha is 56th in the WORLD and has only ever submitted the benchmark in every competition. Looks good for his CV but has he ever done any data analysis. There's one thing ranking the competitors but not then adding means of fair monitoring is not good. If I have noticed about how the system can be played then business and academics will and it will damage Kaggle's reputation. Anyway since the ranking system was introduced it has caused me additional stress!</p>
3517, <p>Hi Is it just the test set that has changed?</p>
3517, <p>How come in the winning team there is a novice who joined Kaggle an hour ago? The rules clearly state Mergers are disallowed within 7 days of the competition deadline. Teams were also merged in the last week of the Cause and Effect competition.</p> <p>I wish Kaggle would stick to their own rules it's as if they encourage cheating</p> <p>&nbsp;</p>
3517, <p>It was a definitely a merge in the other competiton as both teams had submitted. And effectively it is still the creation of a new team on the day of the deadline</p>
3517, <p>[quote=ParagonLight;29831]</p> <p>WTF.... So I can create many accounts and merged into the term in the first place and get the achievements such as 'Top 10%' etc...with high probability. It is very suck...</p> <p>[/quote]</p> <p>&nbsp;</p> <p>yes and it's also a good way to deal with one's multiple accounts!</p>
3517, <p>Daniel*</p> <p>* (P.s. good name ;) )</p> <p>&nbsp;</p> <p>I had thought that too. A &quot;normal&quot; name to chose! ;)</p>
3517, <p>[quote=DirtyCheater;29878]</p> <p>Just to make things obvious. I wrote simple crawler and got 1.20755 on public leader board for less than 30 minutes of work. Good luck everyone with this contest.</p> <p>[/quote]</p> <p>&nbsp;</p> <p>lol</p>
3517, <p>I realised 2 things today - my data preprocessing skills are slow and I make mistakes when under pressure. Realised I had made a mistake at the preprocessing stage only noticed an hour before deadline. All good fun</p>
3517, <p>I used a Random Forest - 880 attributes all based on differences and percentages. Though I noticed I had made a mistake in the distinct categories (didn't realise they were cumulative!)</p>
3517, <p>Hi are the test files one continuous recording or are there lines dedicated to each microphone? When I open a random file there are 16 distinct lines</p> <p>thanks</p> <p></p> <p>EDIT: realised it is continuous. Couldn't find the commas at line ends but I think it's my editor!</p>
3517, <p>I represented the data badly - really overfits. Got 98% 10CV and 84% on leaderboard. I don't have access to resources at the moment so can't convert to boolean</p>
3517, <p>Hi It's because there are a lot of cases in the test set that aren't in the training set. It's a hard one to evaluate the training on - can easily get 100% but then it overfits. The model needs to be loosely trained to make it more general. Use a 10 fold  cross validation as well - this will give you a better estimate on how it will perform on the test data</p>
3517, <p>Have you changed the Max Heap option in Weka? May need more memory</p>
3517, <p>Hi Finally installed Python. I think I've installed all the modules but I'm getting a syntax error - &nbsp;File &quot;logistic_regression_updated.py&quot; line 62&nbsp;&nbsp;&nbsp; print 'Saved'&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ^SyntaxError: invalid syntax</p> <p></p> <p>Sorry for something trivial but anyone know what this could be? ta</p>
3517, <p>Thanks very much. Yes I installed the latest Python though IT people said they had to put an earlier version on as well. I'm not used to Macs</p>
3517, <p>Yes I asked for a semantic diagram but was told to look at the data. But looking at the data only gives correlation and not dependency and hierarchy. I was going to use relational learning but couldn't suss out the hierarchy</p>
3517, <p>Hi</p> <p></p> <p>I've decided to make a first-step look at&nbsp; Python. Usually use R and Perl. I have a Mac. Which add ons would I need? I know about numpy scipy sckit-learn - what others would I need? I tried to sort things out before but some of the add ons only worked  in previous versions of Python.</p> <p></p> <p>thanks</p>
3517, <p>I did something similar for my first go. I used a LM of each column lm(action~factor(resource)) and used the p value of each category as the input into a RF. I got 98% 10 fold CV but 84% on leaderboard</p>
3517, <p>I use Mac TextEdit for R and Python (though I only started using Python today so this may change)</p>
3517, <p>I just do it for fun. There's real-world data analysis and there's Kaggle where we chase the 0.001s. For example in the real world if I achieved 80% with a simple ensemble then there's no point adding on several hours of computational time and manual effort for 1%. But in Kaggle that's important. Kaggle is great for learning the subject area - I learn something new each competition.</p>
3517, <p>Hi</p> <p>Would you be able to provide a little semantic diagram please? I'm having trouble working out the hierarchy - isa and hasa</p> <p></p> <p>thanks</p>
3517, <p>It's a ML problem because it is a regression / classification task. Based on categorical variables (roles / titles etc) which just happen to be represented as numerical IDs one needs to predict the ACTION which is to either Allow or Deny the employee (row  in data) access to a RESOURCE. You need to find a ML method that copes with categorical variables and a numerical class</p>
3517, <p>Hi. Thanks for code. I'm not sure how to run it. What input files are you using? It looks like train10.mat but where did you get this file from?</p>
3517, <p>Sorry no - miml_knn_train.py (3.65 KB)</p>
3517, <p>ah ok thanks - thought I was missing a file</p>
3517, <p>Bump. Are the correct Alltagginginfos.csv loaded? I don't want to have to download the data again as it was huge just get the tagginginfos.</p>
3517, <p>grep? head? sed? http://stackoverflow.com/questions/191364/quick-unix-command-to-display-specific-lines-in-the-middle-of-a-file</p>
3517, <p>seems so. waiting to hear</p> <p>&nbsp;</p> <p>https://www.kaggle.com/c/belkin-energy-disaggregation-competition/forums/t/5156/taggedinfo-from-csv-files/27572#post27572</p>
3517, <p>I was able to run it as is. Have you installed the netcdf4 package?</p> <p>Also have you checked the files are in the correct location?</p>
3517, <p>I had to revert back to python 2.7 to run it (can't remember why) so I installed netcdf4 for 3.3 and 2.7</p>
3517, <p>Thanks for paper. I used a Random Forest on Alec's code but I got a worse result than his linear model and it took ages to run. Having to rethink it.</p> <p>&nbsp;</p> <p>EDIT: Though I'm new to Python so might not have done it correctly!</p>
3517, <p>More of a Python question. I've been trying to run other algorithms&nbsp; using your data transformation. Is there a way to change the shape as a lot of the algorithms complain about it? Newbie to Python usually use R</p>
3517, <p>ofile&nbsp; = open('testtrans.csv' &quot;wb&quot;)&nbsp;&nbsp;</p> <p>&nbsp;writer = csv.writer(ofile delimiter='' )&nbsp;&nbsp;&nbsp;</p> <p>&nbsp;for row in testX:&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;</p> <p>&nbsp;writer.writerow(row)&nbsp;&nbsp;&nbsp;</p> <p>&nbsp;ofile.close()</p> <p>&nbsp;</p> <p>and do for trainX as well</p>
3517, <p>This might help http://www.kaggle.com/c/ams-2014-solar-energy-prediction-contest/forums/t/5057/understanding-data</p> <div>&nbsp;</div> 
3517, <p>Thanks for tutorial Foxtrot. Finally managed to extract the data I need</p>
3517, <p>We got good ensembling (averaging?) of a stacked model (nearest station 5113 * 98 rows) 196 LB with models based on single stations (203 LB and 204 LB) This was 192 on LB</p>
3517, <p>Hi it's here http://www.kaggle.com/wiki/ResearchCompetitionStandardRules</p> <p>One week before the end</p>
3517, <p>I would prefer to keep the one week rule and extend the rule to new teams too.</p>
3517, <p>We had a single model of 196:</p> <p>Take the 2 nearest stations for each meso (averaged over the 11 ensembles) so 75 columns per nearest station</p> <p>Stack them all so over 500k rows (5113 * 98)</p> <p>Add in month and rolling month (123234345etc)</p> <p>Add in elevation</p> <p>Add in whether EWNENWSWSE</p> <p>Run a GBM 2000 trees 0.05 shrinkage depth 10 minobs 1000</p> <p>196... on public Leaderboard</p>
3517, <p>They are the trainlabels for half of the files in the data folder. Each file belongs to one row in the trainlabels. The rest of the files (fileid) are for the test set.</p>
3517, <p>yes it took me a while.</p> <p>If you want recreate the leaderboard submission I think (but haven't tried) you would take the last line from each test file and submit as predictions</p>
3517, <p>Hi Can we fill the Alchemy gaps by using AlchemyAPI? I applied for an API key and they seemed very keen for me to use it for the competition</p> <p>Is this acceptable?</p>
3517, <p>ROC is the AUC</p>
3517, <p>I think 93.75</p>
3517, <p>I have - I can get about .883 but everytime I do something &quot;smart&quot; the results are worse than the benchmark (.878). So I have 3 models .86.877.869 and an ensemble gives .883</p> <p>I still reckon this benchmark ruined the competition!</p>
3517, <p>lol - did you see the reults of the big data competition. some people dropped nearly 400 places! I ended up worse than benchmark and only one person in the top 10 ended up in the top 10</p>
3517, <p>I tried many ratios at first then because I'm worried about overfitting - I decided a new tack and actually used different data for each model. A GBM does well on all the data but then I took &quot;intelligent&quot; subsets and ran a GLM and RF. So I just took an average - not my overall best score but the safest score. I've also averaged with the .878 benchmark but only improve by .0025. I want to try new things but the 0.878 benchmark was a bit soul-destroying so I've kind of lost interest :(</p>
3517, <p>I did. I haven't submitted any of the benchmarks so my score is the standard data with extra 2 columns for Alchemy. Improved my score by 0.02 which is a lot. You need to clean up the URL first (I used the Perl API so I used URI)</p>
3517, <p>I first used URL only - I added two columns to original data &quot;2nd run Alchemy cat&quot; &quot;2nd run Alchemy score&quot; This is the score improvement 0.78</p> <p>&nbsp;</p> <p>I then tried to get the keywords from the boilerplate but I didn't clean up the text properly and also if it fails I hadn't coded the record number. So ended up with an XML file that couldn't be matched back easy. I need to recode (very easy to do so annoyed with myself)</p> <p>The company kindly gave me 30000 hits. I'm just finishing up a couple of other competitions and then I will go back to this one. I will be using Alchemy though rather than the given benchmarks though</p>
3517, <p>Already asked https://www.kaggle.com/c/stumbleupon/forums/t/5429/using-alchemyapi-to-fill-in-the-gaps</p>
3517, <p>I've been using standard 10 CV and my results on leaderboard are about the same. I think you may have done something wrong with data or ordering of submission data?</p>
3517, <p>My 10 CV is still pretty much bang on. The only difference (probably?) is that I have drastically reduced the term matrix</p>
3517, <p>Make sure you haven't been left with a bunch of cookie recipe words! Recipes may be the most frequent but are they the most representative?</p>
3517, <p>Probably would be easier to use the Alchemy API and fill in the gaps with the categories. There will still gaps left but much less</p>
3517, <p>Hi Just to make you aware the My Submissions link takes you to &quot;Make a Submission&quot;</p>
3517, <p>Great - here come the unintelligent sheeples</p>
3517, <p>Because I'm fed up with having 200 opponents when really I had one. 87.7 was too high to post. Thanks a bunch for wasting my time</p>
3517, <p>Well go and enter on the &quot;Knowledge&quot; competitions -they are for learning. When someone just submits your code - what have they learned? How to cheat and pretend they're good at data mining? I reached 87.7 by original thought and method now 200 or people will beat me because of your code!!! Total waste of my time and effort</p>
3517, <p>If it was that simple  how come the 200 people who didn't have this score not already do it? Kaggle's becoming a joke. Multiple accounts cheaters illegal team mergers and hangers on who just submit other people's code.</p>
3517, <p>It wouldn't have bothered me if it wasn't a very high score. Takes the mick</p>
3517, <p>[quote=Travis;30333]</p> <p>@Abhishek - thanks for posting I learned something from this. &nbsp;Always appreciate when I can learn something new :).</p> <p>[/quote]</p> <p>&nbsp;</p> <p>and get a high score for doing nothing</p>
3517, <p>Well the 220 people who weren't above this score could have actually Googled text mining and worked it out themselves. Unprofessional? as unprofessional as people who pretend to be data miners but just submit other people's code competition after competition?</p>
3517, <p>Why would I want to use someone else's code? The top 10 models will end up being 99% the same. YAWN</p>
3517, <p>This a FEATURED COMPETITION - this isn't the Knowledge section!</p>
3517, <p>using his brain and his data mining skills? Shock</p>
3517, <p>Precisely</p>
3517, 
3517, <p>lol so all the people who think &quot;beat the benchmarks&quot; are good for learning - this benchmark is based on bad practices!</p>
3517, <p>lol everyone now back pedal - making features from test data is bad practice and funny how the term &quot;semi-supervised learning&quot; is only now used - bet the OP and everyone who posted benchmark didn't realise they had peformed semi-supervised learning!!!</p> <p>&nbsp;</p> <p>I see you've gone from 11th to 40th - couldn't hold your rank then?</p>
3517, <p>[quote=Abhishek;30554]</p> <p>[quote=Domcastro;30549]</p> <p>lol so all the people who think &quot;beat the benchmarks&quot; are good for learning - this benchmark is based on bad practices!</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Its called semi-supervised learning and is not wrong in anyway!</p> <p>[/quote]</p> <p>lol missed that fact out from your original post - bet you've only just realised that's what you have done! ;)</p>
3517, <p>accusations of cheating? about semi-supervised learning? I think you have wrong person - not sure where have you got that baseless accusation of baseless accusations from!</p> <p>&nbsp;</p> <p>EDIT: and if it makes you feel better I used semi-supervised learning (knowingly)</p>
3517, <p>lol</p>
3517, <p>&nbsp;</p> <p>&nbsp;</p> <p>[quote=Zach;31421]</p> <p>[quote=Stergios;31420]</p> <p>I agree with everything you say. I insist however that this should be done after the competition ends. If you want to work with others and share your code you could form a team. Anyway.........</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Kaggle has a long history of code sharing on the forums. &nbsp;It's an acceptable practice for every competition.</p> <p>Use the code to make your own model better. &nbsp;</p> <p>Use the code to select teams to propose mergers with.</p> <p>&nbsp;</p> <p>[/quote]</p> <p>That's not true Zach. I've been with Kaggle since the beginning and it is only recently there are &quot;Beat the Benchmarks&quot; ready to run code. Yes Kaggle WAS about learning too but people posted snippets of code. These benchmark codes are new. Years ago in the Don't overfit people shared but that's because there was a prize for it!</p> <p>&nbsp;</p> <p>Also I used to only hire Data Analysts who enter Kaggle competitions - however I won't be anymore as I don't know who's genuine or who just enters benchmark code. This is a bad practice and it makes Kaggle look 3rd rate. Snippets and ideas are fine. Full code is not. Too many cheaters and freeloaders.</p> <p>&nbsp;</p> <p>And... why should I have to submit crap bad practice dirty code just to keep my position on the leaderboard? </p>
3517, <p>[quote=Zach;31428]</p> <p>[quote=Domcastro;31427]</p> <p>&nbsp;<span style="line-height: 1.4">And... why should I have to submit crap bad practice dirty code just to keep my position on the leaderboard?&nbsp;</span></p> <p>[/quote]</p> <p>Because it's better than what you've written so far...</p> <p>[/quote]</p> <p>&nbsp;</p> <p>No&nbsp; it wasn't actually. I haven't submitted any benchmarks. However now this new one has actually done a better job (stemming etc) then I will build an ensemble. BUT he's only done what I've already done but Python is better for logistic regression than R and my score is still better!</p>
3517, <p>I haven't done any feature extraction. My CV scores are spot on. There are better things you can do with the text - I've only used Train data so far but I will be running your code so will have some test features too. Also if you do the text analysis properly recipes aren't a problem ;)</p> <p>&nbsp;</p> <p>HINT: Most recipe words are neutral - do they need to be included?</p>
3517, <p>I agree - I'm actually going to just keep the benchmark aside and use it as an ensemble at the end. I am worried about the closeness of the top 50 - could easily slip down 50 places</p>
3517, <p>It's something to do with the regularization or optimization in R. There's a post on it somewhere on this forum and the Amazon one. I get 0.018 better running logistic regression in Python than in R</p>
3517, <p>I think the problem with text analysis is the Random Factor. For example I had mucked up the order of the main Train file without realising. I then joined the correctly ordered text matrices to this file. I still got .80 on the leaderboard! and the words didn't match the rest of the data or labels!</p>
3517, <p>[quote=Upul Bandara;31451]</p> <p>&quot;HINT: Most recipe words are neutral - do they need to be included?&quot;</p> <p>&nbsp;</p> <p>This seems like thing I want to try next (removing words which are common to both categories). Before that I would like to know did you manage to get a significant performance gain by removing common words.</p> <p>[/quote]</p> <p>&nbsp;</p> <p>I haven't gained any &quot;significant&quot; improvement but I gained a few decimal points. Also by removing lots of columns you can now use Random Forest and other algorithms that don't work on sparse matrices. So a gain can be made by an ensemble.</p>
3517, <p>[quote=LI Wei;31446]</p> <p>I am curious how you will interpret the results as an employer. I think two kinds of person will end up high on the overall user rankings(or a mix of two) those who entering many competitions and get 10%/25% finally and those who wins money in a few competitions. You will prefer to hire those has general high ranks or those who can finally get the money in some competitions?</p> <p>[/quote]</p> <p>&nbsp;</p> <p>If they enter lots of competitions it means they LOVE data analysis - an amazing quality to have in an data analysis employee. I was more interested in the participation than results - though of course a high rank is desirable. And you need to remember in the real world 88 is as good as 88.2 which might have been the difference between 1st and 10th</p>
3517, <p>I carried a text analysis on the positives 1s and a separate text analysis for the 0s. I then merged them together calclulated z-scores and removed the words with Z-scores of around 0. Didn't really improve the score much (I think you get the &quot;Random Factor&quot; using all words) but it does mean I could run RF and other memory hungry algorithms</p> <p>&nbsp;</p> <p>EDIT: I wrote my own analyser in Perl to carry this out</p>
3517, <p>Are they features you have generated or features provided in the dataset? If they're features you have generated eg how many times the website has been reviewed then they will overfit the training data - good training bad leaderboard.</p>
3517, <p>Could someone post another good Beat The Benchmark please? With GTA and the new Rayman my priorities have changed! ;)</p>
3517, <p>Hi I've noticed teams forming but according to the rules this isn't allowed? Has there been a rule change?</p>
3517, <p>The differences could just be the choice of random seed - that's how close things are</p>
3517, <p>At the beginning I went a different route and used Alchemy API to extract entitities and concepts. I converted these to a boolean matrix and ran the usual suspects - RF GLM GBM. This was 87.7 on the LB. No text frequency analysis was performed.</p>
3517, <p>[quote=Abhishek;33006]</p> <p>&nbsp;</p> <p>maybe because of the benchmark people didnot work hard enough!</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Having a change of 0.005 dosn't mean people overfitted the leaderboard - that's expected. It was always going to be random luck - some go up 0.005 some go down 0.005. This is all that has happened. The problem was that everyone used the &quot;beat the benchmark&quot; as part of their model. The winner I think possibly gave up after the benchmark code was posted (by looking at date). And Abhishek it's extremely cheeky to say people didn't work hard enough - it was all down to random luck at the end. And there are a lot of people who now achieve a 25% badge by just submitting your code and as there were a lot of them - this has pushed people with slight changes in the scores to be further down the board.</p> <p>Anyways congrats to Fchollet.</p> <p>The biggest lesson I learnt: Wait a few weeks before entering Kaggle competitions just in case someone posts high performing code</p>
3517, <p>Abhishek - many people love you for the benchmark. You are being oversensitive and this benchmark thing was started on another thread about the Cause and Effect competition and the Belkin one. xxxxxxxxxxxxxxxxxxxxxxxxx</p>
3517, <p>I wasn't being a bad loser! He said he was &quot;hated&quot; - I was just saying that the benchmark rankings was unrelated to his benchmark. Not sure how that makes me a bad loser?</p>
3517, <p>Did you try as few variables as possible? I tried a mini competition with myself where I tried to get the highest LB score with the least amount of varables added to the original data. I added 10 in and got .86 on LB so effectively my other 15000 only added in .02 (.88)</p>
3517, <p>With all the text mining going around you could identify &quot;serious&quot; cheaters who give their second accounts &quot;personalities&quot; on the forums by an&nbsp;Author Identification model built on forum posts.</p>
3517, <p>We are predicting &quot;booking_bool&quot;. So how does this then get converted into the submission format? thanks</p> <p>&nbsp;</p> <p>ps. Not everyone uses Python - I shouldn't have to go through the benchmark code to get submission format data and find out what we are actually trying to predict</p>
3517, <p>Ok thanks. A little further forward. So the &quot;position&quot; field in the training set is redundant ? and there are no ranks&nbsp; in the training set ? We will run models on Train to predict Click_bool and boolean bool make predictions on the test set and then rank these predictions and use the properyid field of the predictions as the submission format? X</p>
3517, <p>thanks</p>
3517, <p>I managed to inflate from a Mac using the command line (it wouldn't work through the window)</p> <p>&nbsp;</p> <p>and I've just unpacked on a Centos (?) unix system</p>
3517, <p>It's an interesting competition - it needs deep thinking! Bring your creative side out</p>
3517, <p>This is going to be an interesting competition. I like competitions that are different. It's reminding me of the really fun Dunn Humby shopping one a couple of years ago (anyone remember that one?). Now to figure out the best approach....</p>
3517, <p>They are in there. possibly around column 52ish</p> <p>EDIT: Last 3 columns</p>
3517, <p>srch_iddate_timesite_idvisitor_location_country_idvisitor_hist_starratingvisitor_hist_adr_usdprop_country_idprop_idprop_starratingprop_review_scoreprop_brand_boolprop_location_score1prop_location_score2prop_log_historical_pricepositionprice_usdpromotion_flagsrch_destination_idsrch_length_of_staysrch_booking_windowsrch_adults_countsrch_children_countsrch_room_countsrch_saturday_night_boolsrch_query_affinity_scoreorig_destination_distancerandom_boolcomp1_ratecomp1_invcomp1_rate_percent_diffcomp2_ratecomp2_invcomp2_rate_percent_diffcomp3_ratecomp3_invcomp3_rate_percent_diffcomp4_ratecomp4_invcomp4_rate_percent_diffcomp5_ratecomp5_invcomp5_rate_percent_diffcomp6_ratecomp6_invcomp6_rate_percent_diffcomp7_ratecomp7_invcomp7_rate_percent_diffcomp8_ratecomp8_invcomp8_rate_percent_diffclick_boolgross_bookings_usdbooking_bool</p> <p>12013-04-04 08:32:1512187NULLNULL21989333.512.830.04384.9527104.77023246104011NULLNULL1NULLNULLNULL00NULL00NULLNULLNULLNULL00NULLNULLNULLNULLNULLNULLNULL00NULL0NULL0</p> <p>12013-04-04 08:32:1512187NULLNULL2191040444.012.20.01495.0326170.74023246104011NULLNULL1NULLNULLNULLNULLNULLNULL00NULLNULLNULLNULL01NULLNULLNULLNULLNULLNULLNULL00NULL0NULL0</p> <p>&nbsp;</p>
3517, <p>oops that hasn't come out right - just trying fix. I used the Head command on Mac</p>
3517, <p>I don't think Team Mergers are allowed (Rules page) I had a look for same reason!</p>
3517, <p>Do we know the total number of submissions allowed? The last Hackathon was fun but was only for a few hours. In Uk we're BST at the moment so possibly it's Saturday 3am to Sunday 3am but need to check. I'll have a look at the meetup page</p>
3517, <p>Hi you don't by chance have a zip version of the data as I'm having to trawl the net looking for a free app to open a 7z</p>
3517, <p>lol did you just add a stuffed Boo hahahahaha</p>
3517, <p>Delete numpy from the decaf egg. I had this error - it's looking for multiarray in decaf version of numpy.</p>
3517, <p>Really confused. I have read all the instructions FAQs etc. I managed to install FSharp on Linux and get the simulator running. I have successfully got it running on the sept_10 files and partial simulator. I thought I was ready for the train and test data but they are in a totally different format. I can't find any of the files the simulator needs. Seemingly the data is &quot;aggregated&quot; but it's not exactly clear how to extract the relevant data for the simulator. Any starter points please?</p>
3517, <p>Thanks - I had missed that forum post. I had a look at the wiki but it just confused me more! I think the forum post will help.</p>
3517, <p>Abhishek - what MacOS are you using? Out of interest have you manged to get Theano and Pylearn2 working with GPU? I had to install Anaconda Python to get Python on my Mac. I'm on Mac OS X Lion 10.7.5 (11G63) - there was a conflict with scipy (xcodes/gcc/lvcc ?&nbsp; etc) when I tried to install packages individually</p>
3517, <p>ah no- I have set up Theano and Pylearn2 on the Mac but uses CPU.&nbsp; I have a 64 bit windows machine too so bought a GEforce graphics card and installed that. Having trouble getting theano to work on winows though</p>
3517, <p>There's some tutorials at deeplearning.net http://deeplearning.net/reading-list/tutorials/</p> <p>Also there was some good sample code from Dogs and Cats from Kyle Kastner and Foxtrot at fastml has provided some tutorials too. Kyle also has provided code for the Kaggle cifair knowledge competition - same github as below</p> <p>https://github.com/kastnerkyle/</p> <p>http://fastml.com/</p>
3517, <p>Black Magic - I've literally just installed a new graphics card in my PC so that I can learn deep learning on the gpu. I plan to use pylearn2 and theano. Is this what you use? I'm going to play with the dogs and cats competition first then will move to this one. You said that your net has been running for 5 days - is this usual for deep learning? I'm excited about learning it and am now spending the rest of the evening installing the software. Any hints or tips or anything you wish you knew before you started?</p>
3517, <p>I think it's f777</p>
3517, <p>According to an Admin post - yes. There was a couple variables that &quot;could be treated&quot; as categorical but I chose to treat them as numerical.</p>
3517, <p>https://www.kaggle.com/c/loan-default-prediction/details/evaluation</p>
3517, <p>No you have over-complicated it. You are submitting predictions for 100% of the test data but only receiving results for a random 20% sample. The results on the 80% make up the private leaderboard and the winners are those who performed best on the 80%. The random 20% never changes (random to start off with)</p>
3517, <p>[quote=David McGarry;39175]</p> <p>I would also add that I don't think this will necessarily help &quot;crowdsource&quot; the solution. Providing these sorts of &quot;beat the benchmark&quot; posts generally lead to many people doing the exact same thing and lots of people doing pretty good on the leaderboard. From a sponsor's perspective they don't care if a only a few teams do relatively well or not they just want to know who did really well and how they did it. And ideally the top solutions are very different so that combining different aspects of them leads to an even better soultion which of course is much less likely to if everyone is just trying to optimize one approach.</p> <p>[/quote]</p> <p>I really think the only&nbsp; [personal] solution is to not enter a competition for a few weeks. When you've made a real effort it's a bit annoying. I once went to bed in the top 10 out of 200 and woke up about 100th! gutted!</p> <p>&nbsp;</p>
3517, <p>I can't decide if there will be a big shake up of the leaderboard. In the Big Data and Evergreen competitions there were huge shakeups but in the recent Flu compettion I expected a shakeup but there was hardly any movement at all. Big shakup or not?</p>
3517, <p>[quote=David McGarry;38112]</p> <p><span style="line-height: 1.4">Well I guess the leakage variables had a very little impact on my score but i'm not loving the leadboard going from ~200 to 25.</span></p> <p>[/quote]</p> <p>&nbsp;</p> <p>Give it 5 minutes and they'll all be back! :O</p>
3517, <p>I was having a look at the hierarchy with a network package. Most of the nodes seem to be disconnected (1967701) - so no parents. Is this correct?</p> <p>Thanks and sorry if being silly</p>
3517, <p>Hi I kept getting that. It was a difficult one to identify. It turned out to be trailing spaces and make sure you don't have a space after the comma</p>
3517, <p>https://www.kaggle.com/c/lshtc/forums/t/7504/error-when-submitting</p>  <p>My similar problem was caused by trailing spaces</p>
3517, <p>Hi Chi When I run the code  it just outputs 0 as predictions - any idea? So the beat_benchmark file = sample submission file. thanks</p>
3517, <p>Ah - no doesn't for me - get the exact score as sample benchmark</p> <p>&nbsp;</p> <p>This is what I get if I run the awk</p> <p>4256 0&nbsp;&nbsp; 1 target</p> <p>&nbsp;</p> <p>Also I get an np error NameError: global name 'np' is not defined so I imported numpy as np - maybe this the problem</p>
3517, <p>OK - fixed but illogical. Maybe different versions. I opened up the samplesubmission file - deleted all the 0s and then ran the code. It successfully updated the target column. That was all odd!</p>
3517, <p>[quote=Jose M.;39143]</p> <p>By the way I'm obtaining a 6879 % of accuracy on the train set using the &quot;last quoted plan benchmark&quot;. This is very different from the accuracy obtained on the test set as reported in the leaderboard. Are you obtaining the same figure?</p> <p>[/quote]</p> <p>This has confused me - I thought the last row was the plan they purchased - wouldn't that give 100% accuracy?</p>
3517, <p>Format cells custom 0000000</p>
3517, <p>[quote=geringer;44827]</p> <p>[quote=Nim J;44810]</p> <p>Prediction Models are all about hit n trial... Code sharing wouldn't get you anywhere buddy...</p> <p>[/quote]</p> <p>Let me reiterate I think it is a BETTER competition without the &quot;Golden Features&quot; type code sharing:</p> <p>https://www.kaggle.com/c/loan-default-prediction/forums/t/7115/golden-features</p> <p>I'm guessing its because the prize money is higher but it may be due to other factors. &nbsp; &nbsp;</p> <p>[/quote]</p> <p>&nbsp;That was Data Leakage. I hate all the sharing - it ruins the competition - because at the end of the day - it is a competition. It's hard to know who actually is good at data analysis&nbsp; and who just takes other people's ideas and codes and presses a button. Golden features my arse - golden leakage!</p>
3517, <p>I reckon (but I'm usually wrong!) that there will be overfitting in this one. This was a really hard competition so kudos to those highly-ranked</p>
3517, <p>There's 2 other posts with this same question! Highlight column Format cells Custom 0000000</p>
3517, <p>Do we select a submission at the end?</p>
3517, <p>It's written above the leaderboard. 20% public 80% private</p>
3517, <p>Just discovered R package data.table for reading in large files. If read.csv is taking too long:</p> <p>install.packages(&quot;data.table&quot;)</p> <p>library(data.table)</p> <p>train &lt;- fread(&quot;train.csv&quot;)</p> <p>train&lt;-as.data.frame(train)</p> <p>Took about 5 minutes compared to 2 hours with read.csv.</p>
3517, <p>Is this for AllState? You may be in wrong forum?</p> <p>edit: ignore - not sure I understand!</p>
3517, <p>Are you running 64 bit R?</p>
3517, <p>I think he meant GB. He's probably running 32 bit R on a 64 bit machine</p>
3517, <p>Check you're calling R64 not R32. R32 has memory limit R64 doesn't</p>
3517, <p>No idea then. I had same problem but it was because typing R on command line called the 32 bit version (in $PATH earlier). good luck</p>
3517, <p>yes - I was thinking that - they have the same kind of dates too</p>
3517, <p>Once you've preprocessed the transactions file the dataset isn't that large. My processed dataset for input into data mining is only 40meg</p>
3517, <p>They never get what they deserve - only the sub-accounts get removed. The master account always stays as they're too clever to link them with the dummies. I was waiting for the obvious Dummy Master in another competition to get deleted - but they never did. And that was sooooo obvious as it was impossible for them to get that score with only 6 submissions. Howver instead they finished in top 10 and became a Kaggle Master.</p> <p>And the cheaters don't care if you remove the accounts because they've got the results and will now use them for their master acoount.</p> <p>People suck and don't believe that they will be dealt with because they won't!</p>
3517, <p>The worrying thing is that some of the dummy accounts are currently being deleted. Havingfun at number 13 exists no more - does this mean someone in the Top 12 is a cheat?</p>
3517, <p>It's evaluated by AUC so if you just reduced / increased probabilities by the same ratio then your score will be the same</p>
3517, <p>It's only a problem if you're Top 3 and even then you'd be given time to recode in something acceptable</p>
3517, <p>Both in the final - good team up</p>
3517, <p>I want Germany to win as I remember http://en.wikipedia.org/wiki/Argentina_v_England_%281986_FIFA_World_Cup%29 with Maradona's Hand of God</p> <p>However I haven't been lucky in my world cup predictions</p>
3517, <p>We added in &quot;general&quot; features. In total had 750 features</p> <p>- customer / dept boolean matrix</p> <p>- brand / category boolean matrix</p> <p>- brand / company counts</p> <p>etc</p> <p>Training by different subsets and averaging increased the score</p> <p>XGBOOST and GBM worked the best. Code coming soon</p>
3517, <p>The solution isn't allowed to be posted. I checked.</p>
3517, <p>Kazanova - I had to get my 2nd place solution code removed from the forum. The anonymous sponsors didn't want us posting it</p>
3517, <p>Do we wait to get contacted concerning code submission?</p> <p>thanks</p>
3517, <p>Ha! thanks. I hope you won the sweepstakes!</p>
3517, <p>ha! Yes  it's ok to hack out someone else's code :D</p>
3517, <p>https://www.kaggle.com/forums/t/2898/ideas-for-competition-changes-to-reduce-cheating-multiple-accounts?limit=all</p>  <p>It reduces cheating</p>
3517, <p>Funny how the people who moan the most are the same people who have annoyed others by &quot;Beating benchmark&quot; posts and sharing on the forums when it specifically said not to (eg Walmart).</p>  <p>They're moaning about &quot;New Trends&quot; of late minute sharing - well their &quot;New Trend&quot; was also annoying to us old Kagglers - but we sucked it up. Now they can't handle &quot;New Trends&quot; lol</p> <p>Well done KesterLester - what you shared was interesting - don't listen to the moaners - seemingly they're the only ones allowed to share.</p>
3517, <p>[quote=KazAnova;54084]</p> <p>[quote=ACS69;54083]</p> <p>Funny how the people who moan the most are the same people who have annoyed others by &quot;Beating benchmark&quot; posts and sharing on the forums when it specifically said not to (eg Walmart).</p> <p>They're moaning about &quot;New Trends&quot; of late minute sharing - well their &quot;New Trend&quot; was also annoying to us old Kagglers - but we sucked it up. Now they can't handle &quot;New Trends&quot; lol</p> <p>Well done KesterLester - what you shared was interesting - don't listen to the moaners - seemingly they're the only ones allowed to share.</p> <p>[/quote]</p>  <p>ouch! Hit below the belt that was (with my Yoda accent)!</p> <p>[/quote]</p> <p>Well truth hurts.</p>
3517, <p>Title says it all.</p>  <p>EDIT as per usual I didn't read the instructions properly. I see it is you wish to attend conference</p>
3517, <p>Regression. Have a look at the &quot;beat benchmark&quot; thread - will get you started</p>
3517, <p>[quote=Selfish Gene;55273]</p> <p>I also had experienced a similar situation where a team member was submitting from another&nbsp;account that I didn't know about.</p> <p>I cannot describe the frustration I felt when I found out having no&nbsp;control over your own situation&nbsp;like that.</p> <p>In my case this was a last minute team up just for blending opportunity so I asked kaggle to break our team and ignore any submissions after the merge and even raised the point that this could be used in the future as a malicious tactic to get rid of competition but to no avail.<br><br>I think the SMS verification is a good way to greatly reduce this issue but just to take an extra measure of safety I don't think I would be joining teams with any one I don't know anytime soon. it was simply too traumatic of an experience for me.</p> <p>[/quote]</p> <p>I thought that was what happened to your team - I was a bit shocked but glad it wasn't you</p>
3517, <p>It is clear - in lots of places. Maybe you should read these things?</p>
3517, <p>[quote=rcarson;52767]</p> <p>[quote=James King;52765]</p> <p>Great answer I nominate Triskelion for <em>de facto&nbsp;</em>competition admin. Also for best avatar...</p> <p>[/quote]</p> <p>Absolutely</p> <p>[/quote]</p> <p>Yes Triskelion can be the Competitor's Union Rep! He has my vote</p>
3517, <p>Max email compliance@kaggle.com and explain the situation</p>
3517, <p>After I import the files into Python I can't find the field names 'Data'  'data_length_sec' 'sampling_frequency' etc etc. There's an array with</p> <p>'preictal_segment_1': array</p> <p>etc</p> <p>but I can't access by the field names given in data description. Any idea? I can see fields in R but not Python</p>
3517, <p>Thanks all. Can I confirm that for each segment (eg Patient 1) I would end up with a 300000 * 15 data frame (in R)? ta</p>
3517, <p>Hi First submission deadline is in the rules </p> <p><br>Competition Timeline<br>Start Date: 8/25/2014 4:15:33 PM UTC<br>Merger Deadline: 11/10/2014 11:59:00 PM UTC<br>First Submission Deadline: 11/10/2014 11:59:00 PM UTC<br>End Date: 11/17/2014 11:59:00 PM UTC</p>  <p>I</p>
3517, <p>All you have to do is submit the samplesubmission.csv after accepting the rules.</p>
3517, <p>https://www.kaggle.com/c/seizure-prediction/forums/t/10972/the-one-with-only-21-entries-removed</p>
3517, <p>I found this http://www.noble.org/ag/soils/phosphorusbehavior/</p> <p>At a soil pH above 5.5 most of the phosphates react with calcium to form calcium phosphates. Below pH 5.5 aluminum (Al3+) is abundant and will react more readily with the phosphates. Calcium phosphates are relatively more water-soluble than aluminum phosphates. The lack of water solubility of aluminum phosphates means that these compounds are not readily available for plant use. In other words in strongly acid soils most of the P is bound and not released.</p>
3517, <p>LB 0.42715</p>  <p>Changed all Ps to 0</p>  <p>LB 0.44220</p>
3517, <p>I think the scoring problem is also not helped by non-random train/test split. I reckon there will be big leaderboard changes at the end.</p>
3517, <p>In my personal opinion I think the method you've described is valid and is of use to the organisers. There's a risk here though of overfitting because predictions will be dependent on how good your other predictions are. It's something I will be trying</p>
3517, <p>[quote=Abhishek;54148]</p> <p>AngryTomato please don't be angry. </p> <p>[/quote]</p>  <p>That just made me really chuckle!</p>
3517, <p>out of interest why do you feel the need to post high performance benchmark code? You pretty much have ruined Kaggle for me. thanks</p>
3517, <p>are you taking the mick? it was .43 score. it was a top 20 score. You always do it - all you need to do is tune your .43 to get even higher.&nbsp; Foxtrot was the expert at Beat Benchmarks - they were posted not long after the competition started  and weren't much bigger than the benchmark and there was a blog tutorial. Kaggle is now no fun</p>  <p>Did you not see the major leaderboard moves after posting? some people went up 300 places?</p>
3517, <p>argghhhh&nbsp; grumpf!</p>
3517, <p>&nbsp;so 39/40 out of 600 is not high? You complained at Foxtrot for posting benchmark at position 57 in the avito!!! After your post 4 people entered the top 10 after going up over 100 positions. Anyway I know you have good intentions but I'm going off Kaggle because of this.</p>
3517, <p>yes of course it does - it's the same argument.</p>
3517, <p>And look at the Avito forum - you'll see Abhishek complaining about benchmark code</p>
3517, <p>[quote=Abhishek;54439]</p> <p>Just because it was posted a week before the competition deadline.</p> <p>[/quote]</p>  <p>So why are your personal preferences more important than mine? You also therefore have &quot;unwritten&quot; rules about suitable timing for benchmark codes - but  to me your timing is also wrong. How you felt when Foxtrot posted that code is how you make me feel when you post your code 3 weeks after the competition starts.</p>
3517, <p>[quote=Rohan Rao;54441]</p> <p>There's a difference between posting a benchmark early in a competition and towards the end.</p> <p>[/quote]</p>  <p>yes but we all have different definitions of &quot;early&quot; and &quot;late&quot;. To me &quot;early&quot; is first week only &quot;Late&quot; is anything afters</p>
3517, <p>I don't want an appreciation society - I was just moaning! Thanks anyway</p>
3517, <p>Anyone know?</p> <p>&quot;We suggest you to remove spectra CO2 bands which are in the region m2379.76 to m2352.76 but you do not have to.&quot;</p> <p>from data page</p>
3517, <p>Maybe it's how you're treating Depth? I'm getting higher scores than you on a 5 fold using same estimators. I've just changed Depth to 1 or 0</p>
3517, <p>Very good tutorial.</p>
3517, <p>yes 3 is a bit tight for 5 targets.</p> <p>ps. I also think it's tight of people to negative vote you the way they have. sorry about that</p>
3517, <p>[quote=Ants;55539]</p> <p>It's not so much about disagreeing but about the way it was communicated.</p> <p>[/quote]</p>  <p>You need to take into account that English isn't everyone's first language</p>
3517, <p>I paired up and got worse results</p>
3517, <p>Slightly different if you use first derivative calibrated spectral data only (CO2 removed)</p>
3517, <p>I used First Derivative. Are you using One-dimensional Gaussian filter? I couldn't find where you getting derivative in code</p>
3517, <p>I binded train and test</p>
3517, <p>sorry -&nbsp; Like in Breakfast pirate's code&nbsp;  we joined (stacked in python) the train and test sets then took first derivative.</p>
3517, <p>I used R and I hacked the first derivative code given in the BART benchmark. Maybe compare the BART benchmark &quot;first derivative&quot; and Python's &quot;First Derivative&quot;</p>
3517, <p>I removed non-spectral depth and CO2</p>
3517, <p>Choose a bigger dataset with 1 target and a random train/test split</p> <p>This dataset has non-random train/test split which adds complexity</p>
3517, <p>Be careful Abhi and Joao. In the Aquired Value competition it was bad news for the people who teamed up through the forum. You 2 would make a great trustworthy team</p>
3517, <p>[quote=Arthur B.;56345]</p> <p>Yes there is some over-fitting but it's not just that. My number of submissions is higher than many which rank much higher than me so there is definitely something else at play.</p> <p>[/quote]</p>  <p>There's so much code floating around that the later you enter the competition the fewer submissions you need to make to get same scores.</p>
3517, <p>[quote=phunter;56369]</p> <p>Congratulations! And I want to know the magic from&nbsp;Charly B. * who managed jumping up 886 ranks in the private LB!</p> <p>[/quote]</p> <p>Those arrows aren't correct. We went up 90 places not down 3 and Charly went up about 480</p> <p>Congrats to the winners</p>
3517, <p>[quote=Jan Kanty Milczek;56386]</p> <p>Congrats Yata and Charly B!</p> <p>May I ask if you have used the non-spectral features? Or should I wait for your writeups?</p> <p>Regards :)</p> <p>[/quote]</p> <p>Not a winner but 5th - we used non-spectral data. I haven't seen private scores per submission but we found that building models with spectral data and ensembling with models built with spectral and non-spectral produced better results on the public LB</p>
3517, <p>This was a particularly bad competition. Small data = leaderboard shakeup</p> <p>Just try&nbsp; a competition with bigger data and it won't be like this.</p>
3517, <p>The arrows are being odd for everyone (I think). We went from 96th to 5th but it says -3 on private board. There's theories on the other thread</p>  <p>EDIT: Actually I think I see what you mean.</p>
3517, <p>[quote=Abhishek;56453]</p> <p>Sorry guys &nbsp;as long as rules don't change about public sharing &nbsp;I'll keep on posting benchmarks... ;)&nbsp;</p> <p>[/quote]</p>  <p>Always the bridesmaid never the bride</p>
3517, <p>[quote=Abhishek;56482]</p> <p>[quote=ACS69;56465]</p> <p>[quote=Abhishek;56453]</p> <p>Sorry guys &nbsp;as long as rules don't change about public sharing &nbsp;I'll keep on posting benchmarks... ;)&nbsp;</p> <p>[/quote]</p> <p>Always the bridesmaid never the bride</p> <p>[/quote]</p> <p>&nbsp;Haha.... &nbsp;Without you Kaggle and benchmarks are no fun! :D</p> <p>[/quote]</p> <p>lol - same ;) I said to Kazanova that I wanted you to win this one as you deserved a prizewinner badge. But in the end glad I beat ya :P</p>
3517, <p>[quote=superfan123;56493]</p> <p>You want to run together with the top guy. </p> <p>[/quote]</p> <p>But that's just a psychological illusion that benchmark codes provide!</p>
3517, <p>why?</p>
3517, <p>For my part of UK Calling Africa:</p> <p>1. Use 3 data transformations: First derivative gap derivative and the SG. Remove CO2</p> <p>2. Run 2 datasets per transformation: with / without non-spectral</p> <p>3. For each dataset run BayesTree Bayesian Ridge and GBM</p> <p>4. Ensemble by straight averaging regardless of individual leaderboard results</p>
3517, <p>All the benchmarks on the forum overfitted.</p>
3517, <p>On First Derivative data you could get .44 with a GBM</p>
3517, <p>[quote=lewis ml;56477]</p> <p>[quote=ACS69;56475]</p> <p>On First Derivative data you could get .44 with a GBM</p> <p>[/quote]</p> <p>Thanks yes - I'd got that far or somewhere similar. But I made the mistake of thinking it wasn't close enough to the svm score of 0.40 to be worth merging the two.</p> <p>[/quote]</p> <p>oh no - rule of data analysis - ensemble in everything. Even if you did a (svm * 0.75) + (gbm * 0.25)</p>  <p>EDIT: I even ensembled in scores of 0.47</p>
3517, <p>yes - I would say this was overfitting. If the Ps in the private set were high then you'd be in trouble.</p>
3517, <p>Get it from the CRAN Archive</p> <p>http://cran.r-project.org/src/contrib/Archive/BayesTree/</p>
3517, <p>The code is a thing of beauty - classy and elegant</p>
3517, <p>The improvement might have come from the increase in weights and the number of passes in the modified code</p>
3517, <p>[quote=gmilosev;57257]</p> <p>Very nice thx @laserwolf. Can you also post the LB score.</p>  <p>[/quote]</p>  <p>Changing the number of weights to 24 gives 0.0133331 LB</p>
3517, <p>This was mentioned earlier - it's an error. It is 45 191 in total</p>
3517, <p>Thanks for this code. I really enjoyed playing with it. I only had enough memory for 2 ^ 24 though. By adding in x1 and x142 into the hash_cols could get &lt; 0.60 (3 passes)</p>
3517, <p>There was a bug in the Kaggle system when someone became a Master but then got removed from the leaderboard</p>
3517, <p>I think it sometimes just crashes out. I had that on y10. At the moment I'm on y32. I'm on 16gig mac</p>
3517, <p>zip your submission. Still takes ages but it's a big file</p>
3517, <p>Most competitions are reactivated after the deadline so you should be able to. It's all free by the way</p>
3517, <p>I'm in UK and I have had no problems</p>
3517, <p>ooo I wonder if the taking down of part of the Dark Web has caused issues.</p>  <p>EDIT: Actually there is a problem with the Kaggle Forum. It's lagging on the overview page and hasn't picked up the most recent posts</p>
3517, <p>Achillis you can get to below 7 by making Triskelion's adjustments and changing the weights to 2^24. I also have been focusing on online learning. My performance is slightly worse than laserwolf I can get approx&nbsp; 0.0058</p> <p>EDIT: and run more than 1 pass/epoch</p>
3517, <p>I've just been experimenting with the online learning code. Hardly need any memory and can still get a competitive-ish score.</p>
3517, <p>[quote=rcarson;57641]</p> <p>I don't get why the down vote.~</p> <p>[/quote]</p>  <p>I thought it was funny but I suppose people who haven't had the upload problems probably didn't get it.</p>
3517, <p>[quote=rcarson;57677]</p> <p>I'm Chinese too!</p> <p>[/quote]</p> <p>You're The Hulk - don't pretend you're Chinese Bruce!</p>
3517, <p>the drama</p>
3517, <p>How exciting. Well done all</p>
3517, <p>I'm going for 0.007 (same as Yelp)</p>
3517, <p>I reckon it may be the least shakeup yet</p>
3517, <p>Thanks Abhishek</p>
3517, <p>It's because the file is in sequence order.</p>
3517, <p>I'm no expert but I would think it's because they're in similar groupings (hour) - the way the online code works will pick up the similar features - so for the same hour the log loss gets better then the hour changes and there's new features so the log loss may get worse then level out again or improve and so on . If you randomly shuffle the file before running the code you will get the pattern that you are expecting</p>
3517, <p>Yes - it will happen to everyone who runs the online code on the sequenced data.&nbsp; Shuffle the data to get consistent log loss improvement. I used &quot;hour&quot; as an example - I haven't looked at the details to see what is causing the drops</p> <p>edit: I didn't enter the other competition so I don't know what the data was like</p>
3517, <p>It's harder to see the NULL fields now - I think they're encoded per column?</p>
3517, <p>Bless you!</p>
3517, <p>yes - I was just working it out when laserwolf wrote their post</p>
3517, <p>[quote=B Yang;57686]</p> <p>Why bother with hashing at all ? In my view the right way is&nbsp;to map each unique value to an incrementing integer ID:</p> <p style="padding-left: 30px"><code>int new_id=0;<br>dictionary d;<br>for (each datum in data column) {<br>&nbsp; if (!d.contains_key(datum)) {<br>&nbsp;&nbsp;&nbsp;&nbsp; d[datum]=new_id;&nbsp;new_id++;<br>&nbsp; }<br>&nbsp; mapped_value=d[datum];<br>}<br></code></p> <p>This approach has the following advantages:</p> <ul> <li>Faster processing compared to non-trivial hashing.</li> <li>Smaller resulting dataset.</li> <li>Resistance to cracking.</li> </ul> <p>[/quote]</p>  <p>Agreed. I usually do this for long hashes anyway as it frees up more memory</p>
3517, <p>Email support@kaggle.com</p>
3517, <p>https://www.kaggle.com/forums/t/9883/sms-account-verification</p>
3517, <p>When anonymised data is used an extra rule should be used &quot;Cracking of anonymised data will be considered as cheating&quot; or alike. At least this means when people do crack the data they don't share it on the forums</p>
3517, <p>[quote=James King;57826]</p> <p>Laserwolf is understandably skeptical about whether it is possible to go forward but if new data is being pulled it sounds like the contest will proceed which makes me very happy.</p> <p>755f85c2723bb39381c7379a604160d8&nbsp;867c4235c7d5abbefd2b8abd92b57f8a</p> <p>ed881bac6397ede33c0a285c9f50bb83 !!</p> <p>[/quote]</p> <p>71d3e8b42792b5e476804f4f7fbddc58</p>
3517, <p>[quote=laserwolf;57892]</p> <p>2) keep the contest fair to competitors who have not downloaded the old data.</p> <p>they can't do both.<br><br></p> <p>[/quote]</p> <p>I downloaded the old data and totally ignored all the decrypting. Sometimes that kind of knowledge is more distracting than worthwhile.</p>
3517, <p>You don't want it becoming a burden on you</p>
3517, <p>lol! Your code is too beautiful to become a burden!</p>  <p>edit: Homer in a West Ham kit</p>
3517, <p>strange - are you sure it's not writing the submission file wrong? as it's the sample submission score</p>
3517, <p>4 GB is not enough as the file is bigger than that. I usually use library(data.table) and the function fread (I have 16GB mac)</p>
3517, <p>[quote=rcarson;58566]</p> <p>Thank you. I am about to write that script anyway. Integer should help.</p> <p>I'm actually encoding pandas data frames into sparse matrix.</p> <p>[quote=Konrad Banachewicz;58562]</p> <p>- have you tried using sparse matrices instead of pandas as input? I do believe they are acceptable as input&nbsp;</p> <p>[/quote]</p> <p>So how to get a sparse matrix input without encoding? :P</p> <p>[/quote]</p>  <p>I use Perl to make my matrices - I haven't for this competition yet but that was what I used for Aquired Value which was 22GB file.</p>
3517, <p>I've ran lots of validation programs but haven't made an entry yet. What I've found interesting is that 0.39 seems to be very common. I have totally changed the data to be unrecognisable but still get 0.39. I expected better or worst not the same considering the changes I had made. I then removed each variable and ran Tinrtgu's v3 - you get 0.39 all the way down to only 4 features left. You can get .42 using just 2 features from the original dataset. I think a breakthrough may come through data prep not code this time. Anyone else notice oddities?</p>
3517, <p>I think there's room for improvement but haven't figured it out yet. I'm not sure if the sampling strategy has introduced oddities. I totally changed data - only used prior probabilities and still got .39 on validation - didn't even use any original features. I'm going to manually go through some of the data. I reckon there's a &quot;golden feature&quot; somewhere</p>
3517, <p>[quote=Abhishek;58783]</p> <p>More than half of the LB is based on same benchmark code. If people try new ideas the score should improve....</p> <p>[/quote]</p> <p>Giulio has used XGB - not in benchmark code. I've tried several different things. I only ran the &quot;remove variable&quot; code using the benchmark and that was only because it was quick</p>
3517, <p>What's Thanksgiving?!</p>
3517, <p>[quote=James King;59035]</p> <p>An American version of Harvest Day devoted to gustatory excess and American football.</p> <p>[/quote]</p> <p>and balloons seemingly too. http://www.telegraph.co.uk/news/worldnews/northamerica/usa/10480219/A-British-guide-to-Thanksgiving.html. It seems more like a British Christmas though we have &quot;soccer&quot; played</p>
3517, <p>lol you get the sprouts too!! ha the enemy of kids at Christmas</p>
3517, <p>Blimey - the Americanism Black Friday has come to Britain. There was chaos in the shops today. I blame Walmart http://www.bbc.co.uk/news/uk-30241459</p>
3517, <p>There's a new dataset. You need to download the new lot of data from the Data page</p>
3517, <p>This will help https://www.kaggle.com/c/avazu-ctr-prediction/forums/t/11001/submission-issues-errors</p>  <p>Use library(data.table) and fread</p> <p>but read the above post because otherwise the ids will get translated wrong</p>
3517, <p>Ha just seen Abhishek misses me! He misses our arguments. Have a rubbish Christmas holiday  Abhishek!</p>
3517, <p>Mine's simple. Takes 2 hours maximum and 8gb memory.</p>
3517, <p>No - I'm blagging my way through. But I think it's data prep not algorithms that's more important</p>
3517, <p>I have a Mac. I have pypy and Anaconda python 2.7. I also use R.</p> <p>Start by running the Beat Benchmark with less than 1mg .... code</p> <p>If I remember rightly I just downloaded pypy and just call it from where I downloaded it to</p>
3517, <p>[quote=simeng;59615]</p> <p>The original code will give scores for sure. I guess you did not specify train and test file locations correctly. You have to change the path accordingly.</p> <p>[/quote]</p>  <p>Agreed</p>
3517, <p>are you getting any errors?</p>
3517, <p>According to the leaderboard you didn't enter this competition or you got removed (again) so not sure what your point is. Maybe you should explain why you were removed from the leaderboard rather than distracting attention away from that fact</p>
3517, <p>130? it was 1616 earlier and 1604 now - I must have missed the original number. (I bet as usual the puppetmasters remain on the LB!)</p>
3517, <p>Blimey - I've been decorating for a couple of months so out of the loop! That's a large number of cheaters!</p>
3517, <p>It is scary teaming up. I remember offending Kazanova asking if he had ever cheated before we teamed up in the Africa one. It's got to be done though. There's also been some Masters and people with lots of competitions who have been in teams where someone has cheated - so it really is hard to discern &quot;trustworthyness&quot;. I'd be mortified if I got removed from the leaderboard. But saying that I've been in teams with 7 different people and have had no problems. So I think it is a minority rather than majority</p>
3517, <p>ha!</p>
3517, <p>[quote=KazAnova;64230]</p> <p>[quote=superfan123;64227]</p> <p>ask her</p> <p>[/quote]</p> <p>I do have long hair now but I would still call it a &quot;him&quot; :) . True I am good at disposing.</p> <p>[/quote]</p>  <p>hhahahahah cruely laughs</p>
3517, <p>[quote=rcarson;58688]</p>  <p>Just one thing please don't downvote me I have a fragile heart. o_o</p>  <p>[/quote]</p> <p>Ha! I get downvoted a lot because of my benchmark dislike - it makes me laugh though.</p>
3517, <p>[quote=inversion;58882]</p> <p>That was an expensive mistake.</p> <p>[/quote]</p> <p>Not really - because if Kamil hasn't used FICO software then there will be more popularity in their tool</p>  <p>EDIT: unless he nows reprograms using FICO tool! ha then it will be a hat-trick</p>
3517, <p>[quote=skwalas;58895]</p> <p>&nbsp;It's like expecting a puppy for Christmas and getting a rock.</p> <p>&nbsp; [/quote]</p>  <p>hahahaha</p>
3517, <p>Kamil - congratulations and I think you broke a Kaggle record. Don't do anything until the Admins say something. People are just upset because of our puppy going! Well done</p>
3517, <p>[quote=William Cukierski;58918]</p> <p>I just got off the phone with the North Pole. After a few hours on hold (call center scheduling problem in 2015 anybody?) Santa told me that he wants Kagglers to sit tight while he negotiates with the Elf Union over unfair large-present working conditions. Mrs. Clause is furious that one of the reindeers used all her EC2 credits for the month not to mention half of the elves are busy digging a&nbsp;trench to run fiber internet to the workshop.</p> <p>[/quote]</p> <p>Please pass my letter to Santa:</p> <p>Dear Santa</p> <p>Whatever you decide about the Kaggle competition please award Kamil a special something.</p> <p>From M</p> <p>xx</p>
3517, <p>bromance?</p>
3517, <p>Thanks . I got this error on submitting</p> <p>Evaluation Exception: Toy 405 is not complete: startMinute = 555 workDuration = 183 elf = 199 rating = 1.0050.</p>  <p>edit: Actually I think it may be my problem as I'm using the old code</p>
3517, <p>thanks - just downloaded and rerunning.</p>
3517, <p>bump</p>
3517, <p>Thanks for this. I'm stuck working out the &quot;largest start + duration up to that point&quot; I keep getting my maths / variable choice wrong. Any idea?</p>
3517, <p>lol thanks I went off at a very deep end!</p>
3517, <p>Chill out bro. It's the Christmas competition - always has a Rudolph prize and different criteria to other competitions. Go help those elves</p>
3517, <p>[quote=Muks;66098]</p> <p>Can someone help me in understanding what is x and y in the data files?</p> <p>[/quote]</p>  <p>It's too late for you to enter this competition. x and y are coordinates of driver position</p>
3517, <p>[quote=Aaron Sheldon;61592]</p> <p>While I regard the rules very seriously I am far more concerned that the data is not geographically anonymized.</p> <p>Quite frankly with this data set I am the least of your concerns. I am much more interested in the academic proof of concept then revealing the neighborhoods city blocks or streets on which the drivers live.</p> <p>[/quote]</p>  <p>Why did you even try to deanonymise the data? Why don't you make your own synthetic dataset if you want &quot;proof of concept&quot;? Now I'm thinking is it worth me putting effort in yet as the competition may have to be restarted because once again someone has been a smart@rse</p>
3517, <p>I'm blindly overfitting until I think of a better way!</p>
3517, <p>Order doesn't matter</p>
3517, <p>I'm having trouble matching routes - flipping and rotating. Are there any packages or libraries out there that people are using? thanks</p>
3517, <p>Thanks - I've done all the &quot;reasonable stab&quot; things. I think it was the &quot;trigonometry&quot; that I'm missing. I haven't done that for over 30 years and had forgotten what it was even called!</p>
3517, <p>I have &quot;Christmas Family&quot; tasks the next few days but then I'll give it a go. You could use tree searching on your strings (each level the car goes down the left branch middle branch right branch) I'll have a think. Thanks for your help. I'm now going to learn (relearn)&nbsp; some trigonometry</p>
3517, <p>Write a separate function that sapply calls</p>
3517, <p>I (and benchmark code!!) dealt with it all separately</p> <p>speedDistribution &lt;- function(trip) {<br> vitesse = 3.6*sqrt(diff(trip[1]201)^2 + diff(trip[2]201)^2)/20<br> return(quantile(vitesse seq(0.051 by = 0.05)))<br>}</p> <p>max &lt;- sapply(1:length(trips)function(i) max(sqrt(diff(trips[[i]][1])^2 + diff(trips[[i]][2])^2)))<br> mean &lt;- sapply(1:length(trips)function(i) mean(sqrt(diff(trips[[i]][1])^2 + diff(trips[[i]][2])^2)))<br> times &lt;- sapply(1:length(trips)function(i) nrow(trips[[i]]))</p> <p>sd = sapply(1:length(trips)function(i) speedDistribution(trips[[i]]))</p>
3517, <p>cbind them into a row afterwards!</p>  <p>Have you got 2 accounts because the original poster was Goldman?</p>
3517, <p>Have a look at the 0.66 logistic regression code. This will show you how to do it</p>
3517, <p>&quot;data&quot; covers it. Ie anything you've noticed about the data is &quot;data&quot;. So it is against the rules to privately contact another team and have a brainstorm</p> <p>&quot;Privately sharing code or data outside of teams is not permitted. It's okay to share code if made available to all participants on the forums.&quot;</p>  <p>EDIT: A bunch of people have just been disqualified from the BCI challenge for working independently but within a bigger group. They brainstormed with each other and even though they didn't share code they have been considered as &quot;breaking the rules&quot;</p>
3517, <p>is Expected in mm? so for example 1953.8 is mm but we're only measuring to a maximum of 69mm?</p>
3517, <p>I'm a bit confused. How come so many of the values in &quot;Expected&quot; column are above 69? I think I've missed something?</p>
3517, <p>mm</p> <p>https://www.kaggle.com/c/how-much-did-it-rain/forums/t/11479/expected</p>
3517, <p>Do we have the regular season result for October 2014 - March 2015?</p>
3517, <p>For testing purposes. For example if you put aside tourney and regular season for 2014 then you will train on the regular season and tourneys of years &lt; 2014. Then you can use the seeds &lt; 2014 in the train set and the rest in the holdout test set</p>
3517, <p>At the moment I've built a tournament history dataset for 10 years of tournaments and then only use the regular season for that year.</p>  <p>For 2011 I've used 2004 - 2010 tournament data but only 2011 regular season data</p> <p>For 2012 I've used 2004 - 2011 tournament data but only 2012 regular season data</p> <p>etc</p> <p>However I haven't done distance calculations yet and I'm also only doing slightly better than the seed benchmark (not using leaderboard)</p>
3517, <p>I've had a quick look at the tourney results file and the wloc (winning team location either HA or N) is always N. Is this correct?</p>
3517, <p>For automation purposes can I assume that the release of 2014/2015 regular season data will be in the same format as the historical regular season data?</p>
3517, <p>I don't understand this! How can you evaluate matches that never existed?</p>  <p>EDIT: is it because they might meet in next stage?</p>
3517, <p>Thanks. In reality if you're filling in brackets for gambling would you only predict the group stages? Then after the group stages do the next bracket? or do you have to do the whole lot at beginning?</p>
3517, <p>I understand how there could be play ins at seed 16 (16a 16b) but how can you get them for 12s and 11s?</p> <p>My gut is that the loser of 12a vs 12b would then become seed 13 but I know this doesn't follow from the data</p>
3517, <p>thanks</p>
3517, <p>Hopefully I've calculated it correctly. Just posting if anyone else is interested.</p> <p>[1] 2004<br>[1] 0.5210246<br>[1] 2005<br>[1] 0.5465187<br>[1] 2006<br>[1] 0.5785918<br>[1] 2007<br>[1] 0.4668978<br>[1] 2008<br>[1] 0.531841<br>[1] 2009<br>[1] 0.507833<br>[1] 2010<br>[1] 0.5722903<br>[1] 2011<br>[1] 0.5915076<br>[1] 2012<br>[1] 0.5828431<br>[1] 2013<br>[1] 0.6071851<br>[1] 2014<br>[1] 0.6057572</p>
3517, <p>My worst predictions are for 2011 (0.62 compared to sb 0.59). </p>
3517, <p>Here's R code for logloss</p> <p>LogLoss &lt;- function(actual predicted eps=0.00001) {<br> predicted &lt;- pmin(pmax(predicted eps) 1-eps)<br> -1/length(actual)*(sum(actual*log(predicted)+(1-actual)*log(1-predicted)))<br>}</p>  <p>so</p> <p>LogLoss(actualrf$predicted)</p>
3517, <p>How do you upload a bracket? The site confused me</p>
3517, <p>on March 15th?</p>
3517, <p>I might need help with this tomorrow as I still haven't sussed the site out</p>
3517, <p>thanks. I'll holler if I get stuck</p>
3517, <p>It was easy after all that!</p>
3517, <p>Did you mean this Jason &quot;Fill out the bracket just like a normal bracket with one major exception: you do NOT have to pick a team that won in the previous round&quot;</p>
3517, <p>Just read the update on the site. There's 3 scenarios where I can win Machine Madness though I am dependent on Kentucky losing somewhere along the line (!) :</p> <p>MSU champion = Bluefool</p> <p>Duke champion = Bluefool</p> <p>Wisconsin over Duke = Bluefool</p>
3517, <p>yayayay thanks. Over the moon.</p>
3517, <p>Thanks Net Prophet that was so much fun. Michigan State did me well. If only I had post-processed my predictions I could have been in the Kaggle running. Thanks</p>
3517, <p>I'm going to give it a go but I haven't thought about the conversion process too much yet. The competition Machine Madness that Dr. Pain is running allows you to &quot;not select&quot; a previous winner. I think this is because you get more points for &quot;shock&quot; predictions. I'll have a think over the weekend</p>
3517, <p>&quot;Last year's winner had a log-loss score of 0.52 and the median score was around 0.58. If your predictor is getting performance significantly better than those numbers then you're either (1) a genius or (2) have a problem. It's up to you to decide which.&quot;</p> <p>Hasn't more data been provided this year? I would expect the 2014 predictions to be better than last year?</p>  <p>EDIT: Also the methods from last year have been published or on forum so that means an improvement is to be expected</p>
3517, <p>mmmmm I can get 0.50 on 2014. I'm not tuning by year but by all tournaments - one model for all 4 tournaments. I'm picking the model with the best average rather than cherry-picking years. I'm a little worried about leakage but I've gone through my code and can't see where it is (if any). I've only used provided data and I'm still using my original data set which got 0.57 and all I've done since is a backward selector which reduced my score to 0.51.</p> <p>I will think about it over the weekend</p>
3517, <p>VALID&nbsp; Year : 2011  LogLoss all : 0.5098648 <br>TRAIN&nbsp; Year : 2011  LogLoss all : 0.5185867 <br>VALID&nbsp; Year : 2012  LogLoss all : 0.5217209 <br>TRAIN&nbsp; Year : 2012  LogLoss all : 0.5466057 <br>VALID&nbsp; Year : 2013  LogLoss all : 0.5386175 <br>TRAIN&nbsp; Year : 2013  LogLoss all : 0.5219784 <br>VALID&nbsp; Year : 2014  LogLoss all : 0.5033811 <br>TRAIN&nbsp; Year : 2014  LogLoss all : 0.5218951</p> <p>These are my per year results. Train is on a 10 CV. so I don't think my leakage is coming from part 3</p>  <p>Train is 2004 to year -1</p>
3517, <p>[quote=Dr. Pain;65087]</p> <p>&nbsp; The 0.51 in 2011 is more questionable -- that was a difficult year for prediction with a #8 #11 #3 and #4 in the Final Four for example.&nbsp; My guess would be that you're using some rating or data that's contaminated by the Tournament results.</p> <p>[/quote]</p> <p>I was having terrible trouble with 2011 at beginning but could get &lt;0.50 for 2012. So I sacrificed 2012. I'm not using rating data and I'm not even using the seed numbers as features for the tournament to be predicted. Funny enough the models I'm running at the moment can not improve 2011 but I've got &lt; .50 for the other 3 years. Unfortunately I will have to bin the &lt;0.50s because of 2011</p>  <p>EDIT: Also I came to the conclusion that either the person doing the seeding has got worse or the teams have got closer. Either way I didn't want my model to reflect the success of the seeder</p>
3517, <p>[quote=skwalas;65109]</p> <p>I recall last year quite a few people were trying to predict upsets specifically as if they had some unique differentiator. Wouldn't surprise me if it's a common thing this year too.</p> <p>[/quote]</p> <p>I predicted &quot;shocks&quot; at the beginning but it wasn't overly helpful</p>
3517, <p>Ha! thanks people. I do have part 3 leakage. It shouldn't effect predictions for 2015 but it does mean you can't compare my 2014 predictions with last year. I still would have done well but I wouldn't have come in the Top 3</p>
3517, <p>I read your 15000 sample post - why do you remove early regular season games?</p>
3517, <p>Hi Sorry to cause you extra work but I accidentally introduced leakage into my model during feature selection.&nbsp; Please can you delete my last 3 submissions - all_sub_v14.csv all_sub_v13.csv all_sub_v10.csv.</p>
3517, <p>You can get it from the tourney_slots.csv file.&nbsp; W and Z&nbsp; would only meet in the final I think</p>  <p>R5 is WX and YZ</p>
3517, <p>I've edited my reply. I think it's standard through all tournaments.</p>
3517, <p>I preprocessed the file then used the processed one. I then used MATCH to get the round lookup</p>
3517, <p>This is the file I've been using (but I thought it was all standard throughout)</p> <p>The lookup is done on</p> <p>IF RegionA == RegionB then concatenate seed numbers (eg 1_16 lowest first) then MATCH no match means Round = 4.</p> <p>IF RegionA != RegionB then concatenate region (eg W_X lowest first) then MATCH</p> <p>My code is messy. I'll neaten up tomorrow</p>
3517, <p>[quote=Jeff Sonas;65455]</p> <p>Oh and the other thing is that we can tell from the day numbers in the tourney_compact_results file which round an actual game was played in:</p> <p>Days 134-135 are the play-in games.</p> <p>Days 136-137 are the Round 1 games.</p> <p>Days 138-139 are the Round 2 games.</p> <p>Days 143-144 are the Round 3 games.</p> <p>Days 145-146 are the Round 4 games.</p> <p>Day 152 is the Round 5 games.</p> <p>Day 154 is the Round 6 game.</p> <p>[/quote]</p> <p>Ha. That would have been a lot easier to implement.</p>
3517, <p>yes pretty much same but I dealt in numbers. I have lots of IF statements.</p>
3517, <p>why 8 slots? The reason I worked with &quot;IFs&quot; was because in one of the location files the seeds are also characterised into W1W2W3W4WF. This file might be hard to join as well</p>
3517, <p>[quote=Zach;65466]</p> <p>hmmm nope. &nbsp;I've got a bug somewhere.</p> <p>[/quote]</p> <p>That's why my code ended up a string of IFs! I'd use Jason's code.</p>
3517, <p>It's best if you go to the Knowledge sections which have starter competitions with tutorials. </p>
3517, <p>If a really good player broke a leg before the Sunday would this change the way the team was seeded or is it just done on team performance throughout the regular season?</p>
3517, <p>[quote=Jose M.;65592]</p> <p>[quote=Bluefool;65591]</p> <p>If a really good player broke a leg before the Sunday</p> <p>[/quote]</p> <p>Is your model able to predict that? Let's team up.</p> <p>[/quote]</p>  <p>Ha. no. I was wondering if I was supposed to be reading the Sports News</p>
3517, <p>[quote=Jason Sumpter;65601]</p> <p>http://basketball.about.com/od/ncaatournament/a/injury-seeding.htm</p> <p>[/quote]</p>  <p>Thanks. Looks like I don't have to read the Sports News</p>
3517, <p>As I and others have recommend before why don't you have a look at the knowledge competitions? It sounds like from your other post you want an exact working piece of code where you can change &quot;2014&quot; to &quot;2015&quot; then submit as your own.</p>
3517, <p>The link to the sample code will show you how to create the combinations and matrices. It doesn't matter that it's not in Pandas - the first rule of data science - adapt and learn</p> <p>The tourney_seeds file will tell you the teams that will be playing in each tournament. Use this to make the combinations for each year. In it's simplest this will be a FOR loop.</p> <p>What data you use is up to personal preferences. I have only used tournament games. Net Prophet thinks this is not as good as using at least 15000 regular season games.&nbsp; Each to their own. Second rule of data science - Try it and see</p> <p>You could run 4 separate models.</p> <p>train 1985 - 2010 for test 2011</p> <p>train 1985 - 2011 for test 2012</p> <p>etc</p>
3517, <p>I think you had to start as a team.</p>
3517, <p>It says in the rules that Team Mergers aren't allowed so maybe this is why he doesn't have the link?</p>
3517, <p>[quote=Jason Sumpter;65952]</p> <p>I think you have an error in your formula.</p> <p>R1X1: X16 North Florida (1316) defeats X01 Duke (1181)</p> <p>R1X3: X14 New Mexico St (1308) defeats X03 Iowa St (1235)</p> <p>This happens quite a bit. The lower seed should never be predicted to beat the higher seed when using the seed benchmark.</p> <p>[/quote]</p>  <p>I think it's because of this &quot; I used the seed formula to randomly select the winner of each slot and you will notice for instance that it simulates the first-ever &quot;#16 over a #1&quot; upset in the history of the men's Division I tournament! &quot;</p>
3517, <p>I have a feeling that I'm going to be bed when the new stuff gets released. Hopefully I'll be able to &quot;plug&quot; in new stuff. Do we get locations of W1 W2etc? This is the bit that's worrying me most as it was forum obtained.</p>
3517, <p>thanks ever so Jason</p>
3517, <p>I think the NCAA finalises seeds&nbsp; at 19.00 ET - which I think (but I've been a bit thick this competition) is in about 2 hours http://www.ncaa.com/march-madness</p>
3517, 
3517, 
3517, <p>I'd like to use your predictions in an ensemble with mine. The thing is I get really cross with &quot;Lazy Kagglers&quot; so I'm not sure how I feel about posting my predictions. There are several people on the leaderboard who have just submitted the benchmarks and it is these people who will just take our work and claim as their own. (I know I know I'm grumpy) We have 0.8719279 correlation between our predictions which is interesting as we have used totally different approaches. My model doesn't use the current tournament seeds so I'm hoping I'm protected from any seeding errors (your post earlier) On a 10 CV I get .529 in training using years 2004 - 2014. Standard Deviation of 0.06 across the years. I can get 0.55 for 2014 but 0.59 for 2011.&nbsp; I have only used tournament games for training (tournament history + current regular season)</p> <p>If you have no objection I will ensemble our predictions but without posting my predictions. Please say if this is not acceptable as I have a backup 2nd model that I can also enter (using seeds). I will make it clear on the leaderboard by changing the team name if our ensemble is doing better.</p>
3517, <p>yes - I thought a straight average. </p>
3517, <p>I've just used Jason's code to get my bracket. I was disappointed that even though I haven't used seeds my model predicts that all seed 1s will win their region! Duke to win overall. I have some shocks in the first round though. Ohio Davidson UCLA and Dayton to win their first round matches</p>
3517, <p>[quote=skwalas;66633]</p> <p>I tried a bit of ensembling during stage 1.&nbsp; Simple averages always seemed to make the logloss worse.&nbsp; As best as I can tell averaging of the probabilities causes the values to regress toward 0.5 and the logloss to move toward -log(0.5) especially for those matchups where there are large variances between the models in the predicted win probabilities.&nbsp; So it reduced the cost of wrong predictions but also disproportionately reduced the gain I could have gotten from correct predictions.</p> <p>[/quote]</p> <p>Actually I'm rethinking the ensemble. I just created the bracket of the ensemble and Duke and Kentucky are going out first round!! I think I've made an error somewhere!</p>  <p>EDIT: mmm I've done something weird. Glad I caught that</p>
3517, <p>I think it would make me psychic!</p>
3517, <p>Enjoy. I watched the North Florida match last night on TV - first ever basketball match that I've watched. I couldn't believe my model preferred North Florida - silly model!</p>
3517, <p>Our ensemble has just pipped my own entry for the first time. My entry 0.423542 ensemble 0.421699. (35 games scored)</p>
3517, <p>The ensemble was successful if we use the leaderboard as the measure of success. My model was 24th yours was 42nd. A straight average of them was 8th. My model was better in the Round of 64 but got overtaken by the ensemble at 35 games</p>
3517, <p>[quote=Siddharth Chandrakant;69885]</p> <p>i set the threshold at [3367] for 2015 &amp; obtained 0.41 as i've mentioned above. the same [33 67] wud've had me fucked in 2014 but threshold of [2080] wud've given me 0.544 (which is 4th place on last year's leaderboard).</p> <p>since kaggle allow 2 submissions i'll try 2 different thresholds &amp; submit. one can work well if the tournament is very predictable. other can work well if there are too many upsets.&nbsp;</p> <p>[/quote]</p>  <p>Yes but hindsight is a great thing. You would need to select thresholds that will work well for both years. ie. sacrifice the 2015 score to get a better one for 2014</p>
3517, <p>Loads of us have done it. Embarrassing but not a problem</p>
3517, 
3517, <p>Jeff I think Vlookup does require sorted data. There's a work around using INDEX and MATCH though</p>
3517, <p>ok sorry - I've had trouble before so I always use Index and match.</p>
3517, <p>[quote=J Kolb;67282]</p> <p>Really Baylor?&nbsp; You too?</p> <p>[/quote]</p>  <p>ha! 2 shocks already!</p>
3517, <p>oh bless him. Probably got a broken arm as well as an injured achilles!</p>
3517, <p>I never used tourney_slots file. I just used Jason's seasons_with_locations file</p>
3517, <p>if(all$same_region[x] == 1 &amp;&amp; all$round[x] == 1) {<br> if (all$seed_noA[x] %in% c(11689)) {<br> reg = paste(all$regionA[x]&quot;1&quot;sep=&quot;&quot;)<br> } else if (all$seed_noA[x] %in% c(215710)) {<br> reg = paste(all$regionA[x]&quot;2&quot;sep=&quot;&quot;)<br> } else if (all$seed_noA[x] %in% c(314611)) {<br> reg = paste(all$regionA[x]&quot;3&quot;sep=&quot;&quot;)<br> } else if (all$seed_noA[x] %in% c(413512)) {<br> reg = paste(all$regionA[x]&quot;4&quot;sep=&quot;&quot;)<br> }<br> } else if(all$same_region[x] == 1 &amp;&amp; all$round[x] &lt;= 4) {<br> reg = paste(all$regionA[x]&quot;F&quot;sep=&quot;&quot;)</p> <p>} else if (all$same_region[x] == 0 &amp;&amp; all$round[x] &gt; 4) {<br> reg = &quot;Final&quot;<br> }</p> <p>if (all$round[x] == 2 || all$round[x] == 4 || all$round[x] == 6) {<br> distA = 0</p>
3517, <p>yes - Rounds 1 and 2 are at same location. All depends on the seeds. So seed 1 will play first 2 rounds at W1</p>
3517, <p>The better of the two. Just gives you 2 chances</p>
3517, <p>Using Jason's Printable Bracket in R code. Here are the brackets for Bluefool Net Prophet&nbsp; Bluefool / Net Prophet</p>
3517, <p>To tell you the truth I just blindly ran Jason's code. Jason Sumpter can answer this question better than me. Also I didn't calculate/predict expected scores - just win/lose probabilities</p> <p>EDIT: my model doesn't like Virginia for some reason - predicted to lose in 2nd round</p>
3517, <p>I think it's Duke playing Villanova?</p>
3517, <p>Interesting you have Ohio St to win 1st round as well. I have that and Net Prophet does</p>
3517, <p>My model doesn't use seeds either so there must be something about Ohio St as you have them go even further.</p>
3517, <p>Sorry duplicated post</p>
3517, <p>[quote=hamelg;66872]</p> <p>[quote=Bluefool;66760]</p> <p>EDIT: my model doesn't like Virginia for some reason - predicted to lose in 2nd round</p> <p>[/quote]</p> <p>Michigan State&nbsp;just had a couple good wins at the&nbsp;big 10 tournament and lost in overtime to #1 seed Wisconsin to end the season. Virginia lost 2 of its last 3 games of the season. If your&nbsp;model gives&nbsp;extra weight to the end of the season I could see why it might choose Michigan State over Virginia.</p> <p>[/quote]</p> <p>I only used whole regular season summaries. I only trained on tournament games</p>
3517, <p>Min.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1st Qu.&nbsp; Median&nbsp;&nbsp; Mean&nbsp;&nbsp;&nbsp; 3rd Qu. &nbsp; Max. <br>0.04922 0.30880 0.49030 0.49650 0.68170 0.96280</p>
3517, <p>I've got a play-in team to win the first round in my bracket! My bracket may be ruined before the deadline if they lose the play-in!</p>
3517, <p>I mean for my bracket challenge not Kaggle</p>
3517, <p>heehee First time for me. It's all quite exciting really</p>
3517, <p>We don't really have basketball in the UK. We don't even have College sports (apart from Oxford vs Cambridge boat race). We don't have cheerleaders either so it's all mad to me.</p> <p>I have</p> <p>Hampton to beat Manhattan 0.455665947</p> <p>BYU to beat Mississippi 0.522237595</p> <p>North Florida to beat Robert Morris 0.678540374</p> <p>Boise St to beat Dayton 0.32427828</p>
3517, <p>What do you mean by &quot;over&quot; - beat by?</p> <p>edit: if so you're spot on HT for Hampton match!</p>
3517, <p>Not an overly good start for me! Both play-in games went the other way!</p>
3517, <p>I have this scoring 0.60 in my spreadsheet but it's over 1 on LB and the Seed Benchmark is on 0.576864 according to the spreadsheet</p>
3517, <p>What matches are you at today? Could you hold up a Kaggle banner or something for a laugh?</p>
3517, <p>I have an inconsistency with Net Prophet's benchmark on the spreadsheet and the LB after 19 games. I have his benchmark as 0.53341</p>
3517, <p>2015_1320_1461 1 Public&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2015_1320_1461-1Ignored</p> <p>Apart from the line above yes it's the same. My score is agreeing with the LB but the seed benchmark I have as 0.525817 and Net Prophets Benchmark as 0.533415</p>
3517, <p>Hang on let me check again</p>
3517, <p>Sorry I think I might have got my Virginias muddled as the 19th / 20th games finished ???</p>
3517, <p>mmm now I have the correct seed benchmark score but my score and Net Prophet's doesn't match the LB. I will have another look at what I've done.</p>
3517, <p>ok thanks for that link. This competition has been no good for my ego! I had missed the Iowa / Wyoming match and had one of the Virginia games as the 19th game. oops sorry again</p>
3517, <p>I have .56 probability that Michigan State beats Virginia in the possible match up. I don't know what my model thinks it knows about Michigan State but it also predicts that after beating Virginia they have 0.61 chance of beating Oklahoma! I don't think I'll be in the top 50 for long!</p>
3517, <p>I'm quite pleased that I predicted 27/32 matches correctly. 3 out of 4 of my &quot;shocks&quot; went the right way UCLA Dayton and Ohio St. I was very wrong on the Davidson match though. The other 4 I got wrong were the first #14 vs #3 ones and a couple of #8 vs #9</p> <p>I think I also should have manually increased my probabilities for the #1 seeds as my biggest probability was .93. I think I could have scraped a few more LB places if I'd been more confident.</p> <p>A general thought on basketball - I bet the coaches have heart attacks as much as our Premier League managers do.</p>
3517, <p>Let's have some real Madness</p>
3517, <p>I don't like this Kentucky team or their supporters.</p>
3517, <p>I'm shocked at how bad their supporters are. They are deliberately trying to distract / put off Cincinnati. If they truly believed in their team they wouldn't have to do that. Maybe I'm just British but I've really taken a dislike of them. I would prefer my bracket to get totally busted then have them win</p>
3517, <p>If you make the scoring system complicated or dependent on domain-knowledge then only &quot;experts&quot; will be able to enter rather than general Kagglers</p>
3517, <p>https://www.kaggle.com/c/march-machine-learning-mania-2015/forums/t/12382/welcome</p> <p>There were no points for Stage 1 but half points for Stage 2</p>  <p>edit: replied same time as William!</p>
3517, <p>My Michigan St choice is looking more and more dubious now Villanova have gone. Luckily it's at 0.56 so hopefully won't be too much damage</p>
3517, <p>Nice one GLMnet</p>
3517, <p>My bracket's doing quite well now in the Bracket Challenges. It hasn't made much difference for Kaggle though. I have them to beat Oklahoma too at 0.61 so more stress to come. Definitely fun</p>
3517, <p>Taking into account that I knew nothing about basketball I decided to just use the data we were given. I ended up with 325 features (a lot of which were binarised conference and region)</p>  <p>1. I created a tournament history for each team. Number of tournaments entered tournament match wins if they had ever &quot;shocked&quot; before (based on seed benchmark) average shock level average seed min max std seed. Total match wins etc</p> <p>2. Then I worked out the differences in tournament history between Team A and B</p> <p>3. I then created a current regular season summary for each team (all the variables in the detailed_results file - so from 2004)</p> <p>4. Then I worked out the regular summary differences between Team A and Team b</p> <p>5. I also did distance - I kept it in but it didn't improve model</p> <p>6. I converted the Conferences and Regions to boolean matrices.</p> <p>7. Number of conference / region successes</p> <p>8. The last 2 year tournaments seed benchmark results</p> <p>9. No current tournament seed features</p> <p>Then I ran a GLMNET through it. I could reduce the variables to about 10 per test year but I introduced leakage into model so my final model was on all 325 features</p>  <p>I was quite surprised that it got UCLA Dayton Ohio St and it really likes Michigan St which it thought would get to the last 8 but get beaten by Villanova (!)</p>
3517, <p>When I was selecting features I could get rid of 95% of regular season data and get better results but I got concerned about leakage. Another difference - I also didn't look at if Team A had ever played Team B before. This was mainly due to laziness though. My &quot;shocked before in Round of 64&quot; features were quite relevant though. I had planned to do a &quot;negative shock&quot; &quot;positive shock&quot; feature but I also got lazy and decided that a lot of it was down to luck so I didn't put the extra effort in.</p> <p>I only trained on tournament games from 2004 but doubled up. TeamA vs Team B and then reversed as TeamB vs TeamA</p>
3517, <p>It has Kansas at 0.61 vs Wich St so might be on to a loser at the moment</p>
3517, <p>R code to create tournament history file. Messy but runs!</p>  <p>EDIT: Using the data I used this year would have got a top 10 position last year. It gets 0.55 for 2014</p>
3517, <p>[quote=Bluefool;67632]</p> <p>My bracket's doing quite well now in the Bracket Challenges. It hasn't made much difference for Kaggle though. I have them to beat Oklahoma too at 0.61 so more stress to come. Definitely fun</p> <p>[/quote]</p> <p>Blimey - this Michigan St run has been very good for my bracket. My model thinks it's over now though (0.49 over Louisville)</p>
3517, <p>I was thinking yesterday (and it might seem silly) but I would have liked to have known a &quot;supporter madness&quot; scale. So which teams have the most avid fans and therefore will have a lot of support</p>
3517, <p>Jason distance is no measure for avid fans. Distance will be a distorted measure. For some teams longer distance means less fans for other teams it will make no difference.</p> <p>EDIT: In the Kentucky Vs Cinc match I saw that the distance was about the same but there were way more Kentucky fans</p>
3517, <p>Not saying I've watched many basketball matches but I saw their match against Cincinnati and I didn't rate them. The only difference about them and other teams was their noisy supporters.</p>
3517, <p>I was just looking at the list of past winners - what happened to Connecticut this year? I tried to find information on the net but didn't find any.</p>
3517, <p>Thanks - it was strange because they have a good tournament history so was surprised that the winner from last year couldn't make it through to this year. Unusual. I didn't know if I wasn't getting the proper news because of English IP address. I hadn't found that wiki page just their team Wiki page which didn't say about the departures</p>
3517, <p>wow - what a lot of work. I hadn't realised I could win or come second so happy about that. I've been concentrating on my bracket entries rather than the Kaggle entry. I had already decided to measure my success by Stage 1 score vs Stage 2 score (ie. If I end up between .50 and 0.57 then I know my model was quite stable)</p>
3517, <p>I get &quot;Problem Loading Page&quot; http://drewdez.com/march-madness-metaprediction/</p> <p>you need to add www. to the front of link</p>
3517, <p>I kind of want Kentucky and Duke to go out next round just to muck up the stats!</p>
3517, <p>[quote=Colin Carroll;68662]</p> <p>For those interested in their own fates I updated the repo I was using for scoring the contest here:</p> <p>https://github.com/ColCarroll/ncaacheck</p> <p>Running</p> <p><code>python update.py</code></p> <p>will now produce a file &quot;carroll_outcomes.txt&quot; that lists all 128 possible tournament outcomes along with what place I'd end up in in&nbsp;order. &nbsp;If you change the last line of the file to read&nbsp;</p> <p><code>score_possible(username='glickman')</code></p> <p>It will produce a file &quot;glickman_outcomes.txt&quot; (and so on). &nbsp;Hope this is useful!</p> <p>[/quote]</p>  <p>Thanks. I have best at 4th worst at 98th!</p>
3517, <p>[quote=Jeff Sonas;68928]</p> <p>Also here is a complete listing of everyone's scores across the final possible 8 scenarios.&nbsp; You can use the Excel filtering feature to see what the top list would look like for just one of the 8 scenarios or you can filter on your own submissions to see what your final placement (among submissions not among teams) would be for each of your possible submissions.</p> <p>[/quote]</p> <p>Oh this is very different from Colin's python code. I'll think I'll just wait and see (for example according to Colin's code if Kentucky beat Duke I can get 7th you have me as 55th)</p>
3517, <p>[quote=Colin Carroll;68941]</p> <p>[quote=Bluefool;68935]</p> <p>Oh this is very different from Colin's python code. I'll think I'll just wait and see</p> <p>[/quote]</p> <p>I noticed that too --&nbsp;I'm double-checking my stuff feverishly and getting the same answers as before (which are different from Jeff's) -- will let you know if I find a problem on my side!</p> <p>[/quote]</p>  <p>thanks.</p>
3517, <p>Could you have the left / right side mixed up?</p>
3517, <p>Taken from another thread:</p> <p>Taking into account that I knew nothing about basketball I decided to just use the data we were given. I ended up with 325 features (a lot of which were binarised conference and region)</p> <p>1. I created a tournament history for each team. Number of tournaments entered tournament match wins if they had ever &quot;shocked&quot; before (based on seed benchmark) average shock level average seed min max std seed. Total match wins etc</p> <p>2. Then I worked out the differences in tournament history between Team A and B</p> <p>3. I then created a current regular season summary for each team (all the variables in the detailed_results file - so from 2004)</p> <p>4. Then I worked out the regular summary differences between Team A and Team b</p> <p>5. I also did distance - I kept it in but it didn't improve model</p> <p>6. I converted the Conferences and Regions to boolean matrices.</p> <p>7. Number of conference / region successes</p> <p>8. The last 2 year tournaments seed benchmark results</p> <p>9. No current tournament seed features</p> <p>Then I ran a GLMNET through it. I could reduce the variables to about 10 per test year but I introduced leakage into model so my final model was on all 325 features</p> <p>I was quite surprised that it got UCLA Dayton Ohio St and it really likes Michigan St which it thought would get to the last 8 but get beaten by Villanova (!)</p>
3517, <p>I really enjoyed / enjoying this competition and I've added a new sport to my collection. I have no suggestions for changes but I noticed others might prefer points spread / MOVs for scoring - I'm kind of against this as it would make things more complicated for ordinary Kagglers. Other than that it's all been great</p>
3517, <p>[quote=zdb_88;68658]</p> <p>&nbsp;The sharing of predictions before the contest also gives some incentive to gamble for example one of the top 5 projected finishers appears to have just taken Net Prophet's benchmark and gambled on one team all of the way with both of their entries.</p> <p>[/quote]</p>  <p>Lazy Kagglers always seem to do well</p>
3517, <p>Next year now I know a bit more I probably will post-process my predictions. I've correctly predicted lots of matches but my probabilities were too conservative. I didn't know how good Kentucky were so I think I would have done a &quot;gambling&quot; strategy on the favourites. My model is 100% computational with no manual alterations but this meant I had no extreme predictions which is why I'm falling behind all the extreme Kentucky predictors.</p>
3517, <p>[quote=Net Prophet;68746]</p> <p>[quote=Bluefool;68707]</p> <p>I didn't know how good Kentucky were so I think I would have done a &quot;gambling&quot; strategy on the favourites. My model is 100% computational with no manual alterations but this meant I had no extreme predictions which is why I'm falling behind all the extreme Kentucky predictors.</p> <p>[/quote]</p> <p>Sure this year.&nbsp; But last year the gambles lost and you'd be saying you learned an important lesson not to artificially inflate predictions.&nbsp;</p> <p>[/quote]</p>  <p>That be true! Using the exact model I used this year would have got a top 10 finish (0.55) last year so I think you're correct - this year was a year for the gamblers. I'll let you know next year! ;)</p>
3517, <p>[quote=zdb_88;68740]</p> <p>[quote=Net Prophet;68696]</p> <p>Interesting.&nbsp; Who is that?</p> <p>[/quote]</p> <p>Juho. While you are looking at your predictions compared to Juho's you can also compare pookiebear to ur1pl.</p> <p>[/quote]</p> <p>They must have stopped reading the thread title after &quot;Steal my Entry&quot; - didn't get as far as the ENSEMBLE bit. boooo</p>
3517, <p>[quote=YosarianLives;68770]</p> <p>&nbsp;I&nbsp;would note however that the baseline for my entry was a model that I created and I believe the same is true for ZachB. The fact that we both employed &quot;betting&quot; strategies does not make us &quot;lazy Kagglers.&quot;</p> <p>[/quote]</p> <p>My &quot;lazy Kagglers&quot; only refers to people who didn't even download the files and attempt a model. The people who literally just took someone else's entry and manually adjusted it.&nbsp; In no way was I implying &quot;gambling strategies&quot; was lazy</p>  <p>:( I didn't mean you or Zach :(</p>
3517, <p>:D&nbsp; I feel better now</p>
3517, <p>Wouldn't this make my tournament history model&nbsp; redundant? I see my tournament history part as the&quot;psychological&quot; advantage score</p>  <p>EDIT: I haven't fully analysed it but I think it was this part that predicted Michigan State's successful run</p>
3517, <p>Actually the more I think about it the scoring system is strange and biased. With my best entry which is the ensemble with Net Prophet I have Kentucky to win the next 2 matches but only in the .50s. If my predictions are correct for the next 3 matches then I will slide down the leaderboard and end up 55th. If my predictions are wrong and Duke wins then I can get 13th Winconsin 9th and Michigan St 13th so it's in my interest if my predictions are wrong.&nbsp; In all cases I can get top 20 if Kentucky loses but if they win (like I've predicted) I'll end up 55th. I know it's all about &quot;confidence&quot; in predictions - but it seems odd to me.</p>
3517, <p>[quote=Net Prophet;69078]</p> <p>[quote=Bluefool;69075]</p> <p>I know it's all about &quot;confidence&quot; in predictions - but it seems odd to me.</p> <p>[/quote]</p> <p>&nbsp;But your situation hints at more interesting strategies.&nbsp; </p> <p>[/quote]</p> <p>I think I'll go for the &quot;shouting louder at the telly&quot; strategy! Thanks for giving me the chance of a Top 20 finish with your predictions.</p>
3517, <p>I have a better chance of a Top 20 finish!</p>
3517, <p>I think I need to shout it - COME ON NOTRE DAME</p>
3517, <p>it's not over yet!</p>
3517, <p>the clocks went forward in UK tonight - it's 3.42am!</p>
3517, <p>arrgh Kentucky gone in lead</p>
3517, <p>crap</p>
3517, <p>Notre Dame were more of a &quot;team&quot; I thought.</p>
3517, <p>Mine was the Notre Dame vs Kentucky one. I liked the way Notre Dame played - seem to be different from the others. Kentucky winning in the last few seconds was just thrilling</p>
3517, <p>[quote=Net Prophet;68922]</p> <p>We don't call them &quot;matches&quot; Dr. Bluefool :-)</p> <p>[/quote]</p> <p>lol games?</p>
3517, <p><a href="http://bleacherreport.com/articles/2416920-jimmy-fallon-uses-puppies-to-predict-which-team-will-win-2015-ncaa-tournament?utm_source=twitter.com&utm_medium=referral&utm_campaign=programming-national">Puppies may be better at predicting than us</a></p>
3517, <p>So funny. Poor Duke!</p>
3517, <p>ha - my model was never keen on them!</p>
3517, <p>My model (not ensemble) was .46 this year. Last year the exact model would have been .55</p>
3517, <p>The rules state we can choose 2 submissions for judging. Does this mean we can submit 2 brackets for 2015?</p>
3517, <p>Thanks how will this work for the LB? Will it switch between your 2 submissions after each match? Could we have both on LB? so can compare. I'm thinking of doing the 2nd submission by logical manual guessing so it would be interesting to see my 2 submissions compete on the LB.</p>
3517, <p>It says up to 25 games - isn't it 24? or have I missed another on?</p>  <p>EDIT: are you scoring the play-ins?</p>
3517, <p>The results file was updated an hour ago so I think it's scoring play-ins to get to 25. I'll fork it and take out the play-ins as I'm easily confused</p>
3517, <p>ok - yes it's just a typo from 24 to 25</p>
3517, <p>Did the first match trash all our predictions? lol&nbsp; 2 out 3 shocks already.</p>
3517, <p>Come on Butler and UCLA! Make my score better</p>
3517, <p>WTF? I though UCLA had lost? last second goal? No TV in UK</p>
3517, <p>hahah I had even filled in the spreadsheet with SMU. lol no wonder this is called Madness</p>
3517, <p>it's asking me to install stuff - last night I tried getting it but kept getting IP blocked. It's on ESPN at 10pm in UK so only half an hour to go. Ohio St aren't doing very well</p>
3517, <p>[quote=David Tran;67316]</p> <p>[quote=Bluefool;67311]</p> <p>WTF? I though UCLA had lost? last second goal? No TV in UK</p> <p>[/quote]</p> <p>You may be able to watch games online: http://www.ncaa.com/march-madness-live/</p> <p>It might be region-locked though.</p> <p>[/quote]</p> <p>I tried that but I'm blocked :(</p>
3517, <p>bloody football's gone into extra time so another half an hour to wait</p>
3517, <p>yes! it's on now. My dogs are looking at me weirdly as I shout at the ref in the Ohio match. Yes - I wasted half an hour of my life the other day reading baseball news lol</p>
3517, <p>lol can I blame it on being British?</p>
3517, <p>Come on Ohio</p>
3517, <p>[quote=Sun'sOutFun'sOut;67335]</p> <p>Bluefool love your enthusiasm. &nbsp;Careful with calling Ohio State Ohio. &nbsp;Unless you want to be a Michigan fan. &nbsp;</p> <p>[/quote]</p> <p>ah ok - don't know the nicknames yet. I'm loving this. I do actually love sport but have never crossed over to an American one. Bedtime soon though so will miss the others</p>
3517, <p>I think they're submission counts - I've read them as &quot;About 80 submissions give a 0.65 probability that Duke will win&quot;</p>
3517, <p>Most competitions are fine for a 16GB machine. Apart from 1 competition a 16gb mac has been fine for me</p>
3517, <p>That's weird you being the admin Josef. I was playing with the data last night and the results&nbsp; reminded me of the Loan Default competition - which you won!</p>
3517, <p>Try it and see.</p>
3517, <p>Some of my models take 2 weeks to run.</p>
3517, <p>[quote=Goldman;66951]</p> <p>when I calculate crossvalidation it takes hell lot of time...without crossvalidation it takes 5 minutes..</p> <p>is it necessary to calculate CV?</p> <p>[/quote]</p>  <p>yes  it's for the best. Try a GBM first as much quicker than an RF</p>
3517, <p>Goldman you have set the 93 features to be numeric and not factors? Factors can really slow things down</p>
3517, <p><br>LogLoss &lt;- function(actual predicted eps=0.00001) {<br> predicted &lt;- pmin(pmax(predicted eps) 1-eps)<br> -1/length(actual)*(sum(actual*log(predicted)+(1-actual)*log(1-predicted)))<br>}</p>  <p>I think you work out logloss by class then divide by 9</p>
3517, <p>[quote=Dean McKee;67044]</p> <p>Assuming you have nine columns in a df named 'preds' corresponding to the predicted probabilities and a vector of ground truth labels with numbers ranging from 1-9 (for each class):</p> <p>preds &lt;- cbind(predsgroundTruthLabel)</p> <p>rowTotals &lt;- apply(preds1function(x) log(x[x[10]]))</p> <p>logLoss &lt;- sum(rowTotals)/length(rowTotals)*-1</p> <p>Of course this isn't applying the max/min constraints of the contest but you probably won't exceed those anyway unless you're post-processing.</p> <p>[/quote]</p>  <p>thanks I'll use this</p>
3517, <p>If a row can only belong to one class why are the probabilities rescaled? &quot;The submitted probabilities for a given product are not required to sum to one because they are rescaled prior to being scored (each row is divided by the row sum).&quot;</p>
3517, <p>If you do one_against_all they will never add to one. I get good logloss per class (9 binomial models) but awful score using the evaluation metric</p>
3517, <p>glm VALID target : Class_1  alpha : 0.5  LogLoss : 0.09276369 <br>glm TRAIN target : Class_1  alpha : 0.5  LogLoss : 0.08689497 <br>glm VALID target : Class_2  alpha : 0.5  LogLoss : 0.322279 <br>glm TRAIN target : Class_2  alpha : 0.5  LogLoss : 0.3147334 <br>glm VALID target : Class_3  alpha : 0.5  LogLoss : 0.2818557 <br>glm TRAIN target : Class_3  alpha : 0.5  LogLoss : 0.2834728 <br>glm VALID target : Class_4  alpha : 0.5  LogLoss : 0.1495732 <br>glm TRAIN target : Class_4  alpha : 0.5  LogLoss : 0.1518968 <br>glm VALID target : Class_5  alpha : 0.5  LogLoss : 0.01824936 <br>glm TRAIN target : Class_5  alpha : 0.5  LogLoss : 0.01536175 <br>glm VALID target : Class_6  alpha : 0.5  LogLoss : 0.1222117 <br>glm TRAIN target : Class_6  alpha : 0.5  LogLoss : 0.1166334 <br>glm VALID target : Class_7  alpha : 0.5  LogLoss : 0.1106946 <br>glm TRAIN target : Class_7  alpha : 0.5  LogLoss : 0.1153422 <br>glm VALID target : Class_8  alpha : 0.5  LogLoss : 0.1065663 <br>glm TRAIN target : Class_8  alpha : 0.5  LogLoss : 0.1073392 <br>glm VALID target : Class_9  alpha : 0.5  LogLoss : 0.09153483 <br>glm TRAIN target : Class_9  alpha : 0.5  LogLoss : 0.09663007 <br>[1] 0.1439698</p> <p>This gives over 2 on the leaderboard. Very confused</p>
3517, <p>[quote=Dean McKee;67051]</p> <p>Log-loss makes no sense as a metric in a context where the possible solutions don't add up to 1.&nbsp;</p> <p>[/quote]</p>  <p>In other competitions logloss has been used for multi-classes where a row can belong to more than one class - so that doesn't follow</p>
3517, <p>[quote=Dean McKee;67055]</p> <p>Are you using R (I'm guessing so because of GLM) -</p> <p>You can use my (or anyone else's code) from this post to calculate log-loss.</p> <p>http://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/12895/compute-score-before-submission</p> <p>[/quote]</p>  <p>So this problem has to be dealt with as multinomial because of the metric?</p>
3517, <p>Abhi are you doing 1 multinomial model?</p>
3517, <p>thanks. I need to rethink this</p>
3517, <p>[quote=Abhishek;67070]</p> <p>Have you seen the class distribution? Its pretty messed up ;)&nbsp;</p> <p>[/quote]</p>  <p>yes  that's why I thought 9 models would be better! lol back to the beginning for me</p>
3517, <p>Does XGBoost do class stratification on CV?</p>
3517, <p>Josef was the winner of Loan Default and is the admin of this competition. So I doubt there's leakage</p>
3517, <p>? I'm not talking about the leader. Josef Feigl won the Loan Default competition and is the admin of this competition. If you see above  people have referred to &quot;golden features&quot; and &quot;leakage&quot; like there was in the Loan Default competition. My point is Josef out of everyone would have been prepared for this</p>  <p>EDIT: ok</p>
3517, <p>Look everyone has just focused on the &quot;Beat the Benchmarks&quot;. Soon everyone will have a score like that. That's why I don't like the Benchmarks - they make people lazy and unimaginative</p> <p>edit: that was for Chris! Getting timing wrong. Giovanni - possibly but I think it's too early to think of leakage</p>
3517, <p>[quote=Giulio;68753]</p> <p>My two cents: I do not understand why everybody is thinking leakage. First we have no idea what Otto's own best model is. Maybe they were looking at the LB and thinking &quot;how long is it going to take to these morons to get to .2 ?&quot;... :-)</p> <p>All we know is that the usual methods landed most in the low .4 zone. I have put 0 imaginative effort in this competition and just copying pasting code from old competitions I got easily to .41. I have not done a single plot of the data nor any feature engineering. The few things I've looked at seemed to point out to clear opportunities to improve the score. I haven't looked into any of those yet but obviously at least one person has.</p> <p>For me the fun part of the competition starts now. Everything I've done so far was just self gratification to be in a good spot faster than other people. But that really means nothing. Maybe the only advantage is that when you sprint well out of the blocks you get better opportunities to team up with other good players. But obviously it seems like good will be very relative in this competition...</p> <p>[/quote]</p>  <p>Totally agree</p>
3517, <p>Man what has the world come to? You're now picking on me because I'm telling you about a known cheat! lol. I made a handful of submissions and got bored. Maybe come back when you have the same overall profile that I do!</p>
3517, <p>rbind train and test to account for factors. Then split into train and test then run model. I'm only guessing you're getting &quot;factors in test set&quot; error</p>
3517, <p>The problem is that the dataset is so small you might actually be 1st on the private LB. You need to ignore the public LB and concentrate on local validation scores.</p>
3517, <p>This was a particularly hard competition so I'm not sure if any wisdom would have helped you with this one. I personally downloaded the data ran a 10 CV changed the random seed ran another 10 CV and the scores were so far apart I didn't spend any more time on it and didn't submit. In most competitions choosing the best Public score is normally ok. It's only a handful of times where another model was better but never would have changed my position by one or two positions. You get feel for it after a while</p>
3517, <p>[quote=Lilia Knyazeva;83397]</p> <p>some participants and newbies in particular do not understand that &quot;cheating&quot; in the Kaggle sense is totally different compared to the concept of cheating in Uni sense where cheating is a clearly negative action.</p> <p>[/quote]</p> <p>You need to get a better dictionary. The fact you're still harping on means you must have been caught cheating and now bitter. You're trying to justify your cheating. Cheating is cheating - at Uni and Kaggle. Get over it and don't cheat (unless of course you're a &quot;name&quot; then no-one gives a toss if you're a cheat and you can repetitively cheat and everyone still thinks the sun shines out of your cheating unethical arse)</p>
3517, <p>oops double post</p>
3517, <p>You haven't taken the Kaggle-in-class accounts into account. Over 50 of the &quot;non-points&quot; accounts will be from my students! They need an account to take part in the competition for their assignment. As they are students they receive no points and never enter again. I think 1000s of these accounts will be from Kaggle-in-class competitions.</p>  <p>Also KDD&nbsp; has a&quot;No multiple account&quot; rule so once again I'm not sure what your point is</p>
3517, <p>I find it strange that the species &quot;UNSPECIFIED CULEX&quot; is in every year of the test set but in no years in the train set. Have they been removed from the train set?</p>
3517, <p>Yes I know it's not a species as such! It's not a natural pattern that it's in 200820102012 and 2014 but not in 2007 20092011 and 2013</p>
3517, <p>If that was the case then there would be no species in the test data at all. Most of the test set has a species entry so the species are known. It's only a minority where &quot;unspecified&quot; is used it's just that is never used in the training data</p>
3517, <p>Yes&nbsp; I also thought the majority of unspecified culex is fake! Only the real combinations are scored though</p>
3517, <p>It all's going to depend on how many multiple rows (with duplicated classes) are in the 30% and the 70%. I take a sample before I submit to check and the difference between the 30% 70% is sometimes over 10. This whole competition hasn't been thought out properly</p>
3517, <p>lol Abhi you're going to be back below 1000 votes at this rate! ps. I haven't down-voted you</p>
3517, <p>You can have a sympathy vote up! :P</p>
3517, <p>yay! but you won't be able to help yourself. There will be other comps with swag prizes. I think this one was chosen to attract people as it's crazily data munging.</p>
3517, <p>I've assumed that the padding of the test data is for 2 reasons:</p> <p>1. Manual tuning</p> <p>2. The organisers want a model based on weather data only.</p> <p>However the fake data totally ruins any time-series analysis neighbourhood analysis and cluster analysis. You can't base a prediction on anything that's gone before because the test data is fake. The worst thing is I get a better score on the leaderboard chucking out all weather data and using fake record counts instead! Also once an epidemic has started weather is irrelevant - so if you can't use time series clustering neighbourhood analysis or the weather then what data is left?</p> <p>EDIT: and because of the duplicate records with opposite classes you can't even work out the probability there's wnv present. If there's 40 duplicated rows with 2 predicted cases the prob = 2/40 but as 10 of those records could be fake the real probability of the data appearing for that species in that trap is 2/30</p>
3517, <p>They've &quot;padded&quot; it so every species is in every trap at every visit.</p>
3517, <p>You can't. I should have been clearer and said &quot;counts that include fake data&quot; rather than &quot;fake record counts&quot;</p>
3517, <p>I have built models on the training data yet they can't be applied to the test data because it is fake which means you have to ignore time. You have duplicated records with opposite classes and a 30/70 split. This competition is random.</p> <p>and yes my models have a +-10 either way up/down in a 30/70 split so I know my model isn't great.</p> <p>Having duplicated rows with opposite classes is hard enough but then they've added fake duplicate records to make this a totally random competition. And what's the point of even given us time? any pattern will fall over in the fake test set</p> <p>And if there's no point giving us time what's the point of duplicated records? They might as well make the target be the probability of wnv appearing in that species in that trap on that date. Instead of - here's 50 duplicated rows 1 probably has wnv - though it might not because it may be fake.</p>
3517, <p>1. We don't have spray data for test years</p> <p>2. For example  you always have a couple of multi-row traps before an outbreak of wnv begins. This symbolises that the number of mosquitos is increasing. However how do you know which is the date with the 2nd multi-row entry in the test set? If fake multi-row entries have been inserted before the real 2nd multirow then the whole extrapolated pattern will then be incorrect</p> <p>3. Usually fake data is irrelevant but not in a time-series</p> <p>4. If you can't use patterns based on time then why keep the duplicated row format?</p>
3517, <p>yes I know. Oh well</p> <p>EDIT: Totally regret it too. Going to move on to another competition and watch myself slide down the leaderboard on this one!</p>
3517, <p>Trust your validation scores not the leaderboard</p> <p>As well as producing the validation score also produce the score on a random 30% / 70% of the validation set - this will give you an idea of distortion to expect in the leaderboard. So in validation I end up with 3 scores - 100% validation set (1 year) and then a score on 30% and a score on 70%</p>
3517, <p>[quote=Victor;79296]</p> <p>&nbsp;There is no possibility (in this competition) to obtain substantially different&nbsp;public and private LB scores.</p> <p>[/quote]</p>  <p>I disagree - I think there can be huge differences in the public / private leaderboard. It's all down to where the duplicate records end up. Even on validation if you split into 30/70 you can sometimes get pretty even scores other times you can get a .1 discrepancy</p>
3517, <p>I'm having same difficulty so I have no advice. My 0.77 validation model got 0.83 on LB and my 0.83 validation model got 0.77 on the LB! It's psychological trauma - do I trust validation or get sucked into the leaderboard? lol All fun</p>
3517, <p>&nbsp;I would ignore my LB score - I originally had 4 models. In validation 0.750.810.820.77. On LB 0.750.810.810.83. I then added features got 0.83 in validation and got 0.77 on LB</p>
3517, <p>Validate by a whole year. I can pretty much get 1 on training 10 CV over all the data but validating on a whole year I get about .82 average. This is the score that matches the LB more</p>
3517, <p>was that the score of the CV or the score on the year you left out?</p>
3517, <p>Yes big difference. Train on the 3 years (forget the CV error) and predict on the year left out. It is this score that should match the LB closer. Then change Train parameters (regardless of CV score)  predict on the year left out etc and so on. Choose the model that has the better score on validation not the Train CV. Though there may be better ways of doing it this is how I'm currently doing it</p>  <p>EDIT: and if you're doing this and getting big differences it means you haven't tidied up the data</p>
3517, <p>Bit of a silly script but I thought I'd try out the Script functionality. This is a bit of code I've added in to my models as I'm distrustful of the Public Leaderboard in this competition. It gives the AUC over your valid set and then takes a 30/70 stratified split and gives you the AUCs on the splits.</p> <p>https://www.kaggle.com/users/2242/bluefool/predict-west-nile-virus/check-your-validation-30-70-split</p>
3517, <p>I never managed to install it on my Mac. I used devtools in R and use XGBoost in R instead</p> <p>https://github.com/dmlc/xgboost/tree/master/R-package</p>
3517, <p>Such a good script</p>
3517, <p>Unfortunately Kaggle fully supports late benchmarks regardless of how the other competitors feel.</p>
3517, <p>I doubt that - it's an Old School Master's Top 10.</p>  <p>Anyway Kaggle doesn't care about the hard work we put in - they only care about attracting people to Kaggle. All these benchmarks are good for Kaggle. I just take solace in the fact all the benchmarker writers must have been bedwetters as children</p>
3517, <p>Kaggle fully supports late benchmarks.</p> <p>I thought &quot;negative voting&quot; would be the answer but there are too many lazy people who positive vote.</p> <p>Unfortunately these people are just malicious. I personally think it's vile that top 5% code is posted near the end of competition. </p>
3517, <p>[quote=the1owl;81786]</p> <p>&nbsp;Yup one publication could make you work harder for the 10% but lets be honest if your not sharing you are aiming for the 1st place and in so no possible sharing should deter you from that goal at the very least it should make your script stronger if a good one is shared because you have the luxury of copying things that work without anyone knowing.&nbsp; Top 25% wow that's like an average student might as well share if that is your only goal.&nbsp; Anyway to some the takeaway is a sandbox to test ML and to others yes half the scripts may be mere copies but there are a few jewels out there that make the search well worth it. Maybe a rating system based on functionality and merit of of script selected by a panel not all benchmarks with the first publish first to rate option.&nbsp; Anyways I'm new and the whole one click debate is getting old.&nbsp; Thanks to those sharing and contributing valuable content in the forums hope to reciprocate some day or at the very least challenge you for the top 3.</p> <p><a href="http://jwitteport.weebly.com/uploads/2/7/2/3/27233783/eng345km.pdf" target="_blank">An just for fun here is one of many references that can help with your tension.</a></p> <p>[/quote]</p> <p>For someone who only joined in April you seem to know everything. I can only assume you are a puppet account. And getting a 25% badge is a great achievement - who do you think you are saying it's worthless?</p>
3517, <p>For those who are starting out: I was really proud the first time I set myself the target of a 25% badge - it took me a couple attempts but I did it (this was before benchmarks). I also still remember the first time I dared to set myself the 10% target. I clearly remember the desperation of trying to stay in the Top 10 in the last week to become a Master in Cause and Effect (thanks AL) . It's all been worth it.</p>
3517, <p>I just hope we haven't overfitted and get beaten by it!</p>
3517, <p>https://www.kaggle.com/c/stumbleupon/leaderboard Here a benchmark had &quot;underfitted&quot; look at all the entries with same score - I was gutted and got beaten by all these people!</p>
3517, <p>[quote=Lilia Knyazeva;82243]</p> <p>sure creation of an alternative platform of about the same size as Kaggle will make a lot of headaches for Kaggle admins but it seems to be absolutely necessary. The reputation issue is a very subjective and it appears going from bad to worse.. (a lot of totally uncontrollable administrative regulations inconsistency of the results and so on). Yes I am also taking part in one competition outside Kaggle.. (and have no time for more)</p> <p>[/quote]</p> <p>I use other platforms but they're all &quot;all or nothing&quot; - you either win or get nothing. I usually give up when I realise I have no chance of getting a prize (like KDD Datadriven). The advantage Kaggle has is the whole Ranking thing - you still have something to &quot;win&quot; even if you're not going to get prize - you aim for badges and points. I liked that KDD and DataDriven had no forum or community but then again I also disliked this!.</p>
3517, <p>Lilia - Kaggle's ok. Just chill out and have some fun</p>
3517, <p>yes - Random Forest needs to know all levels (ie categorical Species) before predicting. You have 2 options: 1. rbind train and test into a data.frame then split into train and test again</p> <p>2. Change &quot;Unspecified Culex&quot; to something that already exists</p>
3517, <p>You submit predictions for all the test set</p> <p>Kaggle have a predetermined 30% of these predictions for the Public Leaderboard</p> <p>The other 70% (of the predictions you have already submitted) are used for the Private LB</p> <p>The Private LB is revealed on Wed 17th June 12 UTC (as soon as competition ends)</p>
3517, <p>[quote=NxGTR;81669]</p> <p>I have compete a few times already XD I ignore LB most of time but still wonder on LB it states:<br>&quot;The final results will be based on the other 70% so the final standings may be different.&quot;<br><br>Is that correct? I mean are the results of the LB completely ignored? I thought final rankings include the LB + the other 70% not just the other 70%.</p> <p>Does it mean that in Taxi competition Final rankings will ignore half the data :/?</p> <p>[/quote]</p> <p>yes</p>
3517, <p>No - all your submissions will have the Private Score (an added column in Submissions Section) when the competition is finalised</p>
3517, <p>[quote=Davut Polat;81696]</p> <p>this is how it looks like</p> <p>[/quote]</p> <p>Looks like you would have been upset at your model choice!</p>
3517, <p>[quote=Davut Polat;81701]</p> <p>[quote=Bluefool;81698]</p> <p>[quote=Davut Polat;81696]</p> <p>this is how it looks like</p> <p>[/quote]</p> <p>Looks like you would have been upset at your model choice!</p> <p>[/quote]</p> <p>not really!! choosed best models ;) &nbsp;it is logloss results on Otto</p> <p>[/quote]</p> <p>lol didn't realise - looked at the highest scores!</p>
3517, <p>I think : there will be some overfitting underfitting good luck bad luck. So I think there will be some LB changes (maybe not the ones over 0.87). I'm not sure if &quot;overfitting&quot; is right term though because a lot of it depends on how many of the duplicate rows are in the 30/70</p>
3517, <p>I think my teammate D3PO has some very useful stuff for the organiser. He had some awesome weather-based features that performed better than my simple basic leaderboard feedback.</p>
3517, <p>I think they were just having fun ideas. I've got nervous now - I want to stay in top 10 so D3PO gets his Masters realistically I'm thinking Top 20 but could be horrifyingly drop&nbsp; to Top 100</p>
3517, <p>lol I didn't move!</p>
3517, <p>[quote=Leustagos;82198]</p> <p>We moved up! Hurray for useless overfitting models!</p> <p>[/quote]</p> <p>Congrats. Was it really useless?</p>
3517, <p>[quote=inversion;82197]</p> <p>I would have lost LOTS of kaggle points! &nbsp;lol</p> <p>Now the question is . . . how much Hand Labeling???????? &nbsp;:-)</p> <p>[/quote]</p> <p>hand-labelling? in this comp?</p>
3517, <p>[quote=Devin;82202]</p> <p>Looks like the best benchmark is just out of the top 10%. I got the largest drop of 889 places. I think a simple ensemble of the best benchmarks would have gotten into the top 10%</p> <p>[/quote]</p> <p>you dropped 800+ ? did you go for feedback model?</p>
3517, <p>[quote=Leustagos;82198]</p> <p>&nbsp; &nbsp; &nbsp;I didnt like this amount of hand adjusting. We have a good underlying model that gets 0.83 on public lb without multiplier the rest was just probing. The main issue here is that we didnt used extreme multipliers to avoid overfitting and it worked pretty well for us.</p> <p>[/quote]</p> <p>We could get nearly 84 using multirows and weather. We didn't use multipliers but we did cap the tails (june and October)</p>
3517, <p>It was silly making unspecified as pipiens - all you had to do was check how many multirows of unspecified there were - there was none. I put it as Erraticus (I changed this before ensembling in the model with mine)</p>
3517, <p>[quote=Leustagos;82278]</p> <p>We just set all non pipens/restuans predictions to 0. We didnt use those records in the train phase. There was non zero records on lb for those but its was negligible.</p> <p>[/quote]</p> <p>I had done that originally before the LB was leaked - I thought this might have been why I had gone from 1st to 5th so I put the other species back in</p>
3517, <p>I'm on a Mac with anaconda and I can't run it either</p>
3517, <p>Alexandros - do you know that it's bad etiquette to post code in the last few days of a competition? </p>
3517, <p>Using Leaderboard feedback has already been asked - the Admins didn't respond therefore it must be ok. As for the External Data Peng has just admitted to cheating!</p>
3517, <p>The competition finishes tomorrow - your issue existed 2 months ago. Why the wait?</p>
3517, <p>[quote=Solstice;82018]</p> <p>This is my first time in a competition - is it always like the wild west here with rule changes?</p> <p>[/quote]</p>  <p>It never used to be</p>
3517, <p>Doesn't this give an advantage to those in a later time zone. They have 5 submissions to exploit this data  I have none</p>
3517, <p>You should end the competition. Everyone's entries could be taken as those submitted before Peng decided to ruin the fun</p>
3517, <p>The only reason you don't care is because you've already been using this data</p>
3517, <p>And remember people don't use up all your submissions because you know some smartar@e is going to post a late benchmark tomorrow :P</p>
3517, <p>[quote=Herra Huu;82040]</p> <p>[quote=Dmytro Lystopad;82036]</p> <p>Data available but using them still can(and my opinion should) be forbidden.</p> <p>[/quote]</p> <p>I don't understand all the hate for Peng. Making the link public (instead of just secretly using it by himself to win this competition) is a sign of high moral character. If someone made a mistake here it was Kaggle/host.&nbsp;</p> <p>[/quote]</p> <p>It was the timing. I can only assume that he used the data realised he had chance to win and then at last minute made it public so the rest of us don't have time to utilise it and his back is now covered</p>
3517, <p>The problem I have is that we have no 0s in our best prediction. However now I've seen the external data (in the hour it was allowed) I now think I can risk some 0s</p>
3517, <p>[quote=barisumog;82060]</p> <p>[quote=Abhishek;82059]</p> <p>Great! Its good and works only for the top 3 (since models are evaluated after the competition closes). What can be done about other participants who just care about &quot;good&quot; rank?</p> <p>[/quote]</p> <p>As a general rule I agree but in this competition at this moment I disagree.</p> <p>It's hard to tell how many of the top rankers&nbsp;have already been using this data. I'd guess anyone over 0.80 (without using external data) now has at least a chance to win a&nbsp;prize spot.</p> <p>Personally I wouldn't risk choosing an illegitimate submission just for gaining rank points and lose the opportunity to actually win a prize.</p> <p>[/quote]</p> <p>yes but you're forgetting that as LB feedback is allowed you can just say I knew there were no cases that week because the leaderboard told me</p>
3517, <p>lol shows we don't read!!</p>
3517, <p>[quote=inversion;82073]</p> <p>[quote=SkyLibrary;82039]</p> <p>Whatever this competition is totally ruined.</p> <p>[/quote]</p> <p>I even spent $200+ for a private consultation with one of the module authors. [/quote]</p> <p>I'm so tight I won't even hire AWS</p>
3517, <p>My method:</p> <p>Exploited multirows extensively</p> <p>- how many multirows by species date week year-to-date other species with multirows</p> <p>- aggregate rows with a &quot;no_of_row&quot; feature (reduced dataset by about 2000) so no duplicates</p> <p>- WnvPresent as binary target</p> <p>- no weightings but capped June and October</p> <p>- 2012 automatically was picked up because of multirows</p> <p>- could get 0.82 - 0.835 using gbm xgb and random forest</p> <p>D3PO:</p> <p>- adjusted longitudes and latitudes of traps so they had a better personal weather forecast</p> <p>- calculated weather fronts floods distances from floods</p> <p>- calculated Cold Fronts that American Robins don't like</p> <p>- with tail capping could get about .835 too</p> <p>- target wnvpresent + (nummosquitos/360)</p> <p>Ensemble of 2 approaches : 0.85</p> <p>Then strangely enough ensembling in (70/30) our 0.85 model with the .80 Lasagne model got us to 0.86</p> <p>So no real leaderboard feedback at all</p>
3517, <p>I wrote this on other post:</p> <p>Here's a badly written file that exploits the multirows (and other stuff) - don't ask me about bugs and efficiency! you're on your own! It includes all species</p>
3517, <p>I always make NAs -1000</p>
3517, <p>I subsequently decided that the multirows in the test set weren't fake - 2 rows meant between 50 and 100 mosquitos. The more mosquitos  the bigger risk of WNV - therefore the more multirows the bigger risk of WNV. It's not leakage if they use the model for short term forecasting - ie a week ahead</p>
3517, <p>Here's a badly written file that exploits the multirows (and other stuff) - don't ask me about bugs and efficiency! you're on your own! It includes all species</p>
3517, <p>there's some here https://www.kaggle.com/c/predict-west-nile-virus/forums/t/14781/model-and-overfit-sharing</p>
3517, <p>mmmm you're forgetting some people only have limited time to work on the competitions - ie weekends only. That would put them at a serious disadvantage. I was always keen on the &quot;max submission for whole competition&quot; idea but this was argued against about 4 years ago!</p>
3517, <p>yes - that was what I meant but I should have written &quot;max number of ..&quot; I can't remember why it was outvoted. I think it's ok like it is. It's only in time-series comps where probing is a issue - in other comps people just end up overfitting the public LB</p>
3517, <p>lol http://www.independent.co.uk/news/business/news/greece-crisis-crowdfunding-campaign-crashes-indiegogo-raises-half-a-million-in-three-days-10357000.html</p>  <p>UK are donating!</p>
3517, <p>The problem with Democracy - the people voted him in</p>
3517, <p>What's the &quot;Lambda Magic&quot;? What do those lines of code do please?</p>
3517, <p>thanks</p>
3517, <p>It's not a good idea really. However these benchmarks always seem to be lucky for the leaderboard. Bad practice but good leaderboard score. Happens all the time.</p>
3517, <p>nice code</p>
3517, <p>I had real trouble doing that task in Python!! That was why I wrote out the components to csv and went back to using R! My script is in the Script section. R is easier for munging and preprocessing than Python but Python is faster at ML</p> <p>EDIT. and by using E1071 package you can replicate the benchmark score but also have the benefit of cbinding all your other features</p>
3517, <p>I gave up on that too. Adapt my SVD to File script with the components you want then write to csv and cbind them to your file in R.&nbsp; This turned out to be the easiest and quickest way round.</p>
3517, <p>wow thanks - have you asked for the package in the Feedback section? It would be good to have that in the Script section (Edit - just realised it is! just not runnable)</p>
3517, <p>For the R package you need the ScoreQuadraticWeightedKappa for this competition</p>
3517, <p>I managed to open them both in R. I'm on a MAC. I used:</p> <p><br>train &lt;- read.csv(&quot;kaggle_data/train.csv&quot;stringsAsFactors=FALSE)<br>test &lt;- read.csv(&quot;kaggle_data/test.csv&quot;stringsAsFactors=FALSE)</p>
3517, <p>[quote=Peter Hrvola;80855]</p> <p>(cunting head)</p> <p>[/quote]</p> <p>snigger</p>
3517, <p>In case you want to use other languages this script dumps the SVD components to csv</p> <p>https://www.kaggle.com/domcastro/crowdflower-search-relevance/utility-write-svd-components-to-file</p>  <p>In R you need to cbind the file to any other file</p>
3517, <p>Yes - I just ran my copy and I just selected and copied the above into a new text file and ran. Both times the train and test files were written out correctly and as expected</p>  <p>EDIT: bad timing - my &quot;Yes&quot; and reply was to Ben</p>
3517, <p>A more efficient and better version of using R for SVD is here</p> <p>https://www.kaggle.com/gmilosev/crowdflower-search-relevance/r-version-of-benchmark-script</p>
3517, <p>Yes it does to some extent. However even when I look at high variance results I just think What planet were these graders on?</p>
3517, <p>As a warning to Mac users you may get an error when calling RWeka. It's something to do with the Parallel package</p> <p>I've had to call it explicitly</p> <p>BigramTokenizer &lt;- function(x) {RWeka::NGramTokenizer(x RWeka::Weka_control(min = 2 max = 2))}</p> <p>and not load the library</p> <p>http://stackoverflow.com/questions/17703553/bigrams-instead-of-single-words-in-termdocument-matrix-using-r-and-rweka</p> <p>(The format of code has gone weird when displayed)</p>
3517, <p>Go away</p>
3517, <p>Have you won yet? :P</p>
3517, <p>Thanks I will have a look at the Json. I had no luck using External data so I decided that reality was irrelevant and it was the ranker's perception of reality that was important. But I'm still going to have a look at the Json!</p>
3517, <p>No way! Yes it happened before. At least you know you're overfitting. Did you use benchmarks at all? (get them out of my ensemble!!!)</p>
3517, <p>did you see West Nile?</p>
3517, <p>hahah yes I think my jaw would have dropped at that kind of drop. At least you can change direction now (if it was the private LB you saw)</p>
3517, <p>[quote=Ben Hamner;82168]</p> <p>No changes have happened to the site in the past 6&nbsp;hours and there's no indication that private leaderboards have been leaked.&nbsp;</p> <p>[/quote]</p>  <p>He must have Kaggle-blindness!</p>
3517, <p>I can't get this out of my head! I'm having to go over everything I've done just in case</p>
3517, <p>You're 35th out of 900+ - I doubt people are going to help your ascent to greater heights!</p> <p>EDIT: and the hint is in your message &quot;I haven't started ..&quot; maybe start?</p>
3517, <p>[quote=StatMach;82264]</p> <p>Thanks Bluefool.. So Most of them with LB score above 0.67+ are using&nbsp;ensembles?&nbsp;</p> <p>[/quote]</p> <p>I couldn't get to 0.66 without ensembling so you're already doing better than me!</p>
3517, <p>[quote=Artem;83100]</p> <p>Can someone from top-6 confirm success without external data?</p> <p>[/quote]</p> <p>They just did. If they are getting 0.693 for single models without external data then averaging them would give them their current score. Can you confirm that you haven't please?</p>
3517, <p>Don't use the test set to make features as it's been padded with fake data</p>
3517, <p>[quote=Stephen McInerney;83083]</p> <p>@NxGTR:&nbsp;Your post comes across as incredibly rude and unhelpful even if that wasn't your intent:</p> <p>I wouldn't insult or disrespect people for not joining in&nbsp;the game of tweaking benchmarks.</p> <p><a href="https://www.kaggle.com/c/crowdflower-search-relevance/forums/t/14923/advice-on-generating-features-from-training-test-sets-in-cv">Advice on generating features from training + test sets in CV?</a></p> <p>I didn't get decent answers to that.</p> <p>[/quote]</p>  <p>So saying my answer isn't &quot;decent&quot; isn't rude?</p>
3517, <p>It is bad practice to use the test set in any competition (or real world) that is not based on semi-supervised learning. It's particularly bad when the test set is known to be padded with fake data and the domain is text mining. So I did actually answer 2 of your questions as 2 of them were based on using the test set. The answer to your first question is - try it and see. Remove the non-common words make a model and submit. You will then have answered your own question. The answer to your last question - we only have the training data to rely on so yes we have to rely on it for feature selection and validation</p>
3517, <p>[quote=Dimitris Leventis;83528]</p> <p>Good luck to all from Athens Greece!</p> <p>[/quote]</p>  <p>Good luck to all Greeks in general</p>
3517, <p>It would be nice to be able to quote - but even this has changed. Rcarson - That was bad advice - the reason the competitions are &quot;unpopular&quot; is because they are harder - Otto competitions are easy. It is much harder to get in the top 10 in &quot;unpopular&quot; competitions because they tend to be more specialist. &quot;Unpopular&quot; competitions also don't tend to have benchmarks. More points should be given to the competitions with least competitiors because you know the people have actually put effort in. Unlike this competition and Otto.</p>  <p>Anyway congrats all. I adjusted the Query and Title based on median relevance then did the same as everyone else</p>
3517, <p>SkyLibrary - next time if you are doing well team up with someone. and don't worry about the paper academia is full of rejection (and corruption).</p>
3517, <p>Let's bring back the abacus!</p>
3517, <p>Others came to the same conclusion https://www.kaggle.com/c/icdm-2015-drawbridge-cross-device-connections/forums/t/14485/goodbye-any-remnants-of-privacy</p>
3517, <p>I wonder if it will end up an online competition with Tinrtgu's code?</p>
3517, <p>[quote=Abhishek;80791]</p> <p>yes:&nbsp;https://www.kaggle.com/abhishek/avito-context-ad-clicks/beating-the-benchmark</p> <p>[/quote]</p> <p>ha I'm psychic and you're quick</p>
3517, <p>Will you have NULL targets where objecttype &lt;&gt; 3?</p>
3517, <p>Do I need to use any special unicode encoding to one-hot-encode the Russian? I've googled it but can only find stuff for Windows not Mac</p>
3517, <p>thanks</p>
3517, <p>Another shining example of my communication skills</p>
3517, <p>I've only just started but I've been using Perl</p>
3517, <p>Takes 30 minutes to run on a 16 GB Mac.</p> <p>I had trouble merging and joining in R because of memory constraints. I was even having trouble with a 25% sample.</p> <p>This script stores category and ad into hashtables then reads the searchstream files line by line and joins the relevant fields. As I'm only using Contextual Ads Location is ignored and only Contextual Ads are written out. This can be changed by removing thisline[3]==3 condition.</p> <p>I have pretty much used the same code to also add in the user and search info - just edit the column numbers (Perl indexes from 0)</p> <p>I have included the add_user file too but it takes hours. It can only be run AFTER add_ads.pl as the output of one is the input to the other</p>
3517, <p>I'm trying to write out Alex's features to file as I'm more comfortable using R. </p>  <p><a href="https://www.kaggle.com/domcastro/grasp-and-lift-eeg-detection/write-the-features-to-file">https://www.kaggle.com/domcastro/grasp-and-lift-eeg-detection/write-the-features-to-file</a></p>  <p>I can't get it to work as I get stuck on the arrays/matrix/frames. I tried hstack append and now converted to data frame but still can't get it to work. Can anyone help out? ta</p>
3517, <p>Thanks - just going to give it a go. - Might take me a while if I have to install stuff for Python</p>
3517, <p>Taking longer than expected as I have to download data again for the new events file</p>
3517, <p>Thanks ever so much. It worked and all files are written.</p>
3517, <p>How do you change the filter please? I tried :</p>  <p>def data_preprocess_train(X):     X_prep_normal = scaler.fit_transform(X)</p>  <pre><code>X_prep_low = np.zeros((np.shape(X_prep_normal)[0]10))  for i in range(10):      X_prep_low[:i] = butterworth_filter(X[:0]12-(i*0.2)3)      X_prep_low[:i] = scaler.fit_transform(X_prep_low[:i])  X_prep_low_pow = X_prep_low ** 2  X_prep_med = np.zeros((np.shape(X_prep_normal)[0]10))  for i in range(10):      X_prep_med[:i] = butterworth_filter(X[:0]02-(i*0.2)3)      X_prep_med[:i] = scaler.fit_transform(X_prep_med[:i])  X_prep_med_pow = X_prep_med ** 2  X_prep_high = np.zeros((np.shape(X_prep_normal)[0]10))  for i in range(10):      X_prep_high[:i] = butterworth_filter(X[:0]22-(i*0.2)3)      X_prep_high[:i] = scaler.fit_transform(X_prep_high[:i])  X_prep_high_pow = X_prep_high ** 2  X_prep = np.concatenate((X_prep_lowX_prep_normalX_prep_low_pow X_prep_med X_prep_med_pow X_prep_high X_prep_high_pow)axis=1)  #do here your preprocessing  return X_prep </code></pre>  <p>but getting error. </p>  <p>File &quot;3sigs.py&quot; line 163 in </p>  <pre><code>X_train=data_preprocess_train(X_train) </code></pre>  <p>File &quot;3sigs.py&quot; line 68 in data_preprocess_train</p>  <pre><code>X_prep_med[:i] = scaler.fit_transform(X_prep_med[:i]) </code></pre>
3517, <p>Thank you. I think it's the fourth argument that I've got wrong. Thanks for explanation </p>
3517, <p>EDIT: Resolved by installing Lasagne from source file and compiling rather than using the stable version</p>  <p>Hi thanks for code. I've set up the relevant packages on my MAC but I'm having trouble with this one line</p>  <p>from lasagne.objectives import aggregate binary_crossentropy</p>  <p>I get error</p>  <p>ImportError: cannot import name aggregate</p>  <p>I can import everything else. I've Googled it but can't find an answer</p>
3517, <p>I have it running. thanks. Took me a while. The score is fluctuating between subjects (0.49 to 0.74) so far but I only increased the parameters you suggested and I haven't learnt how to add in layers yet (one step at a time). I think I added in too many epochs (if I've interpreted the scores being printed to screen correctly) I was quite pleased with myself really! lol. Thanks for the chance of playing with Lasagne</p>  <p>Update: the last 2 subjects  score has been .80 and 0.97 (if this is the valid score?)</p>
3517, <p>OK that was successful LB 0.84255</p>  <p>SAMPLE_SIZE = 4000 # Larger (2048 perhaps) would be better</p>  <p>max_epochs = 100 (checking validation scores to see if this was a good thing)</p>  <p>dense = 1024 # larger (1024 perhaps) would be better</p>
3517, <p>Update:</p>  <p>LB 0.90211 &quot;replacing the line computing tseries in train with the commented out line just above it.&quot;</p>  <p>I'm stuck on  increasing the TRAIN_SIZE. I didn't understand why it was 5*1024 rather than just 5120? (I might be stupid but I don't mind) I'm then going to look at tutorial to play with layers </p>
3517, <p>[quote=Tim Hochberg;86891]</p>  <p>[UPDATE] There's not much point in playing with the TRAIN_SIZE and max_epochs at the same time in this code. The total number of points seen is just going to be TRAIN_SIZE*max_epochs.  If you keep that product constant the training will be the same but the intervals between updates will increase / decrease.  If you start changing the learning rate each epoch or doing early stopping then you'll have to pay more attention to this.</p>  <p>[/quote]</p>  <p>Thanks - I had just worked that out funny enough. Just playing with Filters for conv1D I can't remember if I set it to 10 for that LB result (bad versioning - edited it whilst it was running) </p>
3517, <p>Download the code again as you've done something funny. This code definitely gets the scores above with the changes discussed above. </p>
3517, <p>I think you have leakage     X_prep_normal = scaler.fit_transform(X) should be X_prep_normal = scaler.transform(X) for preprocessing test</p>
3517, <p>I'm only learning so a silly question - is this code for 1 electrode or all electrodes?</p>
3517, <p>Thanks - that's why I got confused. Is it this:</p>  <p>X0 = [x[:0] for x in X]</p>  <p>?</p>
3517, <p>Finally sussed it. Thanks guys</p>
3517, <p>Example not attached</p>
3517, <p>I don't think an &quot;epoch&quot; is an epoch in Lasagne. Tim's written about it on the script discussion</p>  <h1>We train on TRAIN_SIZE randomly selected location each &quot;epoch&quot; (yes that's</h1>  <h1>not really an epoch). One-fifth of these locations are used for validation</h1>  <h1>hence the 5*X format to make it clear what the number of validation points</h1>  <h1>is.</h1>  <p>? sorry about format</p>
3517, <p>If you're interested in per-subject why is the evaluation metric not mean AUC subject-wise? Calibrating the AUCs is a whole separate problem but it sounds like it's unnecessary for the real-world and only necessary for the competition? </p>
3517, <p>Congrats everyone. Not much LB movement (we didn't move at all) and congrats to Sky Library who has become a Master</p>
3517, <p>Thanks ever so much for all your help. I'm pleased with the no 1 and no 2 on LB at the moment - the helpful experts in their rightful place!!</p>
3517, <p>NxGTR - you need to bite the bullet and team up with someone. You'll get your masters then</p>
3517, <p>nice</p>
3517, <p>Considering about 13 people have submitted this -&nbsp; 2 votes ( 1 of which is mine and I haven't submitted) is extremely tight. People could at least vote you up. (or is pressing 2 buttons too much work)</p>
3517, <p>I didn't down-vote you but I can only guess the reason could be :</p>  <p>&quot;just submit and see&quot;</p>  <p>&quot;you could have submitted it by the time you wrote that post&quot;</p>  <p>&quot;Just use a submission&quot;</p>
3517, <p>Gaussian in GBM usually requires lots of trees. </p>
3517, <p>Ah I'm not using Scripts so I don't know what data is being used. On my data on a 10CV and a 10% holdout my best is 2000 trees. I always end up using loads of trees for Gaussian and Laplace (on a CV). My GBM results are bang on to LB score (~.25)</p>
3517, <p>Is it cheating? : Yes - you have multiple accounts on the Leaderboard. I would contact Support if I was you otherwise you might get a shock after the end of the competition</p>
3517, <p>Huge ?</p>
3517, <ol> <li>Make a model predicting Cost using the Training data (in Data section)</li> <li>Apply this model to the Test data (ie Predict the Cost of the Test rows)</li> <li>Submit your predictions</li> <li>Your predictions are then split into 2 subsets - 1 for the Public Leaderboard and 1 for the Private Leaderboard (hidden) We do not know what ids are in the subsets but it is the same 2 subsets used each time you submit.</li> <li>Repeat</li> </ol>
3517, <p>Yes - use cross validation to tune training parameters  Calculating the error locally on your computer will give an estimate of the Leaderboard (LB) score - do this BEFORE you submit. Sometimes the LB score can be misleading so it is important to set up an appropriate CV method - there are other posts on this. </p>
3517, <p>I think Franc means his holdout set which he's treating as a Private LB</p>
3517, <p>What have you imported XGBoost as? in the import line?</p>  <p>import xgboost as xgb</p>  <p>xgtest = xgb.DMatrix(test)</p>
3517, <p>Seeds and Operating Systems</p>
3517, <p>[quote=woshialex;90872]</p>  <p>Interesting... I feel different and think  the leader board will be very stable given it is so hard to over fit the leader board ...</p>  <p>[/quote]</p>  <p>Really? I think this could be a big overfitting one</p>
3517, <p>It's split by tube assembly. This has been mentioned previously in the competition (and easily confirmed by comparison of CVs to the LB) Judging from the standard deviations of folds then I believe there could be overfitting if you don't protect yourself. It's not wishful thinking all I can do is protect myself from it - I'm  more worried about dropping than going up the LB!</p>  <p>EDIT: seemingly I'm probably wrong!</p>
3517, <p>I have strangely underfitted in my last 3 competitions!</p>
3517, <p>ha! I just had that in the Grasp competition (7th on both) and West Nile (8th on both).</p>
3517, <p>Congrats Leustagos and team</p>
3517, <p>tube assemblies <a href="https://www.kaggle.com/c/caterpillar-tube-pricing/forums/t/16035/why-i-m-observing-a-bias-between-cross-validation-score-and-kaggle-public">https://www.kaggle.com/c/caterpillar-tube-pricing/forums/t/16035/why-i-m-observing-a-bias-between-cross-validation-score-and-kaggle-public</a></p>
3517, <p>ah crap</p>
3517, <p>yes I've started to agree with you. I totally misunderstood that other forum post! It's going be very bad if by random chance the expensive tubes with 1 quantity are all in the private set</p>
3517, <p>It's the number of leaves rather than the &quot;depth&quot; so Depth 1 would have 2 leaves at a split depth 2 would be 3 leaves etc. Have a look at the GBM Tree and it will become clearer</p>
3517, <p><a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3885826/">http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3885826/</a></p>  <p>The idea behind a decision tree is to partition the space of input variables into homogenous rectangle areas by a tree-based rule system. Each tree split corresponds to an if-then rule over some input variable. This structure of a decision tree naturally encodes and models the interactions between predictor variables. These trees are commonly parameterized with the number of splits or equivalently the interaction depth. It is also possible to have one of the variables be split in a particular several times.</p>  <p>A special case of a decision tree with only one split (i.e. a tree with two terminal nodes) is called a tree stump. Therefore if one wants to fit an additive model with tree base-learners it is possible to do this using the tree stumps. In many practical applications small trees and tree-stumps provide considerably accurate results (Wenxin 2002). Moreover there is much evidence that even complex models with rich tree structure (interaction depth &gt; 20) provide almost no benefit over compact trees (interaction depth &#8776; 5).</p>
3517, <p>I've never been able to get xgboost to work in Python on a MAC - I use the R version</p>  <p>EDIT: ps. mucking with OpenMP on Mac will make other things not work</p>
3517, <p>Well done - what python are you using? I'm using Anaconda and it clashed with the open mp changes. </p>
3517, <p>Have some fun and play with non-benchmark scripts.</p>
3517, <p>You have to take into account the little score differences - but this isn't &quot;shake-up&quot; as such. For the Crowd competition I looked at scores and thought Top 20 was realistic for me (nothing to do with overfitting underfitting just random chance) . Also I think Crowd might have been the first competition where we all &quot;underfitted&quot;. Big shake-ups as in huge score differences usually occur with small datasets. Anyway this competition has only just started so I think there may be a break away by the end of the month</p>
3517, <p>[quote=inversion;84160]</p>  <p>[quote=Bluefool;84152] Have some fun and play with non-benchmark scripts. [/quote]</p>  <p>Nah . . . when this gets boring I work on the Caterpillar Tube Pricing competition which is super intricate and interesting.</p>  <p>[/quote] But think of the T-shirts and mugs <a href="https://www.kaggle.com/c/introducing-kaggle-scripts/details/prizes">https://www.kaggle.com/c/introducing-kaggle-scripts/details/prizes</a></p>
3517, <p>Ha This one even made me laugh. I keep repeating to myself &quot;adapt and evolve adapt and evolve&quot;</p>
3517, <p>I'm on the latest version of R on a Mac and I use <a href="https://github.com/dmlc/xgboost/tree/master/R-package">https://github.com/dmlc/xgboost/tree/master/R-package</a> to install XGboost</p>
3517, <p>xgboost is much faster - I usually use xgboost and gbm.</p>  <p>I didn't do any compiling. I just ran: devtools::install_github('dmlc/xgboost'subdir='R-package') then library(xgboost)</p>  <p>I'm version 3.2.1 and only updated this week on Mavericks</p>
3517, <p>Hi I haven't been using Scripts or the data from Scripts but I just ran &quot;count:poisson&quot; on my data (R and xgboost updated this week) and I received no error message on a 10cv 100 rounds</p>
3517, <p>ps. I've not got xgboost to work in Python on the Mac though</p>
3517, <p>I tried very hard! Seemingly it's because I have Anaconda MP optimisations which clashes with openMP. It mucked up a lot of my Mac trying so I was very pleased when the R package came out. It has some functionality missing but I've found it fine.</p>
3517, <p>I'm a bit late to the party - why are you dropping some of the fields? train_s.drop('T2_V10' axis=1 inplace=True) train_s.drop('T2_V7' axis=1 inplace=True) train_s.drop('T1_V13' axis=1 inplace=True) train_s.drop('T1_V10' axis=1 inplace=True)</p>  <p>Was it just that dropping them improved LB score?</p>
3517, <p>When I see the square on the LB and press it and go to the Script I then Fork it but it never runs in the 600 seconds. It must have run for other people? I've just forked one (it had a tick on the version number) but when I run it it gets killed. All I wanted to do was add &quot; ntree_limit=model.best_iteration&quot; into the predict statements (Justfor's forum post)</p>  <p>I've tried others and it's the same the little square box is deceiving me in some way</p>  <p>EDIT: and now I've spelt Persecuted wrong and feel more persecuted because I can't edit title</p>
3517, <p>I still can't get it to run. </p>
3517, <p>Have you set the Random Seed?</p>
3517, <p>That's the reason. If the script hasn't set a seed then reproducing isn't possible</p>
3517, <p>[quote=SteveKane;87896]</p>  <p>why would y'all want to join teams? then you gotta split the money and you can't be nearly as proud if you win</p>  <p>[/quote]</p>  <p>Ah the sound of LB Naivety. Steve the public Leaderboard does not necessarily reflect the private Leaderboard. If your solutions are just tweaks of the scripts going round there's a big chance you've overfitted and not top of the private leaderboard. </p>  <p>People team up to share ideas and try to not overfit. Winning in a team is really good - most competitions are won by teams - still a lot to be proud of. </p>  <p>The competition has a long way to go yet and I wonder if you'll be begging for a team mate in the last couple of weeks!</p>
3517, <p>I tried but the script never runs for me in 600 seconds! Someone in a better time zone needs to do it</p>
3517, <p>Normalised Gini</p>
3517, <p><a href="https://www.kaggle.com/c/liberty-mutual-group-property-inspection-prediction/details/evaluation">https://www.kaggle.com/c/liberty-mutual-group-property-inspection-prediction/details/evaluation</a></p>
3517, <p>[quote=SteveKane;88140]</p>  <p>All I've got to worry about for the next month is not completely neglecting my girlfriend and studying for 3 not very difficult university courses. go me go me</p>  <p>[/quote]</p>  <p>You're not completely neglecting your girlfriend all ready? Shame on you data always comes first</p>
3517, <p>Random seed?</p>
3517, <p>It's known as &quot;fine tuning to the Public Leaderboard&quot;. Just ignore that side it - download the script locally run a proper CV and adjust the preds accordingly (even if it doesn't beat the fine tuned score)</p>
3517, <p>[quote=Bluefool;89098]</p>  <p>It's known as &quot;fine tuning to the Public Leaderboard&quot;. Just ignore that side to it - download the script locally run a proper CV and adjust the preds accordingly (even if it doesn't beat the fine tuned score)</p>  <p>[/quote]</p>  <p>IGNORE: how did I even manage this duplicated quoted post?!!!</p>
3517, <p>[quote=Leustagos;88766]</p>  <p>In R it works but not in python. I think the seed param works in both. But i'm not using R anymore so im not sure if R supports the seed param (set.seed i do know). Python has all the hot algos and it is much faster for data proccessing and less memory hungry so im very happy moving foward!</p>  <p>[/quote]</p>  <p>How are you getting on with slicing/dicing etc? I want to make the move to Python and I've sussed out Scikit Learn but I'm having trouble with data preprocessing in Python. I seem to do all my preprocessing in R (especially for time series comps) then use Python for modelling</p>
3517, <p>I got that when I used the FactortoNumeric script. 39CV/36LB</p>  <p>ps. I'm using the early eval as the validation set and the scores match the LB</p>  <p>No time for cross validation in under 20 minutes for the shared scripts</p>
3517, <p>The top script dropped a lot but it didn't over fit funny enough! I only managed .3902 public LB so gone up quite a few places</p>
3517, <p>Steve dropped too</p>
3517, <p>[quote=Dmitriy Guller;90758]</p>  <p>[quote=Alexandre Millot;90752]</p>  <p>Congratulations Qingchen !</p>  <p>Dropped 22 but 73 isn't that bad for a first hey.</p>  <p>Is there any way to know which of the 2 submissions was the best on the private LB ?</p>  <p>[/quote]</p>  <p>You'll see the private score for all your submissions eventually but it usually takes about a week or so.</p>  <p>[/quote] You can resubmit your top 2 and it will tell you your private score</p>
3517, <p>[quote=Deep;90775]</p>  <p>@Cardal's sorcery didn't work today! He didn't win but still pretty impressive finish! @Abhishek dropped the most :)</p>  <p>[/quote] I don't think Abhi dropped the most? The most (that I can see) is a drop of about 694</p>
3517, <p>Hi I have a couple of datasets that I want to use for this competition. I could hardcode it but it might be messy. Is there away to put small datasets somewhere?</p>
3517, <p>ps. it's interesting data. From my paper <a href="http://www.nature.com/nrc/journal/v15/n3/full/nrc3891.html">http://www.nature.com/nrc/journal/v15/n3/full/nrc3891.html</a> (it made the news <a href="http://www.telegraph.co.uk/news/health/11435977/New-drugs-to-tackle-cancer-cell-weak-spots-could-end-scattergun-chemotherapy.html">http://www.telegraph.co.uk/news/health/11435977/New-drugs-to-tackle-cancer-cell-weak-spots-could-end-scattergun-chemotherapy.html</a>)</p>
3517, 
3517, <p>Whole patient data is here <a href="https://www.kaggle.com/c/introducing-kaggle-scripts/forums/t/15139/is-there-a-way-of-adding-data/83956#post83956">https://www.kaggle.com/c/introducing-kaggle-scripts/forums/t/15139/is-there-a-way-of-adding-data/83956#post83956</a>. Several cancers and patients (over a million patient / gene combinations)</p>  <p>EDIT: if people like this data then I also have Gene expression and Copy Number data for the patients too. </p>
3517, <p>ignore - don't need to install it</p>
3517, <p>No - unfortunately not. You have to earn them! Write a super script - are you good at visualisation?</p>
3517, <p>I don't know - give it a go and if it doesn't work request the packages here <a href="https://www.kaggle.com/forums/f/15/kaggle-forum/t/13728/kaggle-scripts-product-feedback">https://www.kaggle.com/forums/f/15/kaggle-forum/t/13728/kaggle-scripts-product-feedback</a></p>
3517, <p>In the Acquired Value competition Zach had to send in his winning code from the top of a mountain on his honeymoon!</p>
3517, <p>Thanks. That was fun</p>
3517, <p>I thought it might be &quot;after 6pm&quot; or alike</p>  <p>I have assumed:</p>  <p>0 not available</p>  <p>1 available all day</p>  <p>2 available but with conditions</p>
3517, <p>I'm a bit late to the party - but is this metric correct or am I misunderstanding it? I took one of the benchmark scripts removed all coupons from users who had withdrawn before the coupon date (920 users) resubmitted and my score didn't improve (stayed the same) and the benchmark script is still my best submission. I doubt all 920 are in the 70% split - so what am I not getting?</p>
3517, <p>thanks!</p>
3517, <p>If you use R use the Metrics package</p>
3517, <p>From working at hospitals people born in May have a higher risk of MS. I know 3 people with MS and all 3 were born in May</p>
3517, <p>? seems perfect to me! ps. you've spelt &quot;organisers&quot; wrong ;)</p>
3517, <p>You seem to have forgotten where your language came from - just because you went on to mis-spell things doesn't mean your way is right</p>
3517, <p><a href="http://www.wikihow.com/Talk-Like-a-New-Yorker">http://www.wikihow.com/Talk-Like-a-New-Yorker</a></p>  <p>ps. My family just went to New York and were teaching me how to say &quot;coffee&quot; NY style. Seemingly I'm not very good and sound a mixture of Polish and Irish!</p>
3517, <p>He was just very good at writing the meaning of words - he didn't &quot;invent&quot; words. His contributions were good and still stand (I believe) . Though he might have &quot;invented&quot; the word &quot;autopeotomy&quot; because he chopped his own penis off!</p>  <p>EDIT: and strangely enough he was sent to Crowthorne Berkshire which is the village I live in! (He was actually an American but was working in England)</p>
3517, <p>ha Blooming Americans even changed the title of the book</p>  <p>The Surgeon of Crowthorne: A Tale of Murder Madness and the Love of Words is a book by Simon Winchester that was first published in England in 1998. It was retitled The Professor and the Madman: A Tale of Murder Insanity and the Making of the Oxford English Dictionary in the United States and Canada.</p>
3517, <p>Come on James copy it to the Script section. You know you want to really! (ps I've &quot;clicked and submitted &quot; twice today already :O)</p>
3517, <p>it improved my score!</p>
3517, <p>If you look at titles of Scripts it looks like my script has the highest score but it hasn't. Let's try and edit titles after submitting with the actual LB score rather than keeping the historic original's score.</p>
3517, <p><a href="https://github.com/yandexdataschool/flavours-of-physics-start/blob/master/evaluation.py">https://github.com/yandexdataschool/flavours-of-physics-start/blob/master/evaluation.py</a></p>
3517, <p>Hi you need to change tau_data... to /input/train.csv</p>
3517, <p>Not sure - try it and if it returns an error request the package here </p>  <p><a href="https://www.kaggle.com/forums/f/15/kaggle-forum/t/13728/kaggle-scripts-product-feedback">https://www.kaggle.com/forums/f/15/kaggle-forum/t/13728/kaggle-scripts-product-feedback</a></p>
3517, <p>ha yes but it's getting them all running in the 600 seconds!!</p>
3517, <p>Try an earlier version   possibly version 2</p>
3517, <p>Not sure sorry. Make sure you're using my Version 2</p>
3517, <p>Version 2 is the highest scoring version of this script that also passes the correlation and agreement checks. I have made several versions subsequently that failed the checks. So choose Version 2 from the drop down box and Fork that one. If your code still doesn't work locally I'd ask @fchollet as he is the author of Keras</p>
3517, <p>Is there an R equivalent of evaluation.py as I'm having trouble validating in Python</p>
3517, <p>love you! thanks. I'll have a look at it later and give it a comparison test to the Python one</p>
3517, <p>My CVs and LB are way out - using the min cutoff.  I get 0.92 CV and 0.98 LB</p>  <p>EDIT: it would help if I used Weighted ROC and not just ROC</p>
3517, <p>Did you submit the original? there are loads of scripts with that name. </p>
3517, <p>[quote=Sylvia Lee;93141]</p>  <p>I got CV=0.881 while LB=0.972 using RF. I don't understand how this could happen (maybe I did something wrong). How can fix this?</p>  <p>[/quote] Are you using weighted-roc? Your results are indicating a local normal AUC for CV and a weighted AUC for LB</p>
3517, <p>the LB is only for cases where min_ANNmuon &gt; 0.4. Maybe this is why?</p>
3517, <p><a href="https://www.kaggle.com/c/flavours-of-physics/forums/t/15736/classification-with-a-control-channel-don-t-cheat-yourself">https://www.kaggle.com/c/flavours-of-physics/forums/t/15736/classification-with-a-control-channel-don-t-cheat-yourself</a></p>
3517, <p>[quote=Gilles Louppe;88316]</p>  <p>Note: I took the decision alone -- this was not decided nor even suggested by the organizers. See my post above for details :)</p>  <p>[/quote]</p>  <p>Yes but  can you still win the Kaggle competition and get Ranking Points? Because if so then we all might as well play the system</p>
3517, <p>So you're not really &quot;out of competition&quot; you just can't claim a prize. Big difference</p>
3517, <p>As Gilles hasn't been removed from the leaderboard I take it that he is still eligible for the Kaggle ranking points?</p>
3517, <p>Nope you'll get the points</p>
3517, <p>[quote=Abhishek;88328]</p>  <p>[quote=Bluefool;88326]</p>  <p>As Gilles hasn't been removed from the leaderboard I take it that he is still eligible for the Kaggle ranking points?</p>  <p>[/quote]</p>  <p>There will be people in the end using the agreement test dataset just to gain Kaggle points. How do you plan to remove them? ;)</p>  <p>[/quote]</p>  <p>That's my point - we might as well all just use the agreement test.</p>
3517, <p>It's all a bit confusing. I had some feature selection running with the checks built in but I stopped it. Now I'm not sure. At the moment I'm only using checks before I submit but even then I'll be selecting my models based on if they pass or not!</p>
3517, <p>How do you know it's sarcasm? is it done on points?</p>
3517, <p>ah ok - saw the answer in another script</p>
3517, <p>The timeline dates doesn't match the timeline graphic with the little blue dot</p>
3517, <p>Sounds like a case of bad timing so I wouldn't worry about it. I'm just rooting for NxGTR to get his Masters!</p>
3517, <p>I'm having problems processing the text files with the process_html.py file. My Mac keeps crashing with a segmentation fault 11. There's loads of stuff on StackOverflow from about 2013 about Mac Mavericks and Python Readline. I installed the patches but I still get the fault - I think possibly because I use Anaconda  the files that need to be updated are in a different place. I've tried with 2.7 and 3.3 and both bomb out with a fault. Anyone have a solution please?</p>
3517, <p>I tried that. I'm OSX 10.9.5 conda 3.17 py27-0 with py3k environment too I think it must be Mavericks. I think I'll have to update my OS</p>
3517, <p>thanks. I updated Beautifulsoup and it still bombed out with a fault. I know what is happening - some of the files don't have proper end_of_file. If I go in to the file that crashed it and add a new line to bottom then it doesn't crash next time (I tried a bash script to do this but that bombed out as well). I think I'll have to rewrite the preprocessing file. Thanks for your help</p>
3517, <p>really? Are you Mavericks?</p>
3517, <p>Not sure where the seg is. I think it's a Mavericks / readline fault (gathered from Stackoverflow) I might update to Yosemite</p>
3517, <p>yes I keep dreaming of a 64GB mac</p>
3517, <p>No wait and save up for a 32</p>
3517, <p>Not sure - I think it's because I prefer reading black on white. I didn't think about it consciously. Also so I had a local copy (my computer was chugging away on something else). I'll change it</p>
3517, <p>and I don't like all the package stuff printed at top. Looks messy.</p>  <p>EDIT: but just got rid of it as didn't need bit64 any more as changed from fread</p>
3517, <p>Actually sorry to be a forgetful minnie but I've just remembered that I don't think the log shows all of it - I think it gets truncated because of the number of columns</p>
3517, <p>I think it's because of the word &quot;Vomit&quot; - bit disrespectful to the sponsors. (I didn't down vote)</p>
3517, <p>Check that you haven't mismatched the IDs with predictions. If you've done a MERGE the row order might change</p>
3517, <p>[quote=Alexander  Larko;92483]</p>  <p>And I was stuck! </p>  <p>Who would help me ... :))))</p>  <p>[/quote]</p>  <p>I'd help if I had anything to offer. Not having much luck with this one</p>
3517, <p>[quote=quant_actuary;92568]</p>  <p>You're going to get flamed and lectured by the tolatarian establishment. They don't want you to learn anything that they know. They will have no edge otherwise.</p>  <p>[/quote]</p>  <p>You're quite articulate. Have you ever thought of being a politician / philosopher or alike?</p>  <p>EDIT: Though I think you mean &quot;totalitarian&quot;</p>
3517, <p>I know but I was impressed with his antiestablishmentarianism commentary - there's a useful role for this troll somewhere.</p>
3517, <p>ok well earn some money then give up and go into philosophy / law. I love data but you love political philosophy. Hope it pans out for you</p>  <p>EDIT: and this has all started from (if I remember rightly) someone being a bit off with you. You shouldn't react so rebelliously as patience would have rewarded you with more helpful people</p>
3517, <p>I understand but honestly the bad would have been cancelled out if you had sat on it a while :D</p>  <p>EDIT: that's to Quant</p>
3517, <p>Leave it Dean - just let it go and let it all calm down</p>
3517, <p>oi! you too! </p>
3517, <p>Thanks that's kind. This has been a &quot;reactionary&quot; event - I don't think Quant started off with bad intentions </p>
3517, <p>[quote=OttoP;93300]</p>  <p>but whether or not the results can be reproduced exactly is that a requirement?</p>  <p>[/quote]</p>  <p>Yes - need to be reproduced or at least reproduced to above the LB position below you. That's why setting seeds is essential</p>
3517, <p>I've never been able to install xgboost on a mac with anaconda! Other people have - search the forums as there's detailed instructions </p>
3517, <p>ok will give a go  thanks</p>
3517, <p>I love this!</p>
3517, <p>A lot of the Memo fields are in Dutch (assumption) - are there any automatic translation tools available through scripts? The Biology field is quite interesting - my favourite being &quot;saw 2 penguins!!!!!!!!&quot; They liked their exclamation marks too back then!</p>
3517, <p>Also the romantic Captain &quot; The sea in the night sparkles as if we sailed thro fire &quot;</p>
3517, <p>I hadn't realised that the Netherlands had such a naval history. According to Wikipedia in the 17th century you had the most powerful navy in the world. I was intrigued by my War and Fights script as there were a lot of dutch entries (though I didn't have a look at the percentage ratio). I expected Britain to be the warmongers at sea!</p>
3517, <p>I will have to delve deeper into the Braave. Just found this </p>  <p><a href="https://books.google.co.uk/books?id=LUc1AQAAMAAJ&pg=PA515&lpg=PA515&dq=captain+Braave+Dutch&source=bl&ots=R27HqtNXGi&sig=nFcaLq3b9FnDM42GXPG_rEykq6I&hl=en&sa=X&ved=0CB4Q6AEwAGoVChMIw-eq2b_UxwIVx2sUCh3rGwBL#v=onepage&q=captain Braave Dutch&f=false">https://books.google.co.uk/books?id=LUc1AQAAMAAJ&amp;pg=PA515&amp;lpg=PA515&amp;dq=captain+Braave+Dutch&amp;source=bl&amp;ots=R27HqtNXGi&amp;sig=nFcaLq3b9FnDM42GXPG_rEykq6I&amp;hl=en&amp;sa=X&amp;ved=0CB4Q6AEwAGoVChMIw-eq2b_UxwIVx2sUCh3rGwBL#v=onepage&amp;q=captain%20Braave%20Dutch&amp;f=false</a></p>  <p>BRAAVE 40 Guns Captain James Gifford. Taken from the Dutch in Saldnaha Bay in 1796 At the Cape of Good Hope</p>  <p>I wonder if it's the BRAAVE you mention above</p>
3517, <p>[quote=Triskelion;91124]</p>  <p>Currently looking at a captain who claims to have seen a sea-monster of indeterminable size.</p>  <p>[/quote]</p>  <p>Loch Ness Monster on vacation!</p>
3517, <p>[quote=KA;91439]</p>  <p>Is the whale in w_8026.jpg pooping?</p>  <p>[/quote]</p>  <p>hahahahahhahaha</p>
3517, <p>[quote=Christin B. Khan;91519]</p>  <p>&quot;Is the whale in w_8026.jpg pooping?&quot;</p>  <p>@KA - yes! nice find! We marine biologists refer to it as defecation</p>  <p>[/quote]</p>  <p>even a bigger hahahahahahahah</p>
3517, <p>Could it be normalised by number of years a member?</p>
3517, <p>That's better - I think I'm out the Top 100 now! I've been a member for ages so the stats were a bit warped for example I've entered 45 competitions over 5 years others have entered 45 competition in a couple of years</p>
3517, <p>Yes - I also thought that weren't so many competitions in the &quot;old&quot; days. Probably hard to remove all bias</p>
3517, <p>that's great</p>
3517, <p>Click on the box next to their name</p>
3517, <p>This is the box I mean:  Click on the box and it will take you to the script. (3 with boxes 2 without)</p>
3517, <p>Hi I'm kind of enjoying the Script section. However isn't it just going to end up another &quot;Beat The Benchmark&quot; section with the same people who get the forum votes who will now get all the script votes? All the &quot;Beat Benchmarks&quot; scripts so far have the most votes in all the sections. I think all the visualisation scripts may get &quot;lost&quot; between all the &quot;Benchmakers&quot;</p>
3517, <p>Actually I think all &quot;Beat the Benchmarks&quot; should be banned from the Scripts section. It's bad enough having to deal with it anyway - now it's shoved in your face in 2 places</p>
3517, <p>You might as well rename the Script Section to &quot;Beat The Benchmark&quot; section</p>
3517, <p>I'm a bit ashamed asking this but I'm behind on all the GitHub thing. When I press FORK what does it actually do? It doesn't seem to do anything - I thought it downloaded it?</p>
3517, <p>Ah ok. Thanks. I might have &quot;forked&quot; a couple of things without making changes. Sorry!</p>
3517, <p>I like the new Scripts section. Feedback:</p> <p>I get confused on all the differing forks that may have same title. Also it's sometimes not overly clear what changes they made - maybe highlight the original (they might have forked a forked fork and then it really gets confusing)</p> <p>It would also be good for a &quot;new&quot; script star or something. I look to see if there's new scripts think there is but really it was a fork or a run</p> <p>I have been selecting and copying the scripts into a text file and sometimes the indentation goes wrong. I'm not sure if this is me being silly but I couldn't find a handy way to download the script</p> <p>Maybe get people to prefix the title with Script Type. Like VISUALISATION - blah BENCHMARK -blah UTILITY - blah</p>
3517, <p>More feedback:</p> <p>I've put a Utility script on Crowd. It just dumps the train and test SVD components to file so you can mix and match in other languages. 2 text files are produced - one for train and one for test. The Script Function only outputs one file and it gives you the option to submit it (which will disappoint some people if they press that button!)</p>
3517, <p>No we don't! The only Scripts that &quot;deserve&quot; down-votes are late forked benchmarks (and even then most votes will still be up-votes). All the rest automatically deserve a up-vote!  EDIT: It's hard enough to get an up-vote on a non-benchmark script anyway. It's like CTR rate - 1 vote per 100 views!</p>
3517, <p>Feedback - when someone comments on a script post and you press on link from forum page it takes you to the script output rather than the Discussion</p>
3517, <p>Feedback: When you go to &quot;Scripts&quot; and get the sortable list would it be possible to indicate whether you've voted or not. I like to vote for Scripts but I can never remember which ones I've already voted for so I have to go in to each script again</p>
3517, <p>Thanks. I would be happy with just an indicator but sortable would be good too. </p>
3517, <p>Feedback: I think I've seen it written before but is there a way we can delete scripts? I've just done a bad one and don't want it cluttering up my Script section.</p>
3517, <p>ok - thanks. It still appears but with a cross through it though - I've unhidden it again for the time being because of my accompanying forum post. For the future maybe we can have Folders to organise our scripts?</p>
3517, <p>ah ok - I wasn't bothered about others seeing it - I didn't want to see it. Devin has fixed it now so I've forked his and will now Hide mine</p>
3517, <p>Feedback: The script title only gets saved after a run. If you want to go back and update title with LB score you have to run the whole thing again. </p>
3517, <p>Error: my script stuck on the last bit (writing the file I assume) for 31 minutes - the script continued to run. I cancelled in the end. Not sure if it's a disk space thing?</p>
3517, <p>It was my CERN RF+xgb+keras+flatline script. It printed out &quot;Making predictions on test set&quot; at about 9.40minutes then just never stopped. I went for a mini dog walk and when I got back it was still running and said 31 minutes!! I waited a bit longer but then just cancelled it  </p>  <p>EDIT: It has worked since. It was Version 1 that never ended</p>
3517, <p>Will do. Because of the timing (at the end of script) I concluded that it was a write error (as I thought maybe the script doesn't stop if in the middle of writing a file?)</p>
3517, <p>R GBM: I just ran into the Namespace error when updating the gbm library from the devtools/github. I thought I'd tell you in case this is your problem. I can't even load the original version now. I've removed it and installed it again (on my Mac) but I still get the Namespace error. </p>  <p>I fixed it by removing the gbm directory from the harddrive then reinstalled. </p>
3517, <p>I meant at my home</p>
3517, <p>[quote=Ben Hamner;87637]</p>  <p>[quote=Bluefool;87635]</p>  <p>It was my CERN RF+xgb+keras+flatline script. It printed out &quot;Making predictions on test set&quot; at about 9.40minutes then just never stopped. I went for a mini dog walk and when I got back it was still running and said 31 minutes!! I waited a bit longer but then just cancelled it  </p>  <p>EDIT: It has worked since. It was Version 1 that never ended</p>  <p>[/quote] Glad to hear that it's worked sense. Must've hit an edge case on our end - let us know if it repeats</p>  <p>[/quote]</p>  <p>Hi this has just happened again. Same situation where I believe the 20 minute limit happens whilst writing the output file. I stopped the script at 22 minutes and am running again</p>
3517, <p>If you are on the Script listing (ie press on Scripts from the main menu bar) then select a script. If you then press Back it goes to the previous page before you pressed the Scripts button - it doesn't return you to the Script listing</p>  <p>EDIT: but if you take action on the script eg Give Trophy then by pressing Back you can get back to the script listings</p>
3517, <p><a href="https://www.kaggle.com/domcastro/springleaf-marketing-response/xgb-3-trainpreds/edit">https://www.kaggle.com/domcastro/springleaf-marketing-response/xgb-3-trainpreds/edit</a></p>  <p>Hi this is another &quot;never-ending&quot; script (unless you've increased script time?) - it's been running for 21.35</p>  <p>I've left it running in case you would like to investigate</p>
3517, <p>Has something changed with version editing? I used to be able to edit previous versions of the script but now it seems to take you to the latest version with no drop-down version box</p>
3517, <p>I use Firefox on a Mac and my recent activity looks like this: </p>
3517, <p>Hi Now the ranking algorithm has changed in favour of teams would it be good to implement maximum team member limits?</p>
3517, <p>You have to remember to unclick it before posting. When I'm  the OP I edit message afters and turn it off too</p>
3517, <p>They've submitted the same script. Click on the box by their name</p>
3517, <p>yes but they download the file first then submit it. So they get same score but no little box</p>
3517, <p>Not sure I understand but you have to keep your eye on the LB if you want to be a successful Click-and-submitter. </p>
3517, <p>Ha! I averaged a .30 with .12 and got a .14 . Which surprised me but confirmed what I had been thinking</p>  <p>EDIT: I was also surprised that last years sales (or the year before if no 2014 data) only score 0.51 on the LB. I might have done something wrong but didn't bother to check</p>
3517, <p>The Closed stores are ignored in the scoring process <a href="https://www.kaggle.com/c/rossmann-store-sales/details/evaluation">https://www.kaggle.com/c/rossmann-store-sales/details/evaluation</a></p>  <p>&quot;Any day and store with 0 sales is ignored in scoring.&quot;</p>
3517, <p>So basically someone in the Top 4 is cheating? (or going to get someone with a huge uplift on last day) They've probably protected their own account so won't mind all the others getting removed. </p>
3517, <p>Congrats everyone congrats Gert. nicely played</p>
3517, <p>[quote=BreakfastPirate;101270]</p>  <p>Congrats gert! Much deserved!</p>  <p>[/quote] Looking forward to your LB shake-up score</p>
3517, <p>I thought the shake up would have been worse than that. Must have been bad / good for a minority and ok for most</p>
3517, <p>It was lovely to see the group of well-deserved new Masters in this one. Very pleased for Inversion and Data Geek! Well done</p>
3517, <p>[quote=BreakfastPirate;107999]</p>  <p>Since this is a life insurance competition &#8230; some thoughts on life death and Kaggle: When we die is the Private Leaderboard of Life revealed to us? When the Private Leaderboard of Life is revealed will there be a big shake-up? Am I overfit to the Public Leaderboard of Life?  (My apologies in advance for making a philosophical post in a data science forum but LB shake-ups on Kaggle cause me to reflect on these things.) </p>  <p>[/quote]</p>  <p>Yes the Private Leaderboard is revealed to us when we die. We will then realise all our mistakes and how rubbish we were at living (ie should we have been socialising instead of worrying about the Private Leaderboard) and then we will get reincarnated and we start the Kaggle cycle again of Novice to Master. And then we fail life again and become a Novice again. Yeh man it's Monday!</p>
3517, <p>love it Owlie</p>
3517, <p>yes it's pain. You need to convert  ISO-8859-1 to UTF8. I used command line iconv</p>
3517, <p>This fixed the problem in Python</p>  <p>import pandas as pd</p>  <p>train = pd.read_csv('data/train.csv' sep=&quot;&quot; encoding=&quot;ISO-8859-1&quot;)</p>  <p>test = pd.read_csv('data/test.csv' sep=&quot;&quot; encoding=&quot;ISO-8859-1&quot;)</p>  <p>train.to_csv('data/train0.csv' index=False encoding='utf-8')</p>  <p>test.to_csv('data/test0.csv' index=False encoding='utf-8')</p>
3517, <p>Can't help anymore sorry! I managed to get things running after the above conversion (and I also used the iconv command before taking into R and this worked too)</p>
3517, <p>It's because of the unknown searches. Make sure you train with whole search terms in the validation set - ie. don't split your train and validation randomly</p>
3517, <p>From my understanding there's search terms in the test set that aren't in the train set. If you randomly split the train set the chance is you'll proportion the search terms and get a slightly inflated score.</p>
3517, <p>Gender segregation is patronising - being called Women Power and then having the number 1 kaggler who is male helping out makes you look silly. </p>
3517, <p>Data transcends gender</p>
3517, <p>says a man who helped out the little feeble women because he thought they were incapable of doing it on their own</p>
3517, <p>There was no need to call the team &quot;Women Power&quot;</p>
3517, <p>Well as a woman I will disagree with that</p>
3517, <p>[quote=Abhishek;116841]</p>  <p>Bluefool is back! \m/</p>  <p>[/quote]</p>  <p>hahahah bring it on mo-fos!! Marios (rank 1) needs to question why he went out of his way to team up with women (rank &gt; 800)! Positive discrimination does no-one any good</p>
3517, <p>Well at least you got to be a Master from it heh?  edit: I deserve voting down for that</p>
3517, <p>I know what would make me feel better - team up with <a href="https://www.kaggle.com/carloshuertas">https://www.kaggle.com/carloshuertas</a> </p>
3517, <p>It's always a risk. I always have a weird feeling waiting between finishing and the removal. Luckily everything's been fine for me. Has Sky's account gone? Were they in winning team?</p>
3517, <p>Mail William</p>
3517, <p>You need to sort out the Sleep State! I took mine off the Mac. I've got to my Kaggle position with a 4 core 16gb mac so there's no need to go mad - just sometimes have to be patient!</p>
3517, <p>What machine are you on?</p>
3517, <p>is there no Power Management where you can set Sleep to Never? or have I over-simplified the problem?</p>
3517, <p>No to overfitting - my 5 CV models are pretty much spot on. Unfortunately. As they're both in the 0.45s and not .43s lol</p>
3517, <p>[quote=Net Prophet;109961]</p>  <p>If anyone is making use of the coaches data I'd be interested in hearing how.  I feel like there should be some way to estimate the impact of a coach but it's difficult for me to see how.</p>  <p>The &quot;intuitive&quot; approach is to try to identify the impact particular coaches have upon the game outcomes.  But how would one do this?  It's very difficult (maybe impossible) to look for multi-season effects because so much other than the coach changes from season to season.  How do you distinguish the impact of the coach from the (presumably much bigger) impact of changing team composition?  On the other hand if you're just looking within a season then for teams that don't change coaches it's really immaterial who the coach is (since you're learning within that season what impact the coach has).</p>  <p>[/quote]</p>  <p>I would use:</p>  <ol> <li>Have they won tournament before?</li> <li>How long have they been coaching team?</li> <li>Was the last team they coached higher or lower seed than their current team?</li> <li>How long have they been coaching?</li> <li>How many heart attacks have they had?</li> </ol>
3517, <p>Hi Net Prophet. Looking forward to your blogs etc and the bracket competition (is it running?)</p>  <p>Hi Siddharth too. Hope it's as exciting as last year</p>
3517, <p>I thought this was a good story so thought I'd share / show-off I entered this competition last year. My ensemble with Net Prophet came 8th. As a side line to the Kaggle Competition NetProphet ran a Bracket Challenge - the bracket had to be 100% computer predictions and complete before the start of the tournament.</p>  <p>Roll on a year. ESPN 538 were interested in running a story on computer-predicted brackets. NetProphet put them in contact me. The shock of me being a British woman who had never watched a basketball match in her life made them think that they could make more of story of it. So lol ESPN are sending a film crew to the UK and they will be making a mini-documentary about brackets.</p>  <p>15 minutes of possible fame (If ESPN show it!) Thanks William Jeff and Scott (NetProphet) I'll let you know how it goes</p>
3517, <p>[quote=William Cukierski;110310]</p>  <p>Awesome! Make sure to have <a href="http://hackertyper.net/">http://hackertyper.net/</a> ready in full screen mode to make us all look good.</p>  <p>[/quote] hahahah That might be a good idea - they wouldn't know! lol</p>
3517, <p>[quote=Net Prophet;110309]</p>  <p>When they contacted me I considered pretending to be a British woman with no interest in basketball but then I thought &quot;Who would believe that!?&quot;</p>  <p>Seriously that's cool and I look forward to seeing it!</p>  <p>[/quote]</p>  <p>All thanks to you :)</p>
3517, <p>Well I hope it gets aired because that was a very long day! </p>  <p>They sent over Henry Roosevelt <a href="http://www.imdb.com/name/nm3943473/">http://www.imdb.com/name/nm3943473/</a> as the Director. I mentioned Kaggle but even though I said loads most of it will be edited out. lol. I don't think I did Data Scientists any favours because I said never trust your heart or gut when it comes to brackets - trust the data. They then said a 12 year old won the ESPN bracket competition last year and when he was asked how he won he said &quot;I just used my gut&quot; lol lol Shown up by a 12 year old.</p>  <p>They were mad busy because of the short notice so I hope it gets done. They flew to UK this morning flying to Chicago to see the 12 year old tomorrow then on to Vegas to see Vegas Dave (lol ?) They hope to get it done by Sunday. They said they'll let me know. I also didn't realise how hard it is to type randomly at a keyboard. I wrote about 250 lines of crap R.</p>  <p>ps. If it does get aired - yes I know when asked to explain Logistic regression in layman terms I described linear regression - saying &quot;draw a straight line through the wins and losses&quot; was so much easier - believe me!</p>  <p>I attach Kazanova's picture of me in dedication of the event</p>
3517, <p>[quote=AdamWolfeLevin;110416]</p>  <p>But why?</p>  <p>[/quote]</p>  <p>Because it's fun. Thanks NetProphet</p>
3517, <p>Stolen. Done the same as last year. Mine is from a straight non-optimised GLM (bracket will go to bracket challenge) and I've done a straight average with yours for second submission. I will label my team name according to the winning one</p>
3517, <p>Netprophet - I've entered our ensemble bracket into a couple of competitions. I will let you know how it does. Also if we do do well I'm ineligible - I've had to say I live in an office in Boston! </p>
3517, <p>There was some code Jason wrote last year that converted Kaggle submission to a Bracket. Might need changing</p>
3517, <p>thanks Jason</p>
3517, <p>NetProphet - is there time to submit bracket after play-ins on Wednesday? I've got confused on Time zones - I've picked a play-in team to go far but obviously they might not actually end up in tournament!</p>
3517, <p>[quote=Net Prophet;111651]</p>  <p>[quote=Bluefool;111646] NetProphet - is there time to submit bracket after play-ins on Wednesday? I've got confused on Time zones - I've picked a play-in team to go far but obviously they might not actually end up in tournament! [/quote]</p>  <p>Oh so now the big TV star needs help from one of the little people?  :-)</p>  <p>According to the site picks lock on Thursday at 12:00 PM ET.  So yes you can enter after the play-in games.</p>  <p>[/quote]</p>  <p>You're so mean! ;) Thanks for info</p>
3517, <p>The bracket pool is asking me for a password? EDIT: OK actually read the instructions</p>
3517, <p>I printed it fine from Jason's code</p>
3517, <p>oh no I didn't check</p>
3517, <p>Are you sure it's not the North Carolina vs UNC Wellington thing?</p>
3517, <p>I just checked and it seems fine. I attach Netprophet's bracket run through Jason's code.</p>
3517, <p>He's done one for this year too <a href="https://www.kaggle.com/c/march-machine-learning-mania-2016/forums/t/19403/7th-annual-machine-madness-competition">https://www.kaggle.com/c/march-machine-learning-mania-2016/forums/t/19403/7th-annual-machine-madness-competition</a></p>
3517, <p>Selection Sunday yeh! When do we have to submit by? </p>
3517, <p>Jason Sumpter posted some R code here</p>  <p><a href="https://www.kaggle.com/c/march-machine-learning-mania-2016/forums/t/19403/7th-annual-machine-madness-competition">https://www.kaggle.com/c/march-machine-learning-mania-2016/forums/t/19403/7th-annual-machine-madness-competition</a></p>
3517, <p>omg - thanks - going to watch!</p>
3517, <p>I'm quite pleased with it! &quot;Trust the data Trust the numbers&quot; lol</p>
3517, <p>I didn't do any calibration - output from model is my predictions</p>
3517, <p>North Carolina</p>
3517, <p>Obama on UK ESPN picking his bracket</p>
3517, <p>Obama has gone for KANSAS</p>
3517, <p>For my bracket submission I actually have a North Caroline Vs Duke final My code always loves Duke</p>
3517, <p>I have huge risk this year - I've put a play-in team to win the next couple of rounds! (Michigan)</p>
3517, <p>I am holding out for a North Carolina win. It would be good if I predicted the winner 2 years in a row - as some kind of recompense for being rubbish! </p>
3517, <p>[quote=Siddharth Chandrakant;113150]</p>  <p>Let the carnage continue! C'Mon Syracuse!!! :D</p>  <p>[/quote]</p>  <p>hahaa yes Great carnage. </p>
3517, <p>Good luck Charles</p>
3517, <p>[quote=PSThorbell;112015]</p>  <p>The thing I really wanted to talk about though was the method of ensembling. The impression I get from most of the people I have seen discuss this is &quot;simple average' i.e. just an arithmetic mean. This seems close to the worst way of ensembling I can imagine.  [/quote]</p>  <p>I disagree. I find a simple average is the best in these circumstances. Apart from when I ensemble by weighting models I always chose a simple average ensemble. I used to do smarter things but they're not as good really!</p>
3517, <p>Last year (I think) I could see what submission was doing the best - I can't see this year. Can I extract this from Scripts? Do the file numbering coincide with submission entry?</p>
3517, <p>Argh I had Purdue getting to Elite 8 in bracket</p>
3517, <p>Welcome to the Madness. It's 1.47am in the UK on a school night and I'm in bed with the telly on and my laptop!</p>
3517, <p>Loved it. So exciting. They shocked last year and shocked this year! Maybe not big shakeup because most people went for them</p>
3517, <p>Anyone watched the iowa / Temple - won at 0.0. incredible</p>
3517, <p>Amazing</p>
3517, <p>I'm going down. I could cry. </p>
3517, <p>My model also had too much faith in Purdue. It also has a lot of faith in Duke. And had no faith in Virginia Wisconsin Notre Dame </p>
3517, <p>Thanks Jeff. I'm still aiming for a perfect bracket by 2020</p>
3517, <p>How exciting - that has made my evening</p>
3517, <p>hahhaha Kazzie BUSTED</p>
3517, <p>Have you changed the random seed and tried again?</p>
3517, <p>I have to go to bed when the snooker finishes. Even though I think it will be a late one it would be unfair of you to make submissions when I am in bed and before the competition end. Thanks in advance</p>
3517, <p>Selby might choke. It's weird because me and other English people want Ding to win. We feel guilty but Ding is respected and loved over here and we think he deserves to be world champ! Go Ding</p>
3517, <p>I would love that</p>
3517, <p>ha! yes - Spurs mucked up a 2-0 lead. Well done Leicester</p>
3517, <p>Morning! What did I miss? lol</p>
3517, <p>0.841705</p>
3517, <p>I only really have a 0.839 but last time I ignored the so-called &quot;overfitting script&quot; I got beaten by about 200 people who had used it lol. Gone for a straight 50/50 average with one of the public scripts</p>
3517, <p>hhahaha sorry - I'm a bit rubbish - you need to be more direct and rude and then I might understand :)</p>
3517, <p>Heh - I have the same number of votes as your number of posts</p>
3517, <p>Thanks. How do I know my file name? I saw it written somewhere?</p>
3517, <p>Boring middle-of-the-road predictions</p>
3517, <p>@eagle4  They anonymised it for a reason. There's no point anonymising it if people are just going to de-anonymise it. It's a clear breach of the rules. And is it really a polite thing to do to down-vote the competition organiser who is just reminding people of the rules?</p>
3517, <p>lol true!</p>
3517, <p>Awesome</p>
3517, <p>[quote=inversion;120536]</p>  <p>[quote=FernandoProcy;120523]</p>  <p>That seems a reasonable way to spend my 20's</p>  <p>[/quote]</p>  <p>When I was in my 20s all we <em>had</em> was hand labeling. Up hill. Both ways.</p>  <p>[/quote]</p>  <p>When I was in my 20s I was drunk</p>
3517, <p>[quote=NxGTR;119040]</p>  <p>Hi! I am looking for someone to team up in this competition right now I am just overfitting the LB as in the Santander competition but hopefully someone can help me avoid that.</p>  <p>I do have some requirements:</p>  <p>1) You must be a Kaggle Master</p>  <p>2) You must be ranked above me</p>  <p>3) You must be a Kaggle Early Adopter</p>  <p>4) You should have at least 1 Prize Winner achievement</p>  <p>5) You must have at least 35 (Top 10% + Top 25% combined) achievements</p>  <p>6) You should have at least 1000 forum upvotes</p>  <p>Yes I am the same guy that dropped over 1000 places in Santander yet I am very picky :D</p>  <p>So if you are out there and comply with these requirements I would love to team up! let me know <a href="https://www.kaggle.com/domcastro">Bluefool</a>!</p>  <p>[/quote]</p>  <p>LOL I think I'm the only one that fits that!! Let me look at data and I'll get back to you!</p>
3517, <p>Still downloading. You had to pick the biggest data one! lol</p>
3517, <p>My gpu is like this: </p>  <p><a href="https://www.kaggle.com/blobs/download/forum-message-attachment-files/4237/robin_reliant.jpg">https://www.kaggle.com/blobs/download/forum-message-attachment-files/4237/robin_reliant.jpg</a></p>  <p>Lol I am so good at images that I've even uploaded this image wrong</p>
3517, <p>Are you causing trouble already? The Curse of NxGTR ........</p>
3517, <p>We're coming to get ya!</p>
3517, <p>We are awesome! The deed is done!</p>  <p>So grateful to Inversion David Shinn Oleksii and Datageek Thanks guys</p>
3517, <p>[quote=the1owl;127020]</p>  <p>I don't know about that curse NxGTR the day you make it to Masters a new Grandmasters tier is added :) Congratulations!</p>  <p>[/quote]</p>  <p>hahahha that's EXACTLY what I thought</p>
3517, <p>Poor NxGTR</p>
3517, <p>Owlie - congrats on being number 1 for kernels. Well deserved</p>
3517, <p>thanks that's great</p>
3517, <p>Data dictionary here <a href="https://www.kaggle.com/c/avito-duplicate-ads-detection/data">https://www.kaggle.com/c/avito-duplicate-ads-detection/data</a></p>
3517, <p>Good catch.  With Data Science - the philosophy is &quot;Try it and see&quot;</p>  <p>EDIT: I think they may already be in there?</p>
3517, <p>The image numbers are in the &quot;images_array&quot; column in the info files. You seem to be mixing the itemid with the imageid. </p>
3517, <p>Congrats everyone! Especially to the new Masters including NxGTR (has the Curse been lifted?) and Oleksii Renov.</p>
3521, <p>Suppose that there is no wind (or wind resistance) and neglect all the other airplanes and assume constant speed and altitude. What is the optimum path for an airplane to get to its destination? Drawing a straight line from (easting northing) of origin to (easting northing) of destination doesn't do the trick(I am guessing due to the radius of earth). Does anyone have good tips for this problem perhaps maybe some reference?</p> <p>Thanks</p>
3521, <p>Thanks for this thread. I have&nbsp;3 questions:</p> <ul> <li>Is Flight Quest 2 over or is the Milestone 2 over? &quot;This competition has completed. You can still submit an entry to see what your score would have been.&quot;</li> <li>If it is not when will the submissions be enabled back?</li> <li>We submitted many submissions and our score doesn't reflect our best submission. Shouldn't the system choose the best score?</li> </ul> <p>&nbsp;</p>
3521, <p>Hello everyone</p> <p>We just joined the quest and our first bump is getting the simulator to work(windows):</p> <p>1. The Version 4 of the simulator was looking for many actualLandings actualTakeoffs and groundConditions csv files for different dates ... Those files<br>don't even exist in any of the folders provided in the Data download section.</p> <p>2. The simulator is asking for a route file name from C:/Temporary/FlightQuest. We couldn't find this file.<br>By looking at the forum Q&amp;A this should be an easy one for the gurus who already submitted their initial quests</p> <p>We have attached a screenshot for the problem.<img alt></p>
3521, <p>Thank you for the response glazed. Our question is not about the location of the &quot;basepath&quot; though. We don't know where to get the following files:&nbsp;actualLandings actualTakeoffs and groundConditions csv files for different dates. Can you point us where to download these files or if we have to create them (and if yes what is the process). Anyways thanks for the help.&nbsp;</p>
3524, <p>I'm trying to use this exercise to learn using some of the things on Deep Learning and wanted to start put with a Multi-Layer Perceptron.</p> <p>The tutorial is rather good&nbsp;<a href="http://deeplearning.net/tutorial/mlp.html">http://deeplearning.net/tutorial/mlp.html</a>&nbsp;and I pickled my data to match the code example (took 900 examples as train data and 100 as both validation and testing). But since  there's no printing logging or monitoring to speak of I'm lost debugging it. It loads the data alright and does something but the validation error stays almost constant around 80%.</p> <p>&nbsp;</p> <p>Any pointers on how I would go about debugging this especially with the theano functions?</p>
3524, <p>Thanks for tips that's good starting points - especially since to me it looked like it was basically not doing anything.</p> <p>Hmm I took a look at some of the activation scripts for pylearn2 they seemed quite complicated that plus the rebel in me lead me to go to try something else. But maybe I should reevaluate that :)</p>
3524, <p>I haven't gotten time to do it and I don't know if I will so here's my idea. I was thinking about constructing a network with NetworkX and add additional network characteristics like measures of centrality. Anyone tried that?</p>
3524, <p>Thanks for sharing this I got kind of stuck at .866 since I'm rather new to mining texts. And unlike domcastro et al. who get jealous I appreciate that this is seen more as a collective learning experience. Squeezing the last couple of percent points out has nothing to do with real world applications anyway so collectively learning to get to a good (enough) point is worth way more.</p>
3524, <p>[quote=Zhao Yilong;30400]<span style="line-height: 1.4">While many people mainly seek for a learning experience here they are others who take this more seriously. [/quote]</span></p> <p><span style="line-height: 1.4">I think that learning is a very serious experience...</span></p> <p><span style="line-height: 1.4">&nbsp;[quote=Zhao Yilong;30400]he dramatic change in the leaderboard some people do have their efforts undermined in terms of rank and future motivation.[/quote]</span></p> <p><span style="line-height: 1.4">And some are motivated to keep going some more after getting stuck. Sorry but: Duh.</span></p> <p>&nbsp;</p> <p>&nbsp;</p>
3524, <p>From reading and comparing the various methods it seems like the averaging of multiple models was the key to stabilizing the final score between public and private leaderboard. Not quite sure why that is?</p>
3524, <p>I fell from 33 to 295 and I am still one place ahead of Jared Huling who was leading the whole contest. Thankfully I learned a lot so it's not a complete waste but it still feels like a bad joke. Was hoping for my first Top 10% finish but this is just like playing the lottery (at least there you know your odds)...disappointing.</p>
3524, <p>I started from the benchmark code and improved it in both CV and public leaderboard by a good margin so if the final leaderboard switches that around that's not good. I get and appreciate all the points about how how to safeguard against it but drops this big also indicate that the test set split was at least partially at fault. Ah well maybe next time.</p> <p>At least the things I came up with that I was sure would improve the score and didn't before would have given a better final score - nothing good enough though.</p>
3524, <p>[quote=Domcastro;33056]</p> <p>Abhishek - many people love you for the benchmark. You are being oversensitive and this benchmark thing was started on another thread about the Cause and Effect competition and the Belkin one. xxxxxxxxxxxxxxxxxxxxxxxxx</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Please stop telling people how they are supposed to feel. Like at all. Don't do it. Just don't. It makes you a jerk plain and simple.</p>
3524, <p>sklearn has tools to handle build multi-class and/or multi-label classifiers from other classifiers take a look at:</p> <p>http://scikit-learn.org/stable/modules/multiclass.html</p>
3524, <p>I tried all of these suggestions to directly download into an EC2 instance and nothing worked. What finally worked for me is actually very easy: Install the lynx text-based web browser Kaggle works nicely in it!</p>
3524, <p>not Hanover...</p>
3524, <p>Wow I really didn't know this. Didn't even think Hannover had an English name...</p>
3526, ----
3532, ----
3535, ----
3540, ----
3582, ----
3586, <p>Hi James</p> <p>I am new to Kaggle. I use SAS for analysis and modelling. I cannot email you from Kaggle because I dont have enough points but I would like to be part of your team.</p> <p>thanks</p> <p>Art</p>
3586, <p>You will end up with 2^k choices where k= the number of input variables in this case it will be 780 or more. &nbsp;</p>
3586, <p>After looking at the min and max values of the variables except the date I have several questions.</p> <p>1. the min for dept category brand and purchase size is 0. Does 0 in this case mean that the real value is unknown? Or it mean that there actually is a brand 0 dept 0 category 0 and productsize 0? 0 productsize for example could represent a service offered.</p> <p>2. are the extreme values of purchasequantity and purchaseamount valid?&nbsp;</p> <p>3. would it be possible for the competition admins to provide the min max values of the variables?</p> <p>4. did the competition admins add noise to anonymize the data?</p> <p>VAR &nbsp; MIN &nbsp; &nbsp;MAX</p> <p>id &nbsp;86246 4853598737</p> <p>chain 2 526</p> <p>dept 0 99&nbsp;</p> <p>category 0 9999&nbsp;</p> <p>company 10000 10999999999&nbsp;</p> <p>brand 0 108689&nbsp;</p> <p>productsize 0 6000&nbsp;</p> <p>purchasequantity -32255.00 54800.00&nbsp;</p> <p>purchaseamount -8593791.00 58658.76&nbsp;</p>
3586, <p>Triskelion</p> <p>Your thoughts of creating new features from the transaction data are absolutely right. One note though. Brand is associated with category and company So if a shopper buys a brand he/she buys the category and the company as well. So you can combine these three into one feature. To be more accurate I think category and company could be left out because the offers are made on brands not categories or companies. I think a new binary feature that indicates if a shopper has purchased the brand prior to the offer would be sufficient. In other words a shopper who has prior exposure to the brand is more likely to respond to the offer than the shopper who has not had the exposure.&nbsp;</p>
3586, <p>Abhishek</p>  <p>When you get a coupon it is for a specific brand. You cannot redeem that coupon to get another brand of the same company or category or department.</p> <p>There is also another issue here. What type of association we have between each shopper and coupons? Is it one to one? Were the coupons provided as a collection(coupon book newspaper insert etc) or they were provided in some other way?</p>
3586, <p>Hi Decipher</p>  <p>Your thinking process is absolutely correct. That is a correct approach to extract intelligence and create variables. I have created 114 variables so far.</p>
3599, <p>Hello all</p> <p>My first submission is constant prediction (it is not mean or median) for all records. RMSLE for my method is 0.486600 whereas the best score currently is 0.456575<br> <br> I find it a bit &quot;funny&quot; that there are over 100 team scoring below &quot;my non-public ranking 258&quot; (and I bet some of those are more complicated algorithms). This is because I find my method to be somewhat &quot;starting point&quot;/worst possible one that one can do (that  is after a bit thinking).</p> <p>My next move will be convert my algorithm from constant prediction to conditional prediction (that is to utilize given variables) however I think I will do discretized prediction. But not sure if I manage to do this by coming deadline.</p> <p>I have two questions in my mind:<br> - Is anyone using prediction discretization for daysinhospital value 0 (as it is pretty common). I mean that one would for example first predict whether daysinhospital is zero or non-zero (using logistic regression or mlp etc). And if it is zero then place  value 0 to it and otherwise place continuous predicted value to it.<br> - Is anyone using principal component analysis or independent component analysis or any projection methods? (I am thinking of Year1 Year2 Year3 &quot;time series&quot;)</p> <p>Finally happy predictions &amp; good luck in the challenge! :)</p>
3599, <p>Jeff</p> <p>Two things: <br> 1) Thanks - I did not notice that constant method before good to know about it. (it also verifies that my constant is also &quot;pretty good constant&quot; :)). Well actually I am not interested in this particular constant but more from the formula that calculated it.<br> 2) Isn't that optimized constant method a bit like &quot;cheating&quot;? It is using scores from leaderboard (for epsilon0 etc) which are not available prior evaluation.This is a bit like reducing the number of free parameters (if we are thinking that to be predicted  days in hospital values are free parameters). I know it is only using 2 or was it 3 scores from leaderboard but I still think it like &quot;reverse engineering&quot;. My constant method did not use any scores from leaderboard. Not to judge but I don't like such constant  method - prefer mine :)</p> <p>boooeee<br> Sounds interesting especially the random forest method.</p>
3599, <p>Is IdLookupTable.csv's business_id field supposed only to point business IDs in test data set? Namely I have a problem that it looks like that ~50% of it is pointing to training data's business_id's and 50% to test data's business_ids.<br>(I have not verified this using yelp_test_set_review.json I will do that tomorrow to see if business_id's in that file match with IdLookupTable's business_ids)</p>
3599, <p>Ok I found this just already written in the forums:<br>IdLookupTable.csv has the conversion from the old pairs of ids to the new ones.</p>
3599, <p>What prediction methods are you using?</p> <p>Currently I am using Multivariate Adaptive Regression Splines (MARS). In near future I plan to use Kriging k-nn kernel regression and SVM. And in the end will form an ensemble.</p> <p>&nbsp;</p>
3599, <p>Results for running very simple predictions (ignoring user information):<br>1. If test review businessId found from training data then use training data stars for the business<br>2. Else predict using test data predictors see below for details <br><br>* Multivariate Adaptive Regression Spline MARS: score 1.28901<br>Inputs: longitude latitude open review count</p> <p>* Linear &quot;level&quot; model: score 1.28905.<br>Inputs: &quot;openTRUE&quot; and derived category indicators: &quot;SalonsOrSpasTRUE&quot; &quot;RestaurantTRUE&quot;&nbsp; &quot;ActiveLifeTRUE&quot; &quot;HealthAndMedicalTRUE&quot; &quot;BankTRUE&quot; &quot;MuseumOrArtTRUE&quot; &quot;AutomotiveTRUE&quot; &quot;HotelTRUE&quot; &quot;ShoppingTRUE&quot; &quot;BlankTRUE&quot; (+ intercept term)<br>BlankTRUE indicator means that categories field is blank. Some categories are merged (like museum or art) due to correlation or due to similar level effect.<br><br>I have gotten similar result (on training data) using ensemble of MLPs. As next I am moving to Kriging and &quot;level&quot; Kriging models.</p>
3599, <p>Below is small R script for visualizing training samples on top of satellite map. Naturally you should not use external data in predictions/competion. However it was interesting to plot such a map. <br><br>- Stars are rounded to complete numbers (rounding down to nearest integer). Colors are from red &quot;worst/bad&quot;&nbsp;- yellow&nbsp;&quot;neutral&quot; &nbsp;- green &quot;good/best&quot;. <br>- Change get_map zoom value (say to 13) to zoom in.</p> <p><code>library(ggmap)<br></code><code># Remark: at this point I am assuming that you have <br># training data loaded in dtrain data frame and having <br></code><code># longitude latitude and stars fields<br><br></code><code># Calculate center point for map picture extraction<br>mlon &lt;- mean(dtrain$longitude)<br>mlat &lt;- mean(dtrain$latitude)<br># round to complete stars (for easier visualization)<br>dtrain$label &lt;- as.factor(as.integer(dtrain$stars)) <br><br></code><code># Get map data from google at zoom level 9 <br>mapImageData &lt;- get_map(location = c(lon=mlonlat=mlat)<br> color = &quot;color&quot; source = &quot;google&quot; maptype = &quot;satellite&quot; zoom = 9)<br><br></code><code>p &lt;- ggmap(mapImageData)+geom_point(data=dtrain<br>aes(x=longitudey=latitudeshape=labellabel=labelcolor=label)size=1.5)<br>p &lt;- p+ xlab(&quot;Longitude&quot;)+ylab(&quot;Latitude&quot;)<br>p &lt;- p + labs(title=&quot;Training data samples&quot;)<br>p &lt;- p + scale_colour_manual(values=c(&quot;#770000&quot; &quot;#EE0000&quot; &quot;yellow&quot; &quot;#007700&quot; &quot;#00EE00&quot;))<br><br></code><code># Display (Remark: warning about removed rows is ok as it means that <br># points not fitting screen window were removed. Change zoom value at <br># get_map call to smaller value to see all points<br>p</code></p>
3599, <p>To get a bit different&nbsp;view one can try&nbsp;source=&quot;stamen&quot; and maptype=&quot;toner&quot; in get_map call.</p>
3599, <p>[quote=Kapil Dalwani;29182]</p> <p>can you post the image using R I would be interesting in seeing how it looks like.&nbsp;</p> <p>[/quote]</p> <p>To my understanding I cannot post such as it would reveal data from the competion.</p>
3599, <p>According to my experiences MGR_ID attribute has most information about Action response variable (when choosing single predictor variable from original data set without doing variable selection to categories). </p> <p>&quot;Isn't it too late for that now?&quot;<br>Hmm I guess there is still 43 hours to go :D</p>
3599, <p>I have used very simple models. Namely I have done &quot;direct estimation&quot; of Pr(Action|Zi) where Zi is a i:th predictor variable in the data set. I have not done any categories selection thus always using all categories of a variable.</p> <p>My best ensemble is of type (below Z denotes all predictors):<br>Pr(Action|Z) = w1*Pr(Action|Role_Code) + w2*Pr(Action|Role_DEPTNAME)+ w3*Pr(Action|MGR_ID) + Pr(Action|Action|ROLE_FAMILY_DESC).</p> <p>Thus this ensemble is just approximation of joint distribution Pr(Action|Z) as linear combination of marginal distributions Pr(Action|Zi) where Zi is i:th predictor variable. Remark that I have used only 4 of predictors and not all of them.<br><br>I have tried 3 methods for calculating ensemble weights:</p> <p>1) average: w1=w2=w3=w4=1/4<br>2) QP problem: w1+w2+w3+w4=1 wi &gt;= 0 for i=1...4<br>3) OLS estimation<br><br>Out of these the average corresponds to my best result but just of 0.83621. Thus far from leaderboard top.</p>
3599, <p>Yes you are absolutely right. I did not have time to do better things (and did not want to use logistic regression as so many have used it :D). Anyways I did &quot;my best&quot; to minimize overfitting by using marginal distributions only (no interaction terms). Naturally there is still quite a much overfitting.</p>
3599, <p>Or Generalized Boltzmann Machine:)</p>
3599, <p>Some silly(?) questions:</p> <p>Ok so we are supposed to predict time-serie of following for each flight: <br>Latitude Longitude Altitude AirSpeed<br><br>Few questions that come in my mind:<br>1) How evaluation formula/simulator penalizes if physics of airplane are not taken into account (smooth tangent of flying path vs ufo teleporting etc)? The &quot;ufo teleporting&quot; is related to next question 2 (that is what are limits of changing the airspeed between two time points).<br><br>2) What is time frame between the time serie (latlongaltairspeed) pairs? If it is fixed say s milliseconds then naturally airspeed between point t and point t+1 cannot change more than physical limits of flying airplane (and weather conditions).<br><br>3) If object is to land to say point E then how simulator penalizes solution if flight path does not end at point E. (thinking about &quot;perfect solution&quot; taking weather conditions like headwind etc 100% perfectly into account and landing to E versus &quot;bad solution&quot; which ignores weather conditions and say there is heavy tailwind and lands far off point E - although the algorithm itself may think that &quot;we&quot; (simulator) lands at point E.<br><br></p>
3599, <p>What is time interval between ordinals of a one flight? Or is this time interval varying and determined by path&nbsp;simulator based on delta coordinate and delta speed?</p>
3599, <p>[quote=joycenv;29160]</p> <p>[quote=sfin;29115]</p> <p>What is time interval between ordinals of a one flight? Or is this time interval varying and determined by path&nbsp;simulator based on delta coordinate and delta speed?</p> <p>[/quote]</p> <p>You are correct there is no fixed time interval between ordinals in the submitted flight route. Time between ordinals will depend on the route airspeed and wind conditions.</p> <p>[/quote]</p> <p>So how I am penalized (by cost function)&nbsp;if at interval 0 I am at point say p with velocity v&nbsp;and at interval 1 I am at point p+epsilon (consider epsilon vector&nbsp;almost 00 longitudelatitude change)&nbsp;and velocity v1 such that v1 &gt;&gt; v. So my coordinates say that I have moved &quot;epsilon&quot; (almost zero time) but velocity v1 says that I must have moved more than zero time. Thus basicalyl this is case of &quot;breaking physics&quot; (accelerating plane's speed from v to v1 in &quot;almost zero time&quot;).</p> <p>&nbsp;</p>
3599, <p>Below are some (hopefully) useful terms/formula links. Feel free to add suitable links:</p> <p>* <strong>Air speed/ground speed computations</strong>: http://virtualskies.arc.nasa.gov/navigation/6.html</p> <p>* <strong>Headwind/tailwind computations</strong>: http://en.wikipedia.org/wiki/Tailwind</p> <p>* <strong>Latitudelongitude pair formulas</strong> (Haversine distance bearing etc): http://www.movable-type.co.uk/scripts/latlong.html<br><br>* <strong>Aircrafs weight change at a given altitude:</strong> https://www.grc.nasa.gov/WWW/k-12/airplane/wteq.html</p>
3599, <p><strong>Some publications <br></strong>(remark optimization problems in below publications may have nothing to do with this competition - but still may contain&nbsp;useful&nbsp;information to read):<br>* <strong>Aircraft Flight Path Optimization The Hamilton-Jacobi-Bellman Considerations</strong><br>&nbsp;http://www.m-hikari.com/ams/ams-2012/ams-25-28-2012/khardiAMS25-28-2012.pdf</p> <p>* <strong>Flight path optimization for an airplane<br></strong>http://www.diva-portal.org/smash/get/diva2:436563/FULLTEXT01.pdf<br><br>* <strong>Mathematically Modeling Aircraft Fuel Consumption:<br></strong>http://sciencecases.lib.buffalo.edu/cs/files/flight_fuel.pdf<br><br>* <strong>Relationship between fuel consumption and altitude for commercial aircraft during descent: Preliminary assessment with a genetic algorithm <br></strong>http://www.sciencedirect.com/science/article/pii/S1270963811000459<br>(remark above link requires purchasing article. However there are some unfortunately low resolution pictures displaying some interesting data)</p>
3599, <p>Hi<br>Unfortunately I do not have tableau license. However to my understanding at least Tableau 7's modelling capabilities are very limited compared to say R Python etc. In Tableau 8 situation is better however still far from R etc. But&nbsp;Tableau is great for&nbsp;plotting longitudelatitude paths&nbsp;on top of map (however similar plots can be done using&nbsp;googlemaps api&nbsp;&amp; R).</p> <p>I am probably testing something in R and if it seems working (and well enough) then I will implement it in C++ C# or Java.</p>
3599, <p>Negative latitude coordinate means south (positive north) and a negative longitude means west (positive east). So I guess data is focused to west side then?</p>
3599, <p>Below R-macro draws planes&nbsp;corresponding to test flights and all airports. It also draw&nbsp;s great circle path from current plane location to target airport. <br>Remark: currently it draws just first 50 test flights. Change parameter&nbsp;flightsToDraw if/when needed:</p> <p><code># R-macro to plot planes and (all) airports corresponding to <br># test flights. Draws great circle path from current plane <br># location to&nbsp;target airport.<br></code><code><br>#</code><code> Required libraries<br>library(maps) <br>library(geosphere)</code></p> <p><code># Load in data sets<br>setwd(&quot;d:\\kaggle\\ge&quot;)<br>airports &lt;- read.csv(&quot;airports.csv&quot; header=TRUE)<br>testFlights &lt;- read.csv(&quot;testFlights.csv&quot; header=TRUE)</code></p> <p><code># Calculate bounding box for test flights' longitudes <br></code><code># and latitudes<br>xlim &lt;- c(min(testFlights$CurrentLongitude)max(testFlights$CurrentLongitude))<br>ylim &lt;- c(min(testFlights$CurrentLatitude)max(testFlights$CurrentLatitude))</code></p> <p><code># Draw map using the bounding box<br>map(&quot;world&quot; col=&quot;#ffffff&quot; fill=TRUE bg=&quot;white&quot; lwd=0.05 xlim=xlim ylim=ylim)</code></p> <p><code># Plot desired number of test flights<br>flightsToDraw &lt;- 50 # max value is nrow(testFlights) <br># </code><code>but drawing becomes too messy<br><br></code><code>for(i in 1:flightsToDraw){<br>&nbsp; targetLongitude &lt;- airports$longitude_degrees[airports$airport_icao_code==testFlights$ArrivalAirport[i]]<br>&nbsp; targetLatitude &lt;- airports$latitude_degrees[airports$airport_icao_code==testFlights$ArrivalAirport[i]]<br><br>&nbsp;</code><code>inter &lt;- gcIntermediate(c(testFlights$CurrentLongitude[i]testFlights$CurrentLatitude[i]) <br>&nbsp; c(targetLongitude targetLatitude) n=50&nbsp; addStartEnd=TRUE) <br>&nbsp; lines(interlty=2 col=&quot;#CC0000&quot;) <br>}</code></p> <p><code># Plot planes <br>for(i in 1:flightsToDraw){<br>&nbsp; inter &lt;- gcIntermediate(c(testFlights$CurrentLongitude[i]testFlights$CurrentLatitude[i])<br> c(testFlights$CurrentLongitude[i]testFlights$CurrentLatitude[i]) n=1 addStartEnd=TRUE) <br>&nbsp; lines(inter col=&quot;#0000FF&quot;lwd=5) <br>}</code></p> <p><code># Plot target airports<br>for(i in 1:nrow(airports)){<br>&nbsp; inter &lt;- gcIntermediate(c(airports$longitude_degrees[i]airports$latitude_degrees[i])<br> c(airports$longitude_degrees[i]airports$latitude_degrees[i]) n=1addStartEnd=TRUE) <br>&nbsp; lines(inter col=&quot;#00FF00&quot;lwd=5) <br>}</code></p> <p><code># Plot legend<br>legend(xlim[1] + (xlim[2]-xlim[1])*0.75ylim[1] + (ylim[2]-ylim[1])*0.9 <br>&nbsp; legend=c(&quot;Plane&quot;&quot;Target airport&quot;)text.col=c(&quot;#0000FF&quot; &quot;#00FF00&quot;)ncol=1)</code></p>
3599, <p>I have not checked if licenses of packages&nbsp;maps or geosphere are according to the competition rules (5 allowed&nbsp;licenses) - my guess is that they are not. So prepare to program (mostly gcIntermediate function) by&nbsp;yourself in case if you use above functionality&nbsp;as part of path estimations in your competition submissions.</p>
3599, <p>My guess is that they want to avoi following scenario: you are using GNU R is GPL and you might be using 3rd party GPL licensed R package. Then suppose you would win competition and after that 3rd party would contact the host of competition and request full source code of their final solution. (As it would have eaten GPL component inside and according to GPL license source code must be available). Well this is just my two cents :)</p> <p>Anyways I have no clue why those 5 licenses are allowed and others not. My guess is that some lawyer with knowledge of software technology licenses has thought them and decided that using only those 5 are &quot;safe&quot; for host of competition (safe = final solution cannot be &quot;force extracted&quot; by 3rd party from them after they have implemented it).</p> <p>Addon:<br>Hmm I guess that python 2 license is among the 5 licenses because: <br><em>&quot;GPL-compatible doesn&#8217;t mean that we&#8217;re distributing Python under the GPL. <strong>All</strong></em><br><em><strong>Python licenses unlike the GPL let you distribute a modified version without</strong></em><br><em><strong>making your changes open source</strong>. The GPL-compatible licenses make it possible to</em><br><em>combine Python with other software that is released under the GPL; the others</em><br>don&#8217;t.<em>&quot;</em> source: http://docs.python.org/2/license.html</p> <p>This web page might be useful too: http://en.wikipedia.org/wiki/License_compatibility</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p>
3599, <p>Hi</p> <p>I have not utilized those dependencies yet. I think&nbsp;that one should use those rules to post edit &nbsp;estimates so that one replaces the worst RMSE component using deterministic rule. Say if in your example rule Class 10.3 would have highest RMSE (in training set) then I would l use:</p> <p>Class 10.3 = Class4.1 - Class10.1 - Class 10.2</p> <p>However there is one but. Namely let's assume that 4 classes are estimated independently (which is perhaps not the optimal case - as they are negatively correlated due to sum rule) but for simplicity. Then prediction variance would be:<br>VAR[Class 10.3] = VAR[Class 4.1 estimate] + VAR[Class 10.1 estimate] + VAR[Class 10.2 estimate]</p> <p>Now the question comes that what was the initial MSE of Class 10.3 estimate vs MSE of Class 10.3 &quot;based on rule&quot;. Depending which of these has smaller MSE then I would use it. Also estimation of MSE is likely&nbsp;biased which makes things a bit more complicated.</p>
3599, <p>Hi<br>Sorry for late response. Anyways below is sample code I used to extract features from training data set. Similarly it needs to be run for test data set.&nbsp;<br><br><em>Remarks:</em> <br>- run time can be 3-6 days (separately for both training and test) depending from your hardware.<br>- images are not properly pre-processed (like histogram equalized). However I have checked that marginal distributions of features are roughly same for training and test data sets.<br>- Fourier transform is not proper (should be 2D and not 1D)<br>- I am converting RGB data to gray using formula: 0.21*red + 0.72*green + 0.07*blue - thus color information is lost (feel free to add colour channel features too)<br>- I my experiements I am fitting regression tree per class this example just has sample code for Output1.1 class at the end. So you need to do either 37 separate trees (to output all classes) or you could do multivariate regression tree (which predicts all 37 simultaneously) - I am not sure how well it would perform.<br>- This sample fits only one regression tree (thus no ensemble). I have also tested linear regression logistic regression support vector regression and random forest. However I have found sgbm = stochastic gradient boosting to work best so far.<br><br><em>Required files:</em><br>-&nbsp;training_solutions_rev1.csv and training data jpg files must be in folder where R is being run (you can set working directory using setwd(&quot;c:\\temp&quot;) -&nbsp;if you have all files in c:\temp.</p> <p><em>Required R packages </em>(these you must install running R as admin using command install.packages and if I recall correctly EBImage required a bit more complicated install)<em>:&nbsp;</em><br>-fractaldim<br>-biOps<br>-EBImage<br>-rpart</p> <p><br><strong>Sample R Code:</strong></p> <p># Include needed libraries<br>library(fractaldim) <br>library(biOps) <br>library(EBImage)<br>library(rpart)</p> <p># Set random number generator seed (so that results can be replicated later)<br>set.seed(1234)</p> <p># get training solutions (to figure out jpeg filenames and also to get Y values = ClassX.Y)<br>trSolutions &lt;- read.csv(&quot;training_solutions_rev1.csv&quot; header=TRUE)</p> <p># Basic features<br>distinctValues &lt;- NULL<br>meanv &lt;- NULL<br>logmean &lt;- NULL<br>medianv &lt;- NULL<br>sd &lt;- NULL<br>qr &lt;- NULL<br>logsd &lt;- NULL<br>q25 &lt;- NULL<br>q75 &lt;- NULL</p> <p># Fractal features<br>fd1 &lt;- NULL<br>fd2 &lt;- NULL<br>fd3 &lt;- NULL</p> <p># Fourier features<br>fftConcentration &lt;- NULL<br>fftReal &lt;- NULL<br>fftImaginary &lt;- NULL</p> <p># Gray tail mass distribution features<br>linFitAlpha &lt;- NULL<br>linFitBeta &lt;- NULL</p> <p># Blob features<br>blobs &lt;- NULL<br>meanMass &lt;- NULL<br>medianMass &lt;- NULL<br>sdMass &lt;- NULL<br>maxMass &lt;- NULL<br>countSD1Mass &lt;- NULL<br>countSD2Mass &lt;- NULL<br>countSD3Mass &lt;- NULL<br>countSD4Mass &lt;- NULL</p> <p>#<br># Loop over all training data (= images)<br>#<br>for(imgi in 1:nrow(trSolutions)){</p> <p># Load JPEG file<br>print(sprintf(&quot;Processing %s (%d/%d)&quot; trSolutions$GalaxyID[imgi] imgi nrow(trSolutions)))<br>flush.console()<br>x &lt;- readJpeg(sprintf(&quot;%s.jpg&quot; trSolutions$GalaxyID[imgi]))</p> <p>#<br># Extract features from images<br>#</p> <p># Calculate number of blobs<br>y = thresh(x 10 10 0.10)<br>y = opening(y makeBrush(5 shape=&quot;disc&quot;))<br>z = bwlabel(y)<br>blobs[imgi] &lt;- max(z)</p> <p># Calculate blob segments<br>blobMassDistribution &lt;- table(z)<br>segment &lt;- rownames(blobMassDistribution)<br>mass &lt;- as.numeric(blobMassDistribution)<br>massTable &lt;- data.frame(segmentmass)</p> <p># Remove background blob<br>massTableFiltered &lt;- massTable[ massTable$mass &lt; max(massTable$mass)]</p> <p># Calculate blob mass distribution features (simple statistics)<br>meanMass[imgi] &lt;- mean(massTableFiltered$mass)<br>medianMass[imgi] &lt;- median(massTableFiltered$mass)<br>sdMass[imgi] &lt;- sd(massTableFiltered$mass)<br>maxMass[imgi] &lt;- max(massTableFiltered$mass)</p> <p># Calculate number of &quot;big blobs&quot; defined by 123 and 4 standard mass deviations from mean mass<br>countSD1Mass[imgi] &lt;- sum(massTableFiltered$mass &gt;= meanMass[imgi] + 1*sdMass[imgi])<br>countSD2Mass[imgi] &lt;- sum(massTableFiltered$mass &gt;= meanMass[imgi] + 2*sdMass[imgi])<br>countSD3Mass[imgi] &lt;- sum(massTableFiltered$mass &gt;= meanMass[imgi] + 3*sdMass[imgi])<br>countSD4Mass[imgi] &lt;- sum(massTableFiltered$mass &gt;= meanMass[imgi] + 4*sdMass[imgi])</p> <p>dims &lt;- dim(x)<br>plot(x) <br>img &lt;- data.frame(x)</p> <p>xsize &lt;- min (dim(img))<br>ysize &lt;- xsize</p> <p># get RGB channels data<br>red &lt;- (img[ 1:ysize])<br>green &lt;- (img[ (ysize+1):(ysize*2)])<br>blue &lt;- (img[ (ysize*2+1):(ysize*3)])</p> <p># convert to gray levels<br>gray &lt;- as.integer(unlist(0.21 * red + 0.72 * green + 0.07 * blue))</p> <p><br>plevels &lt;- table(gray)<br>distinctValues[imgi] &lt;- length(rownames(plevels))<br>levelValues &lt;- as.integer( rownames(plevels) )</p> <p>densityValues &lt;- as.numeric (plevels)<br>cumulativeValues &lt;- cumsum(densityValues)<br>cumulativeProbabilityValues &lt;- cumulativeValues / max(cumulativeValues)</p> <p>standardizedLevelValues &lt;- (levelValues - min(levelValues)) / (max(levelValues)-min(levelValues))<br>massd &lt;- data.frame(rowIndex=1:length(levelValues)standardizedLevelValuescumulativeProbabilityValues)</p> <p># Left truncate by 10% (to remove &quot;darkness from background&quot; and focus to more interesting part of mass distribution)<br>dropIndex &lt;- as.integer(length(levelValues) * 0.10)<br>massd &lt;- massd[massd$rowIndex &gt; dropIndex]</p> <p>massd$standardizedLevelValues &lt;- massd$standardizedLevelValues - min(massd$standardizedLevelValues)<br>massd$standardizedLevelValues &lt;- sqrt(massd$standardizedLevelValues)<br>linfit &lt;- lm(cumulativeProbabilityValues~standardizedLevelValues data=massd)<br>linFitAlpha[imgi] &lt;- as.numeric(linfit$coefficients[1])<br>linFitBeta[imgi] &lt;- as.numeric(linfit$coefficients[2])</p> <p># Calculate basic features<br>meanv[imgi] &lt;- mean(gray)<br>logmean[imgi] &lt;- mean(log(1+gray))<br>medianv[imgi] &lt;- median(gray)<br>sd[imgi] &lt;- sd(gray)<br>logsd[imgi] &lt;- sd(log(1+gray))<br>q25[imgi] &lt;- quantile(gray 0.25)<br>q75[imgi] &lt;- quantile(gray 0.75)<br>qr[imgi] &lt;- q75[imgi] - q25[imgi]</p> <p># Calculate Fourier transform features<br>fourierTrans &lt;- fft(gray)<br>fftConcentration[imgi] &lt;- mean( abs(fourierTrans) ) # the more peaked around 00 at complex space the more periodical<br>fftReal[imgi] &lt;- mean( abs(Re(fourierTrans)) )<br>fftImaginary[imgi] &lt;- mean( abs(Im(fourierTrans)) )</p> <p># Calculate fractal dimension (3 diff. estimates)<br>dim(gray) &lt;- c(xsize ysize)<br>d1 &lt;- fd.estim.isotropic(gray p.index = 1 direction='hv' plot.loglog = FALSE plot.allpoints = FALSE)<br>fd1[imgi] &lt;- d1$fd</p> <p>d2 &lt;- fd.estim.squareincr(gray p.index = 1 plot.loglog = FALSE plot.allpoints = FALSE)<br>fd2[imgi] &lt;- d2$fd</p> <p>d3 &lt;- fd.estim.filter1(gray p.index = 1 plot.loglog = FALSE plot.allpoints = FALSE)<br>fd3[imgi] &lt;- d3$fd<br>}</p> <p>#<br># Attach training data and gathered features<br>#<br>outputData &lt;- trSolutions<br>outputData$distinctValues &lt;- distinctValues<br>outputData$meanv &lt;- meanv<br>outputData$logmean &lt;- logmean<br>outputData$medianv &lt;- medianv<br>outputData$sd &lt;- sd<br>outputData$qr &lt;- qr<br>outputData$logsd &lt;- logsd<br>outputData$q25 &lt;- q25<br>outputData$q75 &lt;- q75<br>outputData$fd1 &lt;- fd1<br>outputData$fd2 &lt;- fd2<br>outputData$fd3 &lt;- fd3<br>outputData$fftConcentration &lt;-fftConcentration<br>outputData$fftReal &lt;-fftReal<br>outputData$fftImaginary &lt;- fftImaginary<br>outputData$linFitAlpha &lt;- linFitAlpha<br>outputData$linFitBeta &lt;- linFitBeta<br>outputData$meanMass &lt;- meanMass<br>outputData$medianMass &lt;- medianMass<br>outputData$sdMass &lt;- sdMass<br>outputData$maxMass &lt;- maxMass<br>outputData$countSD1Mass &lt;- countSD1Mass<br>outputData$countSD2Mass &lt;- countSD2Mass<br>outputData$countSD3Mass &lt;- countSD3Mass<br>outputData$countSD4Mass &lt;- countSD4Mass<br>outputData$blobs &lt;- blobs<br>outputData$GalaxyID &lt;- NULL</p> <p><br># Save outputs<br>write.csv(outputData &quot;outputData.csv&quot;)</p> <p># Fit sample regression tree to predict Class1.1 (you need to do similarly for others classes).<br># and feel free to add variable selection etc<br>fit &lt;- rpart(Class1.1 ~ meanMass+medianMass+countSD1Mass+countSD2Mass+countSD3Mass+countSD4Mass+blobs+linFitAlpha+linFitBeta <br>+fftConcentration+fftReal+fftImaginary+meanv+sd+logsd+medianv+fd1+fd2+fd3+distinctValues+maxMass+sdMass data=outputData)<br># Calculate mean squared error for tree prediction and for unconditional mean prediction<br>mseTree &lt;- mean( (predict(fit) - outputData$Class1.1)^2 )<br>mseMean &lt;- mean( (mean(outputData$Class1.1) - outputData$Class1.1)^2 )</p> <p># If improvement is clearly &lt; 1 then fitting regression tree was worth it provided it did not over-fit <br># to data which you need to check separately&nbsp;(= result is better than unconditional mean estimate)<br>improvement &lt;- mseTree/mseMean</p>
3599, <p>Hmm not sure If I understood correctly. Namely if you can estimate Q1.1 then why you cannot estimate Q1.2 too? Hmm perhaps this is related that I have seen in some softwares K-nn able to do only univariate Y although predictors X can be multivariate. In such case you need to find better software/knn algorithm and do it itself or you can try predicting outputs one by one. One by one prediction may not preserve correlation between outputs.&nbsp;</p> <p>There are logical constraint rules e.g. some probabilities etc summing to 1 or almost always. This means that there is negative correlation among those probabilities (random vectors = answers to questions). Therefore I would say that this is multivariate regression problem. Hence&nbsp;you need to predict vector (Q1.1 ... last question) given predictors. And I suppose here predictors are features computed from images.</p> <p>Typically in nearest neighbour algorithms you define metric between predictors and for a given sample you look k nearest neighbour. Then you can calculate average of Q1.1...Last question over the samples. Selecting k is critical as the lower the k is the higher the variance will be but less bias and vice versa. In K-nearest neighbour the &quot;effective&nbsp;number of parameters&quot; is p*n/k where p is dimension of questions vector (Q11. ... last question) n is number of training samples and k is number of nearest neighbours to look for.&nbsp;</p> <p>For best performance it might be that you need to split questions into subgroups and do multivariate regression to them separately (as some features may work better for some question sets and less well others). However it might be that you find a decent set of predictors that work simultaneously for the whole set of questions.</p>
3599, <p>Hmm I was wondering how long image feature extraction from training data took for others?</p> <p>Processing on my system takes 17 hours (on i7/920 with 16GB RAM and running R 3.0.2/64-bit). I am calculating mostly simple features and a few more complicated (Fourier Fractals and shape of mass distribution). However my implementation is likely using only 1 core out of 4.</p> <p>I guess this is one reason why some are using those GPU libraries to speed-up processing significantly.</p>
3599, <p>Hmm yeah it seems that extracting blob (= &quot;thresholding and labeling B&amp;W image to segments&quot;) location and mass distribution takes most time. Fractal measures and fast fourier transform take second most of the time.</p> <p>I am storing things to memory only and storing them only at the end of whole extraction proces. I also just noticed that my run time increased to ~72 hours for training data (and about 80 hours for test data feature extraction).</p> <p>Cannot say any specific R vs Python speed differences. But I would imagine that it is easier to write slow code with R than using Python :)</p>
3599, <p>I did small test to see if there is difference between using only gray channel features (features computed from rgb-&gt;gray converted images) vs RGB features. I would think that converting RGB to HSV might provide better&nbsp;insights but I did not try that yet.</p> <p>I found out (using gradient boosting variable importance) that many Green and Blue features come out as first in variable importance before any gray channel only features. This I just tested for class1.1 class 1.2 class1.3 but I suppose similar findings can be done for them.</p> <p>Also I noticed that (some) green channel features are significantly more important than blue or red channel features [again checked only for 1.11.2 and 1.3 class outputs]. This might be related to fact that human eye observes blue&nbsp;wavelength worse than green or red. See for example&nbsp;http://www.normankoren.com/Human_spectral_sensitivity_small.jpg</p> <p>(that is why I used originally heavy downweighting of blue channel when converting RGB images to gray scale).</p> <p>&nbsp;Anyone else tested importance of RGB channels (and willing to share any information :))?</p>
3599, <p>[quote=Armando Vieira;38146]</p> <p>Nice :)</p> <p>I'll not advice SVM (unless you use some technique to deal with dimensionality and create sparser representations of support vectors like Relevance SVM)</p> <p>[/quote]</p> <p><span style="line-height: 1.4">Hmm if your &quot;Relevance SVM&quot; refers to RVM then isn't it patented by Microsoft? (not sure if we can use it then?):</span></p> <p><span style="line-height: 1.4">http://worldwide.espacenet.com/publicationDetails/biblio?CC=US&amp;NR=6633857&amp;KC=&amp;FT=E&amp;locale=en_EP</span></p> <p>&nbsp;</p>
3599, <p>I guess scoring is referring for example to building scorecard using logistic regression:</p> <p>http://plug-n-score.com/learning/logistic-regression-for-scorecard-calculation.htm</p>
3599, <p>I want to sort data by f275 f521 key-pair. However I run into issue that integer64 data type implementation supports only single key sorting (in R so far).&nbsp;</p> <p>So far I have been casting f275 and f521 but naturally that causes loss as double cannot present all integer64 values. So is there other than below way to do sorting so that f275 and f521 values are preserved (at integer64 accuracy) ?</p> <p>REMARK: in below R code ending paranthesis are not always displayed correctly not sure why. They &nbsp;convert to greater than sign. Also some assignments are corrupted and only display as hyphen -</p> <p><strong><span style="line-height: 1.4">Currently I load and sort data as:</span></strong></p> <p><code><strong><span style="font-family: monospace serif; font-size: 1em; line-height: 1.4;">require(data.table)<br></span><code>require(bit64)<br></code><code>DT2 &lt;- fread(&quot;train_v2.csv&quot; header=TRUE sep=&quot;&quot;)<br></code></strong><strong><code>DT2sub &lt;- data.frame(f275=DT2$f275 f521=DT2$f521 loss=DT2$loss)<br></code></strong><strong><span style="line-height: 1.4">sortnames -</span><span style="line-height: 1.4">&nbsp;c(&quot;f275&quot;&quot;f521&quot;)<br></span></strong><strong># cannot dual sort integer64's thus cast to doubles (with loss of precision)<br></strong><strong>DT2sub$f275 - as.double(DT2sub$f275)<br></strong><strong>DT2sub$f521 - as.double(DT2sub$f521)<br></strong><strong>DT2sort - DT2sub[do.call(&quot;order&quot; DT2sub[sortnames]) ]<br></strong><strong>DT2sort$rank - as.integer( ave(DT2sort$f521 DT2sort$f275 FUN=rank) )</strong></code><code><strong><code>If I do not do above as.double casting I get error:<br>Error in order.integer64(f275 = c(6.12641400843146e-313 2.67489959352367e-314 : <br> can only order one vector at the moment<br><br>I looked sort64 implementation and it indeed seems to have currently this limitation:<br>http://r-forge.r-project.org/scm/viewvc.php/pkg/bit64/R/sort64.R?view=markup&amp;revision=132&amp;root=ff&amp;sortby=date</code></strong></code></p>
3599, <p>Thank you for both of the responses did not know about that key parameter in data.table call.</p> <p>&nbsp;</p>
3599, <p>Hmm sounds suspiciously high AUC like over-fitting as you mentioned or because of class imbalance. However I do not have comparison AUC from my predictions yet.</p> <p>Hmm was just thinking that should you use weighted AUC instead of normal AUC (as there is so heavy class imbalance between no-default vs default). Take a look of this paper if interested:&nbsp;http://www.jofcis.com/publishedpapers/2012_8_1_371_378.pdf</p> <p>&nbsp;</p>
3599, <p>I guess test set's time-series (per customer) have been &quot;truncated&quot; and this process is probably not completely at random - which could preserve distributions :)</p>
3599, <p>If I checked correctly then there are over 1800 combinations of target classes (when considering conditional joint distribution Pr(AB...G given background variables).</p> <p>I was wondering that has anyone checked if targets A ... G are independent of each other (or some subsets of them are). As it would allow simpler classifiers to be used and parameters over-fitting could be avoided easier.</p> <p>&nbsp;</p>
3599, <p>I wrote below simple R code to check difference between joint distribution of target classes vs product of marginal distributions. In ad-hoc manner it seems to suggest that target class (vectors A...G) are not independent.</p> <p><code>require(data.table)<br>require(bit64)</code></p> <p>library(data.table)<br>system.time(DT2 &lt;- fread(&quot;train.csv&quot; header=TRUE sep=&quot;&quot;))</p> <p>DT2$output &lt;- paste(DT2$ADT2$BDT2$CDT2$DDT2$EDT2$FDT2$Gsep=&quot;&quot;)<br>classes &lt;- sort(unique(DT2$output))</p> <p># Calculate joint distribution<br>pr &lt;- table(DT2$output) / nrow(DT2)</p> <p># Calculate marginal distributions<br>prA &lt;- table(DT2$A) / nrow(DT2)<br>prB &lt;- table(DT2$B) / nrow(DT2)<br>prC &lt;- table(DT2$C) / nrow(DT2)<br>prD &lt;- table(DT2$D) / nrow(DT2)<br>prE &lt;- table(DT2$E) / nrow(DT2)<br>prF &lt;- table(DT2$F) / nrow(DT2)<br>prG &lt;- table(DT2$G) / nrow(DT2)</p> <p># Calculate uniform (= non-weighted) pr (= weighted) and max error between joint and marginal product probabilities<br>uniformError &lt;- 0.0<br>prError &lt;- 0.0<br>maxError &lt;- 0.0<br>for(i in 1:length(classes)){<br> print(sprintf(&quot;Class %d/%d&quot; i length(classes)))<br> flush.console()</p> <p># joint probability<br> prJoint &lt;- pr[classes[i]]</p> <p># marginal product probability<br> prMarginal &lt;- prA[[substring(classes[i]11)]] * prB[[substring(classes[i]22)]] * prC[[substring(classes[i]33)]] * prD[[substring(classes[i]44)]] * <br>prE[[substring(classes[i]55)]] * prF[[substring(classes[i]66)]] * prG[[substring(classes[i]77)]]</p> <p>error &lt;- abs(as.numeric(prJoint)-as.numeric(prMarginal))<br> uniformError &lt;- uniformError + error<br> prError &lt;- prError + error * as.numeric(prJoint)<br> if(error &gt; maxError){ maxError &lt;- error}<br>}<br>uniformError &lt;- uniformError / length(classes) # normalize</p> <p>plot(as.numeric(pr)xlab=&quot;Class index&quot; ylab=&quot;Pr&quot; xlim=c(1length(classes)) ylim=c(0max(pr)))<br>par(new=TRUE)<br>abline(h=maxErrorcol=&quot;red&quot;lty=2)<br>abline(h=uniformErrorcol=&quot;purple&quot;lty=2)<br>abline(h=prErrorcol=&quot;#990000&quot;lty=2)</p>
3599, <p>To my understanding it refers only to single policy of C.</p> <p>I earlier checked at least some of policies A-G are dependent (joint distribution of A..G is not product of marginals A...G).&nbsp;In addition I did just study for records with record_type=1 at training data set and found:<br><strong><span style="line-height: 1.4">If you look probability Pr(C | C_Previous) you will see that probability mass is peaked at C=C_previous except for case C_previous NA then its peak value is 3.</span></strong></p> <p>I got around 71% match for predicting C given C_Previous in training set however for other policies I got much weaker results around 50%.</p> <p>So previous C refers to policy C only however it contains (&quot;weakly&quot;) information about other policies too.</p>
3599, <p>You need to predict whether a customer is going to be repeat buyer (= that is doing at least 2 purchases in his/her lifetime) given his/her history data and information about offer.</p> <p>Thus you need to model (estimate) Probability(&quot;Customer i is going to be repeat buyer given data about his/her past transactions and data about chain offer market and offerdate&quot;) for each customer i (that i you can think to be all id in testHistory.csv file).</p> <p>With some simple checking (on trainHistory) one can find that this probability is dependent (at least) on chain offer and market variables.</p>
3599, <p>Hi Decipher</p> <p>Customer is given offer about a product (or I guess it can be multiple products?) of a company. So you need to estimate if this customer is going to purchase again of that product.</p> <p>From leaderboard you can see that best prediction done by Kaggle is &quot;Prior (Brand &amp; Company &amp; Category) Benchmark&quot;. If you look it it says &quot;The customer has purchased at least one item of the same brand company and category previously&quot;.&nbsp;</p> <p>To my understanding repeater='t' means that customer has done at least 2 purchases of same thingy and value 'f' must mean that he/she has done it only once (or not at all - I am not sure about if &quot;not at all&quot; part is included). I guess from given transactions history you can check which of those is the true case.</p>
3599, <p>Predicting whether customer becomes repeat buyer is &quot;sort of&quot; sub-task in estimation of&nbsp;customer's lifetime value. Best models in that field (to my understanding) are based on Bayesian models which model timing of first purchase 2 3 ...  amount in first purchase  2 3 ... customer churning etc etc.</p> <p>Remark that in this competition one does not need to do full lifetime value modelling (which would be perhaps a bit &quot;overkill&quot;).</p> <p>Anyways it might be worth taking&nbsp;a look at the following things:</p> <ul> <li><span style="line-height: 1.4"><em>&quot;Modeling Customer Lifetime Value&quot;</em> http://www.anderson.ucla.edu/faculty/dominique.hanssens/content/JSR2006.pdf</span></li> <li><em>&quot;Measuring Customer Life Time Value: Models and Analysis&quot;</em> http://www.insead.edu/facultyresearch/research/doc.cfm?did=51835</li> <li><em>Custora's CLV related material:</em><br>https://www.custora.com/university/for-marketers/clv/advanced/bayesian-inference<br>and<br>https://www.custora.com/university/for-marketers/clv/advanced/pareto-nbd</li> <li>&quot;A modified Pareto/NBD approach for predicting customer lifetime value&quot;<br>http://www.essec.edu/faculty/showDeclFileRes.do?declId=8555&amp;key=Publication-Content</li> <li>&quot;Improved Pareto/NBD Model and Its Applications in Customer Segmentation based on PErsonal Information Combination&quot; http://www.sersc.org/journals/IJDTA/vol6_no5/16.pdf</li> </ul> <p>In above NBD is negative binomial distribution which was used in early phases of computer modelling . See wiki page&nbsp;http://en.wikipedia.org/wiki/Negative_binomial_distribution<br>Remark that NBD modelling approach has flaws but it is better than nothing :)</p>
3599, <p>If you can live just with 15 first variables then a way to fast load in R is following: (it takes around 3-5 minutes on my ssd / I7 with 16 gigabytes ram peak memory usage is less than 8 gigabytes - might be even less than 3.5 gigabytes cannot recall exactly):</p> <p>mycols &lt;- rep(&quot;NULL&quot; 41)<br>mycols[1:15] &lt;- c(&quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot;&quot;numeric&quot;&quot;numeric&quot;&quot;numeric&quot;&quot;numeric&quot;&quot;numeric&quot;&quot;numeric&quot;&quot;numeric&quot;&quot;numeric&quot;&quot;numeric&quot;&quot;numeric&quot;&quot;numeric&quot;&quot;numeric&quot;)<br>inp &lt;- read.table(&quot;train.csv&quot; header=TRUE skip=0 colClasses=mycols nrows=45840617sep=&quot;&quot;) <br># save to internal R compressed format as loading it later is much faster<br>save(inp file=&quot;train_numeric.dat&quot;)<br><br># later load using load(file=&quot;train_numeric.dat&quot;)<br># and then use inp data<br><br>For test data set change 41 to 40 and mycols from 1:15 to 1:14 and remove one &quot;numeric&quot; from array and change nrows to&nbsp;6042135 or leave it out. I am unable to read much more variables as my 16 gigabyte memory runs out quite fast. However loading could be split into parts (and then later merged to one big datafile and resaved to R compressed format) - but that is a bit more work.</p>
3599, <p>Hi</p> <p>I am using R to fit logistic regression. In my test I have one day of training data (about 61 million records). And my data set has only id label and 13 id (numeric) variables. Size of this data in R (3.0.1 64-bit) in memory is around 13 gigabytes.</p> <p>When I am running R's standard glm I am getting peak memory usage of 14 gigabytes. However when using optimized package speedglm the memory usage is only 66 gigabytes. I guess glm training should scale with O(p^2) for memory requirements where p is number of regression predictors.</p> <p><br>So few questions:<br>1) Does anyone know even more memory optimized glm package than speedglm for R? (asking this as I typically play a game and let R run in background and want to minimize resources it is using)?</p> <p>2) &nbsp;How much (less) memory other more memory optimized implementations (Python machine learning kits) are using for something similar?</p>
3599, <p>Just checking obvious: did you put country prefix&nbsp;+91 before gsm phone number.&nbsp;(also here in Finland I have also to remove first zero from local gsm number before adding&nbsp;prefix +358)</p>
3599, <p>I am using logistic regression with just some of those 13 numeric variables. Score is clearly worse than&nbsp;yours only 0.52x something. Anyways check if you have not included these:<br>intercept+l2+l5+l7+l8+I9+l11+I10miss+I12miss+I13miss+l10+l12+l13+s1+s2+s3+t1</p> <p>where&nbsp;I am using logarithmic transformed I2 I5 I7 I8 I9 I11 I10 I12 i13 variables. In addition I am using indicators for I10 I12 and I13 being NA (missing) 2nd order interaction terms s1 (I10 &amp; I12 missing simultaneously)  s2 (I10 &amp;&nbsp;I13 missing simultaneously) s3 (I12 &amp; I13 missing simultaneously) and 3rd order interaction term t1 (I10 I12 and I13 missing simultaneously). Adding those interaction terms introduces some multicollinearity however with this large sample size that can be &quot;somewhat&quot; ignored.</p> <p>I have done mean imputation for all NA values.&nbsp;Hopefully some of those can improve your score a bit :)</p>
3599, <p>Perhaps you can find something from here (not sure):<br>http://quant.stackexchange.com/questions/390/is-variable-binning-a-good-thing-to-do<br><br>However I guess in above link most people think that after &quot;binning&quot; / creating indicators you would omit the original variable from your model training data but I bet that is not the case here. Anyways your predictor variable and indicator variable might get heavily correlated depending on mass of x=0 value. I think your model validation should reveal if such model would be good or bad compared to model with no such indicator variable.</p> <p>In the past I have handled semi-continuous variables Y with say value -9=not applicable and values&gt;= 0 normal continuous&nbsp;such that I have created indicator variable Y_-9 = 0 if Y &lt;&gt; -9 and 1 if Y=-9 and have put missing data value in original Y. Then I have used incomplete data training algorithm to learn conditional mean E[Y|x] and conditional probability Pr(Y is -9|x) models. However I have done this mostly with unsupervised (clustering) algorithms. With supervised models it may get a bit more tricky unless perhaps if using Bayesian modelling.</p>
3599, <p>Hi</p> <p>I am using R 3.0.2 64-bit on Windows 7 and had first a bit problems reading in the Russian data using it (I have been told UTF R read.csv etc works much better/easier at Linux/Macintosh).</p> <p>I noticed that easiest way to read Russian UTF8 to R is to change locale first to Russian and then just do read.csv with UTF-8 encoding (without locale changing it was always failing as my normal locale in Windows is Finnish). This can be done as:</p> <p># Read first 10 rows<br>Sys.setlocale('LC_ALL' 'russian');<br>d2 &lt;- read.csv(&quot;avito_test.tsv&quot; encoding=&quot;UTF-8&quot; sep=&quot;\t&quot; header=TRUE nrows=10)</p> <p>Update:<br>- Above has issue that reading of data breaks (when going to say nrows=1000). I thought changing UTF-8 to UTF8 helped but it did not it reads all in but data in cyrillic font is totally broken.</p> <p>So any help how to get data loaded in at R is appreciated :)</p>
3599, <p>Solved above issue - loading with locale set to Russian only without encoding specification seems to be working. However cyrillic characters are just displayed totally wrong - atleast on my machine.</p> <p>Below is R sample code if someone has any use for it. It does not score well but is just simple example. It does not use title keywords in modelling just category subcategory price and urls_cnt.</p> <p><code># Load in data<br>setwd(&quot;H:\\kaggle\\avito_prob_Content&quot;)<br></code><code>Sys.setlocale('LC_ALL' 'russian');<br>dTrain &lt;- read.csv(&quot;avito_train.tsv&quot; sep=&quot;\t&quot; header=TRUE &nbsp;nrows=1562937)<br><br></code><code># Pre-process (replace NA values by zeroes)<br>dTrain$price[is.na(dTrain$price)] &lt;- 0<br>dTrain$is_blocked[is.na(dTrain$is_blocked)] &lt;- 0<br>dTrain$urls_cnt[is.na(dTrain$urls_cnt)] &lt;- 0<br><br></code><code># Create new fields: log price zero price indicator and <br># merged category field<br>dTrain$logPrice &lt;- log(1+dTrain$price)<br>dTrain$zeroPrice &lt;- as.integer(dTrain$price == 0)<br>dTrain$mergedCat &lt;- paste(dTrain$categorydTrain$subcategory sep=&quot;/&quot;)<br><br></code><code># compute E[is_blocked|categorysubcategory] to lookup &quot;table&quot;&nbsp;<br>ts1 &lt;- tapply(dTrain$is_blocked dTrain$mergedCat mean)<br>categories = as.numeric(ts1)<br>names(categories) = rownames(ts1)<br><br></code><code># Prepare training data<br>dSample &lt;- data.frame(is_blocked=dTrain$is_blocked)<br>dSample$mergedCat &lt;- dTrain$mergedCat<br>dSample$logPrice &lt;- dTrain$logPrice<br>dSample$urls_cnt &lt;- dTrain$urls_cnt<br>dSample$mergedCatScore &lt;- categories[dSample$mergedCat] <br>dSample$Z1 &lt;- dSample$logPrice * dSample$mergedCatScore <br><br></code><code># Train logistic regression model<br>glm.out1 = glm(is_blocked ~ mergedCatScore + logPrice + urls_cnt + Z1 family=binomial(logit) data=dSample)<br><br></code><code># Evaluate model (just visually)<br>outputData &lt;- data.frame(is_blocked=dSample$is_blocked estimated=fitted(glm.out1) mergedCat=dSample$mergedCatScore)<br>sortedData &lt;- outputData[order(-outputData$estimated<br>)]<br>cs1 &lt;- cumsum(sortedData$is_blocked)<br>plot(cs1col=&quot;#880088&quot;cex=0.1xlim=c(0500000))</code><code>&nbsp;<br><br></code><code># Load test data in<br>dTest &lt;- read.csv(&quot;avito_test.tsv&quot; sep=&quot;\t&quot; header=TRUE)<br><br></code><code># Pre-process test data similarly as dTrain<br>dTest$price[is.na(dTest$price)] &lt;- 0<br>dTest$urls_cnt[is.na(dTest$urls_cnt)] &lt;- 0<br>dTest$logPrice &lt;- log(1+dTest$price)<br>dTest$zeroPrice &lt;- as.integer(dTest$price == 0)<br>dTest$mergedCat &lt;- paste(dTest$categorydTest$subcategory sep=&quot;/&quot;)<br><br></code><code># Save R objects (they are faster to load and compressed) which<br># are not used in this example but in case you want to use later.<br>save(dTest file=&quot;dTest.dat&quot;)<br>save(dTrain file=&quot;dTrain.dat&quot;)<br><br></code><code># Check for covariate shift just via numbers / eyes :)<br># (if there is such then that is indication that weighted logistic <br># regression should be applied)<br>summary(dTrain$urls_cnt)<br>summary(dTest$urls_cnt)</code><code>summary(dTrain$logPrice)<br>summary(dTest$logPrice)<br># -&gt; tail of dTest$urls_cnt is longer than dTrain$urls_cnt but <br># otherwise no clear covariate shift observed<br></code><code><br># Prepare test data<br>dSample &lt;- data.frame(mergedCat=dTest$mergedCat)<br>dSample$logPrice &lt;- dTest$logPrice<br>dSample$urls_cnt &lt;- dTest$urls_cnt<br>dSample$mergedCatScore &lt;- categories[dSample$mergedCat] <br>dSample$Z1 &lt;- dTest$logPrice * dSample$mergedCatScore<br><br></code><code># Do predictions<br>output &lt;- predict(glm.out1type=&quot;response&quot;dSample)<br></code><code>outputData &lt;- data.frame(id=dTest$itemid estimated=output)<br>sortedData &lt;- outputData[order(-outputData$estimated<br>)]<br>sortedData$estimated &lt;- NULL<br><br></code><code># write output to file<br>write.csv(sortedData&quot;submission1.csv&quot;row.names=FALSE)</code><code><code><br></code></code></p>
3599, <p>It may be related to Windows operating system (or Windows version of R). Namely I also managed to display data correctly. However only if I limited loading of data to first few hundred records (after then comes some unicode that my R version cannot handle). Anyways with correctly displaying data d  if I write edit(d) then I see that R is not using UTF8 font. It displays cyrillic characters incorrectly.</p> <p>However in my experiments I found out that even using incorrectly displaying data the things are working. Naturally it is impossible to read text - that is why I have limited only to category and subcategory text columns.</p> <p>If possible try R under Linux or Macintosh I have been told that unicode support / fonts are there working much easier. However I have no personal experience with that.</p>
3599, <p>If you are using Windows and want to batch resize images (probably to smaller size) then you can use following nice script:<br>http://www.sergeyv.com/Projects/ResizeImageScript.aspx</p> <p>To get it &amp; run it do the following:<br>1. Download link at the web page does not work at least in my browser. Therefore just copy paste the script at the end of page (starting from row <strong>&lt;job id=&quot;ResizeImage&quot;&gt;</strong>) to file&nbsp;resize_image.wsf</p> <p>2. Modify resize_image.wsf file row&nbsp;<br><strong>var fileMask = &quot;jpg&quot;</strong> <br>to&nbsp;<br><strong>var fileMask = &quot;jpeg&quot;</strong></p> <p>3. Then create&nbsp;to script folder run.bat file containing following:<br>resize_image.wsf /indir:&quot;C:\pictures&quot; /width:512 /height:512</p> <p>where c:\pictures must be folder containing jpeg files (it does not process sub-folders) and width and height are wanted target resolution.It creates subfolder c:\pictures\512x512 so it does not overwrite existing files. I have found out that approximately starting from 512x512 resolution there is enough information to do some &quot;accurate&quot; modelling :)</p> <p>4.&nbsp;Execute run.bat file at the folder</p>
3599, <p>I first did 9 x logistic regression models (one per output class). Then I wanted to try multinomial logistic regression. However it looks like that is performed a lot worse than 9 independent logistic models.</p> <p>I guessing&nbsp;that there is convergence problem and/or Hessian matrix has too many parameters. (I am using all the features). Did anyone else try multinomial logistic regression &quot;successfully&quot;?</p>
3599, <p>Hi I am just getting:<br>- 0.9 multinomial regression (for this I am using the multinom function from nnet neural network R package)<br>- 0.72 with 9 separate logistic regression (for this I am using the normal glm fit in R)</p> <p>I am not using any penalties although I probably should be - especially with the multinom regression.</p>
3599, <p>Hi</p> <p><br>I am using R nnet package:&nbsp;http://cran.r-project.org/web/packages/nnet/nnet.pdf<br>Also I am not using penalty.</p>
3599, <p>I am using very stiff linear model and I am not accepting any predictor which is not significant or which results in more than 0.6 ratio in Std Error(Coefficient)/abs(Coefficient estimate). I started with 0.3 ratio limit but it was rejecting almost all :)</p> <p>Naturally this does not avoid collinearity (except a bit indirectly) which needs to be handled separately.</p>
3599, <p>[quote=jmwoloso;74613]</p> <p>[quote=sfin;74319]</p> <p>I am using very stiff linear model and I am not accepting any predictor which is not significant or which results in more than 0.6 ratio in Std Error(Coefficient)/abs(Coefficient estimate). I started with 0.3 ratio limit but it was rejecting almost all :)</p> <p>Naturally this does not avoid collinearity (except a bit indirectly) which needs to be handled separately.</p> <p>[/quote]</p> <p>Collinearity isn't bad if all you care about is whether the prediction is correct or not (which we do)...if you need to explain the features (which we don't) then collinearity might be an issue.&nbsp;</p> <p>[/quote]</p> <p>In my opinion collinearity can be very bad given very small sample size (with larger sample size it could be ignored better).</p>
3599, <p>&quot;...the standard errors for one or more of the coefficients <strong>can be large</strong> but that's it.&quot;<br>I think that can be large is the exactly what I was referring. If your linear prediction at point X=x is&nbsp;is Beta * x then the variance of prediction is naturally VAR[Beta * x] = x^T VAR[Beta] * x. And when one diagonal elements in VAR[Beta] matrix is large then your prediction variance will easily explode and generalization can be bad. Also with this small sample size the generalization error (log error) variance estimates have itself very high variance therefore I would be careful with any coefficients with large standard errors.</p>
3599, <p>[quote=I can do the splits NO PROB;74827]</p> <p>[quote=sfin;74824]</p> <p>&quot;...the standard errors for one or more of the coefficients <strong>can be large</strong> but that's it.&quot;<br>I think that can be large is the exactly what I was referring. If your linear prediction at point X=x is&nbsp;is Beta * x then the variance of prediction is naturally VAR[Beta * x] = x^T VAR[Beta] * x. And when one diagonal elements in VAR[Beta] matrix is large then your prediction variance will easily explode and generalization can be bad. Also with this small sample size the generalization error (log error) variance estimates have itself very high variance therefore I would be careful with any coefficients with large standard errors.</p> <p>[/quote]</p> <p>Exactly! (and that is precisely the reason why one should rather use Ridge regression in such situations (collinearity)).</p> <p>[/quote]</p> <p>Indeed ridge lasso or perhaps elastic net. Btw do you have any good articles etc about doing&nbsp;ridge regression with correlated binary indicator predictors (preferably non Bayesian version)?</p>
3599, <p>You can map city names to GDP PPP$s using this source:<br>http://commons.wikimedia.org/wiki/File:Turkey_per_capita_income_by_province_2011.svg</p> <p>Then you will get mostly rid of problem that there are city names values in test set not found in the training set. Then creating some indicators e.g. above or below average GDP levels might work. However those have not improved my models much.</p>
3599, <p>I have tried it and at least for me it seems to be working well.</p>
3599, <p>Well you probably have noticed that correlation(median_relevancerelevance_variance) &lt; 0 and value is near -0.3 or so if I recall correctly.</p> <p><strong>How to use SD in prediction:</strong><br>A. Built model 1 to predict relevance variance using predictors X (set of variables)<br>B. Built model 2 to predict median relevance using predictors Z (set of variables) and using relevance variance estimate from model 1.</p> <p><strong>Remarks:</strong><br>- Depending how you fit model 1 and model 2 and select X and Z you may have problems with multicollinearity (estimate of relevance variance from model 1 may be strongly correlated with some/many components in predictors Z of model 2).<br>- I am not saying that above approach works well but you can try it if you like :)</p>
3599, <p>Just wondering if anyone has had suggess with k-Nearest neighbour regression (or classification)?</p> <p>I tried k-NN regression (and &quot;intelligent&quot; continuous output to class post rounding). However I always run into problem that k-NN overfits to data no matter which k value I choose. This might be because I am using lots of binary indicators and not that many continuous variables.</p> <p>Compared to linear regression MARS (Multivariate Adaptive Regression Splines) or MLP the kNN gives&nbsp;very bad results in my tests.</p>
3599, <p>Ok I will probably stop&nbsp;focusing on that algorithm :)</p>
3599, <p>[quote=RosaCaponnetto;78656]</p> <p>In the train excel sheet I can find median relevance and relevance variance; but in the test sheet there isn't what can I do?</p> <p>[/quote]</p> <p>Median relevance is the thing that you need to predict and variance is not either in the test sample. I tried a two step model as follows:</p> <p>STEP 1. predict relevance variance&nbsp;<br>STEP 2. predict median relevance given relevance variance estimate (from step 1) + various other predictors.</p> <p>However at least in my step 2 model the relevance variance did not improve predictions too much.</p>
3602, ----
3611, <p>if you want to learn basic of machine learning you may want to take a look at 'Machine Learning' class by 'Andrew Ng' @ https://www.coursera.org/</p>
3611, <p>you can look at hierarchical softmax / Error-Correcting Output Codes this will help you if you don't want to train 1000 one-vs-rest models. Hope this is what you want if it is about which specific algo (svm / logistic etc) that will depend on data characteristics</p>
3611, <p>Few suggessions (assuming that you would be using R for this analysis):</p> <p>1) to find relevent terms you can manualy build preliminary list</p> <p>2) fetch data from twiter using R package &quot;twitteR&quot; for such terms</p> <p>3) use setiment analysis to find out twittes which are convey negative sentiments. you can use tool available on this link: http://www.nactem.ac.uk/opminpackage/opinion_analysis</p> <p>4) use these twittes and R package &quot;tm&quot; to find new terms associated with terms which were manualy identified to build compehensive list</p> <p>5) then you can again fetch twittes for terms in new list and do sentiment analysis</p> <p>6) you can plot trends and use R package &quot;wordcloud&quot; if you wish to incorporate wordcloud in your report</p>
3611, <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p><span style="line-height:1.4em">in #1 i was trying to say you need to do manual search for terms related to tech support on twitter to begin with then you can work on points 234 to build comprehensive list of terms.<br> </span></p> <p>&nbsp;</p> <p>&nbsp;</p> <p><span style="line-height:1.4em"><br> </span></p>
3611, <p>Data is like a shaggy dog. It needs grooming every now and then!</p>
3611, <p>For the members I make it the following:<br> 51482 Females<br> 43966 Males<br> 17552 Unspecified<br> 113000 Total</p>
3611, <p>Anyone who studies the Milestone descriptions cannot but be impressed by the cleverness the persistence and the comprehensiveness of the methods used. As time goes on you see the best scores on the leaderboard shaving 0.001 or thereabouts off the previous  best score. However it occurs to me that these methods will not be practical in real life situations because they are a composite of several algorithms some of which take considerable time to compute. Is the situation going to arise where the method(s) that  finally win accordin to the RMSLE objective function will NOT be the ones actually used in practice? To me the rules seem to have assumed that each entry will be based on ONE algorithm. Has this issue been discussed anywhere in the forum?</p>
3611, <p>Along the same lines has it been established that all members in the members table were members during year 1?</p>
3611, <p>Surely any serious method of predicting hospitalisation would use the patient's weight Body Mass Index and smoking history as well as age and sex. </p>
3611, <p>In another thread called &quot;2nd Milestone Winners&quot; I posted the following now 24 days ago:-</p> <hr> <p>Anyone who studies the Milestone descriptions cannot but be impressed by the cleverness the persistence and the comprehensiveness of the methods used. As time goes on you see the best scores on the leaderboard shaving 0.001 or thereabouts off the previous  best score. However it occurs to me that these methods will not be practical in real life situations because they are a composite of several algorithms some of which take considerable time to compute. Is the situation going to arise where the method(s) that  finally win according to the RMSLE objective function will NOT be the ones actually used in practice? To me the rules seem to have assumed that each entry will be based on ONE algorithm. Has this issue been discussed anywhere in the forum?</p> <hr> <p>So far no one has responded to this comment on that thread. Since then I have searched and read the forum to see if the issue of a single algorithm has been discussed but found nothing. And I have re-read the Rules a few times. It is noteworthy that the  rules expect a single algorithm.<br> For example:<br> In the Introduction:-<br> &quot;The winning team will create an algorithm...&quot;<br> In Rule 5:-<br> A &quot;Prediction Algorithm&quot; is the algorithm used to produce the data in an Entry taken as a whole (i.e. its particular total configuration) but does not include individual components of the Prediction Algorithm or tools used for analysis or development of the  Prediction Algorithm.</p> <p>In Rule 12:-<br> Entries will be judged based on the degree of accuracy of their Prediction Algorithm’s predictions of DaysInHospital for Y4... </p> <p>Once an Entry is selected as eligible for a prize the conditional winner must deliver the Prediction Algorithm’s code and documentation ...</p> <p>If this issue has been settled and I have missed it would someone (Kaggle Admin?) tell me where and what the conclusion was.</p>
3611, <p>Thanks guys (ChipMonkey DavidC and Chris Raimondi) for your comments. Let me say right away that of course I accept Kaggle Admin's ruling as to whether the milestone winners' methods comply with the rules. (I might use the same techniques myself if my  methods get good enough.)<br> However I still disagree that a combination of several algorithms should be considered &quot;AN algorithm&quot;. It should be called &quot;a method&quot;. Have a look at Market Makers' paper describing their method for their winning round 1 milestone.<br> Here is what they write:-</p> <hr> <p>There were four underlying algorithms used in our models all of which are freely available in the R language for statistical computing. Online references for each algorithm are given in the hyperlinks below.</p> <ol> <li>Gradient Boosting Machines ... </li><li>Neural Networks... </li><li>Bagged Trees... </li><li>Linear Models... </li></ol> <hr> <p>I have deleted the hyperlinks for clarity. Market Makers go on to use &quot;ensembling&quot; which blends the results of the different algorithms.<br> Surely you guys wouldn't argue that a combination of four such different algorithms is AN algorithm. If you do I would have to give away my geographical location and say you are a mob of &quot;bush lawyers&quot;!</p>
3611, <p>Oleg My definition of an algorithm is the same as the dictionary definitions I have seen which are all singular ie refer to &quot;a procedure&quot; or &quot;a method&quot;. Ok I can see that some people could classify a procedure that embodies the use of four totally distinct  algorithms (eg Market Makers' milestone 1 solution) as fitting this definiton and is therefore &quot;an algorithm&quot;. It is just that that is not my position.</p>
3611, <p>Guys I only joined Kaggle a few days ago and thought I would try to come up to speed on this credit problem for practice. Then I discovered what surely must be dirty data yesterday. I thought what seemed to be missing decimal points in colum 'C' might have  been an artifact of the download but a second download revealed the same data of course. It would be good if Kaggle Admin would make some statement whether the data really is dirty and we have to live with it or not! I don't have time to clean it up now so  I am going to look at a problem with more time until it closes. </p>
3611, <p>In real life surely the volume of securities exchanged at a trade would be important and used for prediction of recovery.</p>
3611, <p>I notice in the diagram &quot;Liquidity Replenishment.jpg&quot; posted by Capital Markets CRC that bid and ask prices are the same at a trade (of course as they should be). But in the testing data when a trade takes place eg at time 49 the bid is always less than  the ask. I haven't looked at the training data (I'm almost not game to download such a big file) and it may be different.<br> Further the p_value in row 2 for security 13 is 6963537884 pounds. Should the data dictionary say pence for this column?</p>
3611, <p>Thanks William and Christopher.</p>
3611, <p>Hi William (or anybody)&nbsp; I am starting to get the hang of this!&nbsp; Am I correct in thinking if the trade is initiated by the buyer (initiator = 'B') then the trade price is the ask price ie ask49 and if the trade is initiated by the seller (initiator = 'S')  then the trade price is the bid price ie bid49?</p>
3611, <p>Many thanks Christopher. I wonder what these guys pay the exchanges to let them do all these shenanigans. Not what I would pay I bet.</p>
3611, <p>I have a question about the data. Comparing the values in the column 'trade<em>vwap' with the values in the column 'trade</em>volume' you see that the values in 'trade<em>vwap' do not vary much while those in 'trade</em>volume' do. This doesn't make sense  if the meaning of these values is as given in the data dictionary. Since each row relates to a different trade shock event shouldn't values in 'trade<em>vwap' vary like those in 'trade</em>volume'. So what are the values in 'trade_vwap'? </p>
3611, <p>Quick. At this late stage I think I am having trouble unpacking &quot;training.zip&quot;.&nbsp; Can anyone tell me how many rows there are?&nbsp; I get variable result around 544111 to 544351 !!</p>
3611, <p>On the question of profitability it would be helpful if someone would tell us what are the typical financial arrangements between an exchange and a HFT organisation. I understand the exchange pays the HFT organisation for keeping the market &quot;ticking over&quot;.  They certainly could not afford to pay the brokerage of a retail trader.</p>
3611, <p>Hi Kaggle Admin&nbsp; When are the winning methods going to be published?</p>
3611, <p>Congratulations to all the winners and thanks to the sponsors and Kaggle for an interesting competition.&nbsp; What a marvellous high score the winners achieved!&nbsp; My approach was to use 9 bandpass filters from 70Hz to 310Hz and record their magnitude and time  of occurrence.&nbsp; Then I fed that data into a neural net.&nbsp; Although on the face of it you would think these data carried nearly all the important information I was surprised to find it scored less than all zeros!!!&nbsp; With the shortening of the time to finish  I did not have time to go any further.&nbsp; I used basic desktop languages.&nbsp; Looks like I will have to get R or something.&nbsp; Could I plead for someome to compile a list of acronyms so I can look them up and see what hey are and do.</p>
3611, <p>Isabelle&nbsp; Is that a typo at the beginning of</p> <p>- A is a cause of A means A=f(B noise) [denoted A &lt;- B]</p> <p>in your response to LoveSaves?</p>
3611, <p>What do you gurus say if we substitute an altimeter for a thermometer -- the type of altimeter that is based on air pressure.&nbsp; Then we use the altimeter to determine the height of the city.</p>
3611, <p>It is my understanding that the data in this competition are not time-series.</p>
3611, <p>The rules for this competition are the messiest of all the competitions I have been interested in.&nbsp; The framework setting up the competition&nbsp;is also messy some being outside the Kaggle site.&nbsp; Early contributions to the forum by the organisers contained  confusing typos.&nbsp; Altogether in my opinion it has not been up to Kaggle standard.</p> <p>Nevertheless it is a very interesting problem.&nbsp; I don't expect to score high enough to have to submit my solution but since I am using 'fossil' desktop languages to do the computing I doubt that the organisers would be able to run&nbsp;my programs anyway.</p>
3611, <p><span color="#000000" style="color:#000000"><span face="Times New Roman serif" style="font-family:'Times New Roman'serif"></span></span></p> <p>Like I have written previously this competition is a mess.&nbsp; Kaggle Admin should take control of it.&nbsp; Where are you Kaggle Admin?</p> <p><span color="#000000" style="color:#000000"><span face="Times New Roman serif" style="font-family:'Times New Roman'serif"></span></span></p>
3611, <p>I agree with Sali Mali that some Kaggle competitions have been very unstable in the description the data and the rules for a time after they have been launched.&nbsp; I sympathize with the people setting this competition because the&nbsp;flight of a commercial aircraft is a very complex&nbsp;phenomenon.&nbsp; (If this competition wasn't so interesting I would put it away for a couple of weeks and hope things were more stable when I come back!)&nbsp;</p> <p>I fancy this competition is too big for my humble desktop and my fossil languages so I am making the following observation in case it might be useful to someone else.&nbsp; The structure of the flight would seem to be amenable to 'Dynamic Programming'.&nbsp;&nbsp;Further if the simulator is to be used to evaluate the cost function of answers submitted and if it accepts starting point&nbsp;and destination information&nbsp;and if it is made available for use by competitors then it could be used to evaluate the cost function at intermediate points and using Dynamic Programming give an answer that should be the best in the competition.</p> <p>Am I wrong?</p>
3611, <p>Has anyone noticed there are 89 flights (rows) in 'testFlightsRev3.csv' with zero ground speed -- all above 21000 ft altitude.</p> <p>Also the flight with FlightHistoryId = 303137309 has no history record in&nbsp;'test1_flighthistory.csv'.</p> <p>Have these two issues been discussed anywhere?&nbsp;</p>
3611, <p>Am I the last or the first to find out that you don't have to submit all 14989 flights (testFlightsRev3.csv) to the evaluation process?&nbsp; Is this deliberate?&nbsp; If so it enables 'gaming' the system by submitting only those flights that have the best results.&nbsp; This doesn't seem compatible with the competition's objectives.</p>
3611, <p>Thanks Alessandro.&nbsp; I should have realised your explanation.&nbsp; However I'm sure all my flights finish within 75 miles so I don't know why my submission(s) result in such terrible scores.</p>
3611, <p>Hi Admins&nbsp; Is it still the case that the submitted flights as in the Milestone Phase&nbsp;need only go to within 75 miles and &lt;= 18000 ft of the arrival airport?&nbsp; Then the simulator takes over performs the landing and adds the extra cost to that of the submitted flight?&nbsp;</p>
3611, <p>Hi Admin&nbsp; What has happened to the data page?&nbsp; On Oct 15 there were the following (amongst others):</p> <p>- testFlightsRev3.csv</p> <p>- sampleSubmissionRev2.csv</p> <p>- FlightQuest Simulator-Rev4.zip</p> <p>&nbsp;</p> <p>testFlightsRev3 is referred to&nbsp;under 'basic structure of fq2/Getting Started' but I can't find it anywhere now.&nbsp; What&nbsp;has happened (or am I very confused)?&nbsp; Where is it?&nbsp;</p> <p>Also on the data page in the Guide to Files the last bullet point:</p> <p>-test1_flighthistory_rev1: revised version of file flighthistory.csv. See this <strong>forum thread</strong> for an explanation of the revision.</p> <p>The link under the words <strong>'forum thread'&nbsp;</strong>just takes you back to 'Competition Details'!</p>
3611, <p>Thanks joycenv.&nbsp; I have the&nbsp;current files but it was the out-of-date descriptions that confused me.&nbsp; It is a complex problem so anything you can do to&nbsp;make the description accurate would be helpful.</p>
3611, <p>I'm afraid my download of 'FQ2DataRelease2_AggregateTrain' (.zip version) stopped at 99% with 1 second remaining).&nbsp; Would it possible to break this file up into smaller pieces please.</p> <p>Thanks in anticipation.</p> <p>A second try was successful.</p> <p>&nbsp;</p>
3611, <p>Hi Charango&nbsp; This rule was mentioned in Basic Structure of Flight Quest2/Getting Started/Paragraph 5 where it says</p> <p>&quot;One constraint worth mentioning here is the maximum allowed number of instructions for each flight is 200 instructions&quot;</p> <p>Good Luck</p>
3611, <p>Anil airspeed is not an input in the ascent model. Fuel consumed seems to depend on altitude and weight.&nbsp; I took it that the ascent model describes&nbsp;the typical aircraft in&nbsp;ascent and for a given weight the fuel consumed depends on altitude reached.&nbsp; It would be possible with some algebra (and possibly some differentiation) to determine the airspeed at a particular altitude but this is not in the ascent model.&nbsp; Have I missed something?</p>
3611, <p>Thanks Anil.&nbsp; I'm afraid I have missed the subtleties of line 37 in the FuelModel.&nbsp; id speculated on the reality of the ascent/descent/ascent etc path.&nbsp; I have been told that in certain motor competitions to get the maximum distance for a given amount of fuel&nbsp;the optimum is to flatten the accelerator to get to a high speed and then to coast and then to repeat this.&nbsp;&nbsp;The efficiency of the internal combustion engine (work done per unit of fuel) is greatest under high load.&nbsp; I don't know much about the efficiency of a jet engine but it is conceivable it has this sort of characteristic.&nbsp; Not sure the passengers would think much of it!</p> <p>&nbsp;</p>
3611, <p>To all those interested in the peculiarities of the fuel consumption for the GE model of a typical commercial aircraft please see the attached graph.&nbsp; Fuel consumption (lbs/hr) is plotted on the y-axis and altitude (ft) on the x-axis.&nbsp; Series 1 2 and 3 are for cruising at 300 400 and 500 knots respectively.&nbsp; Series 4 is for the aircraft ascending.&nbsp; The aircraft weight is 130000 lbs.&nbsp; All points on the ascent curve are for an aircraft weighing this amount.</p> <p>The calculation ignores line 37 in FuelModel.fs <br>&quot;let ascentFuelBurn = max rawAscentFuelBurn cruiseFuelBurn&quot;</p> <p>It appears that the model has a peculiarity in that at low altitudes ( 10000 to 12000 ft)&nbsp; the ascent fuel burn rate is less than the cruise fuel burn rate for an aircraft cruising at 500 knots.&nbsp; There are probably intersection points for lower altitudes and lower speeds also.</p>
3611, <p>Thanks joycenv.&nbsp; That's a nice improvement.&nbsp; (But now FQ2 competes with Christmas!!!)</p>
3611, <p>skwales nick</p> <p>According to the competition 'Description'</p> <p>Work performed during unsanctioned hours decrease an elf's productivity:</p> <p>p=p&nbsp;&#8242;&nbsp;&#8727;(0.9)&nbsp;m&nbsp;&nbsp;</p>
3611, <p>Nick&nbsp; I am referring to skwales' para</p> <p>The above are the *minimum* required work durations that the elves have to work based on their productivity.&nbsp; <strong>Since productivity increases while an elf works during sanctioned time</strong> one could try making the elf &quot;pad&quot; its work duration in order to boost productivity say while waiting for the next toy to arrive.&nbsp; The rules are designed to prevent this &quot;padding&quot; to artificially boost productivity.</p> <p>I agree with your first post about the ambiguities.&nbsp; I have struck such issues in previous Kaggle competitions.&nbsp; The rules are not gone over carefully enough.&nbsp; Perhaps there is more than one person editing the rules.</p>
3611, <p>apologies to Nicholas Hamilton and skwales for my comments above.&nbsp; I was confused about sanctioned and unsanctioned.</p>
3611, <p>Everything looks correct to me too.&nbsp; What does elf 728 do next?</p>
3617, ----
3634, <p>I am interested as well.</p>
3634, <p>RIP Lucas</p>
3634, <p>Hi bosie<br> Yeswe are in need of a programmer.Can you please tell me your skills?</p> <p>Vikram</p>
3634, <p>Hi Bosie<br> can you tell me your skype ID?<br> I need to talk to you.</p> <p>Vikram</p>
3634, <p>Hi Bosie<br> I cant find you on skype.Can you add me on skype?<br> My id is vikramjha89.</p> <p>vikram</p>
3634, <p>Hi DMK<br> Can you add me on skype?<br> My id is vikramjha89.</p> <p>vikram</p>
3638, ----
3641, <p>Apologies for resurrecting this thread.&nbsp;</p> <p>Note that if you have registered with a gmail address containing periods but signed up with gravatar with the same gmail address lacking periods it will not 'match' email addresses. In my case removing the periods from my Kaggle registered gmail address allowed my gravatar to update.</p>
3641, <p>Having the same issue inviting a team member requires their email address and I don't have enough points to do so. Will email b@kaggle.com.</p>
3641, <p>For Python I've found Christian Borgelt's FIM library to be robust and has a variety of association set mining algorithms. See: <a href="http://www.borgelt.net/pyfim.html">http://www.borgelt.net/pyfim.html</a>. You can pip install from the tarball.</p>
3645, ----
3649, <p>Hello everyone</p> <p>I have just started using R a few days ago and run into a problem. I hope someone with a bit more&nbsp;experience in R can help me. I'm using cforest (from party package) with</p> <p><code>controls=cforest_unbiased(ntree=1600 mtry=5 maxdepth=19)</code></p> <p>The goal is classification in two classes 0/1.</p> <p>My dataset is around 300k examples and 30 features the whole data consists only of integers. I have trained a RandomForestClassifier (from sklearn python) with the same dataset and for much larger forests (at max it took around 2G of RAM)&nbsp;</p> <p>But in R the process is constantly being killed for using all of my memory (which is 32G). One more thing that I tried is building a forest with only 10 trees and it still uses all of my memory.</p> <p>Am I doing something wrong is this too much data or?</p> <p>I'm using R v 3.1.0 and&nbsp; party_1.0-13 on a ubuntu x64 machine.</p>
3649, <p>Hi </p> <p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; I &nbsp;was wondering how much time it took for you to run(train) &nbsp;the word2vec model. I installed cython and it is taking 4 hours for me and did not see any output yet.&nbsp;</p>
3649, <p>[quote=Amine Benhalloum;69211]</p> <p>Actually I am using a Random search it gives me great results in way less time.</p> <p>[/quote]</p>  <p>Can you tell me which package do you use (python -sklearn &nbsp;  R packages &nbsp;or something else ) &nbsp;or you are using your own implementation ?</p> <p>Thanks :-)</p>
3649, <p>I was trying to use xgboost cv(python) &nbsp;and I am doing the following steps:</p> <p>1.)Loading the tables dtrain dtest &nbsp;( both are in libfm format)&nbsp;</p> <p>2.) &nbsp;Initializing the parameters&nbsp;</p> <p># setup parameters for xgboost<br>param = {}<br>param['objective'] = 'multi:softprob'<br>param[&quot;eval_metric&quot;] = &quot;mlogloss&quot;<br>param['eta'] = 0.1<br>param['gamma'] = 2<br>param['max_depth'] = 27<br>param['min_child_weight']=3<br>param['max_delta_step'] = 4<br>param['subsample']= 0.75<br>param['colsample_bytree']=0.85<br>param['silent'] = 1<br>param['nthread'] = 4<br>param['num_class'] = 9</p> <p>bst_cv = xgb.cv(param dtrain num_round=10 nfold=3 );</p>  <p>Initially I got &quot;mlogloss&quot; is not valid error. &nbsp;So I changed it to &quot;logloss&quot; . My output verbose is :</p> <p>[0] cv-test-logloss:nan+nan cv-train-logloss:nan+nan<br>[1] cv-test-logloss:nan+nan cv-train-logloss:nan+nan<br>[2] cv-test-logloss:nan+nan cv-train-logloss:nan+nan<br>[3] cv-test-logloss:nan+nan cv-train-logloss:nan+nan</p>  <p>Also I am not sure if i can use lists in my cross validation for tuning eg.&nbsp;param['eta'] = [0.1 0.20.3] or should it be a number and i need to use for loops ?</p> <p>In the code after&nbsp;xgb.cv()  most of them are again using xgb.train() immediately . I do not see they use xgb.cv for anything else. I am totally confused how to interpret the whole thing. Can anyone share an clear example &nbsp;how to use xgb.cv() and then how to train after tuning.&nbsp;</p>  <p>Thanks a lot</p> <p>Guru</p>
3649, <p>[quote=TomHall;67623]</p> <p>[quote=larry77;67622]</p> <p>[quote=TomHall;67452]</p> <p>The attached benchmark code includes a 3-fold cross-validation step and the usual training-prediction procedure. The 2nd step cost me <strong>less than a minute</strong> on an 8-core CPU.</p> <p>To install the R-package of xgboost please first install the latest version of <a href="http://cran.r-project.org/web/packages/devtools/index.html">devtools</a> then run</p> <p><code>devtools::install_github('dmlc/xgboost'subdir='R-package')</code></p> <p>Windows users may need to install&nbsp;<a href="http://cran.r-project.org/bin/windows/Rtools/">RTools</a> to compile.</p> <p>XGBoost also offers <a href="https://github.com/dmlc/xgboost/tree/master/demo/guide-python">python interface</a>. A detailed guide is here:<a href="https://github.com/dmlc/xgboost/tree/master/demo/kaggle-otto">https://github.com/dmlc/xgboost/tree/master/demo/kaggle-otto</a></p> <p>For more information of XGBoost please visit <a href="https://github.com/dmlc/xgboost">https://github.com/dmlc/xgboost</a>.</p> <p>[/quote]</p> <p>Thanks again for your script! There is one part which puzzles me: at some point you carry out a cross validation</p> <p># Run Cross Validation<br>&nbsp;cv.nround = 50<br>&nbsp;bst.cv = xgb.cv(param=param data = x[trind] label = y <br>&nbsp;nfold = 3 nrounds=cv.nround)</p> <p>but bst.cv is no longer used in the rest of your code and if I remove those lines the final predictions are identical. Is it just a &quot;leftover&quot; of a more fine-tuned version of the script?</p> <p>Many thanks</p> <p>Larry77</p> <p>[/quote]</p> <p>Generally we suggest to trust the cross-validation more than the leaderboard. Therefore the cv part is used to tune the model.</p> <p>[/quote]</p>  <p>So In general how do you use bst.cv after that step. &nbsp;When u run xgb.cv  does it store the best parameters in param and run it automatically ?</p>
3656, ----
3670, <p>can you tell us the exact version numbers of numpy pandas python etc you used in 64-bit?</p>
3670, <p>Will you guys post/share&nbsp;the code?</p>
3670, <p>nice</p>
3670, <p>Added a sample script to estimate homogrpahy between two images using SIFT. I will add using Kanade-Tomasi soon. <a href="https://www.kaggle.com/asymptote/draper-satellite-image-chronology/homography-estimate-stitching-two-imag">https://www.kaggle.com/asymptote/draper-satellite-image-chronology/homography-estimate-stitching-two-imag</a></p>
3670, <p>It seems SIFT and SURF are note available on Kaggle scripts which makes sense that SIFT and SURF are examples of algorithms that OpenCV calls &#8220;non-free&#8221; modules.</p>  <p>I am curious if it's possible to install those.</p>
3706, <p>[quote=Bruno Santos;37635]</p> <p>Hello everyone<br>I am facing a similar problem. What can I do to get a team together using kaggle?<br>thank you.<br>Regards<br>Bruno</p> <p>[/quote]</p> <p>Go to my team when you are competing in a competition. Click on &quot;My Team&quot;. Add a person to your team by his/her Kaggle email address.</p>
3706, <p>the multi-modal gesture recognition ended yesterday. first everyone got a rank 1 in that competition and points were awarded according to the rank(that was 1 for all). The ranking has been fixed but not the points. It seems to be some bug and I think Kaggle people are working on it. :)</p>
3706, <p>This post deserves a reply from a Kaggle Admin</p>
3706, <p>yeah. now it seems fixed :D</p>
3706, <p>same problem with me. works on my mac with chrome but not on my android&nbsp;</p>
3706, <p>are these knowledge competitions never going to end?</p>
3706, <p>please see the attached screenshot. this happens when I create a new topic in the forums. Forum name is not displayed properly</p>
3706, <p>All members are allowed to make submissions. The maximum number of submissions per day will be the maximum number of submissions for the whole team.</p>
3706, <p>IMO yes</p>
3706, <p>In the introductory e-mail text:</p> <p>To block all future contact attempts by all Kaggle users please check the &quot;Block all user contact emails&quot; preference in your user profile <strong>here</strong></p> <p>&nbsp;</p> <p><strong>'here'&nbsp;</strong>doesn't work (404 error)</p>
3706, <p>Please Read:&nbsp;https://www.kaggle.com/forums/t/8335/scoring-outage</p>
3706, <p>Master Status: Two top 10% one of which is also Top 10!</p>
3706, <p>Works in Germany!</p>
3706, <p>all work and no perks won't get many competitors ;)</p>
3706, <p>Yes. &nbsp;It helps you to find your post I think&nbsp;</p>
3706, <p>Quick solution: Build a time-machine. Go back in time and dont create a duplicate account to have more submissions per day!</p>
3706, <p>Can I redeem my earlier beating benchmark scripts for a hoodie? :D :D&nbsp;</p>
3706, <p>[quote=ACS69;56895]</p> <p>Also I don't want dates put on my profile because I'm supposed to be working!! ;)</p> <p>[/quote]</p> <p>haha.. yeah me too :D</p>
3706, <p>I started this competition&nbsp;http://inclass.kaggle.com/c/predict-movie-ratings for students of my university who are taking RecSys lecture. If you are interested in this give it a try as its open for all :)</p>
3706, <p>[quote=Vincent Firmansyah;57399]</p> <p>Much appreciated! Shall give this a shot&nbsp;</p> <p>[/quote]</p> <p>seems your shot was a big one :P</p>
3706, <p>Just one competition left to work on??? Is kaggle announcing any new competitions soon? If not I'll have to go out and play football in my free time :(</p>
3706, <p>[quote=ACS69;57767]</p> <p>[quote=Abhishek;57760]</p> <p>Just one competition left to work on??? Is kaggle announcing any new competitions soon? If not I'll have to go out and play football in my free time :(</p> <p>[/quote]</p> <p>It's awful isn't it? I'm going to have to decorate my spare room!</p> <p>[/quote]</p> <p>I should clean my one and only room :P&nbsp;</p>
3706, <p>[quote=beluga;57763]</p> <p>playing football sounds great :D</p> <p>Btw I am starting to see&nbsp;Xmas decoration&nbsp;in the shopping malls so probably a combinatorial optimization problem is also on the way...</p> <p>[/quote]</p> <p>in December I think...&nbsp;</p>
3706, <p>[quote=lnicalo;57762]</p> <p>I can guess that one of the next competitions will be http://neuro.embs.org/2015/bci-challenge/ :-)</p> <p>[/quote]</p> <p>I dont see Kaggle as a sponsor there...</p>
3706, <p>come on guys! I dont have kids :P and out of the three screens I use Ive reserved one for kaggle. So its kaggle during all breaks I take and full time in the evening :D</p>
3706, <p>[quote=Giulio;57809]</p> <p>I want a hackathon for Xmas (well not literally on XMas day...)</p> <p>[/quote]</p> <p>+2 for hackathon. but it would be nice to get an email about it 24hrs in advance. I already missed two hackathons and dont wanna miss any future ones.... :)</p>
3706, <p>&quot;NN (Neural Networks) will be rendered ineligible for a prize&quot; LOL!!</p>
3706, <p>there will be only one competition left after a few hours :( (sadmax)</p>
3706, <p>I'm so upset :(</p>
3706, <p>[quote=Trevor Stephens;57845]</p> <p>After being harassed on Twitter this morning by&nbsp;<a href="http://www.kaggle.com/users/5309/abhishek">Abhishek</a>&nbsp;to update the data...</p> <p>[/quote]</p>  <p>hahahaha.... so atleast you know now that people use it :P&nbsp;</p>
3706, <p>+1</p>
3706, <p>Check this out:&nbsp;https://www.kaggle.com/solutions/competitions</p> <p>Click on &quot;get in touch&quot; and they will let you know!</p>
3706, <p>+1</p>
3706, <p>random number generator challenge :P :P&nbsp;</p>
3706, <p>Yayyyy!!! :D :P</p>
3706, <p>[quote=inversion;69006]</p> <p>I'd like to ask the Admins to have this re-scored. &nbsp; :-)</p> <p>[/quote]</p> <p>use this cheat code:&nbsp;&#8593; &#8593; &#8595; &#8595; &#8592; &#8594; &#8592; &#8594;</p>
3706, <p>factorization machines.&nbsp;</p>
3706, <p>Awesome! Waiting for python support.... :)</p>
3706, <p>some problem with the scripts? my script is stuck waiting... tried lots of times....</p>
3706, <p>I think we need to wait. edX website says competition begins on 14th.</p>
3706, <p>[quote=Giulio;71394]</p> <p>BTW- why would a private entry competition issue standard Kaggle points and count towards achievements?</p> <p>[/quote]</p> <p>and thats why we are waiting ;)</p>
3706, <p>[quote=Giulio;71394]</p> <p>BTW- why would a private entry competition issue standard Kaggle points and count towards achievements?</p> <p>[/quote]</p> <p>Also the &quot;Masters Competitions&quot; (Im missing them) are private and award kaggle points and tiers.</p>
3706, <p>Ahh. So I lost my &quot;highest rank&quot; award (3rd)... Hmmm.. So much for nothing..</p>
3706, <p>[quote=Mike Kim;78232]</p> <p>Is it possible to keep the highest rank ever under the old system? I don't care about the current rank as it'll change.</p> <p>I just ask because if you were higher under the old system and you ever made a documented claim of being higher you might just look like a liar under this new one.</p> <p>[/quote]</p> <p>+1</p>
3706, <p>[quote=Leustagos;78338]</p> <p>Thumbs up!</p> <p>[/quote]</p> <p>Thats coz you benefited!&nbsp;</p>
3706, <p style="text-align: left">Can u please tell us?</p>
3706, <p style="text-align: left">Did anyone see rain too?</p>
3706, <p>[quote=inversion;78523]</p> <p>[quote=rcarson;78509]</p> <p>Did you see anything in the otto contest?&nbsp;</p> <p>[/quote]</p> <p>It was really quick but I could have sworn Abhishek was #11</p> <p>[/quote]</p> <p>Damn! Again? :-/</p>
3706, <p>I dont think its a good idea. It gives an advantage to lazy competitors.</p>
3706, <p>Now my scripts will never be on top except for first couple of weeks of a competition&nbsp;:(</p>
3706, <p>Sorry I missed this. However the wizard doesnt allow me to change the type. :(</p> <p>Screenshot attached.</p>
3706, <p>Is there any feature like that? Sort by votes replies views etc?</p>
3706, <p>They are always respected! Benchmarks never win ;)</p>
3706, 
3706, <p>Ohh... Maybe :D </p>
3706, <p>N I'm getting downvotes for all my posts recently.... I demand bonus votes :P</p>
3706, <p>[quote=Bluefool;86699]</p>  <p>What's good about 1337?</p>  <p>[/quote]</p>  <p>Leet</p>
3706, <p>Dunno y u use 369 when 42 is the answer to everything :D</p>
3706, <p>[quote=inversion;86709]</p>  <p>[quote=Bluefool;86706]</p>  <p>Tesla said:</p>  <p>&quot;If you only knew the magnificence of the 3 6 and 9 then you would have a key to the universe.&#8221;</p>  <p>[/quote]</p>  <p>Yeah but Tesla died with zero upvotes.</p>  <p>[/quote]</p>  <p>Lol</p>
3706, <p><img src="http://i0.wp.com/venturebeat.com/wp-content/uploads/2015/08/owen-zhang.jpg?fit=930%2C9999" alt="enter image description here" title></p>  <p><a href="http://venturebeat.com/2015/08/19/this-guy-is-the-superman-of-data-scientists/">http://venturebeat.com/2015/08/19/this-guy-is-the-superman-of-data-scientists/</a></p>  <p>:)</p>
3706, <p>I have used SVMs with great results in some of previous kaggle competitions</p>
3706, <p>DMatrix(np.array(dataframe))</p>
3706, <p>Takes 6 weeks. Sometimes more</p>
3706, <p>it seems everyone is asking these kinda questions... and i wanted to be first from Germany :D :P </p>  <p>So who is? ;)</p>
3706, <p>We are not working on Kaggle challenges at the moment (or are we?) :P</p>
3706, <p>Such a sad news :( ..... Rest in Peace Lucas.</p>
3706, <p>Oh yeah! She is gonna love it. You are gonna get some  . . . . . . .beating :P</p>
3706, <p>[quote=Ben Hamner;118518]</p>  <p>The goals of the ranking update will be to</p>  <ul> <li>adjust the competition ranking system as may be appropriate</li> <li>add a second ranking system that rewards collaboration (discussion and scripts)</li> </ul>  <p>[/quote]</p>  <p>Will the final Kaggle ranking (on the users page) be a combination of these two? </p>
3706, <p>BTW how old is the data being used in the script?</p>
3706, <p>The thread is abour revamping user profiles. Its not about adding new features. Please dont spam here..</p>
3706, <p>404 error</p>  <p>[quote=beluga;119002]</p>  <p>Maybe it would be useful to visualize your historic ranking instead of showing  the current/best only.</p>  <p><img src="https://www.kaggle.io/svf/228803/9837fdf605b9bc70eb212e564855b88e/__results___files/__results___4_0.png" alt="enter image description here" title></p>  <p>You could use <a href="https://www.kaggle.com/gaborfodor/d/kaggle/meta-kaggle/ranking-history">this</a> script to check your development.</p>  <p>[/quote]</p>
3706, <p>[quote=Bluefool;127100]</p>  <p>Anyone else Top 1% in all 3 tiers? lol I'm on a mission to rule!</p>  <p>[/quote]</p>  <p>I am.. :P</p>  <p>And my top forum post is StumbleUpon :P</p>
3706, <p>[quote=Bluefool;127154]</p>  <p>I voted Abhishek up this morning. Within 30 seconds someone must have downvoted him to get rid of my vote. Also I had 3 downvotes (and 1 friendly upvote) for my silly joke.</p>  <p>The forums have gone brutal</p>  <p>[/quote]</p>  <p>nobody wants me to become forum grandmaster :(</p>
3706, <p>@Myles why does my profile say &quot;unranked&quot; for Kernels? Have I not shared anything ? </p>
3706, <p>[quote=ololo;128191]</p>  <p>Yeah why not. It's a t-shirt after all. </p>  <p>[/quote]</p>  <p>I'd like this quote on tshirt...lol</p>
3706, <p>I wrote a post on approaching machine learning problems. I hope Kagglers will find it useful :)</p>  <p>&quot;Approaching (Almost) Any Machine Learning Problem&quot; <a href="https://lnkd.in/enkgmX7">https://lnkd.in/enkgmX7</a></p>
3706, <p>yes. thats right! I'm not using Windows :)</p>
3706, <p>has the winner published their technologies used and the code ?</p>
3706, <p>[quote=liubenyuan;28734]</p> <p>I think I won't be able to improve much above 0.75 I tried to dig more features out of the scatter data but currently I am using only one RF regressor. Could any one hint some light on improving further the results using some more **advanced** regressors ?&nbsp;</p> <p>And I am very keen to see the method that was used to reach Rank1!</p> <p>[/quote]</p> <p>&nbsp;</p> <p>did you also use the supplimentary data?</p>
3706, <p>[quote=liubenyuan;28738]</p> <p>Yes both SUP1 and SUP2 are used.&nbsp;</p> <p>I found that the CV test on SUP2 provided a good match to the final valid dataset.&nbsp;</p> <p>[/quote]</p> <p>&nbsp;</p> <p>what about SUP3? any specific reason for not using it? would you mind telling on how many features you are training your model?</p>
3706, <p>[quote=Eoin Lawless;28798]</p> <p>I fixed an asymmetry in my model and my score improved slightly. Still searching for the magic sauce that the people above 0.8 are using...</p> <p>&nbsp;</p> <p>Eoin</p> <p>[/quote]</p> <p>&nbsp;</p> <p>damn u took our position..</p>
3706, <p>[quote=Eoin Lawless;28800]</p> <p>I'm sure there'll be a lot of activity over the next few days... I don't expect to hold it for long...</p> <p>[/quote]</p> <p>&nbsp;</p> <p>yeah seems so. new teams coming up and huge increase in ranks and all...</p>
3706, <p>[quote=LucaToni;28827]</p> <p>Hi</p> <p>it is often very slow to upload the model via kaggle website.</p> <p>Do you have any alternative way?</p> <p>Thanks</p> <p>[/quote]</p> <p>&nbsp;</p> <p>works fine with me. what is the total size of your model? mine is around 12mb</p>
3706, <p>if the valid labels are released are we allowed to retrain the model on the new data?</p>
3706, <p>if you submit a model and go up the leaderboard once the test set is available they will verify it with the model you submitted. If you dont hamper the positions being held by top contenders (after the test set releases) you will keep your rank and thus the kaggle points without any prize. However according to the Kaggle rules your model can be challenged by any other competitor in that case (as this is a two stage competition).&nbsp;</p>
3706, <p>I had one submission left in the evening before the deadline but the training took unusually high amount of time. I have attached a post deadline submission and would like to ask the toppers/organizers/kagglers if this is even possible?</p>
3706, <p>this clearly seems like a bug to me</p>
3706, <p>[quote=kinnskogr;29076]</p> <p>Congratulations&nbsp;Abhishek it seems you have broken the rules of causality! ;)&nbsp;</p> <p>My experience from previous 2-stage competitions was that submissions after the public dataset was closed but before the private dataset was unveiled were not even accepted. After the private dataset was closed submissions to both the private and public datasets were once again possible to see what score they would get.</p> <p>[/quote]</p> <p>I dont think its breaking the rule. I just submitted an old file to see the leaderboard score. I think anyone who is a member of Kaggle can do that to see how would they have performed in case the competition was still open :D. I just think its a bug that didnt show me the exact place I would have got or maybe it has been done intentionally.</p>
3706, <p>thanks. noticed that a bit later...</p>
3706, <p>wasn't the key supposed to be released today?</p>
3706, <p>I dont see a point in pushing the final submission date as I dont think any model will take more than a day for processing (I may be wrong about this). Moreover it will give the teams more time to make changes to &quot;not win&quot; &amp; improve the kaggle rank.</p>
3706, <p>Shouldn't all the scores be 0.0 or 1.0(if submitted) like it used to happen in previous two-stage competitions? Otherwise its impossible to know who submitted or who didnt.</p>
3706, <p>hi</p> <p>Will the final results be announced just after the competition ends?</p>
3706, <p>ahh... okay so we cannot know how much others got... :P</p>
3706, <p>[quote=Domcastro;30083]</p> <p>Thanks Isabelle. I'm very very happy with our result :D</p> <p>[/quote]</p> <p>me too :D</p>
3706, <p>now all this made me think of something else. Will only 69 teams be counted towards the final Kaggle rankings? If yes it means to be in top 10% we should have a rank of less than 7?</p>
3706, <p>[quote=Isabelle;30094]</p> <p>The rest of the people did not submit test results and have a score of zero on the private leaderboard. I do not know the implications on the Kaggle ranking.</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Thanks Isabelle. Maybe Kaggle Admins can answer that. :)</p>
3706, <p>[quote=Jeff Moser;30559]</p> <p>The private leaderboard is now available.</p> <p>[/quote]</p> <p>So the final rankings and badges have been given by taking only 68(or 69) teams as total number of participants?&nbsp;</p>
3706, <p>[quote=Domcastro;30562]</p> <p>[quote=Abhishek;30560]</p> <p>[quote=Jeff Moser;30559]</p> <p>The private leaderboard is now available.</p> <p>[/quote]</p> <p>So the final rankings and badges have been given by taking only 68(or 69) teams as total number of participants?&nbsp;</p> <p>[/quote]</p> <p>&nbsp;</p> <p>They are all ranked at 69 - all the competitiors have been included so your rank is out of over 200</p> <p>[/quote]</p> <p>oh okay thanks. I was expecting a new badge but i cant see that in my profile.</p>
3706, <p>[quote=Domcastro;30565]</p> <p>Well done for new badge. I should get Master now but that hasn't been updated yet. What you up for?</p> <p>[/quote]</p> <p>yeah I think I should also get a master's badge but it seems to be the same for me. congrats to you too :)</p>
3706, <p>Lol :P</p>
3706, <p>[quote=Sitmo;30585]</p> <p>I have a hard time understand the &quot;master&quot; criteria. Can someone help me understand it?</p> <p>The description says &quot;2x top 10% 1x top 10&quot;.</p> <p>I ran my model on historical scores here and there seems to be a very strong causal relation between [top 10 -&gt; top 10%] (avgAUC = 0.97!). Can someone verify this with their model?</p> <p>If so what is the minimal number of competitions a master has participated in: 2 or 3?</p> <p>[/quote]</p> <p>2 top 10% one of which is in top 10</p>
3706, <p>Our features include five subsets:</p> <p>1. signal related features including the types of A types of B length #unique of A #unique of B pearson correlation and the absolute correlation value discrete entropy of A and B hsic features mutual information of A;B conditional entropy A|B and B|A normalize mutual information normalize variation information KL divergence. And further some linear/non-linear combination of these features. The initial guess was to extract some 'information' out of the pairs of A and B.</p> <p>2. IGCI features and its derivatives. Including the normalized entropy and the normalized integral and their linear/non-linear combinations. I also devided the numeric axis (A or B) into 20 bins calculate the mean and the variance of each bins and calculate their entropy. I convert the A-B plot into re-scaled (resize to 64x64 using sparse.m in MATLAB) images and also do the entropy calculation as before. This intuition was that the IGCI used to work well and also some of its derivatives.</p> <p>3. Standard solvers provided by the webmaster. Include LINGAM and GPI. The gpi code is rather slow and the improvement using this feature is not evident.</p> <p>4. Convert the A-B plot to a re-scaled image(resized to 64x64 using sparse.m in MATLAB) calculate the vertical line test on the resized image. I also do write a code to test whether the image is 'thin' on the A-axis or B-axis.</p> <p>5. calculate the Kolmogorov-Smirnov test and Chi2test.</p> <p>6. calculate the fit(xy) and compare the sum of its residules the fit algorithm are linear fit and RVM fit. I calculate the spectrum of sorted A and B and extract some features from it.</p> <p>In addition to these features we had a number of features: length types of samples different types of correlations different types of distances ANM and a number of percentile scores. All of them can be found in scipy.stats.</p> <p>The final model was an ensemble of six Random Forest Regressors and was optimized for AUC. Ensembling and optimization improved the score to a great extent approximately by 0.03 which was too high in this competition. We also tried gradient boosting but it did not improve the score in our case.</p> <p>The MATLAB part was fully done by Liu (https://www.kaggle.com/users/70859/liubenyuan) and I was responsible for feature extraction using scipy.stats sklearn metrics and the learning part</p>
3706, <p>same here. when will the submissions start.</p>
3706, <p>wasnt it 72hrs before the deadline?&nbsp;</p>
3706, <p>are submissions only allowed on the full data?</p> <p></p> <p>I get this error while submitting a model: &quot;<span>ERROR: Expected 7178 rows but only 3589 rows found&quot;</span></p>
3706, <p>is the leaderboard being updated in realtime?</p>
3706, <p>i also submitted twice using the new dataset but the leaderboard is empty for me. Previous submissions with error are understandable as they are not on the full dataset.</p>
3706, <p>hi</p> <p></p> <p>I wanted to know whether the probability values are either 0 or 1.?</p>
3706, <p>do we need to submit the model before the deadline?</p>
3706, <p>Can anyone tell me how is the private leaderboard being calculated as the model was not asked to submit? I'm confused. :D</p>
3706, <p>less than a minute</p>
3706, <p>yes im using python</p>
3706, <p>CV is cross validation. The number of times you do cross validation is called fold.</p> <p>more info: http://en.wikipedia.org/wiki/Cross-validation_(statistics)</p>
3706, <p>maybe you are overfitting the data?</p>
3706, <p>can you tell me how you made the cv_loop multiprocessing?&nbsp;</p>
3706, <p>the competition is intense :D</p>
3706, <p>hi whats B_test?</p>
3706, <p>[quote=ryank;26924]</p> <p>Here is the rough equivalent to Leustagos' code in python:</p> <p>def fopt_pred(pars data):<br> &nbsp; &nbsp; return np.dot(data pars)</p> <p>def fopt(pars):<br> &nbsp; &nbsp; fpr tpr thresholds = metrics.roc_curve(y_train fopt_pred(pars B_train))<br> &nbsp; &nbsp; return -metrics.auc(fpr tpr)</p> <p>x0 = np.ones((n_models 1)) / n_models<br> xopt = fmin(fopt x0)<br> preds = fopt_pred(xopt B_test)</p> <p>where n_models is the number of models in the blend B_train is a matrix of predictions from each model and y_train are the labels. Note that if you have scipy version &gt; 0.11 you should replace 'fmin' with 'minimize' and set the method to&nbsp;‘Nelder-Mead’.</p> <p>[/quote]</p> <p>To use it on the final test data what should B_train be taken as? the original labels of the test data?</p> <p></p>
3706, <p>pydev with eclipse. now planning to switch to sublime text2</p>
3706, <p>has anyone tried SMOTE (Synthetic minority oversampling technique) ?&nbsp;</p> <p>by using SMOTE I get a cross validation score of 0.97 but it seems that the data overfits now. I would like to know if someone else has had any experience with this algorithm..</p> <p></p> <p>The paper can be found here:&nbsp;http://arxiv.org/pdf/1106.1813v1.pdf</p>
3706, <p>@afroz I divide the dataset into 0.8 0.2. This has been done after applying SMOTE</p>
3706, <p>yeah I was planning to do that. However I wanted to know whether SMOTE is good for this scenario :)</p>
3706, <p>I tried SMOTE in the cross-validation loop as @Paul said. It doesnt improve AUC mine came around 0.6 after I applied it on the original features plus some of mine. (or maybe im doing something wrong :P)</p>
3706, <p>anybody knows how can this be done with sklearn in python?</p>
3706, <p>[quote=cacol89;27335]</p> <p>Hello guys</p> <p></p> <p>I'd like to open this thread for sharing the models we have found in scikit-learn that are compatible with sparse matrices. These are the ones I have spotted so far (no particular order):</p> <p></p> <li>linear_model.LogisticRegression() </li><li>svm.SVR() </li><li>svm.NuSVR() </li><li>linear_model.LinearRegression() </li><li>neighbors.KNeighborsRegressor() </li><li>naive_bayes.MultinomialNB() </li><li>naive_bayes.BernoulliNB() </li><li>linear_model.PassiveAggressiveRegressor() </li><li>linear_model.PassiveAggressiveClassifier() </li><li>linear_model.Perceptron() </li><li>linear_model.Ridge() </li><li>linear_model.Lasso() </li><li>linear_model.ElasticNet() <p></p> <p>[/quote]</p> <p></p> <p>did you get better results using SVR than Logistic Regression?</p> </li>
3706, <p>has anyone tried grouping data in more than 3 levels? 4/5/6 and then using greedy feature selection? if yes how were the results?</p>
3706, <p>till which level did u try? how much time did it take for greedy?</p>
3706, <p>[quote=Przemys&#322;aw Skibi&#324;ski;27750]</p> <p>My results from the leaderboard:</p> <p>1st+2nd+3rd degree features -&gt; 0.91521<br>1st+2nd+3rd+4th degree features&nbsp;-&gt; 0.91558<br>1st+2nd+3rd+4th+5th degree features -&gt; 0.91694</p> <p>&nbsp;</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Can you tell me what was the time required for greedy feature selection for 1234 and 12345? Also your system specification?</p>
3706, <p>[quote=Yiqun Hu;27817]</p> <p>Przemystaw Skibinski for 1st+2nd+3rd degree I only achineve 0.904. Can you provide some hints to get 0.915?</p> <p>[/quote]</p> <p>&nbsp;</p> <p>I was wondering about that too. Maybe he is using some extra &quot;goodies&quot; :D</p>
3706, <p>did u try the removal from first degree(original) too?</p>
3706, <p>by the way did you remove them or just used some common(new) category for them?</p>
3706, <p>[quote=Garret Vo;27776]</p> <p>Can anyone explain the MGR_ID attributes? Meanings roles etc.&nbsp;</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Isn't it too late for that now?</p>
3706, <p>hi</p> <p>&nbsp;</p> <p>Can you guys tell us your final cross validation score and the leaderboard scores please?</p>
3706, <p>cv fold: 0.9138</p> <p>leaderboard: 0.91884</p> <p>I have observed that as CV fold increases the leaderboard score becomes closer to it. Did anyone else observe that?</p>
3706, <p>Is there any way to compare new predictions with the old ones that I have already uploaded on Kaggle without uploading the new ones? (a vague comparison as only 4 uploads are left)</p>
3706, <p>[quote=sfin;27881]</p> <p>I have used very simple models. Namely I have done &quot;direct estimation&quot; of Pr(Action|Zi) where Zi is a i:th predictor variable in the data set. I have not done any categories selection thus always using all categories of a variable.</p> <p>My best ensemble is of type (below Z denotes all predictors):<br>Pr(Action|Z) = w1*Pr(Action|Role_Code) + w2*Pr(Action|Role_DEPTNAME)+ w3*Pr(Action|MGR_ID) + Pr(Action|Action|ROLE_FAMILY_DESC).</p> <p>Thus this ensemble is just approximation of joint distribution Pr(Action|Z) as linear combination of marginal distributions Pr(Action|Zi) where Zi is i:th predictor variable. Remark that I have used only 4 of predictors and not all of them.<br><br>I have tried 3 methods for calculating ensemble weights:</p> <p>1) average: w1=w2=w4=1/42) QP problem: w1+w2+w3+w4=1 wi &gt;= 0 for i=1...4<br>3) OLS estimation<br><br>Out of these the average corresponds to my best result but just of 0.83621. Thus far from leaderboard top.</p> <p>[/quote]</p> <p>&nbsp;</p> <p>you are overfitting when you are using these probabilities for action/role_code etc. and thus your score is less.</p>
3706, <p>IMO its Gradient Boosting...</p>
3706, <p>it was built on the last day. :P so keep trying :D</p>
3706, <p>I would also like to congratulate all the winners and thank their support to other participants. It would really be great to see the approach of the winners :)</p>
3706, <p>[quote=Leustagos;27949]</p> <p>I think one that also really deserves a special thanks is <strong>Miroslaw&nbsp;</strong>! His code shaped part of many winning solutions. With some modifications on it one could attain near 0.92 with a single model.</p> <p>Thank you Miroslaw!&nbsp;</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Yes <strong>Miroslaw</strong> deserves a special thanks. I learnt so much from him and his code was the base for me to achieve a good rank in this competition. I would also like to thank <strong>Nick Kridler</strong> who gave the idea of removal of infrequent data from the dataset and gave me a reason to come back and not give up in the competition when my rank was around 150 for a long time :)</p>
3706, <p>[quote=IzuiT;27970]</p> <p>Congrats to the winners!</p> <p>Miroslaw thank you for your python code! I think you should be awarded by some sort of special prize for this competition :) &nbsp;</p> <p>[/quote]</p> <p>&nbsp;</p> <p>+1</p>
3706, <p>My final CV score on last day was around 0.9124 which gave me a leaderboard score of 0.920 and 0.917 on private. There was a lot of variation between the min and max in the CV scores and I think I should have worked harder to increase the overall CV score more.</p>
3706, <p>I used some different types of models combined with Miroslaw's code.&nbsp;</p> <ul> <li>Grouped original data into 4 -&gt; greedy selection -&gt; logistic regression</li> <li>Manually added three new features -&gt; group into 4 -&gt; Greedy -&gt; infrequent feature removal -&gt; GBM (previously LR)</li> <li>Divide each feature into sub features -&gt; greedy -&gt; combine with the above features -&gt; Log Res</li> <li>Original data -&gt; remove/replace infrequent(1st 2nd and 3rd degree) [I included ACTION also and this enhanced my score] -&gt; Log Regression</li> </ul> <p>I found out on the last day that blending with GBM improved the AUC on the leaderboard. I didnt have much time to play with it though.</p>
3706, <p>https://www.kaggle.com/c/amazon-employee-access-challenge/forums/t/5283/winning-solution-code-and-methodology</p>
3706, <p>such an interesting competition comes to an end. good luck to everyone :) looking forward to know what has been used by the toppers&nbsp;</p>
3706, <p>Seems to be a great idea. I have a few questions.</p> <p>&nbsp;</p> <p>This is only for the competition winners or for everyone who participated? what will be the deadline for submission and where is it going to be published?</p>
3706, <p>[quote=Paul Duan;28590]</p> <p>I'm in.</p> <p>[/quote]</p> <p>&nbsp;</p> <p>+1</p>
3706, <p>One more question. Is there any format on how to write and what to include?&nbsp;</p>
3706, <p>any updates on this one?</p>
3706, <p>Do we have to predict for the blanks in the rec_labels_test_hidden.txt? What do the blanks represent?</p>
3706, <p>so the blanks have to be considered as negative samples. right?</p>
3706, <p>use&nbsp;</p> <p class="x_p1"><span class="x_s1">wavread from scikits audiolab</span></p>
3706, <p>[quote=Rafael;27879]</p> <p>Hi</p> <p>We have managed to use only the &quot;histogram of segments&quot; features provided with the dataset under python. Our best shot was .86337 on the public leaderboard. If someone has managed to use efficiently the multi-instance features of segments drop us a mail to see if we can form a team.</p> <p>Any idea on how to use the multi-instance under python is greatly appreciated</p> <p>[/quote]</p> <p>&nbsp;</p> <p>i have implemented MIML in python...</p>
3706, <p>the new parser says that we need to submit IDs and Probabilities. I have understood how IDs are being created. Can someone tell me what probabilities are? shouldn't this be predictions (eg 142.... )?&nbsp;</p> <p>&nbsp;</p>
3706, <p>can anyone tell me how to do cross validation on this kind of data? I have some feature vectors in the training set and it looks like:</p> <p>&nbsp;</p> <p>labels features</p> <p>2xxxx</p> <p>5yyyy</p> <p>13zzzz</p> <p>and so on...</p> <p>however the test dataset is different and im confused about both the cross validation and submission part....&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p>
3706, <p>hi thanks for the reply. I have gone through your code but I could not understand it well. Could you tell me how to use it in a cross validation loop??</p>
3706, <p>how is this different from using the different probability values that are predicted by sklearn (eg RF)? Cant you just use the probability value for the label in concern ?</p>
3706, <p>Im looking for a team member. If anyone is interestedsend me a message. I cide in python</p>
3706, <p>hi everyone</p> <p>I was wondering if some of you could tell me your CV scores and leaderboard scores. I am getting around 0.501 as my CV score and leaderboard is 0.7</p>
3706, <p>[quote=Konrad Banachewicz;28124]</p> <p>For the sake of experiment I did leave-one-out. The averaged score is now within ~ 0.03 of my leaderboard one but the error band around it is huge (~0.18).</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Can you please tell us how you are implementing the AUC calculation? I tried but all efforts went in vain :(</p>
3706, <p>[quote=Konrad Banachewicz;28197]</p> <p>Actually it seems to me the problem is not so much with calculating the AUC (just feed a 0/1 vector and a probs vector) but more about what to do when e.g. we have all 0's (a total blank row) as an argument.</p> <p>[/quote]</p> <p>I have the same question..</p>
3706, <p>Hi</p> <p>&nbsp;</p> <p>I have one more doubt with AUC. This time its about how the evaluation is being done for ranks in the leaderboard.</p> <p>Lets say I have probability values for every bird in a given audio file. Similarly I find probability values for all birds in all the files given. Now I want to know whether for scoring in the leaderboard the AUC is calculated on individual samples and then averaged or it is just the overall AUC?&nbsp;</p>
3706, <p>Also what to do for negative/blank labels? How should they be represented?</p>
3706, <p>[quote=William Cukierski;28371]</p> <p>Answered here (which I think you've seen already?)</p> <p>https://www.kaggle.com/c/mlsp-2013-birds/forums/t/4975/auc-evaluation-calculation</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Thanks I guess I overlooked your post. But still the second question remains unanswered :)</p>
3706, <p>[quote=Bojan Vujatovi&#263;;28375]</p> <p>Blank label simply means that there aren't any bird sounds in the recording. Therefore you should put 0 for all 19 species (for that specific recording) in the vector which your probability vector gets compared to.</p> <p>[/quote]</p> <p>&nbsp;</p> <p>So one hack can be to predict all the probabilities and then assign 0s to all the data (0 to 19) where there is no rectangle in the given segmentation data right? But this reduces the AUC rather than increasing it.</p>
3706, <p>is there a reference on how histogram of segment features are created from scratch? I have achieved some good noise reduction and would like to create my own features</p>
3706, <p>[quote=fb;28443]</p> <p>Nice job your noise reduction looks quite good.</p> <p>&nbsp;</p> <p>Histogram of segments is proposed in:</p> <p>&quot;Multi-Label Classifier Chains for Bird Sound&quot;</p> <p>http://arxiv.org/abs/1304.5862</p> <p>Section 5.2</p> <p>&nbsp;</p> <p>The histogram part is relatively simple. Computing the segment features is more complicated. That is described in:</p> <p>&quot;Acoustic classification of multiple simultaneous bird species: A multi-instance multi-label approach&quot;</p> <p>www.fsl.orst.edu/flel/pdfs/Briggs_2012_JASA.pdf&#8206;</p> <p>&nbsp;</p> <p>Sorry but I haven't made all of the code for this public yet. I intend to in the next few months but it is a big task to refactor/generalize/document it all so it is easy to use and I don't get flooded with tech support emails.</p> <p>[/quote]</p> <p>Thanks for the references.</p> <p>&nbsp;</p>
3706, <p>I am on the verge of finishing coding for MIML classifier in python. I have one question maybe people who have implemented it can answer. Do we need to have the same bag size of features for all samples?</p>
3706, <p>[quote=fb;28485]</p> <p>In general it is not a requirement for MIML that all bags have the same number of instances. All instances should be a feature vector with the same dimension.</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Thanks. I was able to implement MIML with k-NN and used it directly on the segmentation features. The IDs for which features were not available were left blank. The CV AUC that I achieved was around 0.50 which is much lower than what I used to get with other features and other classifiers. Is it a usual behaviour? Are the provided features not suitable for MIML?</p>
3706, <p>[quote=fb;28487]</p> <p>In &quot;Acoustic classification of multiple simultaneous bird species: a multi-instance multi-label approach&quot; MIML-kNN was applied to the same type of features (different dataset) and achieved the highest AUC compared to two other MIML classifiers (MIMLSVM and MIMLRBF). My guess is there is a bug in your code.</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Thanks for the comments fb. IMO the score is low not because of any bug (as i checked it on some other data and it works very well) but because of too many empty features in the data. It seems around 308 samples dont have any rectangular segmentation features and thus the score has to go down :)</p>
3706, <p>Thanks. will do. Right now im using MIML - kNN with different kinds of audio features. It seems slow :D</p>
3706, <p>[quote=fb;28526]</p> <p>I think that the behavior of MIML-kNN is not well-defined for bags with 0 instances. Remember it starts by looking for nearest neighbors in &quot;bag space&quot; using a distance measure between bags (i.e. Average Hausdorff distance). Such distances are not well defined (and might even result in a divide by 0) for bags with 0 instances.&nbsp;</p> <p>I suggest that you split the dataset into bags with 0 instances and bags with more than 0. Apply MIML-kNN to bags with some instances and do something different for bags with none.</p> <p>[/quote]</p> <p>With some changes in the MIML and using only the data with the segmentation data provided I was able to get a hamming loss of 0.08 which seems to be pretty good. However the cross validation AUC increased only to 0.66</p>
3706, <p>[quote=fb;28613]</p> <p>Are you generating scores in the range [01]?</p> <p>[/quote]</p> <p>&nbsp;</p> <p>if you are talking about cross validation AUC and hamming loss then yes.</p>
3706, <p>[quote=fb;28613]</p> <p>Are you generating scores in the range [01]?</p> <p>[/quote]</p> <p>&nbsp;</p> <p>The scores generated by MIML have no fixed range</p>
3706, <p>here are the implementations of MIML-kNN MIML-RBF and MIML-SVM</p> <p>&nbsp;</p> <p>http://cse.seu.edu.cn/people/zhangml/files/MIML-kNN.rar</p> <p>http://cse.seu.edu.cn/people/zhangml/files/MIMLRBF.rar</p> <p>http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/annex/MIMLBoost&amp;MIMLSVM.rar</p> <p>These implementations are in MATLAB.&nbsp;</p> <p>I have attached my translation of the MIML-kNN code from MATLAB to python.</p>
3706, <p>[quote=saraswathi;28634]</p> <p>Thanks Abhishek. I am trying to run the RBF version and my leaderboard score is around 0.77 (though the AUC on the train is around 0.93). What should I be looking at the improve the model?</p> <p>&nbsp;</p> <p>The MIML Segment Features dataset has the segments only for 154 test and 154 train rec_id's. I use the train set and predict the test set. For the remaining ones I set the probability to zero.&nbsp;</p> <p>&nbsp;</p> <p>[/quote]</p> <p>&nbsp;</p> <p>I'm also stuck with it so I dont think im the proper guy who can answer your questions.</p>
3706, <p>[quote=Domcastro;28867]</p> <p>Hi. Thanks for code. I'm not sure how to run it. What input files are you using? It looks like train10.mat but where did you get this file from?</p> <p>[/quote]</p> <p>&nbsp;</p> <p>which code are you talking about? the MATLAB one?</p>
3706, <p>the input is the training bags. So you can forget about train10. the bags can be in a list.&nbsp;</p> <p>So input = list containing 2d numpy arrays.&nbsp;</p> <p>the labels are arranged in a specific way. read the documentation of MATLAB code to know about it.</p>
3706, <p>Congratulations to all the winners.&nbsp;</p> <p>I could not do well in this competition even after trying a lot of stuff. I would really like to know from the winners and those above 0.90+ whether it was about the features or learning techniques.&nbsp;</p>
3706, <p>Are the winners going to open source their implementations?</p>
3706, <p>[quote=fb;26534]</p> <p>To review in a binary classification problem AUC is calculated as follows:</p> <p>1. The classifier outputs a score for each instance.</p> <p>2. The scores for all instances are compared to a threshold which is varied from low to high.</p> <p>3. At each different threshold value the score/threshold combination results in a specific predicted label for each instance. That label may be correct or incorrect and can also be described as a true/false positive/negative.</p> <p>3. At each different threshold the classifier achieves some level of sensitivity and specificity which are computed in terms of the number of true/false positives/negatives. Therefore each threshold gives a different point in the ROC curve (which is plotted in the axes of sensitivity vs. specificity).</p> <p>4. AUC is the area under the ROC curve which is a curve formed with one point corresponding to each different threshold.</p> <p>&nbsp;</p> <p>There are at least two variations on how AUC is computed in multi-label classification problems which are sometimes referred to micro and macro AUC (I forget which is which). Kaggle does one of the two which seems to the more commonly used one. As described by William above the way it works is that there is now a chance to make a true/false positive/negative on every class. In other words it is equivalent to computing AUC for a binary problem where there are C times as many instances where C is the number of classes. A single threshold is used across all classes.</p> <p>The other way (not used in the competition) is to compute AUC curves separately for each class then take the average of the per-class AUCs.</p> <p>[/quote]</p> <p>&nbsp;</p> <p>how is this threshold determined?</p>
3706, <p>Also till what rank?</p>
3706, <p>A paper would definitely be a great opportunity for students like me. I would definitely write one IF I achieve a good rank by the end of the competition. that's why the criteria is very important. You should also let us know how far down in rank will this go. Some people might have good feature extraction techniques for audio data but not good rank on leaderboard. :P</p>
3706, <p>do we have to submit results on validation data right now?</p>
3706, <p>does the MATLAB code work with the new data?&nbsp;</p> <p>The file it generates gives me an error while submitting</p>
3706, <p>ok. I have used the validation data as provided. and matlab code without any change but the submission parser wont accept my submission. first few lines of the submission file generated by the provided matlab code are as follows:</p> <p>&nbsp;</p> <p>IdSequence<br>0041017 <br>0041017 <br>004103 <br>0041017 <br>0041017 <br>0041020 <br>0041017 6 <br>004108 6 <br>004103 <br>0041117 8 <br>0041117 <br>0041117 8 <br>0041117</p> <p>.</p> <p>.</p> <p>.</p>
3706, <p>sent</p>
3706, <p>So Kaggle was down for a moment. and now everyone in this competition is ranked 1. I wish I had also submitted :P</p>
3706, <p>yeah my overall rank decreased by almost 40</p>
3706, <p>https://www.kaggle.com/Home/contact</p>
3706, <p>same here but not in this competition</p>
3706, <p>Can someone give a summary of the data? I'm able to view it using netcdf4 python but unable to understand it.</p> <p></p>
3706, <p>Thanks Alec. Can you tell me how train.csv is related to this big array of shape (5113115916) ?</p>
3706, <p>Did anyone achieve a &quot;good&quot; score using RandomForest?</p>
3706, <p>[quote=Herimanitra;33309]</p> <p>[quote=Abhishek;33304]</p> <p>Did anyone achieve a &quot;good&quot; score using RandomForest?</p> <p>[/quote]</p> <p>2036k</p> <p>[/quote]</p> <p>hmm... Im currently running RF regression its been on for last 12 hours lets see. btw this score was achieved after feature engineering or on the basic features?</p>
3706, <p>As many as 23 users in Kaggle with usernames MG133* and 10 in this competition. Can any MG133* explain if they are from a university or something? It looks so weird!</p>
3706, <p>i think they removed cheaters</p>
3706, <p>[quote=Sergey Yurgenson;29438]</p> <p>While we are waiting for description of October 25 event could you please answer one more question:<br>what percentage of test data is used to calculate public leaderboard? (probably it is mentioned somewhere I just cannot find it)</p> <p>[/quote]</p> <p>&nbsp;</p> <p>This is written just above the leaderboard:</p> <p>This leaderboard is calculated on approximately 30% of the test data.<br>The final results will be based on the other 70% so the final standings may be different.</p>
3706, <p>no.&nbsp;</p> <p>from the rules:</p> <p>Team Limits</p> <p>The maximum size of a team is 1 participant.</p>
3706, <p>haven't you tried Lasso in MATLAB?</p>
3706, <p>[quote=Black Magic;30638]</p> <p>funny that ML methods are not able to beat last seen value benchmark!</p> <p>[/quote]</p> <p>I was able to beat the last seen benchmark using ML methods. Though not by much.</p>
3706, <p>[quote=Miroslaw Horbal;30608]</p> <p>Does anyone have any tips on performing consistent cross validation on this competition?</p> <p>I am finding it difficult to find a method that is consistent with performance on the leaderboards. Most methods I try a decrease in my cross validation (using 10-fold cv) often times leads to a significant increase in my leaderboard score. Where as other models that perform weakly on local cross validations perform well on the leaderboard.</p> <p>[/quote]</p> <p>&nbsp;</p> <p>A 10 fold crossvalidation with a split of 90:10 gives me results which are very close to the leaderboard.</p>
3706, <p>it seems its down again :(</p>
3706, <p>translated slides of&nbsp;Alexander D'yakonov. I tried my best :P</p>
3706, <p>how can we take care of the NULL values for body/title/url ?&nbsp;</p>
3706, <p>[quote=Afroz Hussain;29283]</p> <p>yes Domcastro&nbsp;is correct.<br><br>For some understanding of ROC in relation with AUC you may check following :</p> <p><a href="https://www.kaggle.com/wiki/AreaUnderCurve">https://www.kaggle.com/wiki/AreaUnderCurve</a></p> <p>&nbsp;</p> <p>&nbsp;</p> <p>[/quote]</p> <p>&nbsp;</p> <p>I think he meant it should be called AUC and not ROC. AUC is just the area under ROC curve. A number corresponding to ROC doesn't make any sense.</p>
3706, <p>AFAIK in &quot;genfromtxt&quot; you need to specify the &quot;dtype&quot; for the text file. you cant have two different dtypes for one file. So you either read the text or the numbers at a time.</p>
3706, <p>can someone tell how are the raw files arranged? their numbering makes no sense</p>
3706, <p>thanks. I thought its serially arranged</p>
3706, <p>I have the same views as Giulio. Currently I'm using only the train.tsv. I will look into the raw data at a later stage in the competition as only train.tsv not gonna keep a good spot for me in the leaderboard :D</p>
3706, <p>whats your leaderboard score only for naive bayes ?</p>
3706, <p>the code given by BSMan works fine for me.&nbsp;</p>
3706, <p>I work on Mac OSX with python 2.7 and sklearn 0.14</p>
3706, <p>[quote=Kevin Hwang;29581]</p> <p>Yes title and/or url will improve your score.</p> <p>[/quote]</p> <p>&nbsp;</p> <p>title I can understand. How do you intend to use URL?</p>
3706, <p>are we allowed to crawl the websites provided in the database for more data?</p>
3706, <p>ohh :(</p>
3706, <p>since crawling is not allowed I assume that we are not allowed to use the page rank/alexa rank data also. Maybe the Admin can clarify on this.</p>
3706, <p>maybe we can have a competition for that :P</p>
3706, <p>IMO 0.90+ would be a very good score for this competition</p>
3706, <p>and metametametakaggle.com - contests about&nbsp;contests about contests about kaggle contests</p>
3706, <p>im not at all confident about my score. i know my score is going to drop. and to avoid that the only way is to ensemble different model ;)</p>
3706, <p>[quote=duni;32802]</p> <p>anyone wanna guess what rank the 0.87835 benchmark is gonna end up?</p> <p>[/quote]</p> <p>what's your guess? :D</p>
3706, <p>Can people write their CV Score and Leaderboard Score here? As Ive been seeing quite a lot of variation now.</p> <p>Starting with me</p> <p>CV : 0.877</p> <p>Leaderboard: 0.877</p> <p>&nbsp;</p> <p>Last Submissions</p> <p>CV: 0.885 | 0.882</p> <p>Leaderboard: 0.872 | 0.855</p>
3706, <p>my last cv score was 0.893 and leaderboard was 0.866 which was worse than my previous scores. Now I'm confused whether I should trust my CV scores..</p>
3706, <p>I dont think Im overfitting the data. Maybe the distribution is not same for the 20% test data we have and the training set...</p>
3706, <p>these are my new CV scores. Let's see how it performs after a few hours</p> <p>AUC (fold 1/10): 0.907955<br>AUC (fold 2/10): 0.901190<br>AUC (fold 3/10): 0.916381<br>AUC (fold 4/10): 0.913193<br>AUC (fold 5/10): 0.902309<br>AUC (fold 6/10): 0.900302<br>AUC (fold 7/10): 0.901089<br>AUC (fold 8/10): 0.907172<br>AUC (fold 9/10): 0.908583<br>AUC (fold 10/10): 0.902365<br>Mean : 0.906054009445</p>
3706, <p>[quote=Alec Radford;29663]</p> <p>It's a related question but doesn't anyone else experience a really weird spike where the validation performance over time slowly climbs from 0.5 to 0.65 then wildly oscillates around 0.65 to 0.8 then slowly climbs stably once you're above 0.8 again? </p> <p>[/quote]</p> <p>&nbsp;</p> <p>did you experience this during cross validation?</p>
3706, <p>[quote=Abhishek;29662]</p> <p>these are my new CV scores. Let's see how it performs after a few hours</p> <p>AUC (fold 1/10): 0.907955<br>AUC (fold 2/10): 0.901190<br>AUC (fold 3/10): 0.916381<br>AUC (fold 4/10): 0.913193<br>AUC (fold 5/10): 0.902309<br>AUC (fold 6/10): 0.900302<br>AUC (fold 7/10): 0.901089<br>AUC (fold 8/10): 0.907172<br>AUC (fold 9/10): 0.908583<br>AUC (fold 10/10): 0.902365<br>Mean : 0.906054009445</p> <p>[/quote]</p> <p>&nbsp;</p> <p>This gave me&nbsp;0.86768 on the leaderboard</p>
3706, <p>I did an extensive 200 fold cross validation on my earlier submission. The results are as follows:</p> <p>&nbsp;</p> <p>&gt;&gt; mean(auc)<br>0.908236329766<br>&gt;&gt;&gt; max(auc)<br>0.9308211761607238<br>&gt;&gt;&gt; min(auc)<br>0.8891315490791901</p> <p>The leaderboard was around 0.86 as mentioned earlier.</p>
3706, <p>Hi</p> <p>Is the test data relatively more noisy than the given training data? &nbsp;Are the number of missing values in test and training data same? Also I noticed that even for the values derived using Alchemy some categories have missing values even though those values can be generated by using Alchemy API for the same data. Is there any specific reason for this?&nbsp;</p>
3706, <p>thanks William. so it has nothing to do with Kaggle (ofcourse I knew that :D) . but still doesnt answer my second and third question.</p>
3706, <p>Thank you for your reply William. Sorry I couldn't make the question clear. &nbsp;The main problem was why the some of the feature data from test and train are missing when Alchemy can derive them( as I can see the data has been derived using alchemy api) . I wanted to know if this has been done intentionally. The first part of my previous un-understandable question has already been answered</p>
3706, <p>why not use the data that has been provided?</p>
3706, <p>I meant use the &quot;text&quot; data and Alchemy API to generate your own... :)</p>
3706, <p>fyi im not using it yet&nbsp;</p>
3706, <p>[quote=Domcastro;29632]</p> <p>I did. I haven't submitted any of the benchmarks so my score is the standard data with extra 2 columns for Alchemy. Improved my score by 0.02 which is a lot. You need to clean up the URL first (I used the Perl API so I used URI)</p> <p>[/quote]</p> <p>&nbsp;</p> <p>did you fill up the missing values or you created new features using Alchemy?</p>
3706, <p>how did you combine text features with numerical features?</p>
3706, <p>IMO the combination with sparse features will also be poop but a smaller one. :P</p>
3706, <p>[quote=Adam Daum;30100]</p> <p>Smaller poop is better I guess :-)</p> <p>So should I assume you're using something else Abhishek?&nbsp; Are you using the text only or some combination of text + other features? .. Or just the non-text features?</p> <p>&nbsp;</p> <p>[/quote]</p> <p>&nbsp;</p> <p>i would just say that I'm using the smaller poop with some of my extra goodies ;)</p>
3706, <p>I had the same problem. I switched to KFold now and the leaderboard score seems to comply with the CV score now ;)</p> <p>&nbsp;</p> <p>EDIT: It seems I was wrong. It doesnt comply</p>
3706, <p>a simple way to cross validate in python using sklearn is:</p> <p>print np.mean(cross_validation.cross_val_score(model X y cv=20 scoring='roc_auc'))</p>
3706, <p>[quote=Black Magic;30669]</p> <p>[quote=fchollet;30246]</p> <p>Using at least 10% of the training set as validation examples seems to correct the discrepancy.&nbsp;</p> <p>I'm getting essentially the same CV results with k folds and with random shuffling. At this scale I would suppose random shuffling yields test data that is representative enough of the set : )</p> <p>[/quote]</p> <p>&nbsp;</p> <p>fchollet:</p> <p>did'nt get you here. Am facing same problem as you had even with CV.</p> <p>Are you saying that if I randomly shuffle training set and then do a CV - it will help. Not sure how it would given that in k-fold cross-validation we allot the fold numbers to the rows randomly in any case....</p> <p>[/quote]</p> <p>&nbsp;</p> <p>I think its because of the noisy test data we have</p>
3706, <p>ya since last 12 hrs...&nbsp;</p>
3706, <p>Hi folks</p> <p>Ive taken part in a lot of competitions now and used the code provided by others a lot of times. Now I think its my turn to return the favors :D</p> <p>This benchmark will give you a leaderboard score of approximately <strong>0.878</strong>.&nbsp;</p> <p>It has been written in python and uses pandas sklearn and numpy.</p> <p>The basic idea is to use the boilerplate text from the training and test files do a TF-IDF transformation using TfidfVectorizer of sklearn and classify using Logistic Regression.&nbsp;</p> <p>Go nuts! (and don't forget to click &quot;thanks&quot;)</p> <p>&nbsp;</p> <p>&nbsp;</p>
3706, <p>why would you say that?</p>
3706, <p>The idea here is to learn. I learned from the people who kept posting &quot;beating the benchmark&quot; posts. So I thought I would give it a try this time. And if you see the code you will find out that nothing has been done actually there is no preprocessing or feature engineering involved. This is just to give an idea about the functions available and how to use them with the original data. If you were stuck on 0.86 with everything you tried and don't believe in competition and didnt try this basic stuff then I would say that you wrote it for yourself.</p> <p>Also from this competition's rules:</p> <p>No private sharing outside teams</p> <p>Privately sharing code or data outside of teams is not permitted. It's OK to share code or data if made available to all players such as on the forums.</p>
3706, <p>[quote=Domcastro;30549]</p> <p>lol so all the people who think &quot;beat the benchmarks&quot; are good for learning - this benchmark is based on bad practices!</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Its called semi-supervised learning and is not wrong in anyway!</p>
3706, <p>I just want to say one thing.&nbsp;</p> <p>&nbsp;</p> <p>Thanks to everyone for keeping this thread alive for such a long time</p> <p>&nbsp;</p> <p>Special thanks to Domcastro ;) :P</p>
3706, <p>[quote=eidonfiloi;31470]</p> <p>I have tried out both TruncatedSVD and SelectKBest (with different metrics) both with different number of features and I got 20 fold cross validation results over 0.90 but by submitting them got always around 0.86...</p> <p>[/quote]</p> <p>&nbsp;</p> <p>do the feature selection in a cross validation loop.&nbsp;</p>
3706, <p>[quote=Yevgeniy;31476]</p> <p>[quote=eidonfiloi;31470]</p> <p>I have tried out both TruncatedSVD and SelectKBest (with different metrics) both with different number of features and I got 20 fold cross validation results over 0.90 but by submitting them got always around 0.86...</p> <p>[/quote]</p> <p>TruncatedSVD: the result is worse than without it (both CV and leaderboard).</p> <p>SelectKBest: it tends to overfit badly and in CV feature selection loop I didn't see any improvements...&nbsp;</p> <p>[/quote]</p> <p>&nbsp;</p> <p>did you try chi2 feature selection?&nbsp;</p>
3706, <p>try a chi2 feature selection with 90 percentile features</p>
3706, <p>[quote=Jared Huling;31493]</p> <p>I tried incorporating latent dirichlet allocation to no gain</p> <p>[/quote]</p> <p>&nbsp;</p> <p>It did not work for me either.</p>
3706, <p>20 Fold CV Score:</p> <p>score: 0.861140<br>score: 0.856257<br>score: 0.871433<br>score: 0.862105<br>score: 0.881988<br>score: 0.884240<br>score: 0.859123<br>score: 0.876988<br>score: 0.904123<br>score: 0.887602<br>score: 0.890819<br>score: 0.869708<br>score: 0.878216<br>score: 0.867602<br>score: 0.879912<br>score: 0.881393<br>score: 0.857613<br>score: 0.898119<br>score: 0.875911<br>score: 0.895031<br><br>mean = 0.876966231618</p> <p>&nbsp;</p> <p>[quote=Godel;31889]</p> <p>[quote=Triskelion;31887]</p> <p>[quote=Godel;31886]</p> <p><em style="line-height: 1.4">Traceback (most recent call last):</em></p> <p><em> File &quot;&lt;stdin&gt;&quot; line 1 in &lt;module&gt;</em><br><em>TypeError: cross_val_score() got an unexpected keyword argument 'scoring'</em></p> <p><span style="line-height: 1.4">[/quote]</span></p> <p><span style="line-height: 1.4">What is your version of Scikit Learn? I think there were some updates in scoring and CV with last version that beat_bench.py relies on.</span></p> <p><code><span style="line-height: 1.4">import sklearn</span></code></p> <p><code></code><code><span style="line-height: 1.4">print sklearn.__version__</span></code></p> <p><span style="line-height: 1.4">gives:</span></p> <p><code><span style="line-height: 1.4">0.14.1</span></code></p> <p><span style="line-height: 1.4">and I was able to run beat_bench.py without any trouble.</span></p> <p><span style="line-height: 1.4">(If you are on Windows you can find easy installers for many tools at <a href="http://www.lfd.uci.edu/~gohlke/pythonlibs/#scikit-learn">http://www.lfd.uci.edu/~gohlke/pythonlibs/#scikit-learn</a> )</span></p> <p>[/quote]</p> <p>Thanks for the response.</p> <p>I am using version&nbsp;0.13.1</p> <p>What was the cross validation auc score you got on the training data using the beat_bench code?</p> <p>If it is same as what I am getting then there is no issue with sklearn per se.&nbsp;</p> <p>&nbsp;</p> <p>[/quote]</p>
3706, <p>It seems fine to me. What you can do is select a range of k starting from maybe 50 or 100 and then increase it. For every k select the features in the cv loop and then do a 10 fold cross validation. Then select the best k out of all the values for which you have tried cross validation.&nbsp;</p> <p>&nbsp;</p> <p>[quote=Kapil Dalwani;31959]</p> <p>[quote=Abhishek;31477]</p> <p>[quote=Yevgeniy;31476]</p> <p>[quote=eidonfiloi;31470]</p> <p>I have tried out both TruncatedSVD and SelectKBest (with different metrics) both with different number of features and I got 20 fold cross validation results over 0.90 but by submitting them got always around 0.86...</p> <p>[/quote]</p> <p>TruncatedSVD: the result is worse than without it (both CV and leaderboard).</p> <p>SelectKBest: it tends to overfit badly and in CV feature selection loop I didn't see any improvements...&nbsp;</p> <p>[/quote]</p> <p>&nbsp;</p> <p>did you try chi2 feature selection?&nbsp;</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Hi Abhishek wanted to check what do you mean by doing the&nbsp;SelectKBest in the cv loop.&nbsp;</p> <p>Here is the algo. of what &nbsp;I understood.&nbsp;</p> <p>&nbsp;</p> <p>[code]</p> <p><code>k = 10</code></p> <p>import&nbsp;cross_validation.train_test_split as cv_tt</p> <p><code>for i in range(K):<br>&nbsp; &nbsp; &nbsp; &nbsp;X_train X_cv y_train y_cv = cv_tt(X ytest_size=0.2)<br><span style="line-height: 1.4">&nbsp; &nbsp; &nbsp; &nbsp;ch2 = SelectKBest(chi2 k=1000)</span></code></p> <p><code>&nbsp; &nbsp; &nbsp; &nbsp;X_train = ch2.fit_transform(X_train y_train)</code></p> <p><code>&nbsp; &nbsp; &nbsp; &nbsp;X_test = ch2.transform(X_cv)<br>&nbsp; &nbsp; &nbsp; &nbsp;model.fit(X_train y_train)<br>&nbsp; &nbsp; &nbsp; &nbsp;preds = model.predict_proba(X_test)[:1]<br>&nbsp; &nbsp; &nbsp; &nbsp;auc = metrics.roc_auc_score(y_cv preds)<br>&nbsp; &nbsp; &nbsp; &nbsp;print &quot;AUC (fold %d/%d): %f&quot; % (i + 1 K auc)<br>&nbsp; &nbsp; &nbsp; &nbsp;mean_auc += auc<br> return mean_auc/K</code></p> <p>[/code]</p> <p>&nbsp;</p> <p>What I am not sure if how would I pick the best feature?&nbsp;</p> <p>[/quote]</p>
3706, <p>[quote=Kapil Dalwani;31966]</p> <p>In a way I do two loop first one for k and other one for cv.</p> <p>best_score = 0</p> <p>best_k = 0</p> <p>for j in k:</p> <p>&nbsp; &nbsp; for i in cv:</p> <p>&nbsp; &nbsp; &nbsp; &nbsp;score+=pred_score</p> <p>&nbsp; &nbsp;best_score = score</p> <p>&nbsp; &nbsp;best_k = j</p> <p>for the best score I chose the best k and use that to make prediction to my test data.</p> <p>If that seems right then my best score comes out at the k= max_no_features of X.&nbsp;</p> <p>Does that seem right?</p> <p>[/quote]</p> <p>&nbsp;</p> <p>yes</p>
3706, <p>[quote=Marian Dragt;81477]</p> <p>Thanks for sharing&nbsp;&nbsp;@Abhishek!!! I learned a lot thanks to your code.</p> <p>[/quote]</p> <p>Im glad that this code is helping people even after almost two years ;)</p>
3706, <p>there are no teams in this competition. are there any?</p> <p>EDIT: just saw a team. So Ive the same question now</p>
3706, <p>adding the preprocessing code to already existing beat_bench.py will surely improve your CV and LB score. maybe you are something wrong. adding numerical features will decrease your CV and LB and you need to figure out why. There exists a way to stack the numerical features with the text features.! and i think you should also look into feature selection</p>
3706, <p>Kapil your code looks fine. but I this stage I suspect if anyone is going to give you more hints!&nbsp;</p> <p>[quote=Kapil Dalwani;32244]</p> <p>Okie I have some code attached which does&nbsp;SelectPercentile along with&nbsp;lm.LogisticRegression. This is in a pipeline which does GridSearch to find out best parameters.&nbsp;</p> <p>&nbsp;</p> <p>It always end up with C =1 and percentile of 99 or 100.&nbsp;</p> <p>Does the code looks ok?&nbsp;</p> <p>[/quote]</p>
3706, <p>I also had the same problem with cross-validation score and leaderboard score. By the way my best Public score was 0.89447 which got 6th rank when the private data was revealed. I had 40+ submissions which would have got a Top 10 rank in the Private Leaderboard (best being 3rd).</p> <p>Anyways I tried to keep my model as simple as possible and there were only 3 classification models in my ensemble. My ensemble consisted of two Logistic Regression and a k-NN. I used python + sklearn throughout the competition.&nbsp;</p> <p>I divided the data into two parts :</p> <p>#1 Boilerplate: I used the preprocessing.py by Triseklion for preprocessing the boilerplate. In TFIDFVectorizer I used NLTK for stemming and tokenization. So it was basically the same as the beat_bench.py that I had posted except pre-processing and NLTK tokenizer.</p> <p>#2 Raw Data: I used my own data cleaner for cleaning and tokenization and HTML cleaner of NLTK.&nbsp;preprocessing.py by Triseklion was not used here as I had deployed my own pre-processing. I used the same TFIDFVectorizer as the one for Boilerplate data.&nbsp;</p> <p>The next step was SVD. The TF-IDF values obtained from both the data were passed through TruncatedSVD of scikit-learn. Both the SVDs used 120 components.&nbsp;</p> <p>SVD1 ---&gt; Logistic Regression</p> <p>SVD1 ---&gt; k-NN Classifier</p> <p>SVD2 ---&gt; Logistic Regression</p> <p>The final ensemble was a simple mean of these three models.</p> <p>&nbsp;</p> <p>Things that did not work for me (or gave a lower score) :&nbsp;</p> <p>#1 Rapid Automatic Keyword Extraction (RAKE) on both Boilerplate and Raw Data.</p> <p>#2 SVM (I thought it would but it didn't)</p> <p>#3 Naive Bayes worked to a certain extent the results were not satisfactory.</p> <p>#4 Use of Word Embeddings derived using neural network approach on Wikipedia Corpus.</p> <p>&nbsp;</p> <p>Overall it was a very interesting competition for me. Thanks to Kaggle Competition Admins and all the users who contributed their ideas in the forums.</p> <p>&nbsp;</p>
3706, <p>[quote=Yevgeniy;32998]</p> <p>Congratulations to all smart people who used the beating benchmark code. They ended up in top 25% after all. It is much better than people who worked hard on this competition. BTW what is a purpose of the public leader board? I mean your CV score goes up quite consistently with your public leader board score but at the same time your private score jumps randomly. That is all misleading... Mostly I tried to rely on my CV score but it could be my mistake. I had a situation where my CV scores were about the same they both got similar scores on public leader board but on the private leader board difference was very huge. I feel sorry for people who dropped more than 300 places. Domcastro was right after all. It is not that the code was very basic it is because lots of people used it. People who worked hard got nothing not even the top 25% mark.&nbsp;</p> <p>[/quote]</p> <p>&nbsp;</p> <p>maybe because of the benchmark people didnot work hard enough!</p>
3706, <p>also I would like to add one more thing. the basis of my being rank 8th and dropping only 4 ranks(although i was expecting to improve) was the benchmark I posted!</p>
3706, <p>[quote=William Cukierski;33036]</p> <p>FYI we just pushed a change to the way rankings are assigned (see <a href="https://www.kaggle.com/forums/t/6169/skewed-rankings-from-benchmarks">this thread</a>) when people tie. This was done to remove the false &quot;Top 25%&quot; awards from people that submit benchmarks and enter massive ties.</p> <p>[/quote]</p> <p>Hi William</p> <p>Its a really &nbsp;nice decision.!!</p> <p>i really appreciate that. but in imy opinion you cannot implement a new rule for the competitions that have already ended.! &nbsp;I know Kaggle can change the terms and conditions but its not written anywhere that you can change rank of someone just based on what he &quot;actually&quot; submitted!</p> <p>if you wanna change all these this should reflect in your &quot;terms and conditions&quot; and should NOT affect any previous competition!</p> <p>&nbsp;</p>
3706, <p>[quote=William Cukierski;33052]</p> <p>I don't want to argue the legal aspects of this but our thoughts are:</p> <ul> <li>There was greater harm in rewarding false historical achievements than in changing the face value of the rankings (and that's all we are changing).</li> <li>Ranks have never been set in stone. An old user deletes his/her account? You just moved up a rank. We found a ring of cheating? Goodbye old results.</li> <li>Once you have achieved Master status we never demote you (unless it was an error on our part). Everyone who was a Master before this change is still one now.</li> <li>This change only really affects a few competitions and only the relative bottom-ish part of the leaderboard. Yes it's the top 25% in some cases but the affected people can hardly claim they earned it compared to the effort to get top 25% in a non-massive-benchmark-tie competition.</li> </ul> <p>tl;dr - we're trying to do the right thing and sometimes the right thing is not always what makes the most people happy</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Great 4 points for the people who &quot;hate&quot; me here for posting the benchmark!&nbsp;</p> <p>my replies:</p> <p>1. &quot;No private sharing outside teams</p> <p>Privately sharing code or data outside of teams is not permitted. It's OK to share code if made available to all players on the forums.&quot; Your rules. &nbsp;Not mine not anyboy else's. We were allowed to post code and benchmarks and also we can post the zero benchmarks in all the competitions! its nowhere in &quot;rules&quot; that we cannot! And if you wanna change that you can do it for future competitions but not for the ones which have already finished or the ones which are on the verge of finishing!</p> <p>2. I dont really care about &nbsp;&quot;cheating&quot;. &nbsp;I performed well. and yeah i am new. I dont know the users who are ahead of me in this competition. Actually no one except one has replied to any threads here and really a guy with a submission on &quot;16th september&quot; has not replied yet. Then can I say its a false account!?</p> <p>3. Now this should be on your new &quot;rules&quot;</p> <p>4. If I submit some 0 benchmark i know that i have not earned it and even my employers will know that! but once again its the first thing u said. If u wanna change something you cannot do it for the competitions that have already ended and if you are gonna do something like that you should do it for all the competitions hosted by Kaggle till now!</p>
3706, <p>[quote=Domcastro;33056]</p> <p>Abhishek - many people love you for the benchmark. You are being oversensitive and this benchmark thing was started on another thread about the Cause and Effect competition and the Belkin one. xxxxxxxxxxxxxxxxxxxxxxxxx</p> <p>[/quote]</p> <p>Hi Domcastro</p> <p>Your post makes no sense according to Kaggle rules which has been changed just now. I think William can clarify!</p>
3706, <p>Congratualtions &quot;fchollet&quot;.&nbsp;</p> <p>I worked really hard on this competition and I ended up 8th. I would really like to know what you used for your best model and it would be great if you could post the code of your winning model for the Kaggle community :)</p>
3706, <p>has 'fchollet ' replied anywhere?</p> <p>&nbsp;</p>
3706, <p>the readme says it will take approx 5mins to run but on my Mac&nbsp;with 8GB of RAM and 4 cores at 2.7GHz it takes forever to &quot;predict&quot; however the training is quite fast :D</p>
3706, <p>can anybody throw some light on this topic?</p>
3706, <p>I've always wondered how Kaggle decides the timeline of a competition it is hosting. For example in this particular competition Jeff achieved a score of 0.96 just within two days. This score is very competitive and IMO the winning score will be around 0.98. When these kind of scores can be achieved within a short period of time then what's the use of running a particular competition for a period of 4 months? Won't the results be same even if the duration is halved?&nbsp;</p>
3706, <p>Great. I was working with nolearn convnet only but I didnt know that just 100 images can produce an accuracy of 94%. By the way I'm unable to find the example you mentioned.</p> <p>EDIT: nevermind found it</p>
3706, <p>remove that param from decaf/layers/Makefile and compile</p>
3706, <p>i tried decaf in galaxy zoo with random forest on top of it but couldnt get a score of less than 0.15</p>
3706, <p>tried all available ones using grid search</p>
3706, <p>Pip install pil</p>
3706, <p>the exact same code produces a score of 0.30 on the leaderboard. maybe Im using a wrong algorithm</p>
3706, <p>cross validation does give something like 0.17. I was talking about the Leaderboard.</p>
3706, <p>[quote=Giulio;32780]</p> <p>[quote=Abhishek;32454]</p> <p>cross validation does give something like 0.17. I was talking about the Leaderboard.</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Abhishek were you able to get your cv and LB scores closer? My CV is still around 0.17 with&nbsp;my&nbsp;leaderboard&nbsp;of 0.159. Even with TFIDF done within the CV loop...</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Same here... :)</p>
3706, <p>[quote=Giulio;33133]</p> <p>[quote=Abhishek;32781]</p> <p>[quote=Giulio;32780]</p> <p>[quote=Abhishek;32454]</p> <p>cross validation does give something like 0.17. I was talking about the Leaderboard.</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Abhishek were you able to get your cv and LB scores closer? My CV is still around 0.17 with&nbsp;my&nbsp;leaderboard&nbsp;of 0.159. Even with TFIDF done within the CV loop...</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Same here... :)</p> <p>[/quote]</p> <p>Fixed it :-)</p> <p>I hadn't thought through it :-)</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Wanna share how you fixed it? :D</p>
3706, <p>[quote=Giulio;33322]</p> <p>[quote=Abhishek;33321]</p> <p>Wanna share how you fixed it? :D</p> <p>[/quote]</p> <p>Well let me answer with a question. Are you predicting the original labels or a transformation of them?</p> <p>[/quote]</p> <p>Got it! Thanks :D</p>
3706, <p>apply this function to all the rows in your training and test data before you use any vectorizer:</p> <p>&nbsp;</p> <p>def clean_the_text(data):<br>&nbsp; &nbsp; alist = []<br>&nbsp; &nbsp; data = nltk.word_tokenize(data)<br>&nbsp; &nbsp; for j in data:<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; alist.append(j.rstrip('\n'))<br>&nbsp; &nbsp; alist = &quot; &quot;.join(alist)</p> <p>&nbsp; &nbsp; return alist</p> <p>&nbsp;</p>
3706, <p>[quote=LeastSquares;33425]</p> <p>@Abhishek the line:</p> <p>for j in data:</p> <p>should it not read</p> <p>for j in raw:</p> <p>?</p> <p>Cheers</p> <p>&nbsp;[/quote]</p> <p>updated. thanks</p>
3706, <p>[quote=Falconic;33421]</p> <p>How would I do that?</p> <p>[/quote]</p> <p>depends on how you are reading the data. If its a list:</p> <p>for i in range(len(alldata)):</p> <p>&nbsp; alldata[i] =&nbsp;clean_the_text(alldata[i])</p>
3706, <p>whats happening on the leaderboard. I should have logged into kaggle earlier.. :P</p>
3706, <p>[quote=David;33526]</p> <p>[quote=Jack Shih Tzu;33522]</p> <p>I think there's still a lot of improvement to be made at the top of the leaderboard.</p> <p>[/quote]</p> <p>Bah I don't know it probably isn't worth trying. At least not for another 21 days ;)</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Rome wasn't built in a day. It was built on the last day :P . so keep trying&nbsp;</p>
3706, <p>can you give link to SO question? I'll answer there</p>
3706, <p>n there goes my SO points... lol</p> <p>X=scipy.sparse.hstack([tfdif.fit_transform(Train['tweet'])cv.fit_transform(Train['state'])]).tocsr()</p>
3706, <p>but you also need&nbsp;<strong>.tocsr()&nbsp;</strong>:D</p>
3706, <p>[quote=Torgos;35469]</p> <p>Actually it seems to work without .tocsr(). &nbsp;This creates a new error message if used with SVR though:</p> <p>Buffer has wrong number of dimensions (expected 1 got 2)</p> <p>&nbsp;</p> <p>There doesn't seem to be a problem with other regression algorithms. &nbsp;Anyway thanks!</p> <p>[/quote]</p> <p>for SVR:&nbsp;y : array-like shape (n_samples)&nbsp;</p> <p>you can fit SVR only on 1D label</p>
3706, <p>[quote=oztalha;35502]</p> <p><img alt>Thanks. I will try more combinations then. I tried averaging some of them but ensemble CV scores also kind of averaged instead of getting better. BTW attached you can find bar graphs for each category showing class distributions.</p> <p>PS. I couldn't inserted it into the post. Insert/Edit Image button pops up a window but source field does not accept file path or URL?</p> <p>-Talha</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Nice plots. Might help some teams. Thanks</p> <p>&nbsp;</p>
3706, <p>[quote=prof_data;35520]</p> <p>I must be doing something wrong then. Since I am doing exactly that. Features-&gt;TFIDF-&gt; Breaking the label matrix into 24 label vectors-&gt;fit train on every label individually with Ridge -&gt; predict every label individually&nbsp;with Ridge and finally combining those results. But somehow I can't get better results than 0.16200-0.16300.</p> <p>[/quote]</p> <p>Do you scale your results after generating predictions?</p>
3706, <p>[quote=Chi;35590]</p> <p>In my case the gap of CV score and leader board score is high. I noticed there are scientific notation in my submissions is it causing the problem ? I had noticed this caused issue in one previous competition. I am using 'to_csv' from pandas. I am not sure how to handle it in Pandas. Please help how you solve this.</p> <p>[/quote]</p> <p>we are also using pandas .to_csv and our submissions also contain scientific notations. I dont think thats the problem</p>
3706, <p>[quote=David;35595]</p> <p>[quote=Abhishek;35594]</p> <p>[quote=Chi;35590]</p> <p>In my case the gap of CV score and leader board score is high. I noticed there are scientific notation in my submissions is it causing the problem ? I had noticed this caused issue in one previous competition. I am using 'to_csv' from pandas. I am not sure how to handle it in Pandas. Please help how you solve this.</p> <p>[/quote]</p> <p>we are also using pandas .to_csv and our submissions also contain scientific notations. I dont think thats the problem</p> <p>[/quote]</p> <p>Same here.</p> <p>&nbsp;</p> <p>On the CV side of things every single one of my submissions has been 0.006-0.007 worse on leaderboard compared to 10 k-fold cross validation and every submission with better CV has led to a increase in my leadboard score. Given the consistency I don't think this an issue of overfitting to training data and suspect that there just may be some underlying differences between the training and test sets or that leaderboard subsample is just an odd sample. Either way from what I've heard I think just about everyone is expierencing a pretty comparable gap (although based on the number of submissions some teams have I'm going to guess that a few are overfitting to the leaderboard).</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Damn! we have the highest number of submissions :P</p>
3706, <p>hi</p> <p>&nbsp;</p> <p>Im looking for a teammate for this competition. I program in python so someone with python skills would be great. If anyone is interested please message me</p>
3706, <p>Some sklearn models do have multiple columns as input labels eg Ridge Lasso etc</p>
3706, <p>#1 : You are doing something wrong if the fit_transform is not returning you what you need. Im using 0.15-git and its working fine.&nbsp;</p> <p>#2 : Softmax</p> <p>&nbsp;</p> <p>[quote=maoshouse;34857]</p> <p>I think your best bet (per the link) is to get the development version 0.15&nbsp;from git.</p> <p>&nbsp;</p> <p>I also have another question pertaining to the binarizer. I noticed that after running fit_transform(Y) on my data nothing is being transformed. If I print the binarized labels it's still the same as the untransformed one.</p> <p>Subsequently the model can now be fitted but if I try to predict I get a memory error</p> <p>&nbsp;</p> <p>Briggs I assume you're running a linear classifier with Scikit learn as well? How did you account for the weightings for the prediction such that they sum to 1?</p> <p>[/quote]</p>
3706, <p>Ridge from sklearn accepts multicolumn Y</p>
3706, <p>Its a classifier. use:</p> <p>clf.fit(x numpy.round(y) sample_weight=1.0)</p>
3706, <p>more RAM maybe</p>
3706, <p>The data is crowdsourced. Five different people classified one single tweet. Crowdflower then assigned scores to the classification by them. This resulted in predictions which were much closer to the reality. Obviously one user who is rating cannot see tweet#1056 and tweet#1134 simultaneous as you can and thus the discrepancy. IMO it is left for the competitors to decide which one is good and should be used.&nbsp;</p>
3706, <p>cheaters will be removed by Kaggle after the competition ends so no need to worry about it :)</p>
3706, <p>[quote=William Cukierski;35683]</p> <p>We're more or less finished. This is by far the most cheating we've ever seen in a competition.</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Will it affect the overall kaggle ranks and points?</p>
3706, <p>s1-s5 should add up to 1. but your submission will be valid even if they dont.</p>
3706, <p>Partly Sunny With a Chance of #Hashtags</p> <p><strong>Approach for the team (no_name):</strong></p> <p>For classification we treated S W and K separately and created different models for each of them. The dataset was also preprocessed separately for the 3 variables.</p> <p><br><strong>Feature engineering</strong>:<br><strong>Sanitization function</strong> - Each tweet was sanitized prior to vectorization. The sanitization part converted all tweets to lower-case and replaced &#8220;cloudy&#8221; with &#8220;cloud&#8221; &#8220;rainy&#8221; with &#8220;rain&#8221; and so on.<br><strong>Sentiment dictionary</strong> - A list of words for different sentiments and emoticons constituted the sentiment dictionary. <br><strong>Sentiment scoring</strong> - we provided a score to each tweet if the tweet consisted of any words found in the sentiment dictionary.<br><strong>Tense detection</strong> - A tense detector was implemented based on regular expressions and it provided a score for &#8220;past&#8221; &#8220;present&#8221; &#8220;future&#8221; and &#8220;not known&#8221; to every tweet in the dataset.<br><strong>Frequent language detection</strong> - This function removed tweets for infrequent languages was (languages with 10 or less occurences were removed).<br><strong>Tokenization</strong> - A custom tokenization function for tweets was implemented using NLTK.<br><strong>Stopwords</strong> - Stopwords like 'RT''@''#''link''google''facebook''yahoo''rt'  etc. were removed from the dataset.<br><strong>Replace two or more</strong> - Repetitions of characters in a word were removed. Eg. &#8220;hottttt&#8221; was replaced with &#8220;hot&#8221;.<br><strong>Spelling correction</strong> - Spelling correction was implemented based on Levenshtein Distance.<br><strong>Weather vocabulary</strong> - A weather vocabulary was made by crawling a few weather sites which scored the tweets as related to weather or not.<br><strong>Category OneHot</strong> - The categorical variables like state and location were one hot encoded using this function.</p> <p><strong>Types of Data Used</strong>:<br>All tweets<br>Count Vectorization<br>TFIDF Vectorization<br>Word ngrams (12)<br>Char ngrams (16)<br>LDA on the data<br>Predicted values of S W and K using Linear Regression and Ridge Regression</p> <p><br><strong>Classifiers Used</strong>:<br>Ridge Regression<br>Logistic Regression<br>SGD</p> <p><strong>Model</strong>: <br>The different types of data were trained with all the classifiers. The ensemble was created from the different predictions. <br>We used approximately 10 different model-data combinations for creating the final ensemble.<br>The predictions for S and W were normalized between 0 and 1 in the end.</p> <p>We also used the extra data for &#8220;S&#8221; available at : https://sites.google.com/site/crowdscale2013/shared-task/sentiment-analysis-judgment-data</p> <p>Our model scored 0.1469 on the leaderboard.</p> <p>In the end we did an average with <strong>Jack</strong> and ranked 2nd on the public leaderboard and 4th on the final leaderboard.</p> <p><strong>Things that didn't work</strong>:<br>- Building a hand-crafted tense detection using keywords (similar to sentiment detection)</p> <p>Things we should have tried:<br>- Build more diverse models and use ensembling/averaging (similar to what Maarten Boosma did in stumbleupon)<br>- Stacking (e.g. pipeing the predictions of ridge/sgd into a tree estimator)<br><br></p> <p><strong>Things we noted</strong>:<br>- The model for W (when) was performing the worst (RMSE of about 0.19-0.20) whereas S (sentiment) and K (kind) were 0.13 and 0.1 respectively<br>- Most predictions in W related to the current weather situation predictions for &quot;I can't tell&quot; were very difficult</p> <p><strong>Tools used</strong>:<br>- sklearn<br>- nltk<br>- langid<br>- NodeBox:Linguistic</p>
3706, <p>[quote=Matt;36022]</p> <p>[quote=David McGarry;36017]</p> <p><span style="line-height: 1.4">unless you switched your S and K scores (above)</span></p> <p>[/quote]</p> <p>Hi David you're right S and K are mixed up (my bad).</p> <p>[/quote]</p> <p>Yeah S and K are mixed up. Fixed now</p>
3706, <p>I got a score of&nbsp;0.34309 on the LB. when I sort the same submission by IDs and submit the score is 0.68</p>
3706, <p>[quote=William Cukierski;33637]</p> <p>[quote=Abhishek;33635]</p> <p>I got a score of&nbsp;0.34309 on the LB. when I sort the same submission by IDs and submit the score is 0.68</p> <p>[/quote]</p> <p>Check your submission for scientific notation.</p> <p>[/quote]</p> <p>a total of 3 values have scientific notation</p>
3706, <p>seems there will be big changes in the top 50 of Kaggle :P</p>
3706, <p>Congrats to all winners :).&nbsp;</p>
3706, <p>are there usual kaggle points for this competition?</p>
3706, <p>You can use the evaluation version that is valid for 30 days</p>
3706, <p>[quote=Bright future;36296]</p> <p>I'm currious whether Abhishek&nbsp;will beat the&nbsp;1.612.061 score or not :)</p> <p>[/quote]</p> <p>Isnt that impossible?! ;)</p>
3706, <p>[quote=Rudi Kruger;36301]</p> <p>C'mon Abhishek master of the sleigh give us just a teeny-tiny tip ;]</p> <p>[/quote]</p> <p>At this point I better stay quiet ;) I think Ive hit the plateau</p>
3706, <p>[quote=Abdallah Sayyed-Ahmad;36330]</p> <p>1559482 !!! is impossible. There has to be overlap or a bug in the evaluation script.&nbsp;</p> <p>[/quote]</p> <p>the sleigh is magic!</p>
3706, <p>[quote=Victor;36077]</p> <p>[quote=MathWorks;35947]</p> <p>&nbsp;The MATLAB prize is available to MATLAB users..</p> <p>[/quote]</p> <p>I don't understand how is it possible to check whether somebody uses only MatLab since any program may be translated to matlab. So one can initially use C and later translate code to matlab.</p> <p>[/quote]</p> <p>Thats why they are not giving extra time to upload model</p>
3706, <p>[quote=MathWorks;35947]</p> <p>We'd like to see as many competitors as possible so the competition is open to any solution! The MATLAB prize is available to MATLAB users but the Leaderboard and Rudolph prizes are open to any solution (including MATLAB)!</p> <p>[/quote]</p> <p>&nbsp;</p> <p>are we allowed to used MATLAB mex subroutines for C/C++ if competing for MATLAB prize?</p>
3706, <p>[quote=Ben Barsdell;35987]</p> <p>Hi all</p> <p>For those who are impatient like me I've put together some C++ code that uses the data-parallel Thrust library for speed. It's available here: https://github.com/benbarsdell/KaggleSantaPacking</p> <p>On my outdated laptop it takes around half a second to validate a solution and around a tenth of a second to compute its score.</p> <p>Please let me know if you find any bugs or have any questions or suggestions.</p> <p>[/quote]</p> <p>&nbsp;</p> <p>I get the following errors while compiling:</p> <p>In file included from /tmp/tmpxft_000043d0_00000000-1_check_solution.cudafe1.stub.c:1:<br>/tmp/tmpxft_000043d0_00000000-1_check_solution.cudafe1.stub.c:85: error: macro &quot;__cudaRegisterEntry&quot; requires 4 arguments but only 3 given<br>/tmp/tmpxft_000043d0_00000000-1_check_solution.cudafe1.stub.c:85: error: macro &quot;__cudaRegisterEntry&quot; requires 4 arguments but only 3 given<br>/tmp/tmpxft_000043d0_00000000-1_check_solution.cudafe1.stub.c:85: error: macro &quot;__cudaRegisterEntry&quot; requires 4 arguments but only 3 given<br>/tmp/tmpxft_000043d0_00000000-1_check_solution.cudafe1.stub.c:85: error: macro &quot;__cudaRegisterEntry&quot; requires 4 arguments but only 3 given<br>/tmp/tmpxft_000043d0_00000000-1_check_solution.cudafe1.stub.c:85: error: macro &quot;__cudaRegisterEntry&quot; requires 4 arguments but only 3 given<br>/tmp/tmpxft_000043d0_00000000-1_check_solution.cudafe1.stub.c:85: error: macro &quot;__cudaRegisterEntry&quot; requires 4 arguments but only 3 given<br>/tmp/tmpxft_000043d0_00000000-1_check_solution.cudafe1.stub.c:85: error: macro &quot;__cudaRegisterEntry&quot; requires 4 arguments but only 3 given<br>/tmp/tmpxft_000043d0_00000000-1_check_solution.cudafe1.stub.c:85: error: macro &quot;__cudaRegisterEntry&quot; requires 4 arguments but only 3 given<br>/tmp/tmpxft_000043d0_00000000-1_check_solution.cudafe1.stub.c:85: error: macro &quot;__cudaRegisterEntry&quot; requires 4 arguments but only 3 given</p> <p>&nbsp;</p> <p>any idea?</p>
3706, <p>if anybody needs:</p> <p>http://beatingthebenchmark.blogspot.de/2013/12/packing-santas-sleigh-python-code-for.html</p>
3706, <p><span style="text-decoration: line-through">I'm getting a submission exception: present not inside the sleigh although the values look good to me and MATLAB metric calculation gives me the following:</span></p> <p><span style="text-decoration: line-through">&nbsp;</span></p> <p><span style="text-decoration: line-through">&quot;&quot;&quot;Dimensions check PASSED: Input and submission dimensions match</span><br><span style="text-decoration: line-through">Packages fit within sleigh check PASSED: No packages out of sleigh&quot;&quot;&quot;</span><br><br></p> <p><span style="text-decoration: line-through">Can anyone tell me what could be wrong?</span></p> <p>&nbsp;EDIT: Nevermind missed the v2 of MATLAB metric</p>
3706, <p>10 mins. In pure python</p>
3706, <p>Nice visualizations.&nbsp;</p> <p>What did you use?</p>
3706, <p>[quote=razapor;36204]</p> <p>Gravity compaction of an H=1040510 solution reduced H by only 1112. My layer lower surfaces are very random. The Gravity code is small and quick.</p> <p>[/quote]</p> <p>wont gravity compaction affect the order ?</p>
3706, <p>[quote=Sergey Yurgenson;36222]</p> <p><span style="line-height: 1.4">[quote]</span></p> <p><span style="line-height: 1.4">Now you have to tell us what did you do ;)</span></p> <p>[/quote]</p> <p>hehe.. let me enjoy the first place for a few days :P lol</p>
3706, <p>[quote=Hebrew Hammer;36226]</p> <p>I was just about to ask a question about gravity when I saw this discussion. So I'll go ahead and ask it here. Are solutions in which some presents are floating (ie with no support under them) like in some of the examples above valid? Or do presents have to have another present immediately below them in at least one unit square (like in Tetris)?</p> <p>[/quote]</p> <p>Presents can float. There is no gravity</p>
3706, <p>What is the best score that you have attained with one error being zero?</p> <p>Mine is&nbsp;2678476 with ordering error = 0</p>
3706, <p>[quote=Ants;36332]</p> <p>Oh...hmm.... :-/</p> <p>Now as the theoretical limit is broken it would be a good time to fix the official benchmark and reevaluate all :)</p> <p>[/quote]</p> <p>the sleigh is magic!</p>
3706, <p>im being a bit lazy and would like to ask if someone has written any code for metric calculation in c++ and would like to share&nbsp;</p>
3706, <p>[quote=WBTtheFROG;36337]</p> <p>[quote=WBTtheFROG;36316] it may even be possible to beat 1612062 and that would be pretty interesting.&nbsp; After the conversation in this thread I'd be impressed by someone who figures out a way to do that and see that as within the spirit of the competition.</p> <p>[/quote]</p> <p>Congrats Abhishek.&nbsp; The sleigh is magic :-)</p> <p>[/quote]</p> <p>Yep. but I dont see why you are congratulating for I havent won...</p>
3706, <p>origin is (111)</p>
3706, <p>[quote=Kyle Willett;37068]</p> <p>Hi everyone</p> <p>Quick update - I've been working on the dataset much of this evening. It's not 100% fixed yet but there are some bugs in the data that violate the constraints laid out in the decision tree. I've found the source for some of them (a very small percentage of raw data classifications didn't match numbers due to our weighting scheme) but the normalized values for your data still aren't agreeing perfectly. I will continue working on this tomorrow.</p> <p>If we can isolate the cause of the troublesome galaxies we'll either fix or remove them from the data. We'll keep you updated about any changes as soon as we've made a decision. In the meantime if you're thinking about solutions I would assume that the ultimate dataset will obey the constraints in the decision tree that have already been described.</p> <p>[/quote]</p> <p>Does it mean the leaderboard will be re-evaluated?</p>
3706, <p>imo these violations should be eliminated from the data and new data should be given. It wont be helpful for the organization to get predictions and model on a flawed data.</p>
3706, <p>@david have you read &quot;the galaxy zoo decision tree&quot; page?</p>
3706, <p>[quote=Black Magic;37237]</p> <p>I am totally confused. I had some submissions before and now I see leaderboard has only 8 entries.</p> <p>What happened? is there a minimum number of submissions per week. I am re-entering competition after a week</p> <p>[/quote]</p> <p>This happened:&nbsp;https://www.kaggle.com/c/galaxy-zoo-the-galaxy-challenge/forums/t/6794/data-constraints-and-competition-reboot</p>
3706, <p>Since I've been ridiculed previously for the beating the benchmark posts this time I wont post the code but a method which would enable you to beat the central pixel benchmark and would take only 20mins.</p> <p>&nbsp;</p> <p>Step 1 : for all images in train and test resize to 50x50 and vectorize to 1-D array.</p> <p>Step 2: Run the RandomForestRegression from scikit-learn on the train and test set with 10 estimators.</p> <p>Step 3: Submit Results</p> <p>Step 4: You have beaten the benchmark</p> <p>Step 5: Click thanks if this post helped you ;)</p>
3706, <p>aint that true :P&nbsp;</p> <p>[quote=Giulio;37245]</p> <p>[quote=Abhishek;37233]</p> <p>Since I've been ridiculed previously for the beating the benchmark posts&nbsp;</p> <p>[/quote]</p> <p>LOL.</p> <p>[/quote]</p>
3706, <p>[quote=Frank Schilder;37329]</p> <p><span style="line-height: 1.4">The question is whether it is worthwhile to invest the extra time to install cv2. OpenCV seems to offer quite a lot of useful resources for image processing though. Anybody out there who was able to run cv2/OpenCV on a Mac?</span></p> <p>[/quote]</p> <p>&nbsp;</p> <p>I work on a mac and installed opencv using homebrew.&nbsp;</p>
3706, <p>Im using OSX mavericks. I didnt face any conflicts with scipy and all. As I dont have a GPU on my mac I cannot use theano/pylearn2 for convolutional neural networks. However I can use neural networks which do not require GPU and thus theano/pylearn2 are working fine for me. I had a hard time installing opencv but I finally managed to install it. (https://github.com/Homebrew/homebrew-science/issues/402)</p> <p>[quote=Domcastro;37337]</p> <p>Abhishek - what MacOS are you using? Out of interest have you manged to get Theano and Pylearn2 working with GPU? I had to install Anaconda Python to get Python on my Mac. I'm on Mac OS X Lion 10.7.5 (11G63) - there was a conflict with scipy (xcodes/gcc/lvcc ?&nbsp; etc)</p> <p>[/quote]</p>
3706, <p>@domcastro did you install an external GPU for your Mac?&nbsp;</p> <p>[quote=Domcastro;37337]</p> <p>Abhishek - what MacOS are you using? Out of interest have you manged to get Theano and Pylearn2 working with GPU? I had to install Anaconda Python to get Python on my Mac. I'm on Mac OS X Lion 10.7.5 (11G63) - there was a conflict with scipy (xcodes/gcc/lvcc ?&nbsp; etc) when I tried to install packages individually</p> <p>[/quote]</p>
3706, <p>[quote=George Oblapenko;41393]</p> <p>If I understood the decision tree correctly it goes as follows (I'll denote the probability of a class by P(class)):</p> <p>P(1.1) + P(1.2) + P(1.3) = 1.0<br>P(7.1) + P(7.2) + P(7.3) = 1.0 * P(1.1)</p> <p>And so on. Unless explicitly stated that the probabilities should sum to 1 the probabilities of a class should sum to the probability of the previous node in the decision tree.</p> <p>[/quote]</p>  <p>What does the last statement mean? Can you elaborate further with one more example please?</p>
3706, <p>[quote=Triskelion;36633]</p> <p>Have there been done benchmarks on similar (or the same) data sets?</p> <p>What is the state-of-the-art RMSE to beat?</p> <p>[/quote]</p> <p>Yes some papers would be great</p>
3706, <p>I dont know what's the fuss is all about when the same algorithm you used previously can give you similar results. Isnt it so? or am i missing something here?</p>
3706, <p>what just happened?</p>
3706, <p>[quote=Wen K Luo;38951]</p> <p><span style="line-height: 1.4">I assume you haven't actually looked at the data yet or are using some extremely blackbox methods given that you've already submitted but don't know this potential to get a perfect score?</span></p> <p>[/quote]</p> <p>&nbsp;</p> <p>I havent started with this competition yet</p>
3706, <p>I have just made the benchmark submission and tried some small approaches. Not started as in a full-fledged way :)</p>
3706, <p>Has it changed now since the competition is no more a knowledge competition?&nbsp;</p>
3706, <p>So only the last 3 are categorical ones?</p>
3706, <p>[quote=Parthiban Gowthaman;37626]</p> <p>Any one facing this problem&nbsp;</p> <p>In train data</p> <p>Even after filling NA by 0 in scikit using&nbsp;</p> <p>df.fillna(0)</p> <p>I am getting error like</p> <p>ValueError: Array contains NaN or infinity.</p> <p>&nbsp;</p> <p>[/quote]</p> <p>Use X = numpy.nan_to_num(X)</p>
3706, <p>https://www.kaggle.com/c/loan-default-prediction/details/evaluation</p>
3706, <p>I use pandas and it takes me a few seconds to read data and I have no problems with memory. My configuration is same as yours.</p>
3706, <p>Can anyone tell me if cross-validation on the training set is giving them similar results on the leaderboard?&nbsp;</p>
3706, <p>[quote=Triskelion;38064]</p> <p>A problem I find with this approach is that if you predict that nobody will default your plain accuracy is still very high. You'll miss about 9% of defaulters so your accuracy is about 91%.</p> <p><span style="line-height: 1.4">[/quote]</span></p> <p><span style="line-height: 1.4">this dataset is highly skewed and using a metric like accuracy measure wont give you proper results. If you are building a defaulter/non-defaulter system first you should be using area under the ROC curve to evaluate your model.</span></p>
3706, <p>Hi all</p> <p>It seems to be pretty difficult to beat the zero benchmark in this competition. So I wrote a quick and dirty script to achieve that. ;)</p> <p>The main idea is feature selection and a two model approach. One for predicting the loan deafult and another for predicting the LGD.</p> <p>The code is located at my blog:&nbsp;http://beatingthebenchmark.blogspot.de/ and one copy can be downloaded from here. The code doesnot have any comments so feel free to ask about any part which is un-understandable.</p> <p>This version will give you&nbsp;~0.83265 on the leaderboard (Thanks Triskelion for verifying)</p> <p>Dont forget to click &quot;thanks&quot; if this post helped you in any way :)</p>
3706, <p>I havent yet checked how the code performs on the leaderboard. Can someone tell please?</p>
3706, <p>[quote=Triskelion;38231]</p> <p>[quote=Abhishek;38229]</p> <p>I havent yet checked how the code performs on the leaderboard. Can someone tell please?</p> <p>[/quote]</p> <p>~0.83265.</p> <p>An incredibly high score. You are at&nbsp;~0.83261. Took about 30 minutes to run and needs ALL the memory. Latest version of everything (with Scikit-learn this seems to matter).</p> <p>[/quote]</p> <p>&nbsp;</p> <p>Great. So its expected. Btw LinearSVC will select different features on every run so its better to find an alternative ;)</p>
3706, <p>Created a model for predicting default using logistic regression. The features for this model were selected using LinearSVC. The defaulters were kept for further processesing (Logistic regression and all the available features) and the non-defaulters were assigned a prediction of zero. It should be noted that I have only used the numerical features in the code and thus there is a lot of room for improvement.&nbsp;</p> <p>@Triskelion what is your current AUC if i may ask?</p> <p>&nbsp;</p>
3706, <p>my current AUC is around 0.71. After some feature selection I've already achieved 0.74 (results not on LB yet)</p>
3706, <p>One more hint: If you remove the feature selection using LinearSVM the results would be stable and would still beat the benchmark ;)</p>
3706, <p>[quote=Giulio;38242]</p> <p>Thanks Abhishek this is really interesting!</p> <p>I've a question: do you guys believe that this type of approach (which is what I have been trying as well) will lead to substantial improvement if refined further? I just feel that unlike other Kaggle competitions where you keep improving little by little by building on an initial approach it will be hard to refine the approach we have all been trying to move into low 0.7 territory.</p> <p>Any thoughts?</p> <p>[/quote]</p> <p>This approach wont win the competition but wont overfit ;)</p>
3706, <p>[quote=John Galt;38302]</p> <p>What is the peak memory usage of this python code? I can't seem to run on 8GB RAM.</p> <p>[/quote]</p> <p>I run it on my mac with 8gb ram with some other apps open and it works perfectly :)</p>
3706, <p>[quote=Karthik Ramarathnam;40172]</p> <p>Can someone please explain what the column headings stand for ? I am quite confused as all the headings are with &quot;f&quot; and it does not make any business sense to me ..... without an understanding of the business context it will not be possible to proceed with the analysis.</p> <p>Thanks in advance for the help.</p> <p>[/quote]</p> <p>Do you guys never read the other forum posts at all?</p>
3706, <p>did you try L2?</p>
3706, <p>[quote=barisumog;39069]</p> <p>I talked to a friend who works in banking. I explained the premise and told that 0.91 auc can be achieved with just 2 features. He laughed and asked if the features were &quot;probably gonna default&quot; and &quot;most probably gonna default&quot;.</p> <p>I currently have 0.95 auc on the train set with local cv. I told him so and he replied &quot;Nah you'd already be kidnapped by now.&quot;</p> <p>So this is my friendly warning to those at the top of the leaderboard. Lock your doors and windows.</p> <p>=)</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>[/quote]</p> <p>So you took help of an outsider &nbsp;does that mean you broke the rules?&nbsp;</p>
3706, <p>That's normal. But a 1000 &nbsp;places is not :P</p> <p>[quote=&#9757;;39204]</p> <p>I've been seeing this under my username on the leaderboard while the submission is scoring. After it's done scoring it shows the normal message. &quot;Your submission scored ... You moved up 1000 places...&quot;</p> <p>[/quote]</p>
3706, <p>[quote=DataGeek;39208]</p> <p>[quote=William Cukierski;39207]</p> <p>Can you guys clarify what's going on for me? I pulled the stats and the mean scoring duration for this comp is &lt; 2 seconds. The longest ever took 30 seconds. You are saying your submission score reads NULL and then changes to its rightful value?</p> <p>[/quote]</p> <p>Yes William. It says your submission scored NULL and then shows me my current leader board score. It happened with me one time only (last submission).&nbsp;</p> <p>[/quote]</p> <p>&nbsp;</p> <p>The LB always shows &quot;NULL&quot; when its scoring your submission. Its nothing new.</p>
3706, <p>[quote=Giulio;39414]</p> <p>[quote]</p> <p>&nbsp;classification scores 0.99 AUC / 0.95 F-score on validation</p> <p>[/quote]</p> <p>BTW... How many features does your 0.99 AUC model tend to include?</p> <p>[/quote]</p> <p>0.99 AUC here too. 2 features</p>
3706, <p>[quote=yr;39423]</p> <p>[quote=Abhishek;39420]</p> <p>[quote=Giulio;39414]</p> <p>[quote]</p> <p>&nbsp;classification scores 0.99 AUC / 0.95 F-score on validation</p> <p>[/quote]</p> <p>BTW... How many features does your 0.99 AUC model tend to include?</p> <p>[/quote]</p> <p>0.99 AUC here too. 2 features</p> <p>[/quote]</p> <p>The same&nbsp;0.95 F-score? I used 5 features AUC aroud 0.985 and F1-score around 0.88. Guess I might spend some effort to find these golden features</p> <p>Regards</p> <p>[/quote]</p> <p>Obviously not. &nbsp;See my rank :P</p>
3706, <p>[quote=3pletdad;39517]</p> <p>[quote=yr;39508]</p> <p>I see what you mean. I tried a few (50 times) different random splits of train/test while glm may not converge sometimes it does a fair job with my current selected features resulting in average auc around 0.9846 &nbsp;(sd=0.001) and f1-score around 0.8963 (sd=0.0036) on the test set.</p> <p>Given that there are about 100k samples and a few features (about&nbsp;10) I assume that a simple classifier like lr/glm is less prone to overfitting and regularization may do a little help.</p> <p>Regards</p> <p>[/quote]</p> <p>I wonder if I'm doing something wrong. Everyone seems to be getting 0.9846 AUC with ~10 features and I was able to get there with 4. Is there ever a time less feature is bad?</p> <p>[/quote]</p> <p>I have only 3-4 for 0.99</p>
3706, <p>my F1 is 0.91 for AUC = 0.99&nbsp;</p>
3706, <p>[quote=Neil Summers;39353]</p> <p>I got a MSE of 0.47.</p> <p>[/quote]</p> <p>MAE or MSE?</p>
3706, <p>Kaggle usually removes all the cheaters after the competition finishes.</p>
3706, <p>It's just 20% data. &nbsp;You should trust cv</p>
3706, <p>[quote=paparator;40336]</p> <p>i add 250 (can be tuned) zero loss instance to non-zero loss set.</p> <p>[/quote]</p> <p>what does this mean?</p>
3706, <p>[quote=David McGarry;38112]</p> <p><span style="line-height: 1.4">Well I guess the leakage variables had a very little impact on my score but i'm not loving the leadboard going from ~200 to 25.</span></p> <p>[/quote]</p> <p>They will be back. And more people wil join...</p>
3706, <p>In my opinion most people are using the so called leakage variables just to get good ranks and thus the Kaggle points. The problem is for other people who are trying hard to win or get the masters badge without utilizing the leakage as no one is going to verify your model unless u r a winner. I think Kaggle should have changed the whole dataset or shuffled the samples and variables when the leakage was found instead of deleting the old variables</p>
3706, <p>any updates on the python sample code?</p>
3706, <p>I have attached the python code to &quot;Last Quoted Plan Benchmark&quot;. It uses pandas.</p> <p>Click thanks if it helped ;)</p>
3706, <p>if it has errors its not counted towards your daily submission limit and you can submit a new one without errors.</p>
3706, <p>[quote=Luca Massaron;41316]</p> <p>By the way will points be awarded (...) or is the competition really just for fun?</p> <p>[/quote]</p>  <p>Hey u r winning :P</p>
3706, <p>What's the score? &nbsp;;)&nbsp;</p>
3706, <p>tbh 89 variables is nothing</p>
3706, <p>man. I want this competition on main page!</p>
3706, <p>Hi</p> <p>My latest submission shows pending. Are there any competition admins for inclass competitions?</p>
3706, <p>Well the new one got scored but the previous one is still pending. Do I get a submission refund? :P</p>
3706, 
3706, <p>[quote=Frans Slothouber;49941]</p> <p><strong>If the three winners can send me their postal address ( email me at frans.slothouber (at) gmail.com ) I will send them some 'loot' as a reward for their achievements.</strong></p> <p>[/quote]</p>  <p>What's the loot? :D</p>
3706, <p>these variables are the same right?&nbsp;</p>  <p>repeaters = repeattrips &gt; 0 ?</p>
3706, <p>So its reapeat to the shop/brand/category?</p>
3706, <p>[quote=David;44469]</p> <p>It's possible to use repeattrips to train the model but not use it explicitly as a feature.&nbsp;</p> <p>[/quote]</p> <p>Do let us know if you find out how</p>
3706, <p>A negative value in productquantity and purchaseamount indicates a return</p>
3706, <p>Are all test users in testhistory present in transactions.csv?</p>
3706, <p>Yeah. got that. There was something wrong with what I was doing and was getting less number ;) . thanks anyways</p>
3706, <p>Use&nbsp;awk -F '$4 = 706' transactions &gt; trans_cat.csv &nbsp;and so on to avoid junk rows</p>
3706, <p>dict is always faster than the list in python (comparable for small data)</p>
3706, <p>Leave AUC aloneee....</p>
3706, <p>[quote=Art_Ghazaryan;42047]</p> <p>Triskelion</p> <p>Your thoughts of creating new features from the transaction data are absolutely right. One note though. Brand is associated with category and company So if a shopper buys a brand he/she buys the category and the company as well. So you can combine these three into one feature. To be more accurate I think category and company could be left out because the offers are made on brands not categories or companies. I think a new binary feature that indicates if a shopper has purchased the brand prior to the offer would be sufficient. In other words a shopper who has prior exposure to the brand is more likely to respond to the offer than the shopper who has not had the exposure.&nbsp;</p> <p>[/quote]</p> <p>No.</p> <p>Brand is not associated with category (with company?). A sparkling water bottle is a category. A company can make many brands but all brands do not belong to one company. Brand is a trademark of a given company. If you buy kinley still mineral water you are buying from department = water category = still water brand = kinley company = coca cola</p>
3706, <p>statsmodels</p>
3706, <p>0.59104 is also good. You might wanna improve on that.</p>
3706, <p>I'm on a mac too and my score with VW in the begining was also 0.59410</p>
3706, <p>AFAIK Triskelion is using Windows.</p>
3706, <p>split -l 200000&nbsp;transactions.csv&nbsp;part_</p>
3706, <p>Sample submission contains all zeros. The 0.5 score on the leaderboard is Area under the ROC curve (AUC). If you want to learn in detail how AUC is calculated:&nbsp;http://en.wikipedia.org/wiki/Receiver_operating_characteristic</p>
3706, <p>They used latest version of vowpal-wabbit</p>
3706, <p>See. I told you :P</p>
3706, <p>&lt;10 mins. &lt;2gb ram</p>
3706, <p>[quote=Brenton Mallen;44673]</p> <p>Where is the &quot;official Kaggle metric code&quot; located? &nbsp;The evaluation page just points to Wikipedia.</p> <p>[/quote]</p> <p>here:&nbsp;https://www.kaggle.com/wiki/AreaUnderCurve</p>
3706, <p>The sponsor is anonymous in this competition.</p>
3706, <p>[quote=ACS69;50485]</p> <p>The worrying thing is that some of the dummy accounts are currently being deleted. Havingfun at number 13 exists no more - does this mean someone in the Top 12 is a cheat?</p> <p>[/quote]</p>  <p>Yes!</p>
3706, <p>[quote=saikumar allaka;49598]</p> <p>Total number of rows in transactions dataset is 349655789 so i was thinking how to pre-process and extract features.</p> <p>Answer is hadoop and Hive Partitioning and bucketing..</p> <p>Soon i hope to be in top 10.</p> <p>[/quote]</p>  <p>takes me 20-30 mins to extract ~300 features from the&nbsp;349655789 rows transaction data with less than 2 gb ram use.&nbsp;</p>
3706, <p>[quote=Chitrasen;50062]</p> <p>[quote=Abhishek;49617]</p> <p>takes me 20-30 mins to extract ~300 features from the&nbsp;349655789 rows transaction data with less than 2 gb ram use.&nbsp;</p> <p>[/quote]</p> <p>Abhishek how many cores are in use ? Do you split the file for multi-processing ?</p> <p>Only streaming the 22gb file takes me more than 1 hour.</p> <p>[/quote]</p>  <p>without splitting and no multiprocessing. Yes i have an SSD and that makes all the difference ;)</p>
3706, <p>hahaha :D</p>
3706, <p>DELETED</p>
3706, <p>Very high benchmark! Thanks anyways ;)</p>
3706, <p>Hi</p> <p>Will the final rankings be the &quot;real&quot; rankings or would they be like the last competition that we had?</p>  <p>Thank you</p>
3706, <p>Score will be &gt;3.8 :D</p>
3706, <p>Is the distribution of signal and background same in public and private leaderboards?</p>
3706, <p>I was waiting for someone to post something like this about xgboost. This confirms that its not just me.&nbsp;</p>
3706, <p>I'm also trying to test XgBoost with some dummy data and I know AUC cannot be 0.5 as for the same dataset using sklearn I get AUC &gt; 0.9</p>
3706, <p>I'm doing something like:</p>  <p># xgmat = xgb.DMatrix(X_train label=y_train)<br> # watchlist = [ (xgmat'train') ]<br> # num_round = 150<br># bst = xgb.train(plst xgmat num_round watchlist)<br> # bst.save_model('xg.model')</p> <p><br> # xgmat = xgb.DMatrix(X_test)<br> # bst = xgb.Booster({'nthread':8})<br> # bst.load_model('xg.model')<br> # y_pred = bst.predict( xgmat )</p>
3706, <p>Fixes for me too. Thank you!</p>
3706, <p>It seems XgBoost overfits way too much :)</p>
3706, <p>[quote=Peter Williams;47385]</p> <p>Sklearn's&nbsp;<a href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html">GradientBoostingClassifier</a>&nbsp;doesn't appear to have</p> <ul> <li>handling of missing values</li> <li>class weighting&nbsp;</li> <li>an auc target (I don't know if this is important)</li> </ul> <p>I can see at least one <a href="http://www.kaggle.com/users/1955/gilles-louppe">sklearn person</a>&nbsp;competing here. Can the sklearn people tell us how to configure&nbsp; GradientBoostingClassifier for missing values uneven class weights and an auc target?&nbsp;</p> <p>[/quote]</p> <p>- missing values can be handled by using preprocessing.Imputer()</p> <p>- check this for class weighting:&nbsp;https://github.com/scikit-learn/scikit-learn/pull/3224&nbsp;</p> <p>- I dont know what you mean.</p>  <p>By the way there are two sklearn persons that I can see on the LB :D</p>
3706, <p>IMO he means something that can beat the benchmark</p>
3706, <p>IMO its directed towards other competitors for hints and/or discussion about handling missing data.</p>
3706, <p>Whats the leaderboard AMS for this?</p>
3706, <p>@Bing Xu they already listed it!</p>
3706, <p>use:</p> <p>import pandas as pd</p> <p>data = pd.read_csv('file_name')</p>
3706, <p>[quote=Lubo&#353; Motl;48988]</p> <p>Thanks Balazs!</p> <p>I just found a stunningly brutal error in my code which didn't prevent it from getting to #3 at some point. Holy cow it's like treating the numbers in training.csv as having units of meters and test.csv as having feet to recall a NASA (?) incident. My error is really more devastating than that. And this error spoiled most of the features of each test.csv event. How could it work at all? ;-)</p> <p>Everybody: What do you think will happen when I submit a result from the corrected code tomorrow? ;-)</p> <p>[/quote]</p>  <p>Why wait till tomorrow?</p>
3706, <p>Can we see some of these analysis now?&nbsp;</p>
3706, <p>LB score?</p>
3706, <p>Why is this such a big deal?</p>
3706, <p>Kaggle is now verifying accounts and duplicate accounts are deleted at the end of competition anyways. I was saying two people with the same name is not a big deal!</p>
3706, <p>Miroslaw is back.... &nbsp;\m/</p>
3706, <p>[quote=Algoasaurus;53810]</p> <p>[quote=Giulio;53806]</p> <p>&nbsp;... you're putting many folks in a place where they need to decide whether staying true to months worth of own work is ethically worthwhile while tens of people are jumping ahead of you thanks to somebody else's work.</p> <p>[/quote]</p> <p>Exactly Giulio. Thank you for speaking so eloquently to these points.</p> <p>[/quote]</p> <p>I also agree! Sharing is fine but not at the last minute. Unfortunately this is a new Kaggle trend!</p>
3706, <p>All your submissions will be ranked on the leaderboard. But you should remember that the leaderboard we see right now is Public Leaderboard which evaluates and scores only 18% of your submission. The Private Leaderboard (i.e the other 82%) will be revealed as soon as the competition finishes. Thats why you have the choice of choosing two submissions which you think will perform best on the overall dataset and not just the 18% that you are seeing currently</p>
3706, <p>The set seems proper to me. If you want to use pandas to read it ive posted a benchmark code in another thread.</p>
3706, <p>Well this is my favorite forum topic it seems. :D Whats your cross validation score? Mine is very high compared to the leadearboard score!</p>
3706, <p>whats the difference for you guys? mine is approx 0.2!!</p>
3706, <p>Damn!</p>
3706, <p>I wrote a quick script to beat the benchmarks in this competition using only the essay data. The code can be grabbed here:&nbsp;http://beatingthebenchmark.blogspot.de/</p> <p>or downloaded from this post.&nbsp;</p> <p>Dont forget to click &quot;Thanks&quot; if it helped you in any way. :)</p> <p>If you dont understand anything feel free to ask.</p>
3706, <p>The evaluation metric is AUC so you can submit any real values. Why dont you try with essay lengths as responses? You have 5 submissions a day ;)</p>
3706, <p>wow... just noticed it.. great job(?) by the team&nbsp;</p>
3706, <p>still no news from anywhere? (wondering)</p>
3706, <p>Who is that?</p>
3706, <p>Read:&nbsp;https://www.kaggle.com/wiki/AreaUnderCurve</p>
3706, <p>You have to submit probabilities only.</p>
3706, <p>Submit anything you want</p>
3706, <p>This is simply amazing! Is there something wrong or is it genuine?</p>
3706, <p>It seems they fixed it now :D</p>
3706, <p>Take a look here:&nbsp;http://stackoverflow.com/questions/1871524/convert-from-json-to-csv-using-python</p>
3706, <p>Hi</p>  <p>Are the labels in the train data ordered in decreasing order of their significance?</p>
3706, <p>I just submitted a csv with labels as floats eg. 1.0 2.0. Instead of raising an error the system evaluated my submission to 0.0000 and I lost one&nbsp;submission.&nbsp;</p>
3706, <p>Thanks</p>
3706, <p>Whats the LB score for this?</p>
3706, <p>Change to naive bayes to get .42-.43 ;)</p>
3706, <p>Is it for real or some kind of mistake happened here? :D</p>
3706, <p>[quote=Ulo Gulo;48981]</p> <p>Is anybody able to reproduce the benchmark?</p> <p>[/quote]</p>  <p>17 &#8595;3 wenxin zhao <br>0.80804<br>1 Sat 07 Jun 2014 19:52:23<br>18 &#8595;3 Vivant Shen Team <br>0.80804<br>5 Wed 11 Jun 2014 06:33:23 (-3.4d)<br>19 &#8595;2 Ali Ziat <br>0.80804<br>5 Wed 11 Jun 2014 00:00:30 (-23.5h)<br>20 &#8595;2 optimizer</p> <p>0.80804</p>
3706, <p>[quote=Ulo Gulo;48994]</p> <p>Thanks a lot KazAnova I used&nbsp;your approach to find a bug in my data flow. It seems to work now.</p> <p>[/quote]</p>  <p>Could you tell us what the bug was?</p>
3706, <p>Now that you have beaten the benchmark are your CV scores close to LB now?</p>
3706, <p>My CV is off by some extent. Cant say the exact value right now. The problem is my last 3 models scored the same on the LB :D and had different CV. I have a model with CV AUC of 0.97 but I havent tried it on the LB yet as I'm afraid it might be overfitting :)</p>
3706, <p>[quote=Kevin Hu;50127]</p> <p>@Upul I also have this problem before and inspired by David Thaler the reason why is I put the test data to do feature selection the result is very optimistic I don't know if you to do feature selection as I am.&nbsp;usually It must be the test data already&nbsp;exposure before test.</p> <p>[/quote]</p>  <p>how did you use test data for feature selection?</p>
3706, <p>Well I tried the first method long back. Got some good features and CV was 0.97 however LB score was 0.78 ;)</p>
3706, <p>No</p>
3706, <p>I would like to know the views of fellow competitors in this regard. As we know the test data has been inflated heavily what do you think is the actual test dataset size we are dealing with?</p>  <p>I say ~200</p>
3706, <p>:D</p>
3706, <p>after the competition ends all cheaters will be removed from the leaderboard. So dont worry :D</p>
3706, <p>As most of my submissions have the same score I was wondering if I don't choose any submissions which two submissions will be chosen by Kaggle?&nbsp;</p>
3706, <p>I think so too but I think a confirmation from one of the Admins would be nice</p>
3706, <p>[quote=Rogers F Silva;50790]</p> <p>Also make sure to upload all the code (including your feature selection strategies) required to replicate your selected submission models.</p> <p>[/quote]</p> <p>Do we need to upload our model before the timeline? I don't see any links for uploading the model.</p>
3706, <p>Thank you...</p>
3706, <p>and so many cheaters in this one. so dont worry about the ranks :P</p>
3706, <p>24mins to go... and we will know the luckiest person :P&nbsp;</p>
3706, <p>lolllll... my biggest fall...lol</p>
3706, <p>pretty amazing that a person who joined a couple of weeks back won with only one submission. must be a f*cking stable model! would really like to know about it :P</p>
3706, <p>[quote=Giulio;50961]</p> <p>I'm really glad I only spent&nbsp;maybe 6&nbsp;hours&nbsp;on this competition :-)</p> <p>[/quote]</p> <p>me too... per day :P</p>
3706, <p>this is my best:</p> <p>public: 0.59375 &nbsp; &nbsp;private: 0.85641</p> <p>:D im just laughing out loud....&nbsp;</p>
3706, <p>time to concentrate on large datasets.... :D</p>
3706, <p>[quote=Sandro;50977]</p> <p>I would have won if I would had chosen my worst submission according to the public leaderboard. Public: 0.62946 Private: 0.94359 &nbsp;:P</p> <p>[/quote]</p> <p>what model was that?</p>
3706, <p>model for which abs(public-private) &nbsp;~ 0 (or very less)</p>
3706, <p>[quote=Giulio;50972]</p> <p>BTW- congratulation David! Curious to know more about your model.</p> <p>[/quote]</p> <p>Really. The only model I would like to know about in this competition! Congrats David!</p>
3706, <p>[quote=Giulio;51031]</p> <p>No cheaters removed yet?</p> <p>[/quote]</p> <p>weird</p>
3706, <p>use the split command in terminal.</p>  <p>Also you have to use fit first then you have to do partial_fit with classes and then you can use partial_fit</p>
3706, <p>Fit on the first chunk of data that you have and partial_fit on the subsequent chunks</p>
3706, <p>[quote=G Alexander;52052]</p> <p>With current leader values around 0.44xx I don't see how this much better than tossing a coin. &nbsp;This advertising (click vs no-click) is only ~23% successful. I wonder if advertisers know this? &nbsp;</p> <p>This indicates to&nbsp;me that either the algorithms being used really aren't that good or the data is dirty i.e. noisy sampled or categorized poorly.</p> <p>[/quote]</p> <p>23% for click/no-click? I dont think so!&nbsp;</p>
3706, <p>whats the LB score?</p>
3706, <p>It seems the test data set provided is corrupt. Train expands without any problems</p>
3706, <p>Good old auc is like 0.99 already from the sample benchmark. &nbsp;</p>
3706, <p>bummer :P</p>
3706, <p>well it works for me..</p>
3706, <p>omg! you have two train sets!</p>
3706, <p>You stole my copyrighted topic name :P lol</p>
3706, <p>Thats a very high benchmark! &nbsp;:(</p>
3706, <p>Amazing!</p>
3706, <p>Amazing!</p>
3706, <p>^^sarcasm? Right?&nbsp;</p>
3706, <p>Congrats guys! You deserved it!!!</p> <p>Special congrats to Barisumog for becoming a Master!!!</p> <p>Looking forward to reading about your approach!</p>
3706, <p>Hello again!</p> <p>For all the beginners out there I'm providing you with a benchmark which will get you around ~0.29112 on the public leaderboard. The benchmark uses only 7 variables and runs in a couple of minutes. Go nuts!</p> <p>Dont forget to click &quot;Thanks&quot; as usual ;)</p>
3706, <p>8 gb</p>
3706, <p>[quote=ACS69;52770]</p> <p>[quote=rcarson;52767]</p> <p>[quote=James King;52765]</p> <p>Great answer I nominate Triskelion for <em>de facto&nbsp;</em>competition admin. Also for best avatar...</p> <p>[/quote]</p> <p>Absolutely</p> <p>[/quote]</p> <p>Yes Triskelion can be the Competitor's Union Rep! He has my vote</p> <p>[/quote]</p>  <p>+1</p>
3706, <p>a couple of mins after the competition ends!</p>
3706, <p>We still have anything can happen tuesday :P</p>
3706, <p>What about Stumbleupon? :P</p>
3706, <p>Cheaters were removed!&nbsp;</p>
3706, <p>[quote=Max Christ;53261]</p> <p>Based on what do you assume that I or Ben are&nbsp;cheaters? I can only assume that it happened&nbsp;because we uploaded the submissions from the same IP environment. (from work) I assume that there are even more people from our company who are participating in waggle competitions.&nbsp;</p> <p>But we used different servers and wrote different code.</p> <p>I feel bad because I spend a lot of time improving my models and then I get deleted without any notification. Maybe next time I score in the top 10 and I get deleted again. Whats the point in participating if in the end you can get arbitrarily&nbsp;deleted?</p> <p>[/quote]</p>  <p>I never said you cheated. Kaggle removes cheaters after every competition. If you think its a mistake follow ACS69's advice and write to them asap</p>
3706, <p>[quote=Soheil Hooshdaran;57401]</p> <p>What is R?</p> <p>[/quote]</p> <p>Question of the century!</p>
3706, <p>http://www.r-project.org/</p>
3706, <p>[quote=Steven Du;56286]</p> <p>Finally I&nbsp;manage to&nbsp;download these&nbsp;data.&nbsp;</p> <p>[/quote]</p> <p>how?</p>
3706, <p>Just starting this competition... ;)</p> <p>interictal = 0</p> <p>preictal = 1</p>  <p>am i correct?</p>
3706, <p>lol i dont know why i have negative vote in this one. if you know whats correct you can reply to this thread&nbsp;</p>
3706, <p>thanks... whats smb btw? :P</p>
3706, <p>somebody gonna get hurt real bad :P :P</p>
3706, <p>yes</p>
3706, <p>Yeah. &nbsp;It will happen if you don't read the rules properly!&nbsp;</p>
3706, <p>I think that is against the rules...&nbsp;</p>
3706, <p>Ive hit the wall. Cannot improve anymore no matter what I try.</p>
3706, <p>[quote=Rainman;52916]</p> <p>10 folds and standard deviations are:</p> <p>Ca: 0.099</p> <p>P: 0.3211</p> <p>pH: 0.0387</p> <p>SOC: 0.0676</p> <p>Sand: 0.0495</p> <p>(Yes I can't do P either :P)</p> <p>[/quote]</p> <p>this seems weird n incorrect</p>
3706, <p>Ahan I thought they are errors :D. Sorry about that!</p>
3706, <p>Let my rank drop &nbsp;:P</p>
3706, <p>the wait is over! :P</p>
3706, <p>Im afraid this is going to be similar to MLSP</p>
3706, <p>Hi Everyone!</p> <p>I'm back with a simple python script to beat the benchmark. The script is attached and is self explainatory!</p> <p>Let me know if you have any further questions.</p> <p>And please dont forget to &quot;vote-up&quot;!</p>  <p>LB score: 0.43621</p>
3706, 
3706, <p>[quote=AngryTomato;54146]</p> <p>the svm you used here is&nbsp;Multi-class classification? &nbsp;and besides train and test have 3594 cols why you use&nbsp;xtrain xtest = np.array(train)[::3578] np.array(test)[::3578] &nbsp;but no&nbsp;xtrain xtest = np.array(train)[::3594] np.array(test)[::3594]</p> <p>[/quote]</p>  <p>AngryTomato please don't be angry. I have included only the spectral features in the benchmark code ;)</p>
3706, <p>Look at the data indices I'm using</p>
3706, <p>Because its a benchmark code! I didnt want to post something which gives a top 10 rank. Try and include the spatial features and share your views...&nbsp;</p>
3706, <p>[quote=ACS69;54426]</p> <p>out of interest why do you feel the need to post high performance benchmark code? You pretty much have ruined Kaggle for me. thanks</p> <p>[/quote]</p>  <p>Why do you think its a high performing benchmark?</p>
3706, <p>As far as I remember the first person who used the benchmark was around 39-40</p>
3706, <p>[quote=ACS69;54437]</p> <p>And look at the Avito forum - you'll see Abhishek complaining about benchmark code</p> <p>[/quote]</p>  <p>Just because it was posted a week before the competition deadline.</p>
3706, <p><strong>Beating the Benchmark Version 2.0</strong> : If you create the dataset as specified in the data page i.e by removing the CO2 columns you will get a much higher score with the same old benchmark script.&nbsp;</p>
3706, <p>Great stuff. Anyone tried this yet?</p>
3706, <p>Hello all</p> <p>What do you guys think this competition's results are gonna look like? Is this going to be another MLSP?&nbsp;</p> <p>Any guesses on the winning score?</p>
3706, <p>You mean above the baseline? No improvements? That's kind of impossible</p>
3706, <p>Great stuff. &nbsp;What is the leaderboard score?&nbsp;</p>
3706, <p>I'll choose both. &nbsp;You are allowed to choose two submissions ;)&nbsp;</p>
3706, <p>Im looking for a team (individual) to merge with in order to share ideas and combine models. If anyone is interested please mail me :P</p>
3706, <p>&lt;0.40 in &lt;3mins</p>
3706, <p>Congrats to Yasser!!!!&nbsp;</p> <p><span style="line-height: 1.4">Damn &nbsp; yet again I'm not in top 3 :P</span></p>
3706, <p>Sorry guys &nbsp;as long as rules don't change about public sharing &nbsp;I'll keep on posting benchmarks... ;)&nbsp;</p>
3706, <p>[quote=ACS69;56465]</p> <p>[quote=Abhishek;56453]</p> <p>Sorry guys &nbsp;as long as rules don't change about public sharing &nbsp;I'll keep on posting benchmarks... ;)&nbsp;</p> <p>[/quote]</p> <p>Always the bridesmaid never the bride</p> <p>[/quote]</p>  <p>&nbsp;Haha.... &nbsp;Without you Kaggle and benchmarks are no fun! :D</p>
3706, <p>[quote=ACS69;56486]</p> <p>[quote=Abhishek;56482]</p> <p>[quote=ACS69;56465]</p> <p>[quote=Abhishek;56453]</p> <p>Sorry guys &nbsp;as long as rules don't change about public sharing &nbsp;I'll keep on posting benchmarks... ;)&nbsp;</p> <p>[/quote]</p> <p>Always the bridesmaid never the bride</p> <p>[/quote]</p> <p>&nbsp;Haha.... &nbsp;Without you Kaggle and benchmarks are no fun! :D</p> <p>[/quote]</p> <p>lol - same ;) I said to Kazanova that I wanted you to win this one as you deserved a prizewinner badge. But in the end glad I beat ya :P</p> <p>[/quote]</p>  <p>Im also glad that you beat me... :P :P you know why ...haha</p>
3706, <p>Can we see the individual private scores now?&nbsp;</p>
3706, <p>[quote=joycenv;56512]</p> <p>The leaderboard has now been verified. You should see both private and public leaderboard scores on your &quot;My Submissions&quot; page.</p> <p>[/quote]</p>  <p>Thank you. Is there something wrong with the points calculation?&nbsp;</p>
3706, <p>Any info on who is leading currently on private lb? :)&nbsp;</p>
3706, <p>Yep. &nbsp;That's my favorite one. &nbsp;I was in top 10 for the whole time line and then u can see wht happened! Lol</p>
3706, <p>Damn. I forgot the deadline and wanted to post a better model at the end. Anyways it was a good competition for me. I'll upload my model within a couple of days. Do winners get something ? ;)&nbsp;</p>
3706, <p>Great Stuff!</p>
3706, <p>[quote=James King;55838]</p> <p>[quote=Triskelion;55685]</p> <p>Not only is your online learning script able to beat the benchmark. It is able to get to #1!</p> <p>[/quote]</p> <p>Just to confirm - when adding the 46 hash interaction features to the algorithm I get a leader board score of&nbsp;0.0088416 can anyone confirm?</p> <p>[/quote]</p>  <p>Yes&nbsp;</p>
3706, <p>One more addition. &nbsp;Bits =24 with the 46 interaction features will give you my current score ;)&nbsp;</p>
3706, <p>whats the score?</p>
3706, <p>which part takes the most amount of time?</p>
3706, <p>Strange. It took me a few mins before the meta classifier.&nbsp;</p> <p>What score did 300 trees give you?</p>
3706, <p>lol</p>
3706, <p>Which sparse features did you create? Can you share the parameters of your SGDClassifier?</p>
3706, <p>Thats very common in advertising. People sleep at night and thats why there is no click</p>
3706, <p>Inspired by @tinrtgu (http://www.kaggle.com/users/185835/tinrtgu) I present to you a modified version of his script used in tradeshift challenge.</p> <p>LB Score: ~0.0885</p> <p>pypy takes around 5mins on my system.</p> <p>Vote up if this helped :D</p>
3706, <p>[quote=tinrtgu;56735]</p> <p>Do I need to vote up too?</p> <p>[/quote]</p> <p>No we will do that for you :D</p>
3706, <p>[quote=Eugene Nizhibitsky;56737]</p> <p>Do you actually have a cron task which monitors new competitions tests some methods and submits &quot;Beating ...&quot; posts? :)</p> <p>[/quote]</p> <p>Nopes... But thats a nice idea :P</p>
3706, 
3706, <p>It seems the revised dataset is available now! When will submissions open? :D</p>
3706, <p>[quote=Saptarshi Ray;57595]</p> <p>I have not found any click variable in test_rev2.csv.Do you guys also face the same thing</p> <p>[/quote]</p> <p>what will u predict if you have everything?</p>
3706, <p>v3. without change: LB:&nbsp;0.6931472</p>
3706, <p>[quote=ACS69;58335]</p> <p>strange - are you sure it's not writing the submission file wrong? as it's the sample submission score</p> <p>[/quote]</p> <p>sure. all predictions generated by v3 are 0.5</p>
3706, <p>[quote=Herimanitra;59523]</p> <p>If 'train.csv' is not comma separated It raises an error. Any hints?</p> <p>[quote=Yannick Martel;59216]</p> <p>@all</p> <p>....</p> <p>[ like in: python fast_solution_plus.py train --train train.csv -o first.model.gz ]</p> <p>...</p> <p>Yannick</p> <p>[/quote]</p> <p>[/quote]</p>  <p>Use:&nbsp;csv.DictReader(f delimiter='\t' quoting=csv.QUOTE_NONE)</p>
3706, <p>does it have any option to keep header intact?</p> <p>EDIT: found it&nbsp;</p>
3706, <p>shuffling did not give me a better score...</p>
3706, <p>[quote=lewis ml;58781]</p> <p>Just out of curiosity what's XBG? Or do you mean XGB?</p> <p>[/quote]</p> <p>xgboost.&nbsp;https://github.com/tqchen/xgboost</p>
3706, <p>More than half of the LB is based on same benchmark code. If people try new ideas the score should improve....</p>
3706, <p>I went to the store &nbsp;sat on Santa's lap &nbsp;asked him to bring Domcastro all kinds of crap :P</p>
3706, <p>For my current model it takes less than 30mins. I'm currently using a very simple model.</p> <p>EDIT: no feature selection and no subsampling.</p>
3706, <p>Use this:&nbsp;http://patternsonascreen.net/cuSVM.html</p>
3706, <p>just discussing and sharing everything offline...</p>
3706, <p>get 200gigs RAM use SGD from sklearn n u will get below 0.40 easily ;)</p>
3706, <p>There is something wrong with your test.csv</p>
3706, <p>do you have the dev version of&nbsp;libgfortran3?</p>
3706, <p>When will leaderboard be final?</p>
3706, <p>[quote=KazAnova;64170]</p> <p>[quote=Bluefool;64152]</p> <p>It is scary teaming up. I remember offending Kazanova asking if he had ever cheated before we teamed up in the Africa one.</p> <p>[/quote]</p> <p>I was not offended .I felt I had to quickly destroy all the evidence (delete my extra kaggle accounts namely Triskelion Phill Culiton and Abhishek  hide plagiarized papers burn my 3 different passports do a plastic surgery etc &nbsp;)!&nbsp;</p> <p>[/quote]</p> <p>hhahahaa :D :D</p>
3706, <p>Dataset has been updated due to some problems in the rating values</p>
3706, <p>[quote=Yogi Dhiman;58177]</p> <p>Trying this from past 4 days and I guess I am lost somewhere in half way.</p> <p>Some one can give me a Hint if I am using matrix factorization method !!! &nbsp;Abhishek :)</p> <p>[/quote]</p>  <p>http://sifter.org/~simon/journal/20061211.html this might help!</p>
3706, <p>Make it a regression task by one hot encoding the user and movie columns</p>
3706, <p>Thank you for participating. We will share methods soon.</p>
3706, <p>Version 2 is available now:&nbsp;https://inclass.kaggle.com/c/predict-movie-ratings-v2</p>
3706, <p>im confused. can anyone tell me how are the trainlabels related to training files?</p>
3706, <p>I dont think they will. its a pretty simple benchmark.</p>
3706, <p>or use this:&nbsp;http://www.kaggle.com/c/inria-bci-challenge/forums/t/11009/beating-the-benchmark :)</p>
3706, <p>Hello All</p> <p>I'm back with a quick script to beat the Random Forest benchmark using Random Forests :)&nbsp;</p> <p>The script is attached. If you dont understand anything feel free to ask.&nbsp;</p> <p>VOTE UP if it helped you in any way :)&nbsp;</p> <p>Thanks!</p> <p>comment with what score you get on LB</p>
3706, <p>updated and fixed some typos&nbsp;</p>
3706, <p>To the one who gave a - 1 please explain why!&nbsp;</p>
3706, <p>Sorry im not fond of R. :(</p>
3706, <p>or do this: sudo pip install numexpr</p>
3706, <p>that will just increase the chances of overfitting . the dataset is very small...</p>
3706, <p>They remove the cheaters after competition ends. So dont worry!</p>
3706, <p>i think this will be like africa challenge or MLSP challenge.&nbsp;</p>
3706, <p>Or maybe something wrong with the evaluation system like last year's Santa problem ;)</p>
3706, <p>This is crap</p>
3706, <p>I think rank 1 should post his code and submission so that we can verify if its genuine.</p>
3706, <p>[quote=Giulio;58910]</p> <p>[quote=Abhishek;58909]</p> <p>I think rank 1 should post his code and submission so that we can verify if its genuine.</p> <p>[/quote]</p> <p>Yes yes agreed. Just do it around 8PM Pacific Standard Time when most of Europe is asleep and the East Coast is going to bed.</p> <p>[/quote]</p> <p>Who cares. He already won the competition. So he can post the code. Also if its genuine Kaggle should &nbsp;close the competition as they already have optimal solution .</p>
3706, <p>just an advice dont try to get a lower score i already did it last year :P :P&nbsp;</p>
3706, <p>lol one more optimal solution after just one submission ... hahaha</p>
3706, 
3706, <p>There is no private dataset as this is an optimization problem.&nbsp;</p> <p><span style="line-height: 1.4">PP.S. I didn't down vote</span></p>
3706, <p>Always been always will be :D</p>
3706, <p>[quote=Alexander Ryzhkov;58980]</p> <p>[quote=William Cukierski;58961]</p> <p>Our users gave to us the answers without any fuss :-/</p> <p>[/quote]</p> <p>In my opinion it will be great to start another competition (Helping Santa's Helpers Part 2 for example) with corrected&nbsp;parameters and&nbsp;objective function and closed this competition as solved (even without money prize - ranking points and tiers are better present :) ). In this case all people in Kaggle community will be happy - contestants who solved this task get the ranks equal to their positions in the leaderboard and others can do the same&nbsp;in the Part 2 contest.</p> <p>Hope it will be so</p> <p>Alex</p> <p>[/quote]</p>  <p>Its better to restart this competition. And as William said Santa will talk to Kamil. I dont think the competition &nbsp;ran for enough time in order to award points and tiers.</p>
3706, <p>[quote=Alexander Larko;58991]</p> <p>Kamil must receive a cash prize!&nbsp;It is very fast (sparkling) solution !!!</p> <p>[/quote]</p> <p>I never said Kamil shouldn't receive any prize for the hardwork he has done. I meant starting a new competition and awarding ranks and points for this one makes no sense.</p>
3706, <p>[quote=Giulio;59066]</p> <p>How is Kaggle handling points and badges both for the first attempt and the re-start?</p> <p>[/quote]</p>  <p>i think its like other competitions which restart and get revised data. so everything should remain the same (hopefully!)</p>
3706, <p>Thanks ;)</p>
3706, <p>[quote=Skabed;66345]</p> <p>There is one thing I don't quite understand with the given code here. Why is it using the test sentences in the model pipeline? If I understand it correctly the input to the tfidf vectorizer are both the training sentences and the test sentences. I'm specifically refering to:</p> <p><em>tfv = TfidfVectorizer(...)</em></p> <p><em>X_all = traindata + testdata</em></p> <p><em>...</em></p> <p><em>tfv.fit(X_all)</em></p> <p>It kind of feels wrong to me tbh.&nbsp;</p> <p>[/quote]</p>  <p>Why?</p>
3706, <p>can anyone upload the full file for submission (just being lazy) :P</p>
3706, <p>Anyone offering ec2 with Tesla for the whole course of competition? :P :P</p>
3706, <p>Is there any difference between using opencv imread and skimage.io.imread? It seems im unable to beat even 5.0 if i use opencv with a validation score of 4.17</p>
3706, <p>Nevermind. i=0 ruined my model</p>
3706, <p>from the rules: &quot;Semi-supervised learning is permitted.&quot;</p>
3706, <p>Ahh... I wish I had GPU :-/</p>
3706, <p>[quote=gregl;61632]</p> <p>[quote=clustifier;61601]</p> <p>thanks.</p> <p>but why our md5's are different?</p> <p>[/quote]</p> <p>I built im2bin with blas not the default mlk.</p> <p>Also upstream the md5sum of train.lst is:</p> <p>399ab27e818cabcb5bd2158c15451a80 train.lst</p> <p>with the first 5 lines:</p> <p>3406 10 data/train/chaetognath_non_sagitta/23677.jpg<br>22212 90 data/train/radiolarian_chain/157296.jpg<br>19772 83 data/train/protist_fuzzy_olive/147898.jpg<br>23435 98 data/train/siphonophore_calycophoran_rocketship_young/36262.jpg<br>18710 72 data/train/hydromedusae_solmaris/47760.jpg</p> <p>[/quote]</p> <p>how did you build im2bin with blas when mshadow needs mkl?</p>  <p>EDIT: Nevermind</p>
3706, <p>is it possible to compile CXXNET without OpenCV?</p>
3706, <p>[quote=emolson;62784]</p> <p>using the software is fine but using pre-trained caffe networks violates the &quot;no external data&quot; rule.</p> <p>[/quote]</p>  <p>I dont think it does. Maybe some admin can clarify this..</p>
3706, <p>which version are of scikit-learn are you using? In 0.15.2 they improved RF and it doesnt copy the dataset. try 0.15.2 or 0.16-git</p>
3706, <p>Generally submissions&nbsp;are open after final deadline&nbsp;and public/private scores can both be seen for them.&nbsp;</p>
3706, <p>[quote=mike1886;65246]</p> <p>I am using a lasagne + nolearn setup you can check it out here:</p> <p>https://github.com/msegala/Kaggle-National_Data_Science_Bowl</p> <p>[/quote]</p> <p>is this your code? what is the logloss do you get with this?</p>
3706, <p>congrats to Sander et. al.</p> <p>http://benanne.github.io/2015/03/17/plankton.html</p>
3706, <p>n i just need a kaggle mug :-/</p>
3706, <p>[quote=Giulio;66403]</p> <p>[quote=Ben S;66401]</p> <p>they might</p> <p>[/quote]</p> <p>I noticed many folks&nbsp;are&nbsp;referring to Cardal as &quot;they&quot;. He/she is not a team on the LB and would be (I think) against the rule to have multiple people work together under one Kaggle id...</p> <p>Anything I'm missing?</p> <p>[/quote]</p>  <p>Then i think ISFA team should be removed as one member consists of many persons &quot;ISFA_students&quot;</p>
3706, <p>yes</p>
3706, <p>April Fool in advance :P :P&nbsp;</p>  <p><img src="http://d.ibtimes.co.uk/en/full/1371422/april-fools-day-meme.jpg" alt width="700" height="536"></p>
3706, <p>nobody likes jokes *sadmax* :(</p>
3706, <p>Welcome to v2.0 of the competition for University of Paderborn students. The dataset has now more samples and includes a timestamp too. Obviously the competition is open for all.!</p> <p>Happy Competition!</p>
3706, <p>Hi Mary</p> <p>The competition was closed due to some problems with data files. It will reopen as soon as the problems are resolved.</p>
3706, <p>A movie's rating wont remain the same over time. eg. In IMDB movies start with a rating of 8-9 and then obtain their optimum value over a course of time. How you can use this data is upto you!! ;)</p>
3706, <p>Great Stuff! Thanks :)</p>
3706, <p>whats the error?</p>
3706, <p>there is no problem with downloading of any files.</p>
3706, <p>[quote=Rachana Bagde;62874]</p> <p>@Leo Buettiker</p> <p>nrow shows correct values...but on submitting I get this</p> <p>ERROR: Could not parse 'NA' into expected type of Double (Line 1882 Column 9) ERROR: Could not parse 'NA' into expected type of Double (Line 2403 Column 9) ERROR: Could not parse 'NA' into expected type of Double (Line 5280 Column 8)...</p> <p>[/quote]</p> <p>There should be rating for every ID in test set. You cannot submit NA as any rating!</p>
3706, <p>You still have NA in the submission file.</p>
3706, <p>One submission per day is too much... Can we change it to 1 per month?</p>
3706, <p>I'd also like to know the same...</p>
3706, <p>gimme a week ;)&nbsp;</p>
3706, <p style="text-align: left">No link to the paper?</p>
3706, <p style="text-align: left">Thanks everyone and congrats to phalaris srk and Marios.&nbsp;</p> <p style="text-align: left">We are going to keep our approach a secret ;)</p>
3706, <p style="text-align: left">Why does it take so much time these days to finalize leaderboard enable private scores of all submissions and awarding of points?</p> <p style="text-align: left">Cheater removal takes some time for LB and points but private scores should be available as soon as competition ends right?</p>
3706, <p>I just wanted to see my private scores without submitting individual files again :)</p>
3706, <p>If you submit the brackets on his website he might be able to use it on kaggle and may get a better rank than yours.. so beware!</p>
3706, <p>download the cookies of kaggle and use the following:</p> <p>wget -x --load-cookies cookie_file.txt <em>url_to_dataset</em></p>
3706, <p>Well Im not using lynx. I also have google login to kaggle and the above command works perfectly fine for me.</p>
3706, <p>did you try this:&nbsp;</p>  <p>wget -x --load-cookies cookies.txt --no-check-certificate&nbsp;https://www.kaggle.com/c/malware-classification/download/train.7z</p>  <p>or simply:</p> <p>wget -x --load-cookies cookies.txt http://www.kaggle.com/c/malware-classification/download/train.7z</p>
3706, <p>after some processing my training and test dataset size is approx 50mb. Im still trying to figure out how 0.02 logloss is even possible... :D</p>
3706, <p>[quote=WWW BIG - Cup Committee;63696]</p> <p>Yes it is possible :) and can even be improved.</p> <p>Did not expect it to happen so early but this is how sports is.</p> <p>Good luck to all!</p> <p>[/quote]</p> <p>Now I believe you! :D</p>
3706, <p>@jay: any success with FFT features yet?</p>
3706, <p>What does ?? represent in hexdump data?</p>
3706, <p>why not!</p>
3706, <p>Its same for others too...!</p>
3706, <p>nice!</p>
3706, <p>Also you can take a look at spot instances in AWS. there is a risk of losing data (you can always backup) but prices are way cheaper...</p>
3706, <p>[quote=Robert Fontaine;63953]</p> <p>Am I asking the right questions?</p> <p>...&nbsp;</p> <p>[/quote]</p> <p>nop!</p>
3706, <p>you dont have to load any data to dbms or anything. create a script that mines information from individual files and writes it to a separate file. getting a logloss of less than 0.1 will take you only a couple of hours using R or python.</p> <p>P.S. I dont think kaggle will go out of business anytime soon :)</p>
3706, <p>I dont know if you can do that with 7z. I had enough space to decompress the dataset.&nbsp;</p>
3706, <p>Good Job. Did you try not using &quot;??&quot; ?</p>
3706, <p>;)</p> <p>P.S. SVNIT rocks! :P&nbsp;</p>
3706, <p>[quote=UD1989;65025]</p> <p>hey guys how much time does the data consolidation script take to run ? Its been on for 3 hours now.....wondering if the changes i made to the scripts are causing the problems.....</p> <p>[/quote]</p> <p>if you read the very first post carefully you must know that it takes around 6 hours! Come on please read the post instead of blindly using the benchmark scripts!!!</p>
3706, <p>its a reserved space</p>
3706, <p>[quote=eNVy;64166]</p> <p>Hi guys even I m a learner I saw in datasample.7z that we were provided .asm and .bytes but in training.7z there are no .bytes files so now do we have to use &quot;hexdump&quot; to convert the .asm to .bytes?</p> <p>[/quote]</p> <p>there are both asm and bytes files in training.7z. Either your extraction failed or there was some download error.</p>
3706, <p>[quote=Shanky Sharma;64165]</p> <p style="text-align: justify">Hi</p> <p style="text-align: justify">I am new to data science and machine learning a complete novice. I took part in this competition to learn practical applications of ML. When I went through the training data I found it is made up of &quot;.asm&quot; files any ideas on how I can extract features from these files. I am a newbie in the field.</p> <p style="text-align: justify">&nbsp;</p> <p style="text-align: justify">Also is this the right approach?</p> <p style="text-align: justify">Thanks in advance. :)</p> <p style="text-align: justify">&nbsp;</p> <p style="text-align: justify">P.S.: Good-luck everyone.</p> <p>[/quote]</p>  <p>Take a look at the benchmark post in the forums. Although it deals with the bytes files it will give you some idea.</p>
3706, <p>Any guesses?&nbsp;</p>
3706, <p>i meant the score.. :D</p>
3706, <p>already 0.007 :D</p>
3706, <p>given the amount of data in this competition its too easy to overfit...&nbsp;</p>
3706, <p>58kxhXouHzFd4g3rmInB is empty... =&gt; ?????</p>
3706, <p>wont you interpret &quot;??&quot; bytes as only gaps?</p>
3706, <p style="text-align: left">They are one of those 10k files</p>
3706, <p>i meant some of them. I havent checked which ones (havent used this info in my model yet).</p>
3706, <p>http://www.winmd5.com/</p>
3706, <p>[quote=David Tran;64789]</p> <p>Interesting ideas but I would start with something much simpler. How about byte frequencies? There are only 256 different bytes so for every file you can simply record how many times each byte appears. Ignore any location information. That way you can reduce the training/test set from half a terabyte to a few MB. Even with such a drastically reduced data set you can achieve a log loss of less than 0.2 I believe.</p> <p>[/quote]</p> <p>&lt; 0.02!</p>
3706, <p>[quote=LLMSI;65273]</p> <p>1- Is the order of the test files in the submitted probabilities file important? Or can they have any order as long as the Id column is specified?</p> <p>2- Should the first column having the file name always have &quot;quotations&quot;?</p> <p>Thanks in advance</p> <p>[/quote]</p> <p>1-No</p> <p>2-No</p>
3706, <p>None of them are blank.</p>
3706, <p>[quote=Vinh Nguyen;67680]</p> <p>Just wondering whether anyone has tried this idea with generating 255M n-grams :-)</p> <p>[/quote]</p> <p>yes!</p>
3706, <p style="text-align: left">Will this be published somewhere?</p>
3706, <p>How do I cite this dataset?</p>
3706, <p>[quote=WWW BIG - Cup Committee;73140]</p> <p>You can cite BIG at WWW together with a link to the kaggle dataset.</p> <p>Are you doing any work on this dataset that will be interesting fro presentation at the conference?</p> <p>[/quote]</p> <p>I am still doing some work on this competition and writing a paper. I would be interested in presenting at WWW BIG but I dont think my private rank will allow me to do so.... or is it possible? :)</p>
3706, <p>Great!</p>
3706, <p>Is re-scoring on this one over?</p>
3706, <p style="text-align: left">Hand labelling I bet!!!</p>
3706, <p>what about the 2 after top2 ? :P</p>
3706, <p>Lol</p>
3706, <p>[quote=Stergios;71679]</p> <p>[quote=Abhishek;70092]</p> <p style="text-align: left">Hand labelling I bet!!!</p> <p>[/quote]</p> <p>Do you still bet on that one? :)</p> <p>[/quote]</p> <p>$10 bucks says its handlabeling for all teams above us.</p>
3706, <p>I think everyone in top 10 already achieved 99.9% accuracy.</p>
3706, <p style="text-align: left">Its not about 0.00. We never used 0.0. It was done only to show that its possible. There were a lot of other reasons for the drop. And we will release everything soom</p>
3706, <p>yes yes yes....!</p> <p>if selected are flights covered? ;)</p>
3706, <p style="text-align: left">You cannot make a submission to this competition anymore as the first submission deadline is over.</p>
3706, <p>Nothing!</p> <p>It was clearly mentioned in the Rules that you accepted before downloading the data. It is also on top of every page of this competition.</p>
3706, <p>move it to recycle bin</p>
3706, <p>with FFFFFFFF + 9000 features I think we are the ones who created most number of features ;)</p>
3706, <p>Thanks Bing! xgboost helps a lot. Dont know what the private LB will bring but our is solely based on xgboost with a lot of feature engineering and feature selection :D</p>
3706, <p>btw we dont think there is a data leak (or maybe we couldn't find one). Its all about features.!</p>
3706, <p>[quote=Bing Xu;72186]</p> <p>Awesome! I should suggest Tianqi to write a formal publication about XGBoost!</p> <p>[/quote]</p> <p>That would be nice. Since we are using it in one of our research and writing a paper but dont know how to reference it.</p>
3706, <p>We have submissions that could have got a very good rank. I think in the end it was just because of one malware that fucked up our results. Anyways we will post the best result on private set with all the documentation about features soon.... Maybe it will help others...</p>
3706, <p>wait for them to finalize the LB and then u have both public and private scores.</p>
3706, <p>[quote=&#924;&#945;&#961;&#953;&#959;&#962; &#924;&#953;&#967;&#945;&#951;&#955;&#953;&#948;&#951;&#962; KazAnova;72454]</p> <p>[quote=Michael George Hart;72408]</p> <p>&nbsp;I think I could have won this completion easily ....</p> <p>[/quote]</p> <p>There are many other kaggle competitions going on at the moment for you to <em>easily</em> win.</p> <p>[/quote]</p> <p>If only it was so easy. ;)</p>
3706, <p>When will we know all the private scores without submitting each and every file? When will the leaderboard be finalized?</p>
3706, <p>Will write about it soon ;)</p>
3706, <p>Also is it possible to provide md5 of the datasets?</p>
3706, <p>We dont need predictions for all the files in the test set?</p>
3706, <p>as usual the mistake is on my part. Extraction of files got terminated and i was confused why sample submission has less rows....pff!&nbsp;</p> <p>Thanks!</p>
3706, <p>its already on data page.</p>
3706, <p>[quote=Jaco Cronje;64472]</p> <p>Can someone maybe center crop and resize all the images to something like 256x256 or 512x512 and save them as raw .png files please. I really want to have a go at this competition but the download size is just way to large to download. It will take a couple of days or weeks for me to download all the original files.</p> <p>I'm sure the data can be reduced to one file that is less than 8GB.</p> <p>[/quote]</p> <p>Do you also need someone to write a getting-started code with some image transformations and convolutional neural nets?</p>
3706, <p>have you seen the license of scikit-learn? Which rule says we cannot use it?</p>
3706, <p>[RESERVED]</p>
3706, <p>umm? I havent checked how much the code scores yet... :-/</p>
3706, 
3706, <p>Hi All</p>  <p>Attaching a fast image resize script (in case anyone needs it :))</p> <p>Takes 40mins to resize all images (train+test) to 96x96 on 32 cores.</p> <p>Vote up if it helped ;)</p>
3706, <p>[quote=&#924;&#945;&#961;&#953;&#959;&#962; &#924;&#953;&#967;&#945;&#951;&#955;&#953;&#948;&#951;&#962; KazAnova;66737]</p> <p>not many samples</p> <p>predefined set of features</p> <p>Classification</p> <p>All you need is a &quot;beat the benchmark code...&quot;</p> <p>Boom</p> <p>[/quote]</p> <p>and now you have the last one in the list ;)</p>
3706, <p>[quote=&#924;&#945;&#961;&#953;&#959;&#962; &#924;&#953;&#967;&#945;&#951;&#955;&#953;&#948;&#951;&#962; KazAnova;66829]</p> <p>[quote=Abhishek;66762]</p> <p>and now you have the last one in the list ;)</p> <p>[/quote]</p> <p>Haha nice. My trick worked! I don't need to prepare a readcsv-and-model-and-make-predictions' code! :D&nbsp;</p> <p>[/quote]</p> <p>lol</p>
3706, <p>Hi Folks</p> <p>As usual I present to you the beating the benchmark script for this competition. The script is in python and I have commented most of the stuff. Its pretty simple and should give a score of 0.60 on the leaderboard. With some optimization you can achieve my current score.</p> <p>Feel free to ask questions in case of any doubts in the script.</p> <p>Don't forget to <strong>vote-up</strong> in case this script helped you in any way.</p>
3706, <p>Well this code can give you top 3! :D</p>
3706, <p>yes yes i forgot to remove it :D</p>
3706, <p>I did not crossvalidate. The model takes about 10mins on 8 cores.</p>
3706, <p>[quote=Foxtrot;67116]</p> <p>Hey! People! This is Abhishek's BTB thread. One more post about R and I'm gonna enter the competition! Consider yourself warned.</p> <p>[/quote]</p> <p>haha...</p>
3706, <p>[quote=Colin;67114]</p> <p>In text mining idf is used for down-weighting those words that appear too frequently overall like 'the' 'a' etc. I guess here the organizer maybe already did sort of feature filtering as the size of features is not that big. Using idf may not be the best choice to weight the features.</p> <p>[/quote]</p> <p>Could be. I didnt check! I wanted to add something extra to the code except a simple RF on the provided features. ;)</p>
3706, <p>[quote=Foxtrot;67219]</p> <p>[quote=Triskelion;67123]</p> <p>I found this slideshow&nbsp;by&nbsp;<a href="https://www.kaggle.com/users/17379/xavier-conort">Xavier Conort&nbsp;</a>about <a href="http://fr.slideshare.net/DataRobot/final-10-r-xc-36610234">10 R packages to win Kaggle competitions</a>. I didn't know where to put it so I figured this place would be as good as any.</p> <p><sub>And thank you for the benchmark!</sub></p> <p>[/quote]</p> <p>OK that was it.</p> <p>[/quote]</p> <p>aRRRRRRRRRR...........Damn!</p>
3706, <p>Then I think I should write one more :D&nbsp;</p>
3706, <p>[quote=Dipanjan Paul;67737]</p> <p>I am new to Python. Is there anything specific that has to be coded in Python to use multiple cores (when multiple cores are available available).. like using the multiprocessing package. Or does the Python RandomForestClassifier package does it automatically (using multiple cores) while training.</p> <p>[/quote]</p>  <p>use n_jobs in&nbsp;RandomForestClassifier to specify number of cores. n_jobs=-1 means all cores will be used.</p>
3706, <p>[quote=Neil Slater;70121]</p> <p>I don't think the TFIDF transformation adds much&nbsp;- if anything - for this data. See what happens when you comment out those lines . . .</p> <p>[/quote]</p>  <p>As I already mentioned I added tfidf since it was counts data and i wanted to add something extra to the code rather than just putting an RF in there...</p>
3706, <p>okay. I couldnt stop myself after this post. I can get 0.47 with NN</p>
3706, <p>[quote=Daia Alexandru;67197]</p> <p>Hello&nbsp;Abhishek and&nbsp;mproust could &nbsp;you &nbsp;give us some &nbsp;clues &nbsp;about your ann &nbsp;topologies?Tnks</p> <p>[/quote]</p> <p>I cannot give you the exact architecture right now (maybe at a later stage) since im still tuning the network....</p>
3706, <p>[quote=mproust;67210]</p> <p>[quote=Daia Alexandru;67197]</p> <p>Hello&nbsp;Abhishek and&nbsp;mproust could &nbsp;you &nbsp;give us some &nbsp;clues &nbsp;about your ann &nbsp;topologies?Tnks</p> <p>[/quote]</p> <p>Input/Hidden/Output : 93/200/9</p> <p>Activation : ReLU</p> <p>Regularization : L2 with 0.001</p> <p>[/quote]</p> <p>I think a simple RF can give you better results than this one.</p>
3706, <p>[quote=mproust;67213]</p> <p>[quote=Abhishek;67211]</p> <p>[quote=mproust;67210]</p> <p>[quote=Daia Alexandru;67197]</p> <p>Hello&nbsp;Abhishek and&nbsp;mproust could &nbsp;you &nbsp;give us some &nbsp;clues &nbsp;about your ann &nbsp;topologies?Tnks</p> <p>[/quote]</p> <p>Input/Hidden/Output : 93/200/9</p> <p>Activation : ReLU</p> <p>Regularization : L2 with 0.001</p> <p>[/quote]</p> <p>I think a simple RF can give you better results than this one.</p> <p>[/quote]</p> <p>Yeah I think so. Btw I don't have much experience on this area I was just toying around with basic models :)</p> <p>[/quote]</p> <p>Me too. I started NN yesterday.... No good results though !</p>
3706, <p>I dont understand why would it take 2 hours to train a simple model. You must be doing something wrong!</p>
3706, <p>This metric is being used to penalize heavily in case you are doing a wrong prediction.&nbsp;</p> <p>[quote=Bluefool;67058]</p> <p>[quote=Dean McKee;67055]</p> <p>Are you using R (I'm guessing so because of GLM) -</p> <p>You can use my (or anyone else's code) from this post to calculate log-loss.</p> <p>http://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/12895/compute-score-before-submission</p> <p>[/quote]</p> <p>So this problem has to be dealt with as multinomial because of the metric?</p> <p>[/quote]</p>
3706, <p>Im using a OVA approach</p>
3706, <p>Have you seen the class distribution? Its pretty messed up ;)&nbsp;</p>
3706, <p>my submission is stuck :(</p>
3706, <p>now i think i should post 0.45 :P</p>
3706, <p style="text-align: left">Then you are overfitting or doing wrong cross validation. Use stratified cv. The cv and lb scores in this competition are quite close</p>
3706, <p>So now we are giving out all the parameters too? Great!</p>
3706, <p>deja vu (i miss domcastro :P)&nbsp;</p>
3706, <p>not RBM but i tried some black box approaches  (neural networks!)</p>
3706, <p>no good results though . im happy its not image data... beat me CNN! &nbsp;lol</p>
3706, <p>read both the documentations again. xgboost(tree vs. linear). sklearn gbc (init loss) and you will find the answer.&nbsp;</p> <p>also What are your corresponding sklearn gbc params?&nbsp;</p>
3706, <p>found this somewhere (might be useful):</p> <p>gbm ------ xgboost<br>n.trees&nbsp;------&gt; nrounds<br>interaction.depth ------&gt; max.depth<br>bag.fraction ------&gt; subsample<br>distribution ------&gt; objective<br>n.cores ------&gt; nthread<br>n.minobsinnode ------&gt; min_child_weight?</p>
3706, <p>[quote= valerio orfano;71904]</p> <p>Sorry guys which metric are you using to evaluate the best algorithm?</p> <p>[/quote]</p> <p>We are using multiclass-logloss. The metric which is used to evaluate entries on the leaderboard.</p>
3706, <p>golden features? reminds me of loan default prediction....</p>
3706, <p>Good. i can resume now... pff!</p>
3706, <p>from main page: 2000+ players 14000+ submissions</p>
3706, <p>http://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/13102/wow/68989#post68989</p>
3706, <p>I can assure you that no one from our team uses multiple kaggle accounts...</p>
3706, <p>[quote=rakhlin;77703]</p> <p>[quote=skwalas;77699]</p> <p>I also imagine that a large number of the top 1000 are overfitting to the leaderboard</p> <p>[/quote]</p> <p>Impossible - just look at number of submissions half of them is&nbsp;single digits and the other half is&nbsp;low too.</p> <p>[/quote]</p> <p>too hard to overfit in this competition :P :P&nbsp;</p>
3706, <p>too good solution at the end of competition!</p>
3706, <p>Who told you to be away and miss everything?!</p>
3706, <p>[quote=Nicholas Guttenberg;78844]</p> <p>Good luck!&nbsp;Is there a traditional Kaggle celebratory ritual for when a contest is completed?</p> <p>[/quote]</p> <p>All participants do a jungle dance at home and upload it on YouTube after competitions are over :P</p>
3706, <p>which one is the private lb?</p>
3706, <p>@josef what was the inhouse bechmark?</p>
3706, <p>[quote=&#924;&#945;&#961;&#953;&#959;&#962; &#924;&#953;&#967;&#945;&#951;&#955;&#953;&#948;&#951;&#962; KazAnova;79217]</p> <p>Congrats for your victory Gilberto/Semenov it was a stellar performance.&nbsp;</p> <p>I have to stress though that I am a little bit concerned about some top 10 performances with very small number of submissions that also seem to share common past . 3 out of top 10 submissions seem to have come from people that attended the same Russian school?</p> <p>Anyway hope it is ok it would be interesting to see their solutions too and how they differ.</p> <p>[/quote]</p> <p>I agree with you many people have spent so much time in this competition it is important to get it right and shred any glimpse of concern...</p>
3706, <p>Thanks Mike!</p>
3706, <p>[quote=inversion;79593]</p> <p>There was one person in the top 20 I wasn't surprised got removed. Otto was this person's&nbsp;first and only contest. While that's possible of course it also reeks of cheating.</p> <p>[/quote]</p> <p>Who?</p>
3706, <p>[quote=William Cukierski;79612]</p> <p>Cheaters removal is done!</p> <p>[/quote]</p>  <p>Points coming soon?</p>
3706, <p>[quote=C&#233;dric Gouy-Pailler;79774]</p> <p>Tiny bug: Public leaderboard rankings are displayed in profiles</p> <p>[/quote]</p> <p>I confirm that</p>
3706, <p>[quote=inversion;67065]</p> <p>[quote=Jerry Lin;67054]</p> <p>Hey I am not so clear about the features you use to describe each product. Can you provide more explanations about feature?</p> <p>Thanks!</p> <p>[/quote]</p> <p>Would your model be any different if Feature_1 was the number of 5-star reviews or say the number of times&nbsp;someone left a review with the word &quot;banana&quot; in it?</p> <p>[/quote]</p> <p>lol.... mine would be :D</p>
3706, <p>So I think you should trust your CV? ;)</p>
3706, <p>[quote=barisumog;68312]</p> <p>[quote=Abhishek;68306]</p> <p>So I think you should trust your CV? ;)</p> <p>[/quote]</p> <p>That's all we can hold on to. But if the &quot;real&quot; test set is as small as the train set then a single restaurant with 30M revenue will turn&nbsp;the whole competition into a lottery.</p> <p>[/quote]</p> <p>Well in this case I think a transformation of the labels might help (combined with a trusted cross-validation)?</p>
3706, <p style="text-align: left">Are you not allowed to transform twice? Wait for my btb to know more ;)</p>
3706, <p>If you find it please let me know :D</p>
3706, <p>how are you doing CV with such small data? I dont think a 70/30 or 80/20 cv score will correlate to leaderboard score at all.&nbsp;</p>
3706, <p>Kaggle points will always be awarded unless you are kicked out of the LB after the competition finishes. You can choose whether you want to accept the prize or not.&nbsp;</p>
3706, <p>Hi All</p> <p>After Otto challenge this is my benchmark model for ECML/PKDD 2015.&nbsp;</p> <p>The script uses first and last but one latitude and longitude values. I know I have mixed up the names while creating the training set but they are corrected for submission file.</p> <p>The script is in python and I have commented several blocks of code. So it should be easy to understand. In case of any doubts feel free to ask here.</p> <p><strong>LB Score:</strong> <strong>&lt; 3.17</strong> (Better than my current score) [Unverified]</p> <p><strong>Vote Up</strong> if this helped you in any way!&nbsp;</p> <p>Enjoy!</p>
3706, <p>If anyone uses this as it is it would be nice if he/she could confirm the LB score and save me a submission ;)</p>
3706, <p>Why dont you just convert this code :D</p>
3706, <p>[quote=Paul H;73025]</p> <p>Thanks Abhishek.&nbsp; Your benchmark scored 3.22084.&nbsp; Some people may have problems running it as it wanted to use 17 GB RAM...</p> <p>[/quote]</p> <p>Hmm its weird that it scores 3.22 since i used only 10 estimators for my score. maybe random seed problem. It doesnt take 17 gb it takes around 13-14. but yea im working on optimizing it! :)</p>
3706, <p>[quote=NxGTR;73103]</p> <p>I tried as well but is asking 16GB of RAM (I only got 14.5GB free) I managed to run it by sub-sampling the training set and it scored 3.5XXXX</p> <p>[/quote]</p>  <p>Its all because of the stringified list of lists. Must look for a workaround.</p>
3706, <p>Great! Did you see any difference if using asm.literal_eval instead of json.loads. Do you mind if i add this to my script ?</p>
3706, <p>also what happens when u use the full list?</p>
3706, <p>[quote=DEBASHISH ROY;77455]</p> <p>hi abhishek can you plz try Haversine formula instead of Random Forest Regressor ?&nbsp;</p> <p>[/quote]</p> <p>I have no idea what that means....</p>
3706, 
3706, 
3706, 
3706, 
3706, <p>No more benchmarks from me :)</p>
3706, <p>@ben okay :D but let that script be there. I'll modify it soon such that some useful results are produced for the Kaggle community :)</p>
3706, <p>Hi All</p> <p>I present to you a very simple Script to beat the all the current benchmarks and score <strong>0.71+</strong></p> <p>I have commented the script so its easy to understand.</p> <p>In case of any questions feel free to ask.</p> <p><strong>Script</strong>:&nbsp;http://bit.ly/BeatTheBench</p> <p>Thanks!</p>
3706, <p>[quote=Foxtrot;78038]</p> <p>No... Again?</p> <p>[/quote]</p> <p>Couldn't stop myself ;)</p>
3706, <p>[quote=aldente;79708]</p> <p>Edit: One more question why Abhishek is at the 329th position of LB? Does he give up? Thanks.</p> <p>[/quote]</p>  <p>Im taking some rest. Will resume next week :)</p>
3706, <p>yes. u can make submissions even after the deadline.</p>
3706, <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>.</p> <p>Are there any?? :D</p>
3706, <p>seems like people dont like jokes :P&nbsp;</p>
3706, <p>[quote=Gert;82015]</p> <p>[quote=Wendy Kan;82013]</p> <p>since this is now available to anyone you can now exploit this data (and this data only) before the end of the competition.&nbsp;</p> <p>[/quote]</p> <p>I can't believe that... it implies that&nbsp;the rules change one day before the competition is over?</p> <p>[/quote]</p> <p>They dont have any other option</p>
3706, <p>[quote=Bluefool;82026]</p> <p>The only reason you don't care is because you've already been using this data</p> <p>[/quote]</p> <p>+1</p>
3706, <p>Great! Its good and works only for the top 3 (since models are evaluated after the competition closes). What can be done about other participants who just care about &quot;good&quot; rank?</p>
3706, <p style="text-align: left">U stole my trademark topic name :P</p>
3706, <p>Hi All</p> <p>Some of you might remember this script of mine.</p> <p>This gives a Public LB Score of <span style="text-decoration: line-through">0.466&nbsp;</span> 0.54</p> <p>In case of any questions just ask! :)</p> <p>Thanks all!</p> <p>Oh yea the script is located here: https://www.kaggle.com/users/5309/abhishek/crowdflower-search-relevance/beating-the-benchmark</p> <p>EDIT: add new version. more preprocessing feature selection scaling and replace model</p> <p>EDIT-2: Add grid search for parameter selection for SVD and SVM. Optimize for quadratic weighted kappa.</p>
3706, <p>[quote=NxGTR;77984]</p> <p>What happened with the original post? next challenge: &quot;Find the probability to lose a post&quot; :D</p> <p>[/quote]</p> <p>If only you were watching the forum :D</p>
3706, <p>[quote=David Tran;77985]</p> <p>If asking for upvotes is forbidden maybe you can ask for right swipes on Tinder instead.</p> <p>[/quote]</p> <p>hahaha... :D :D&nbsp;</p>
3706, <p>A little birdie told me that it had never seen a SVM benchmark for Kaggle Competitions (I dont know how true is that). But I took it seriously and decided to take it to neext level.</p> <p>The newer version of script uses feature selection from sparse data preprocessing and a better model (SVM). Enjoy!</p> <p>In case of any questions feel free to ask &nbsp;( Triskelion will always be there to answer :P lol )&nbsp;</p> <p>;)</p>
3706, <p>[quote=Bluefool;78753]</p> <p>It's not a good idea really. However these benchmarks always seem to be lucky for the leaderboard. Bad practice but good leaderboard score. Happens all the time.</p> <p>[/quote]</p> <p>It is a benchmark code and its supposed to be dirty. You should be the one to clean it and use it. If that's a problem I'll post a much CLEANER version of the code with much better score both in cross validation and leaderboard.!</p>
3706, <p>UPDATE: I have updated my script with grid search for parameter selection. It is only 2 folds since kaggle doesnt offer&nbsp;enough resources for scripts (for obvious reasons). It directly optimizes for quadratic_weighted_kappa.</p> <p>Fitting 2 folds for each of 4 candidates totalling 8 fits<br>[CV] svm__C=1.0 svd__n_components=120 ...............................<br>[CV] ...... svm__C=1.0 svd__n_components=120 score=0.095201 - 14.0s<br>[CV] svm__C=1.0 svd__n_components=120 ...............................<br>[CV] ...... svm__C=1.0 svd__n_components=120 score=0.109611 - 13.9s<br>[CV] svm__C=10 svd__n_components=120 ................................<br>[CV] ....... svm__C=10 svd__n_components=120 score=0.466751 - 14.3s<br>[CV] svm__C=10 svd__n_components=120 ................................<br>[CV] ....... svm__C=10 svd__n_components=120 score=0.459042 - 14.4s<br>[CV] svm__C=1.0 svd__n_components=140 ...............................<br>[CV] ...... svm__C=1.0 svd__n_components=140 score=0.137393 - 15.9s<br>[CV] svm__C=1.0 svd__n_components=140 ...............................<br>[CV] ...... svm__C=1.0 svd__n_components=140 score=0.138725 - 16.1s<br>[CV] svm__C=10 svd__n_components=140 ................................<br>[CV] ....... svm__C=10 svd__n_components=140 score=0.480929 - 16.6s<br>[CV] svm__C=10 svd__n_components=140 ................................<br>[CV] ....... svm__C=10 svd__n_components=140 score=0.459656 - 16.5s<br>Best score: 0.470<br>Best parameters set:<br> svd__n_components: 140<br> svm__C: 10</p>  <p>best score output will be less than LB because of only 2folds. The Leaderboard score should not change. 0.54....!</p>
3706, <p>[quote=phalaris;78958]</p> <p>Thanks!&nbsp; I was trying to make a kappa scorer for sklearn but hadn't gotten around to finding the correct documentation.</p> <p>[/quote]</p> <p>I faced the same problem when I started :D</p>
3706, <p>[quote=portia brat;79309]</p> <p>Is there any intuition for using tfidf and PCA? &nbsp;My first thought was to just one-hot-encode all the queries and add a few other features.</p> <p>[/quote]</p> <p>Who said I'm using pca.? Anyways u have a better rank than mine so u know better. Did the benchmark help? ;)</p>
3706, <p>[quote=gand;79846]</p> <p>[quote=Abhishek;77983]</p> <p>Hi All</p> <p>Some of you might remember this script of mine.</p> <p>This gives a Public LB Score of <span style="text-decoration: line-through">0.466&nbsp;</span> 0.54</p> <p>In case of any questions just ask! :)</p> <p>Thanks all!</p> <p>Oh yea the script is located here: https://www.kaggle.com/users/5309/abhishek/crowdflower-search-relevance/beating-the-benchmark</p> <p>EDIT: add new version. more preprocessing feature selection scaling and replace model</p> <p>EDIT-2: Add grid search for parameter selection for SVD and SVM. Optimize for quadratic weighted kappa.</p> <p>[/quote]</p> <p>Hi&nbsp;Abhishek</p> <p>starting with your benchmark and stressing it a little bit further I am now 8th in the leaderboard so thanks :)&nbsp;</p> <p>[/quote]</p>  <p>Thanks! Feel free to share your findings in this thread ;)</p>
3706, <p>[quote=Aleix Gimenez;80027]</p> <p>Hi!</p> <p>A small question if refit=True in the GridSearchCV then you don't need to fit the model again with the best parameters isn't it? Thanks for the benchmark btw! :)</p> <p>[/quote]</p>  <p>Yes! That's correct...</p>
3706, <p>[quote=Elena Cuoco;80021]</p> <p>Thanks Abhishek! your Beating the Benchmark code is always a good starting point!</p> <p>[/quote]</p> <p>Glad you liked it...!</p>
3706, <p>I have updated the benchmark for you guys!</p>
3706, <p>[quote=yr;80049]</p> <p>For the current version what's the LB?</p> <p>[/quote]</p> <p>It says 0.57+ but I dont think u need it ;)</p>
3706, <p>[quote=Vectors;82987]</p> <p>Why is Kappa scorer necessary for this ? Just trying to get some idea.</p> <p>[/quote]</p> <p>since kappa is the evaluation metric i use kappa scorer to find optimal hyperparameters which maximize kappa..</p>
3706, <p>let me update my script with how to cross-validate...</p>
3706, <p>done! plesase check the script for cross-validated parameter selection</p>
3706, <p>I updated the code. so it should be fine now...</p>
3706, <p>[quote=Triskelion;80811]</p> <p>Created a script to show some peculiarities with the Quadratic Weighted Kappa metric in comparison to Mean Absolute Error and Classification Accuracy.</p> <p>https://www.kaggle.com/triskelion/crowdflower-search-relevance/kappa-intuition</p> <p><code>ONE EXTREME ERROR<br>Ground truth: [1 2 3 1 4 4 4 4 4 4]<br>Predicted : [4 2 3 1 4 4 4 4 4 4]<br>MAE : 0.3<br>Accuracy : 0.9<br>Kappa : 0.6564885496183206</code></p> <p><code>FIVE SMALL ERRORS<br>Ground truth: [1 2 3 1 4 4 4 4 4 4]<br>Predicted : [1 2 3 1 4 3 3 3 3 2]<br>MAE : 0.6<br>Accuracy : 0.5<br>Kappa : 0.7037037037037037</code></p> <p><code>KAPPA CHANGES WHEN DISTRIBUTION CHANGES<br>Ground truth: [1 1 3 1 4 4 4 4 4 4]<br>Predicted : [1 1 3 1 4 3 3 3 3 2]<br>MAE : 0.6<br>Accuracy : 0.5<br>Kappa : 0.75</code></p> <p>[/quote]</p>  <p>U just wanna snatch my kaggle swag. Dont you? :P</p>
3706, <p>what is the error message?</p>
3706, <p>Ha ha... I don't know what u r doing with the open source stuff.... But I'd love it if a script wins wins one (or beats mine in private)</p>
3706, <p>[quote=Bluefool;82930]</p> <p>Have you won yet? :P</p> <p>[/quote]</p> <p>I'd quit making benchmarks :P</p>
3706, <p>any updates from admins on this one?</p>
3706, <p style="text-align: left">This rule is understood.... U r just hogging the leaderboard....!!</p>
3706, <p>[quote=Mendrika Ramarlina;83092]</p> <p>[quote=Chenglong Chen;82666]</p> <p>In my experience 0.67~0.68 is achievable using single model with careful text cleaning and feature engineering. But that requires a lot of tuning. Ensemble ease the work and some other not very well performed models can also be used to improve the score. It worth a try (even simple averaging).</p> <p>[/quote]</p> <p>I was finally able to get 0.65+ with a single SVM model but it beats me how you guys get 0.68 with one model. It will be interesting to read your write-ups after the competition ends.</p> <p>[/quote]</p>  <p>come on man... I believe some top guys already have 0.69-0.70 using a single model with google/bing and amazon search results... lol..&nbsp;</p>
3706, <p>Have Fun!</p>
3706, <p>Sorry I dont know R. But it should be very easy and similar to the python code. Maybe someone else can share an R code :)</p>
3706, <p>I hope your issue is resolved?</p>
3706, <p>Ids are strings. Try 1.0 instead of 1.</p>
3706, <p>can you post some code snippet so that I better understand your problem?</p>
3706, <p style="text-align: left">Try &nbsp;1.0 instead of 1&nbsp;</p>
3706, <p>It will be the best private score out of those three..... Choose wisely ;)</p>
3706, <p>Make ids like 1.02.03.0......</p>
3706, <p>Could you please add .0 after each prediction? Eg: change 1 -&gt; 1.0 2 -&gt; 2.0 and so on ?&nbsp;</p> <p>Let me know if it works!</p>
3706, <p>give it sometime the competition just started. why dont you create one?</p> <p>anyways its coming soon... ;)</p>
3706, 
3706, <p>yes:&nbsp;https://www.kaggle.com/abhishek/avito-context-ad-clicks/beating-the-benchmark</p>
3706, <p>Based on tinrtgu's code:</p> <p>https://www.kaggle.com/abhishek/avito-context-ad-clicks/beating-the-benchmark</p> <p>All credits to&nbsp;<strong>tinrtgu </strong>!</p>
3706, <p>[quote=Pavitrakumar;80897]</p> <p>is that tinrtgu's code from Avazu&nbsp;competition? :P</p> <p>[/quote]</p> <p>yes</p>
3706, <p>[quote=Leustagos;85524]</p>  <p>It is worth mentioning that in this benchmark bits is defined wrongly. instead of using <strong>bits = 20 we should use bit = 2**20 or 2**24</strong> should be a better starting point. The its defined right now won't do much better than defining a single average.</p>  <p>[/quote]</p>  <p>Its not a bug and nothing wrong there. It was left for the users... ;)</p>
3706, <p>Hi All</p> <p>Attaching a beating the sample benchmark here. Python code. Code will be available here and on scripts (if enabled for this competition). Code is simple and well commented.&nbsp;</p> <p>Ask me questions if you have any!</p> <p>Don't forget to swipe right on Tinder if this post helped you :P&nbsp;</p> <p>Score &lt; 0.40</p>  <p><strong>MOVED TO</strong> :&nbsp;https://www.kaggle.com/abhishek/caterpillar-tube-pricing/beating-the-benchmark-v1-0</p>
3706, <p>Ahh I forgot to write... its only for girls... :P</p>
3706, <p>[quote=inversion;82920]</p> <p>@Abhishek - You are a beast!!</p> <p>[/quote]</p> <p>lol ... :D</p>
3706, <p>Because its a regression problem. Isnt it?</p>
3706, <p>[quote=NxGTR;82979]</p> <p>There is only one way to find out ;)</p> <p>[/quote]</p> <p>Yes! You have to walk 20miles everyday non-stop!&nbsp;</p>
3706, <p>how much does it give on the leaderboard?</p>
3706, <p>downvoter please explain the down-vote..</p>
3706, <p>Next time please choose the topic of your post carefully... I''d suggest you change it to &quot;newbie question&quot; or &quot;blah blah blah i dont know shit..&quot;</p>  <p>[quote=Artificial Brilliance;83823]</p>  <p>I played around with the data sets and ran them through Random Forest Regression and Neural Nets when it dawned on me &quot;we are missing vital data.&quot;</p>  <p>When attempting to predict anything in the real estate market you need more data points. The prime information missing is geographical location. Other vital factors are things like the age of the home size of the home the lot is it 2 stories and the home value. Even the average value of nearby homes is important. All this is readily available on the net via API.</p>  <p>The main reason these are important is home hazards are highly affected by weather. Plus the home value and neighborhood are good indicators of maintenance and up-keep habits.</p>  <p>That being said it seems like the important part of this contest is not what can be predicted from this dataset but what the dataset should include in the first place. 90% of the battle in data science is won or lost in deciding which data points to include and exclude before feeding it to algorithms. In this case the data points are not as relevant to the goal as assumed.</p>  <p>[/quote]</p>
3706, <p>Yep. Thanks!</p>
3706, <p>[quote=Nim J;84003]</p>  <p>He (Abhishek) has already got negative 14 votes on his comment. I guess that's enough.</p>  <p>[/quote]</p>  <p>29!</p>
3706, <p>What do you think?</p>
3706, <p>Did knn work for anyone in this competition?</p>
3706, <p>cold start item recommendation ;)</p>
3706, <p>this is also useful: <a href="https://www.kaggle.com/c/avito-prohibited-content/forums/t/9584/average-precision-at-k-ap-k">https://www.kaggle.com/c/avito-prohibited-content/forums/t/9584/average-precision-at-k-ap-k</a></p>
3706, <p>[quote=deluXe;88344]</p>  <p>@Abhishek I am the one that contacted your boss last week. ;-) </p>  <p>[/quote]</p>  <p>My boss??? I'm the boss! :P :P</p>
3706, <p>[quote=cash_FEG;88527]</p>  <p>@Admin <br> Could you please show us what is the 30% in the public leaderboard? <br> 30% of the users? coupons? or something else?? <br> Thank you.</p>  <p>[/quote]</p>  <p>Users</p>
3706, <p>Great Idea! Lets write a script for that :P</p>
3706, <p>LB score?</p>
3706, <p>Just a guess: maybe because he works for CERN...</p>
3706, <p>This competition is a joke</p>
3706, <p>[quote=Bluefool;88326]</p>  <p>As Gilles hasn't been removed from the leaderboard I take it that he is still eligible for the Kaggle ranking points?</p>  <p>[/quote]</p>  <p>There will be people in the end using the agreement test dataset just to gain Kaggle points. How do you plan to remove them? ;)</p>
3706, <p>Pretty impressive @NxGTR!</p>  <p>Did anyone check the dato benchmark??</p>
3706, <p>[quote=Jiming Ye;88944]</p>  <p>Still struggling with pre-processing</p>  <p>[/quote]</p>  <p>It seems you are using the provided script :P</p>
3706, <p>[quote=khyh;88950]</p>  <p>@Abhishek</p>  <p>Are you going to post the &quot;Beating the Benchmark ;)&quot; script?</p>  <p>[/quote]</p>  <p>Should I ?? :)</p>
3706, <p>Im getting 67536... thats the total number of HTML files?</p>  <p>EDIT: this issue is fixed.</p>
3706, <p>For example:</p>  <p>2851560_raw_html.txt 2309400_raw_html.txt 1650252_raw_html.txt .....</p>  <p>are empty...</p>
3706, <p>96 empty files in 1.zip:</p>  <pre><code>1934233_raw_html.txt 585865_raw_html.txt 1774087_raw_html.txt 3781567_raw_html.txt 2053747_raw_html.txt 2488771_raw_html.txt 2672551_raw_html.txt 1881199_raw_html.txt 3549997_raw_html.txt 1319761_raw_html.txt 3070561_raw_html.txt 1872079_raw_html.txt 2492173_raw_html.txt 3121537_raw_html.txt 449833_raw_html.txt 3314623_raw_html.txt 4011301_raw_html.txt 3737173_raw_html.txt 3965065_raw_html.txt 2265499_raw_html.txt 2639557_raw_html.txt 2886973_raw_html.txt 1352533_raw_html.txt 3200785_raw_html.txt 1282591_raw_html.txt 3914965_raw_html.txt 1198687_raw_html.txt 2933077_raw_html.txt 1432495_raw_html.txt 2796325_raw_html.txt 2165431_raw_html.txt 2962015_raw_html.txt 3458569_raw_html.txt 885133_raw_html.txt 2962183_raw_html.txt 2690749_raw_html.txt 3742045_raw_html.txt 3954739_raw_html.txt 1725697_raw_html.txt 2944663_raw_html.txt 631099_raw_html.txt 1735039_raw_html.txt 3517681_raw_html.txt 2119489_raw_html.txt 1191715_raw_html.txt 1804219_raw_html.txt 1595011_raw_html.txt 1826827_raw_html.txt 581011_raw_html.txt 1754875_raw_html.txt 1467841_raw_html.txt 2281339_raw_html.txt 834403_raw_html.txt 1259353_raw_html.txt 951001_raw_html.txt 3704143_raw_html.txt 50041_raw_html.txt 3744931_raw_html.txt 419689_raw_html.txt 2800789_raw_html.txt 1759495_raw_html.txt 1808281_raw_html.txt 1638835_raw_html.txt 2883499_raw_html.txt 2786701_raw_html.txt 2784949_raw_html.txt 2695483_raw_html.txt 3272413_raw_html.txt 2198791_raw_html.txt 1118797_raw_html.txt 3706777_raw_html.txt 1478647_raw_html.txt 245077_raw_html.txt 1380547_raw_html.txt 4009819_raw_html.txt 573613_raw_html.txt 2254621_raw_html.txt 3905005_raw_html.txt 1140229_raw_html.txt 849271_raw_html.txt 1471291_raw_html.txt 1700191_raw_html.txt 2424883_raw_html.txt 876931_raw_html.txt 892453_raw_html.txt 3572479_raw_html.txt 886957_raw_html.txt 630547_raw_html.txt 553219_raw_html.txt 1549573_raw_html.txt 2740135_raw_html.txt 876217_raw_html.txt 3290887_raw_html.txt 1381189_raw_html.txt 1696759_raw_html.txt 2281003_raw_html.txt </code></pre>
3706, <p>empty files in 0.zip:</p>  <pre><code>2851560_raw_html.txt 2309400_raw_html.txt 1650252_raw_html.txt 3709986_raw_html.txt 2193258_raw_html.txt 629352_raw_html.txt 2579274_raw_html.txt 3938196_raw_html.txt 3637380_raw_html.txt 2521542_raw_html.txt 2357040_raw_html.txt 1080840_raw_html.txt 496788_raw_html.txt 3047736_raw_html.txt 3441636_raw_html.txt 396888_raw_html.txt 1519344_raw_html.txt 1298244_raw_html.txt 1844358_raw_html.txt 551286_raw_html.txt 3159600_raw_html.txt 870672_raw_html.txt 3404412_raw_html.txt 2905926_raw_html.txt 244770_raw_html.txt 1625952_raw_html.txt 12504_raw_html.txt 2183268_raw_html.txt 3946368_raw_html.txt 729306_raw_html.txt 1437642_raw_html.txt 3922974_raw_html.txt 3316254_raw_html.txt 2574126_raw_html.txt 3437664_raw_html.txt 408294_raw_html.txt 716688_raw_html.txt 4047252_raw_html.txt 68640_raw_html.txt 3136470_raw_html.txt 1438740_raw_html.txt 2672976_raw_html.txt 26580_raw_html.txt 733914_raw_html.txt 2919048_raw_html.txt 3176580_raw_html.txt 1872594_raw_html.txt 2976594_raw_html.txt 3588804_raw_html.txt 276450_raw_html.txt 3594540_raw_html.txt 1190082_raw_html.txt 261072_raw_html.txt 1472454_raw_html.txt 450888_raw_html.txt 1607202_raw_html.txt 2683236_raw_html.txt 3570552_raw_html.txt 3904662_raw_html.txt 2285244_raw_html.txt 2621496_raw_html.txt 3328878_raw_html.txt 696324_raw_html.txt 794556_raw_html.txt 1595004_raw_html.txt 1570944_raw_html.txt 134184_raw_html.txt 1289382_raw_html.txt 865632_raw_html.txt 1230408_raw_html.txt 594246_raw_html.txt 1424226_raw_html.txt 2473368_raw_html.txt 258654_raw_html.txt 2749170_raw_html.txt 2810700_raw_html.txt 2953890_raw_html.txt 3282468_raw_html.txt 319926_raw_html.txt 186528_raw_html.txt 3882006_raw_html.txt 252102_raw_html.txt 3852990_raw_html.txt 1564932_raw_html.txt 2922582_raw_html.txt 20910_raw_html.txt 1129050_raw_html.txt 1419432_raw_html.txt 873318_raw_html.txt 2346348_raw_html.txt 995898_raw_html.txt 1990926_raw_html.txt 412794_raw_html.txt 1113630_raw_html.txt 3780390_raw_html.txt 3157794_raw_html.txt 2662128_raw_html.txt 3458538_raw_html.txt 2274462_raw_html.txt 3713004_raw_html.txt 2786064_raw_html.txt 3142446_raw_html.txt 149832_raw_html.txt 3470868_raw_html.txt 390192_raw_html.txt 2295708_raw_html.txt 754176_raw_html.txt 3878472_raw_html.txt 3796110_raw_html.txt 902370_raw_html.txt 2762526_raw_html.txt 3088254_raw_html.txt 3697968_raw_html.txt 2053038_raw_html.txt 3157950_raw_html.txt 3811938_raw_html.txt 732036_raw_html.txt 4001136_raw_html.txt </code></pre>
3706, <p>empty in 2.zip:</p>  <pre><code>1135808_raw_html.txt 971090_raw_html.txt 593474_raw_html.txt 1536974_raw_html.txt 3919184_raw_html.txt 418964_raw_html.txt 3866228_raw_html.txt 1919576_raw_html.txt 2372534_raw_html.txt 769556_raw_html.txt 2328566_raw_html.txt 1724234_raw_html.txt 338654_raw_html.txt 1931306_raw_html.txt 482468_raw_html.txt 1444604_raw_html.txt 3531890_raw_html.txt 1235402_raw_html.txt 3213554_raw_html.txt 3146378_raw_html.txt 3908918_raw_html.txt 1420586_raw_html.txt 2051378_raw_html.txt 3111548_raw_html.txt 1863536_raw_html.txt 1011704_raw_html.txt 2684630_raw_html.txt 634040_raw_html.txt 2249138_raw_html.txt 3750008_raw_html.txt 1199606_raw_html.txt 1676102_raw_html.txt 2883194_raw_html.txt 1004576_raw_html.txt 2487500_raw_html.txt 3360458_raw_html.txt 2272268_raw_html.txt 975344_raw_html.txt 1254494_raw_html.txt 2365064_raw_html.txt 1190630_raw_html.txt 504872_raw_html.txt 3273860_raw_html.txt 55874_raw_html.txt 1152872_raw_html.txt 3871568_raw_html.txt 3441854_raw_html.txt 1142888_raw_html.txt 2572298_raw_html.txt 1483070_raw_html.txt 2143814_raw_html.txt 1315472_raw_html.txt 1330952_raw_html.txt 2243462_raw_html.txt 622610_raw_html.txt 3125108_raw_html.txt 3905306_raw_html.txt 2267090_raw_html.txt 2389874_raw_html.txt 707972_raw_html.txt 1176530_raw_html.txt 3688682_raw_html.txt 1031906_raw_html.txt 498602_raw_html.txt 3462518_raw_html.txt 2350994_raw_html.txt 1960256_raw_html.txt 1189454_raw_html.txt 1134260_raw_html.txt 4039796_raw_html.txt 1912436_raw_html.txt 592346_raw_html.txt 3463868_raw_html.txt 2980340_raw_html.txt 3502502_raw_html.txt 3731624_raw_html.txt 1652_raw_html.txt 3000908_raw_html.txt 3177398_raw_html.txt 3290306_raw_html.txt 3637838_raw_html.txt 1043636_raw_html.txt 2886698_raw_html.txt 2559476_raw_html.txt 3845804_raw_html.txt 3194270_raw_html.txt 1531100_raw_html.txt 971060_raw_html.txt 3223658_raw_html.txt 1879442_raw_html.txt 1869170_raw_html.txt 2117150_raw_html.txt 3812798_raw_html.txt 3705698_raw_html.txt 267722_raw_html.txt </code></pre>
3706, <p>empty in 3.zip:</p>  <pre><code>3373617_raw_html.txt 3599277_raw_html.txt 2849721_raw_html.txt 2599323_raw_html.txt 2084283_raw_html.txt 106173_raw_html.txt 1468653_raw_html.txt 3829563_raw_html.txt 1823073_raw_html.txt 3195081_raw_html.txt 319695_raw_html.txt 145821_raw_html.txt 1602909_raw_html.txt 3733905_raw_html.txt 860805_raw_html.txt 3017865_raw_html.txt 2449773_raw_html.txt 1959999_raw_html.txt 1808253_raw_html.txt 597969_raw_html.txt 699753_raw_html.txt 86241_raw_html.txt 1601775_raw_html.txt 251037_raw_html.txt 1709181_raw_html.txt 3023175_raw_html.txt 484533_raw_html.txt 2518041_raw_html.txt 1063857_raw_html.txt 2878683_raw_html.txt 669309_raw_html.txt 2995881_raw_html.txt 3880113_raw_html.txt 3150987_raw_html.txt 887205_raw_html.txt 3198789_raw_html.txt 252945_raw_html.txt 4046673_raw_html.txt 1884489_raw_html.txt 3265869_raw_html.txt 995511_raw_html.txt 1311273_raw_html.txt 1510245_raw_html.txt 2184789_raw_html.txt 3184719_raw_html.txt 371031_raw_html.txt 389781_raw_html.txt 2120817_raw_html.txt 1315383_raw_html.txt 3219405_raw_html.txt 375633_raw_html.txt 89403_raw_html.txt 2160411_raw_html.txt 2193243_raw_html.txt 963735_raw_html.txt 2648607_raw_html.txt 3626187_raw_html.txt 1179681_raw_html.txt 2298003_raw_html.txt 414825_raw_html.txt 1349829_raw_html.txt 1000047_raw_html.txt 3329469_raw_html.txt 684483_raw_html.txt 2215407_raw_html.txt 2687025_raw_html.txt 3969465_raw_html.txt 3058011_raw_html.txt 707871_raw_html.txt 3442419_raw_html.txt 46641_raw_html.txt 3033825_raw_html.txt 2015091_raw_html.txt 1436799_raw_html.txt 1941681_raw_html.txt 2959365_raw_html.txt 341907_raw_html.txt 3050145_raw_html.txt 3116811_raw_html.txt 359925_raw_html.txt 2977413_raw_html.txt 1360287_raw_html.txt 1100505_raw_html.txt 2686575_raw_html.txt 2509917_raw_html.txt 2407401_raw_html.txt 997689_raw_html.txt 577197_raw_html.txt 3626415_raw_html.txt 1343067_raw_html.txt 3748611_raw_html.txt 2725995_raw_html.txt 678369_raw_html.txt 1619829_raw_html.txt 72417_raw_html.txt 2083197_raw_html.txt 853101_raw_html.txt </code></pre>
3706, <p>empty in 4.zip:</p>  <pre><code>1126840_raw_html.txt 3024136_raw_html.txt 790588_raw_html.txt 1351750_raw_html.txt 3209824_raw_html.txt 1912192_raw_html.txt 241600_raw_html.txt 2153536_raw_html.txt 901858_raw_html.txt 3400534_raw_html.txt 2508124_raw_html.txt 3992716_raw_html.txt 747478_raw_html.txt 1263676_raw_html.txt 243094_raw_html.txt 3017062_raw_html.txt 3807814_raw_html.txt 439438_raw_html.txt 2603704_raw_html.txt 3760312_raw_html.txt 2197408_raw_html.txt 2722138_raw_html.txt 455116_raw_html.txt 1506166_raw_html.txt 746056_raw_html.txt 1575448_raw_html.txt 3329590_raw_html.txt 2224000_raw_html.txt 2879884_raw_html.txt 58366_raw_html.txt 3505192_raw_html.txt 2449936_raw_html.txt 562534_raw_html.txt 3353590_raw_html.txt 2910988_raw_html.txt 3810382_raw_html.txt 791650_raw_html.txt 2720854_raw_html.txt 286492_raw_html.txt 2165734_raw_html.txt 1436752_raw_html.txt 2447362_raw_html.txt 3517036_raw_html.txt 3130546_raw_html.txt 896836_raw_html.txt 2570482_raw_html.txt 2890210_raw_html.txt 2484256_raw_html.txt 3547798_raw_html.txt 2217586_raw_html.txt 769186_raw_html.txt 990490_raw_html.txt 1909270_raw_html.txt 671806_raw_html.txt 521038_raw_html.txt 2942452_raw_html.txt 3684478_raw_html.txt 348406_raw_html.txt 3343180_raw_html.txt 1983634_raw_html.txt 918214_raw_html.txt 259804_raw_html.txt 5818_raw_html.txt 893776_raw_html.txt 2245744_raw_html.txt 3381838_raw_html.txt 36562_raw_html.txt 1815646_raw_html.txt 1202260_raw_html.txt 2340232_raw_html.txt 1130086_raw_html.txt 2633086_raw_html.txt 3570712_raw_html.txt 696508_raw_html.txt 3725908_raw_html.txt 1153672_raw_html.txt 914788_raw_html.txt 669550_raw_html.txt 116104_raw_html.txt 1496524_raw_html.txt 1231480_raw_html.txt 1800586_raw_html.txt 3791008_raw_html.txt 1212622_raw_html.txt 605380_raw_html.txt 784564_raw_html.txt 136492_raw_html.txt 2559496_raw_html.txt 1932502_raw_html.txt 1223236_raw_html.txt 1147198_raw_html.txt 698998_raw_html.txt 2196196_raw_html.txt 2244694_raw_html.txt 246268_raw_html.txt 3438844_raw_html.txt 492826_raw_html.txt 995890_raw_html.txt 1506088_raw_html.txt </code></pre>
3706, <p>Takes me 1 GB!</p>
3706, <p>Lol</p>
3706, <p>Garbage collector. <a href="https://docs.python.org/2/library/gc.html">https://docs.python.org/2/library/gc.html</a></p>  <p>[quote=Glenn Blasius;89834]</p>  <p>[quote=RockBottom;89745]</p>  <p>gc() might be useful. </p>  <p>[/quote]</p>  <p>What is gc()?</p>  <p>[/quote]</p>
3706, <h1>Mine:</h1>  <h1>Kaggler:</h1>  <ul> <li>8gb ram / i5</li> <li>6 competitions for top 10% : Amazon challenge</li> </ul>  <h1>Master:</h1>  <ul> <li>8gb ram / i5</li> <li>9th competition top10: Cause Effect Challenge </li> </ul>
3706, <p>old macbook. its pretty slow now as im used to SSDs now. :D</p>  <p>[quote=inversion;89932]</p>  <p>I obviously need to get one of those 8gb i5 machines. Looks like they perform well in kaggle competitions!</p>  <p>[quote=Abhishek;89910]</p>  <h3>Kaggler:</h3>  <p>8gb ram / i5 6 competitions for top 10% : Amazon challenge</p>  <h3>Master:</h3>  <ul> <li>8gb ram / i5</li> <li>9th competition top10: Cause Effect Challenge  [/quote]</li> </ul>  <p>[/quote]</p>
3706, <p>No. Read the data page. It wont be provided.</p>
3706, <p>ur score is FTRL?</p>
3706, <p>[quote=Glenn Blasius;89938]</p>  <p>Guys there's a big hint in the competition overview: </p>  <p><em>You are challenged to construct new meta-variables and employ feature-selection methods to approach this dauntingly wide dataset.</em></p>  <p>I'm thinking PCA as it is good at deriving orthogonal features from a large number of features....</p>  <p>[/quote]</p>  <p>Yes! so u think if its not written u dont have to do it?</p>
3706, <p>Yes! u shouldn't participate in this competition! Its all black box! lol :P</p>
3706, <p>r u sure its a CSV file?</p>
3706, <p>Since I'm the overfitting expert I would like to hear from others how much they are overfitting with a single model? :P</p>  <p>mine right now: 0.79909</p>
3706, <p>[quote=Kuber@IITB;96405]</p>  <p>Can someone  who is not taking part now buc can submit lend me their credentials? Please..</p>  <p>[/quote]</p>  <p>I dont think its allowed. Its against the rules.</p>
3706, <p>I got a lot of requests during the competition for a benchmark script. So as promised here it is.... After all its all about learning :D</p>  <p>The script will easily give you 0.80+ in  Public LB. In Private LB you can easily get a top 10% rank using this script. With some modifications this script gives top 30 rank out of 2200+ competitors.... not bad ha?! ;)</p>
3706, <p>No. You are not allowed.  From the rules: &quot;External data is not allowed in this competition&quot;</p>
3706, <p>[quote=Sudeep Juvekar;100988]</p>  <p>Any plans of supporting older GPUs/GPUs with compute capabilities &lt; 5.0 on Neon? </p>  <p>Most of us can only get hold of grid K520 on EC2 for example. </p>  <p>[/quote]</p>  <p>I have the same question</p>
3706, <p>Cool.</p>  <p>Did you also get this error:</p>  <blockquote>   <p>2015-12-15 04:04:03589 - neon.data.imageloader - ERROR - Unable to   load loader.so. Ensure that this file has been compiled Traceback   (most recent call last):   File &quot;./localizer.py&quot; line 47 in        point_num=point_num)   File &quot;/home/ubuntu/whale/whale-2015/localizer_loader.py&quot; line 31 in   <strong>init</strong>       subset_pct nlabels macro dtype)   File &quot;/usr/local/lib/python2.7/dist-packages/neon-1.1.3-py2.7.egg/neon/data/imageloader.py&quot;   line 73 in <strong>init</strong>       self.start()   File &quot;/usr/local/lib/python2.7/dist-packages/neon-1.1.3-py2.7.egg/neon/data/imageloader.py&quot;   line 192 in start       self.loader = self.loaderlib.start(ct.c_int(self.img_size) AttributeError: 'LocalizerLoader' object has no attribute 'loaderlib'</p> </blockquote>
3706, <p>[quote=Sudeep Juvekar;101368]</p>  <p>That shouldn't happen. Did your 'make' run successfully? It should create .so in neon/data/loader/. Did you activate venv in neon?</p>  <p>Problem with me is that the cuda kernel fails to compile with Kepler in localize.py</p>  <p>It seems like they used to have a support for older GPUs in an earlier version (0.9.0) but it was deprecated. Try running 'make -e GPU=cudanet' (I doubt it will work with latest commits though). Just in case it succeeds please report back :D</p>  <p>[/quote]</p>  <p>No. It doesnt. On CPU it will take 15 hours lol  ( for one epoch :( )</p>
3706, <p>I think make got interrupted at some point. I have solved this issue :) Thanks!</p>
3706, <p>[quote=Christin B. Khan;103908]</p>  <p>Fingers and toes crossed over here!</p>  <p>[/quote]</p>  <p>Why? I thought Admins can see both public and private LB :D</p>
3706, <p>hahaha.... 5309 :D</p>
3706, <p>You can submit it right after the competition ends and see your score.</p>
3706, <p>I can confirm this for Android!</p>
3706, <ul> <li><span style="line-height: 1.4">Is there a way to delete our own script?&nbsp;</span></li> </ul>  <ul> <li><span style="line-height: 1.4">Is it possible to add a new tab : &quot;Scripts&quot; in user profile page which lists all the scripts created by that user (alongwith the number of votes)? (Just like Results tab lists all the competitions)</span></li> </ul>
3706, <p>So downvotes are allowed now?</p>
3706, <p>[quote=Ben Hamner;78934]</p> <p>[quote=Abhishek;78931]</p> <p>So downvotes are allowed now?</p> <p>[/quote]On scripts? No why do you think that?</p> <p>Do you think they should be?</p> <p>[/quote]</p> <p>clicking up arrow again (after you have upvoted) downvotes it. I just lost a vote &nbsp;# [not that i care ;) ]</p>
3706, <p>ahan! so votes can be taken back! Cool...thanks!</p>
3706, <p>What about a list of users who submitted submission file using a script?&nbsp;</p>
3706, <p>[quote=WhizWilde;80647]</p> <p>I want to ask something related to kaggle scripts. Hope it is the right place.</p> <p>I submitted accidentaly a script in the WNV competition while I just wanted to download the dataset (damn smartphones) and while it helped me reach a better position on the LB I don't want my best submission to be the fruit of someone else work.</p> <p>I still can't beat it even if I am not that far from it from my last submissions but were people I would point on my profile I don't want to be regarded as claimed positions and ability because of someone else work.</p> <p>So it is possible to erase this submission from my profile? (I don't want to refill my submission count it is not the problem).</p> <p>Thanks by advance.</p> <p>[/quote]</p> <p>there is a hide button that you can use....&nbsp;</p>
3706, <p>Need: Down-Votes! :)</p>
3706, <p>lot of sales? </p>
3706, <p>I'm also wondering why we have to submit the model now. The model can be improved a lot in the last week. </p>
3706, <p>Ahh such an interesting competition and i didnt have anytime (also infrastructure) to do it :(</p>
3706, <p>Why do we need spark for this competition? :)</p>
3706, <p>[quote=KSrinidhi;103769]</p>  <p>First timer in the competition please tolerate...</p>  <p>Is there a way to compute scores offline before submission? Thanks</p>  <p>[/quote]</p>  <p>You cannot know the exact score you will get but u can split ur training data into training and validation sets train on training set and calculate error on validation. Its called cross-validation: <a href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)">https://en.wikipedia.org/wiki/Cross-validation_(statistics)</a></p>
3706, <p>Congrats to all the winners! My special congrats to &quot;A FEW with No Clue&quot;. 9 New masters coming up from a single team :) </p>
3706, <p>Very imp variable </p>
3706, <p>[quote=raddar;100056]</p>  <p>Your estimations are way off when you look at LB :)</p>  <p>[/quote]</p>  <p>because there is some problem with the evaluation ;)</p>
3706, <p>Are we allowed to use pretrained networks with open-source license?</p>
3706, <p>Also there is something hidden in that script. Include that and you will be rank 1.</p>
3706, <p>Yet again data was about finding a &quot;leak&quot;. Disappointed.</p>
3706, <p>hahahaha :D</p>
3706, <p>Bluefool is back! \m/</p>
3706, <p>Ahh Marios. Didn't expect this from you :P Kaggle subtracts ur auc from 1 if it is below a certain threshold  I think 0.5 so that no one can trick the leaderboard and come out on top out of no where in the end :D :D</p>
3706, <p>The  public private split is always given at the top of the leaderboard and is decided by the organizers.</p>
3706, <p>hahahaaa... i love it! These kind of posts are very common when a competition is about to end :D maybe we can train a model to generate a post like this in the last week of the competition :P :P </p>
3706, <p>[quote=BreakfastPirate;116955]</p>  <p>[quote=Abhishek;116954]</p>  <p>hahahaaa... i love it! These kind of posts are very common when a competition is about to end :D maybe we can train a model to generate a post like this in the last week of the competition :P :P </p>  <p>[/quote]</p>  <p>Speaking of common posts during the last week I'm still waiting for the typical post that says &quot;Hey! Nobody told me there was a first submission deadline!&quot;</p>  <p>[/quote]</p>  <p>here you go: <a href="https://www.kaggle.com/c/santander-customer-satisfaction/forums/t/20464/cannot-submit-anymore">https://www.kaggle.com/c/santander-customer-satisfaction/forums/t/20464/cannot-submit-anymore</a></p>
3706, <p>I'm so frustated I downloaded data on 2nd March and I've been working 10hours a day on this dataset and have an awesome model that will beat 0.85 and now I cannot submit my model anymore even with 6+ days to go ... so disappointed! :P</p>
3706, <p>i have 0.999</p>
3706, <p>[quote=narsil;115660]</p>  <p>What shall I do to avoid this?</p>  <p>[/quote]</p>  <p>dont use the scientific notation?</p>
3706, <p>So is it a leak?</p>
3706, <p>Someone please upvote me 1000 times to ask about the leak :P</p>
3706, <p>Since now a days all competitions need neural nets and deep learning why not have a competition in which you provide a Tesla gpu to top10? ;)</p>
3706, <p>either overfitting or a leak :)</p>
3706, <p>My validation score is 0.90+ but my LB is my current score 0.77. Is anyone else having the same problem?</p>
3706, <p>Wow... its like when i was applying for jobs first time... 15 years of python experience is something you are missing.... hehehehe :D</p>
3706, <p>[quote=NxGTR;119040]</p>  <p>Hi! I am looking for someone to team up in this competition right now I am just overfitting the LB as in the Santander competition but hopefully someone can help me avoid that.</p>  <p>I do have some requirements:</p>  <p>1) You must be a Kaggle Master</p>  <p>2) You must be ranked above me</p>  <p>3) You must be a Kaggle Early Adopter</p>  <p>4) You should have at least 1 Prize Winner achievement</p>  <p>5) You must have at least 35 (Top 10% + Top 25% combined) achievements</p>  <p>6) You should have at least 1000 forum upvotes</p>  <p>Yes I am the same guy that dropped over 1000 places in Santander yet I am very picky :D</p>  <p>So if you are out there and comply with these requirements I would love to team up! let me know <a href="https://www.kaggle.com/domcastro">Bluefool</a>!</p>  <p>[/quote]</p>  <p>Damn im not an early adopter :(</p>
3706, <p>I can confirm that its idprobability </p>
3706, <p>Since external data is allowed please share it here before you use them.. :)</p>
3706, <p>[quote=DataGeek;118959]</p>  <p>I will be using pre-trained models from this thread <br> <a href="https://www.kaggle.com/c/state-farm-distracted-driver-detection/forums/t/20141/official-pre-trained-models-and-external-data-thread">https://www.kaggle.com/c/state-farm-distracted-driver-detection/forums/t/20141/official-pre-trained-models-and-external-data-thread</a></p>  <p>[/quote]</p>  <p>I think that is too general. One could say I'll use the pretrained data accessible to everyone on www.google.com</p>
3706, <p>Dear Admins</p>  <p>I&nbsp;am probably overlooking something but I was under the impression that I did submit some entries which are now removed. Also when trying to add a new submission I get the message;</p>  <p>&quot;This competition is closed to new entrants&quot;</p>  <p>Does this mean I wasted my time and cannot enter submissions this close to the deadline?</p>  <p>Kind regards</p>
3706, <p>Hmm so... You formally join the moment you make your first submission? On my profile it registers the competition as active (results tab). I think that happens the moment you download the data and agree with the rules.. So this is not really joining a competition then?</p> <p>Not a complain but a bit of a harsh lesson for me just finished my work only to find I cannot submit any more :( Well thats how we learn :)</p> <p>Thanks for your feedback!</p>
3706, <p>Luis</p> <p>thanks for clearing this up (sorry was writing while you were posting your reply)</p>
3706, <p>Same here below is a reproducible example. </p>  <p>I am investigating now but my instinct tells me I have to look at bigger N-grams. (e.g. black pepper is an N-gram of 2)</p>  <h3>#load all libs</h3>  <p>library(jsonlite)</p>  <p>library(tm)</p>  <h3>#load the JSON and convert to list</h3>  <p>train &lt;- fromJSON('data/train.json')</p>  <p>test &lt;- fromJSON('data/test.json')</p>  <h3>#create a matrix of ingredients versus cuisine on the TRAIN set</h3>  <p>train_ingredients &lt;- Corpus(VectorSource(train$ingredients))</p>  <h3>#quick peek at an item in the list (terms still containing the full sentences like &quot;black pepper&quot;)</h3>  <p>str(train_ingredients[97])</p>  <h3>#Put it into a DTM</h3>  <p>train_ingredientsDTM &lt;- DocumentTermMatrix(train_ingredients)</p>  <h3>#show output (here the items are indeed split into &quot;black&quot; and &quot;pepper&quot; instead of &quot;black pepper&quot;</h3>  <p>head(train_ingredientsDTM$dimnames$Terms 100)</p>
3706, <p>The solution here seem to be to put a tokanizer in and detect more the 1-grams. </p>  <p>Tokenizer &lt;- function(x) RWeka::NGramTokenizer(x Weka_control(min = 1 max = 4))</p>  <p>train_ingredients &lt;- Corpus(VectorSource(train$ingredients))</p>  <p>train_ingredientsDTM &lt;- DocumentTermMatrix(train_ingredients control = list(wordlengths=c(1Inf)tokenize=Tokenizer))</p>  <p>However note that my model did not improve much. </p>  <p>I expect that cleaning the data first would have more impact. E.g. change &quot;crushed black pepper&quot; and &quot;ground pepper&quot; to &quot;pepper&quot; or &quot;black pepper&quot;</p>
3706, <p>@Gerome</p>  <p>Same here accidentally  I was testing a similar approach yesterday evening and indeed that seems to perform a lot better. No ~0.8 scores yet but closing in :)</p>  <p>Thanks for the inspiration. </p>
3706, <p>@Pooja:  thanks for spotting. This is a leftover from an earlier experiment and this leads to incorrect mapping between ingredients and cuisine. Will update the script today (hopefully) with the approach that Gerome was suggesting or something along those lines.  </p>
3729, ----
3731, ----
3756, <p>[quote=Neil Summers;40555]</p> <p>I know this is too much personal information and impossible to measure such a broadly defined field but it would be fun to know for those of us aspiring to become data scientists.</p> <p>[/quote]</p> <p>Step 1 to becoming a data scientist: Nothing is &quot;impossible&quot; to measure.&nbsp;</p> <p>I'm fairly new to Kaggle but after seeing some of the performances in these competitions I'm amazed at the seemingly impossible things that are being measured and predicted.</p> <p>I've also read &quot;How to Measure Anything&quot; by Douglas Hubbard. Perhaps it has launched me into a quest to find something that's important to know that truly cannot be measured. I have yet to find such a thing.</p>
3756, <p>According to the description of the submission format we can enter any real number as a prediction.&nbsp; Does that mean we can use 0 as meaning we have absolute certainty in a non-match and 1 as absolute certainty in a match?</p> <p>I was a bit confused by the negative values in the example.</p> <p>&nbsp;</p> <p>Quote from the submission example:</p> <p>&quot;Each line of your submission should contain an QuestionId and a <br>prediction IsTrue. Note that you may submit any real-valued number as a<br> prediction &nbsp;since AUC is only sensitive to the ranking. <br>sampleSubmission.csv shows a representative valid submission. &nbsp;The <br>format looks like this:<br>QuestionIdIsTrue<br>10<br>20.3<br>399999<br>4-0.8<br>etc...&quot;</p> <p>&nbsp;</p> <p>Any help is appreciated.</p>
3756, <p>Excellent. Thank you for the quick reply!</p>
3756, <p>Has anyone found a way to identify teams likely to upset the &quot;better&quot; competitor? I've been looking into the number of times a weaker team has beaten a better team but does anyone else have an algorithm or method they're willing to share? &nbsp;</p> <p>I think this is the most difficult part of this competition since we're basically looking for the first deviation of an existing pattern.</p>
3756, <p>I like the assessment that the equal scoring of games reduces the value of picking any upsets in the first place. Perhaps rather than pick the upset we temper our probability of either team winning closer to 50/50.&nbsp;</p> <p>Calculating volatility will be key as well. ie which teams commonly beat better teams and which teams commonly lose to weaker teams. Coming up with that volatility score will be difficult enough. :-)</p> <p>&nbsp;</p> <p>Thanks Dr. Pain and taenareus. Much improvement left to be made to my methods.</p> <p>&nbsp;</p> <p>&nbsp;</p>
3756, <p>[quote=Gilberto Titericz Junior;39059]</p> <p>Same for R:</p> <p>test &lt;- read.csv('test.csv'header=TRUE)</p> <p>sub &lt;- test[ !duplicated( test$customer_ID fromLast=TRUE )  ]</p> <p>sub$plan &lt;- paste0( sub[18]sub[19]sub[20]sub[21]sub[22]sub[23]sub[24] )</p> <p>write.csv(sub[c(1ncol(sub)]paste0('sub.persistent.csv')quote=FALSE  row.names = FALSE )</p> <p>[/quote]</p>  <p>Thank you for the code Gilberto. &nbsp;It almost ran perfectly as-is but there was one missing parenthesis around your vector for the columns to select from sub. &nbsp;(Not trying to nitpick. Just hoping to help if someone else has trouble running it.) &nbsp;:-)</p> <p>The new last line is below.</p> <p>write.csv(sub[c(1ncol(sub))]paste0('sub.persistent.csv')quote=FALSE  row.names = FALSE )</p>
3756, <p>[quote=Eleftherios Spyromitros-Xioufis;40923]</p> <p>It is obvious that the purchased quote is missing from the test&nbsp;set. My point was that according to the data description:</p> <p>&quot;In the test set you have only a partial history of the quotes and do not have the purchased coverage options.&quot;</p> <p>in addition&nbsp;to not having the purchased quote we also have only partial history. To my understanding this means that we will never have full quote history in the training set! This is in disagreement&nbsp;to some of the quote history example's given above and confirmed as possible by William.</p> <p>[/quote]</p> <p>The training set does have the full quote history. The test set is obviously missing the final purchase choice but may or may not&nbsp;be missing some quotes before the final purchase. &nbsp;</p> <p>I'm guessing the Allstate folks will look at our results and decide how early in the process they should start recommending packages to the visitor.</p>
3756, <p>I chose to consider NA as a separate category but I also chose not to view this variable as an ordinal number. &nbsp;As far as we know the risk categories aren't in any specific order.</p>
3756, <p>Well my best submission is very simple and a kinda stupid.</p> <p>It made the best result on private data and on public data. Idea is simple:</p> <p>1) disregard all features except prices</p> <p>2) build a trend line for each day per instrument</p> <p>3) consider that the more price is off the beginning of the day value the greater probability that trend line will reverse. In case of volitile market and when it is a sideways trend we won't loose pretty much in overal score. But in case there is a big movement we will be reducing an error.</p> <p>4) the final formula is: predicted value = last observed value - 5 * slope of a trend line</p> <p>&nbsp;</p> <p>The R code for the solution:</p> <p>test &lt;- read.csv(&quot;sampleSubmission.csv&quot;)</p> <p># build a trend line<br># I get predicted value using last observed value and reversed slope of trendline<br>my_functor &lt;- function(start1 end1 col day) {<br>&nbsp; mc &lt;- lsfit(start1:end1 day[start1:end1col])<br>&nbsp; a = day[end1col] - 5*(mc$coefficients[2])<br>&nbsp; return(a)<br>}</p> <p># get data from day by day<br>training &lt;- function(indices arr my_functor) {<br>&nbsp; for (i in indices) {<br>&nbsp; &nbsp; a = paste(&quot;data/&quot; arr[i 1] &quot;.csv&quot; sep = &quot;&quot;)<br>&nbsp; &nbsp; day &lt;- read.csv(a)<br>&nbsp; &nbsp; # process each instrument<br>&nbsp; &nbsp; for(j in 1:198) { <br>&nbsp; &nbsp; &nbsp; arr[i j+1] = my_functor(1 55 j day)<br>&nbsp; &nbsp; }<br>&nbsp; }<br>&nbsp; return(arr)<br>}</p> <p>res &lt;- training(1:dim(test)[1] test my_functor)<br>write.table(x = res file = &quot;out_trend_minus_last.csv&quot; quote = FALSE sep = &quot;&quot; row.names = FALSE col.names = TRUE)</p>
3772, <p>It is true that the evaluation is conducted on a subset of all NaN values.</p>  <p>In fact there are two types of NaN values in jester.train</p> <p>1. Those were already missing in the original Jester data set.</p> <p>2. Those had values but were manually erased by me.</p>  <p>The evaluation is only performed on the second type of NaNs. For the first type of NaNs as there is no way to know the groundtruth the evaluation will not be conducted on them.</p>  <p>As a consequence you have both types of NaNs in jester.train but jester.id only index the second type of NaNs.</p>
3774, ----
3785, ----
3788, <p>Can someone tell me how to balance the shopping_pt variable. Thanks in advance.</p>
3788, <p>Can someone help me in finding the stuff for (understanding of the concept) for Ordered/Unordered multinomial Logistic Regression. Please help.</p>
3788, <p>For&nbsp;ordered data start you asked me with start with sklearn in python. I do not know python do we have any package for unordered logistic regression in 'R'.</p>
3788, <p>This won't clear the log as in case you subtract 1 from all the entries there could be some non-clear values. e.g. a shopping_pt with '7' as value and you subtract '1' from it it'll be 6 which is not a corrected record_type. Best way is to download a new test file.</p>
3788, <p>William why do we have&nbsp;more than 1&nbsp;entries for&nbsp;Cust_ID (some ID have one entry some got 2) in test_v2 version.</p>  <p>Why is it?</p>
3788, <p>Thanks for responding. I agree with you and have the same understanding. But the questions is 'why is there repetition in the Cust_id in test set 10000001 comes twice 10000599 come thrice and 10057334 repeats 4 times'. Please explain this phenomenon.</p> <p>Since its a partial (read: truncated history) Which option will I choose let say I predict 0313022 and 1313022 for each entry of 10000001. So which one would I choose finally out of them.</p> <p>Since both the Coverage Plans are different for 'A' Options (01) with predictor variables. Final outcome comes out to be 2 coverage options. I am confused as to which one I choose from the above two plans.</p>  <p>Somebody please respond.</p>
3793, ----
3800, <p>If possible please share something on your approach and&nbsp;techniques used. Would be really helpful for new learners!!</p>
3800, <p>Congrats winners! Congrat Barisumog for becoming&nbsp;Master!</p> <p>Congrats&nbsp;Abhishek&nbsp;for entering&nbsp;&nbsp;into&nbsp;the top 10!</p>
3800, <p>clips.tar. I am still unzipping this file. One time i did it before it gave me a big 46gb document file that is junk..I assume as per my understanding it should be three .mat files. kindly correct me if i m wrong.</p>
3800, <p>@Lalit Thank you..I got it unzipped using gunzip -c clips.tar.gz | tar xopf -<br><br></p>
3800, <p>I got Patient_1.. Patient_8 and Dog_1 to Dog_4 . Each folder has multiple .mat files..</p>
3800, <p>@Lalit Thank you very much for prompt reply.</p>
3800, <p>kindly requesting data..</p>
3810, <p>Hi guys</p> <p>My submission is rejected due to an incorrect number of lines (66292 expected).</p> <p>Yet when I count the lines (e.g. with &quot;wc -l&quot;) I exactly have 66292 lines.</p> <p>Any idea?</p> <p>Cheers</p>
3810, <p>Thank you triskelion.</p> <p>I was actually missing the phrase phrase id &quot;157451&quot; because it's a blank character.</p>
3814, ----
3843, ----
3860, ----
3861, ----
3867, ----
3879, ----
3887, <p>Ho Ho.... that was Some shakeup in the leaderboards. Any pointers why?</p>
3888, ----
3890, ----
3891, ----
3897, ----
3903, ----
3913, ----
3926, <p>Hi</p> <p>I had similar of making 2 submissions but not selecting for final scoring. I have written a post requesting the admins to select the submission in case you have made 1 or 2 selections since you can anyways select a maximum of 2 selections.</p> <p>Please pitch in you support if you agree&nbsp;</p> <p>Link:&nbsp;http://www.kaggle.com/c/march-machine-learning-mania/forums/t/7467/request-to-admins-missed-final-selection-for-scoring</p>
3926, <p>Dear Admins</p> <p>I had made 2 submissions a few hours before the deadline. But the next morning when I woke up I saw an email form the admin asking me to select my submission for scoring. Only then I realized that the submission should be selected (clicked in check box).</p> <p>Since a person can have a maximum of 2 selections for final scoring I was under the impression that the 2 submission of mine will be scored. I thought only if you made 3 or more submissions you nee to select 2 for final scoring. I admit that this was a mistake on my part.&nbsp;However having spent the last few months on the March Mania problem it is a shame that my submission will not be scored.&nbsp;</p> <p>I think there would be other participants like me who might have missed selecting the submission. In order to help our cause <strong>without breaking the rules</strong> of the competition I kindly request the admins to:</p> <p>&quot;<strong><em>If the participant has made only 2 or only 1 submissions the submissions can be score because as per the rules a maximum of two selection is allowed per person</em></strong>.&quot;</p> <p>I believe that this will be fair because in this case the participant was intending to submit more than 2 submissions anyways. Only if the participant made more tan 2 submission he/she would have to select this best 2 for final scoring.&nbsp;</p> <p>Regards</p> <p>Nilos</p>
3926, <p>This was one of my fav competitions and I would love to be back again next year. I had no knowledge of basketball before this competition but now in the hind sight I think I have found where my model gave me a big loss and would love to rectify them next year. The bigger picture would be to expand the model to predict results of different sports.</p> <p>And yeah I would love to see 2nd and 3rd prize too.</p>
3926, <p>[quote=Dr. Pain;40900]</p> <p>It will be interesting to see how this plays out over the remaining games but this suggests that being at the top of the leaderboard (at this point) has a large random element.</p> <p>[/quote]</p> <p>On the contrary I think breaking into the top of the leader-board especially the top 6 is going to very difficult. If I look at the last five score refreshes the worst position of someone in the current top 6 was position 11 which was three refresh back. If I look at the last two refreshes the worst position of someone in the current top 6 was position no. 9. So that the top positions have kind of stabilized.</p>
3926, <p>I have 2 questions:</p> <p>1. Will the sample submission file be updated for this year on day 132 once the 68 teams for this years tournament are announced?</p> <p>2. Is 14th March the last date for submission of our entries stage 1 or stage 2 of the competition?</p>
3926, <p>Hi Jeff</p> <p>Would you be re-uploading the mapping of the team names and the team id or is the mapping between team names and team id remaining unchanged?</p> <p>Regards</p> <p>N</p>
3926, <p>Hi Jeff</p> <p>You have uploaded the 2015 files in the data section. Wanted to check with you if these are any different from the ones you have uploaded in this thread because I have downloaded the 2015 files from this thread and started working on my final prediction. Let me know if there is any change between the 2015 files you have uploaded here and the same files you have uploaded in the data section.</p>  <p>By the way I really appreciate the good job you have done throughout the competition :)</p> <p>Regards</p> <p>N</p>
3926, <p>Last year the winning score was 0.529 by&nbsp;One shining MGF. This year I scored exactly 0.529 and I am not even in the top 200 :-). This year the winning score happens to be 0.438</p> <p>I am curious to know what led to this significant improvement over least year. Are we understanding basketball better and/or are we betting better.</p>
3926, <p>I am looking for a suitable team mate too</p> <p>My email id: nilotpalsinha@gmail.com</p>
3926, <p>I am looking to partner too. My current leaderboard position is 460/916 and would like to go higher by collaborating with a suitable partner:)</p> <p>email id: nilotpal.sinha@gmail.com</p>
3926, <p>@Ekrem</p> <p>I understand that our task is to predict the annual revenue. So the revenue given in the training data set must be some transformation of the annual revenue of the training restaurants. Or is it the transformed revenue of the restaurants since its opening date?</p> <p>Can you please clarify this?</p>
3926, <p>@Admin</p> <p>Are you there? Can you&nbsp;address the above questions?</p>
3926, <p>I stand with Jose.</p> <p>What is the procedure to impeach the competition admin? This is torture !!!! Any human rights activists around.</p>
3926, <p>@Wendy Kan</p> <p>Seriously!!!!!! How can you answer so early? I was hoping that you would answer 5 minutes before the competition deadline :(</p>
3926, <p>I am looking to team up .... always competed as an individual till date.  I am Ex Data Scientist :)</p>
3926, <p>@Garrett - Lets do it</p>
3927, ----
3928, ----
3929, ----
3930, ----
3933, ----
3934, <p>&nbsp;</p> <p>Hi all</p> <p>Has anyone had a problem with the test set?</p> <p>More specifically it seems that some files are not&nbsp;associated with any terms. This set of files consists of the following ids. 102344575943887510265131101774818288194372102722001223432407926173263232664728350297493146636540<br> 37327374263887740124402334080742107</p> <p>Any ideas?</p> <p>Thank you</p>
3934, <p>Hi all&nbsp;</p> <p>I have a question about the final solution. Should&nbsp;all nodes (in the ego-network of a user) be assigned to a circle? Can we ignore some of them in the submission file?&nbsp;</p> <p>Thank you</p>
3934, <p>That's nice.</p> <p>Thank you for the quick response.</p>
3936, <p>Anyone using R for the NN. In that case which R packae are you using. I am having issues with convergence of the NN solution in neuralnet package. Kindly suggest</p>
3940, ----
3943, ----
3948, ----
3949, ----
3951, <p>Dear Admin</p>  <p>Please remove the verification code option or change it. This would not work as it requires me to remove my number from DnD and I am not going to do that. Till then I am not able to make any submissions.</p>  <p>Regards</p>
3953, ----
3954, ----
3959, ----
3960, <p>Can the verification be postponed or made optional for next 7 days?</p> <p>I had already added a request to remove my name from DNC list yesterday but it seems like they will really take around 7 days to remove my name from the list.</p>
3960, <p>Congratulations to the winners!<br>The problem statement&nbsp; was very interesting and attractive to solve. Should have spent more time on this competition instead of last minute rush.</p> <p><br>Here are few things that I tried:<br>1. Stuck to just unsupervised learning and clique percolation for finding circles</p> <p>2. In cases where the circles were too large/dense to cause resource failure reduce the graph size by-</p> <p style="padding-left: 30px">a. Iteratively remove the minimum ranked node from graph till it is small enough to run clique percolation (This gave me my best train/ public score)</p> <p style="padding-left: 30px">b. Graph partitioning and then finding circles (This method didn't perform that well mostly because the graphs were too dense to give good partitions and circles from 2 different partitions might be a one big circle)</p> <p>3. For very small graphs partition the the graphs and give them as different circles</p> <p>&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; </p> <p>Things that I had wanted to try but didn't (Will be trying these soon):</p> <p>- Explore clustering techniques as social graphs might better follow cluster structure than perfect cliques. Plus they would have been light on resources.<br>- Do a clustering on the non-sparse training features and find intersections between feature clusters and Graph clusters to enhance the the Pure-graph Circles<br><br>Would be interesting to find out what other techniques did people try. Were people able to use features to give out better circles?</p>
3960, <p>Ya even on profile results page it shows &quot;Closed - Validating Final Results&quot;.</p> <p>Probably some results validations.</p>
3960, <p>[quote=Run2;54060]</p> <p>Whats going on ? I dropped from 53 in the Private Leader board couple of hours back to suddenly 108 now!! I can see everything changed in the Private Leader board.!!!!</p> <p>Ok so that 108 is the public score ? Not sure what is going on here ......&nbsp;</p> <p>[/quote]</p> <p>Its not the Private leader board being shown now they have just placed the public leaderboard ranks now.</p>
3960, <p>That's some wonderful piece of code!</p>
3960, <p>[quote=Liuftvafas;57037]</p> <p><span style="line-height: 1.4">OK guys. This is embarrassing but I cannot make it work. First of all t</span>his is the&nbsp;first time I'm trying to use scikit-learn or random forests. It seems to work with no changes as expected on the first run. Thank &nbsp;you Dmitry.</p> <p>Going further. If I add more meta features (predictions from VW) I get all cross validation losses during training on metas at least 2x better than the baseline script. However public leaderboard score decreases significantly. Any ideas what am I doing wrong here?</p> <p>[/quote]</p>  <p>There might be some overfitting happening in your cross-validations.</p> <p>Since you are using the the VW predictions as features they might have been trained using the rows form the CV- fold you are running the sk-learn model on. Indirectly the CV fold gets a look at the original labels.</p> <p><br>What might help is to create the VW scores using the same CV indices.</p>
3960, <p>Submissions issue seems to be there in all competitions scoring engine is slow for some reason.<br>Submitted a 15kb file in the Social networks competition and its still pending after 15 minutes.</p>
3960, <p>Hi Ben.</p> <p>&nbsp;</p> <p>I think that prediction horizon of 26459 rows is very difficult to obtain.&nbsp;&nbsp;</p> <p>In my experience&nbsp; next&nbsp; 5000-5200&nbsp; rows&nbsp; are&nbsp; accurate.</p> <p>&nbsp;</p>
3960, <p>Hi Martin</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>I think departure_airport_code is not relevant. &nbsp; Try to extract only 3 requested variables from flighthistory.csv;&nbsp; flight_history_id&nbsp; actual_runway_arrival actual_gate_arrival. &nbsp; You can use MS Excel (data import text). &nbsp;&nbsp; There are many fields with  missings and hiddens.&nbsp;&nbsp; You can try to estimate missing and hidden fields (a bit difficult)&nbsp; or&nbsp; put&nbsp; 0s&nbsp; (errors in predictions wil be high). </p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>regards</p> <p>&nbsp;</p> <p>I. Jaganjac.</p> <p>&nbsp;</p>
3960, <p>Hello.</p> <p>&nbsp;</p> <p>Is the format of predicted arrival data as estimated benchmarks:</p> <p>flight_history_id actual_runway_arrival actual_gate_arrival.</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>Also how many rows should be predicted.</p> <p>Benchmarks have 26460 rows.</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>regards</p> <p>&nbsp;</p> <p>I. Jaganjac</p>
3960, <p>Hi&nbsp; Ben </p> <p>&nbsp;</p> <p>&nbsp;</p> <p>What is the format of model submission.&nbsp;</p> <p>Algorithm pseudocode or detailed algorithm ?</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>&nbsp;</p> <p>regards</p> <p>I. Jaganjac</p>
3960, <p>Hello. This line&nbsp; 57 -----&gt;&nbsp; X = nc.Dataset(path'r+').variables.values()[-1][:::3:73:13]&nbsp;</p> <p>causes an error in netCDF4 module.</p>
3960, <p>Hello.</p> <p>I think error is caused by dimensionality mismatch.</p>
3960, <p>Hi Peter.&nbsp; I have another question.</p> <p>I see in scikit-learn manual that you are the author of sklearn.ensemble package.&nbsp; The computation time for gradient boosting regression is very long (&gt;30 hours) on core-2 Intel uP with 4 GB RAM&nbsp; for say&nbsp; 1000 regression trees of depth 10.</p> <p>Do you get less computation time ?</p> <p>&nbsp;</p> <p>&nbsp;</p>
3960, <p>@Leustagos</p> <p>&nbsp;</p> <p>Hello.&nbsp; Did you perhaps try:</p> <p>&nbsp;</p> <p>model=RandomForestRegressor(n_estimators=500max_depth=5n_jobs=2)</p> <p>for: &nbsp; training data shape (51132160) (511398).</p> <p>It takes very long time on 2-core uP.</p>
3960, <p>@Peter</p> <p>Thanks for the tip.&nbsp;&nbsp; That's it.</p>
3960, <p>@Leustagos</p> <p>Thanks.&nbsp; An error comes up when GradientBoostedRegressor is applied on training data shape (51132160) (511398).</p>
3960, <p>Hello. &nbsp; I tried ensembling extra trees regressor and random forest regressor&nbsp;&nbsp; but did not improve on MAE error. &nbsp; For instance&nbsp; Gaussian Process Regressor requires that&nbsp; trainX and trainY have the same number of rows.</p>
3966, <p>Just a small update to your code for windows batch file I saw aiff files have aif extension. So if anyone is getting errors with this code then try the below;</p> <p>for %%f in (*.aif) do (<br> &nbsp;&nbsp; sox %%~nf.aif &quot;converted/%%~nf.wav&quot;<br> )</p> <p></p> <p>Thanks to chenopod for the help.</p>
3966, <p>Thanks arnaudsj I was beginning to wonder why basic algorithms were failing....</p>
3966, <p>Hi</p> <p>&nbsp;&nbsp;&nbsp;&nbsp; I don't get it don't we already have the labels on the validation data. I have downloaded the new validation datasets which seem to have the labels in the .mat files. Are these labels incorrect and should we ignore them? Thanks.</p>
3966, <p>Thanks Xavier</p> <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; So for now we can submit results on the validation data until the key is released? Thanks.</p>
3966, <p>Xavier</p> <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Is the leader board going to be reset soon?</p>
3966, <p>Imagine my surprise when I woke up this morning.</p> <p>Hope they fix this soon.</p>
3966, <p>seems the issue is fixed but kaggle points are still not updated correctly.</p>
3966, <p>Has anyone notified the kaggle tech support? How does one notify the kaggle support?</p>
3966, <p>Done Raised a support request to award the correct number of kaggle points. It was sweet having 55 rank though :)</p>
3966, <p>Xavier the leader board states</p> <p>This leaderboard is calculated on approximately<strong> 0%</strong> of the test data.<br> The final results will be based on the other 100% so the final standings may be different.</p> <p>Is this the reason for everyone getting 0.000000 score? If so when will the parser start parsing the submission?</p>
3966, <p>Thank you Xavier.</p>
3966, <p>Xavier When will the Kaggle points be awarded?</p>
3966, <p>Thank you sescalera.</p>
3966, <p>Yup This is sad. This was one of the hardest challenges in the recent ones.</p>
3966, <p>sescalera</p> <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; I have enjoyed this competition and have no qualms about it. I was just remarking that it is sad that Kaggle hasn't updated the leaderboard so that we can get the points that will boost our ranks. Rankings along with knowledge help in driving us to do more competitions.</p>
3966, <p>sahi hai bhai.....</p>
3971, ----
3973, <p>Can this work on windows?&nbsp; I am trying to fix an error with timegm</p>
3978, ----
3984, <P>Same with me.&nbsp;I was asked for the submition only without the algorithm - I think it is a kind of cheating and it does not make any sense. <BR><BR>Marcin </P>
3984, Congratulations to all teams especially in first 5 places! It seems I was approaching towards public test data instead of the right solution. I am curious if other participants realized that public test data are not very representative (through for example cross-validation) - I had such intuition once at the beginning of the contest but abandoned this hypothesis - lucky with public RMSE results :).
3984, Aron: I don't know yet.&nbsp;(Will the all hidden data published?). My method should alse be considered naive - I was using linear regression only without historical data.&nbsp;
3984, <p>Hi</p> <p>I have a kind of reversed question to competition organizers. May I publish my algorithms (give the description of algorithm and maybe source code - that every participant would use) on my website/blog?</p> <p>I know that this would not help me to get any prize however I think sharing the existing solutions as foundation for further research &nbsp;may allow concentrate other participants to find better solution. In this competition I think the fusion of 2 worlds is needed - the "data hackers" (this is rather my case) and domain experts (physicians) - so also sharing the algorithms early might help to get better results. Teams that come from these different worlds might share ideas. &nbsp;</p> <p>I remember that during Netflix competition one person published the solution that allows many other people to get higher results and brought more "competition" into the competition (not on the top of the leaderboard but below).</p> <p>Marcin</p>
3984, <p>Today I got a message: &quot;There was a problem with your submission&quot; with info:<br> &quot;The string is not valid date time format&quot;. I have not changed the result format (I think) since my previous succesful submission. What might be the reason?</p> <p>Thanks<br> Marcin</p>
3984, <p>Now it works thanks. <br> And I do appreciate more sophisticated goose picture! </p>
3984, <p>I am not sure If we should also provide at this stage the methodology description? I assume that it is not needed now but later in case of being on top positions. &nbsp;Am I right?</p>
3984, <p>I think it depends on the area. It is not clear for me for this competition.&nbsp; However in case of building predictive model for example in marketing campaign or churn - test data are available. In case of estimating risk of potential claim for insurance policy  for new client - not available.</p>
3984, <p>Hi</p> <p>I am afraid that reversing anonymization process by track matching might cause that the winning models will be not the best ones - from organizer perspective. &nbsp;It may happen that solution with optimal streets map reconstruction algorithm (which is useless for the organizer as they know the exact GPS coordinates) might won or it might be very important part of the winning model. The situation when &quot;other&quot; driver is moving on the same path but with different behaviour is very rare or not exists at all - because just matching the paths (as other users noticed) is enough to obtain good results.</p> <p>This is my observation at this moment - but I am still digging into the data so maybe I missed something.</p>
3984, <p>My pure matching result so far is 0.83 and someone on the forum reported 0.87.&nbsp;</p>
3984, <p>[quote=MarDo;63294]</p> <p>[quote=Marcin Pionnier;63273]</p> <p>My pure matching result so far is 0.83 and someone on the forum reported 0.87.&nbsp;</p> <p>[/quote]</p> <p>where did you read this? So far I only saw those result if trip matching (have not tried it myself yet)&nbsp;was used on top of other features/models.</p> <p>[/quote]:</p> <p>Yes you are right. I have wrongly interpreted this message:</p> <p><a href="http://www.kaggle.com/c/axa-driver-telematics-analysis/forums/t/12321/python-code-to-automatically-draw-repeated-trips/63177#post63177">http://www.kaggle.com/c/axa-driver-telematics-analysis/forums/t/12321/python-code-to-automatically-draw-repeated-trips/63177#post63177</a></p> <p>Thanks for pointing this out.</p>
3984, <p>Chippy</p> <p>I missed your post and Kaggle answer. It seems I should spend more time on reading the forum :)</p>
3984, <p>for verification purposes use only</p>
3987, ----
4000, ----
4004, ----
4024, ----
4031, ----
4032, ----
4037, ----
4043, <p>I dont think there is all that much cheating going on - i made it to 7th place using a really standard&nbsp;machine learning model. Just a shame this one has no prize!</p>
4043, <p>Hi I had a similar problem. Renaming to *.tar.gz and using WinRar (definitely not WinZip!) fixed it :)</p>
4043, <p>Looking at the rules I guess that would not be allowed. You can't alter the model *specifically* for each patient but you can alter the way the software processes according to variances in the incoming data. This way the software should work for patients that are currently unseen to us.</p>
4043, <p>Hi thanks for this. I had success with renaming the files to *.tar.gz and opening in WinRAR (winzip corrupts the .mat files)</p>
4043, <p>Hi why&nbsp;does the&nbsp;FeedBackEvent value only exist for one sample at a time? I would have expected the labels to cover around&nbsp;a second of data. Is&nbsp;FeedBackEvent only indicating where the EEG that corresponds to a feedback starts? If so how long does the feedback last?</p> <p>Thanks! :)</p>
4043, <p>Brilliant thanks :)</p>
4043, <p>Hi just wondering if the submission limit per day can be increased?</p>  <p>Cheers</p>
4043, <p>I am sure going from 3 submissions per day to 5 or 6 wouldnt make too much difference!&nbsp;</p>
4043, <p>Also if you are worried about overfitting then just use 3 submissions</p>
4043, <p>Aahh ok I see :)</p>
4043, <p>Hi all just a bit confused with this line....</p>  <p>&quot;(Note: the actual submitted predicted probabilities are replaced with max(min(p1&#8722;10^&#8722;15)10^&#8722;15).)&quot;</p>  <p>Am I right in saying that this means that the probabilities are altered server-side? So we dont have to actually perform this calculation? Also I read that log loss brutally punishes you for being completely wrong (extreme 1 or 0 classification) so would the above calculation be made to avoid this?</p>  <p>Cheers</p>
4043, <p>Waiting for the &quot;achieve 0.33 on the leaderboard...&quot; post now haha :)</p>
4043, <p>Hi just wondering if it is possible to obtain information about why a RF made a given classification?</p>
4043, <p>I really like the plot :) someone did a t-sne plot a while back and posted on this forum would highly recommend it if you havent seen it&nbsp;https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/13122/visualization/69296</p>
4043, <p>In regards to the feature engineering - I looked at interactions between variables for a while which gave a bit of an accuracy increase. Someone else mentioned counting the number of non-zeros for each vector creates a useful feature. But all in all I didnt really get that far with it; I think this competition is just going to be about who can find the best tuning parameters :(</p>
4043, <p>[quote=Spaceman;79075]</p> <p>I feel very confident that from this day forward every competition you see &quot;<strong>Spaceman</strong>&quot; it will be in the top 10... It is that simple</p> <p>[/quote]</p>  <p>lol :D surely just trolling?</p>
4043, <p>Mind you maybe these competitions do actually suffer from&nbsp;a lack of trash-talking?</p>
4043, <p>Personally I would prefer a contest that gets us to predict if a Portuguese taxi driver will try to get away with charging you 40 euros for a 5 minute trip just because you're a tourist</p>
4043, <p>You could do with increasing the number of trees from 50 to something higher. I find 250 works though haven't really played around with it yet :)</p>
4050, ----
4054, ----
4066, <p>Very very sad news. Rest in peace Leustagos</p>
4066, <p>Many congrats ...you deserve it</p>
4066, <p>@Anil Even i got the same error &quot;AttributeError: 'LocalizerLoader' . There were no issues while sys-wide install (make sysinstall) and i did a 'make' in data/loadeer folder as well</p>
4066, <p>@lhatsk I too had the same issues with loader folder  and after that faced cuMemAlloc failed issue. Changed 'z32' to 'z8' as Sudeep suggested (with 4GB Gpu)  and its running fine now but slow...currently in 9th Epoch and it took around 20 hours :(</p>
4066, <p>its around 12.14   but its taking  4900  seconds for each epoch  :(</p>
4066, <p>Now tried to change the imwidth=256  but getting the below error in Classifying:</p>  <p>File &quot;/home/roshan/anaconda3/envs/python2/lib/python2.7/site-packages/neon/backends/layer_gpu.py&quot; line 1122 in _magic64     magic shift = _magic32(nmax d)   File &quot;/home/roshan/anaconda3/envs/python2/lib/python2.7/site-packages/neon/backends/layer_gpu.py&quot; line 1103 in _magic32     nc = ((nmax + 1) // d) * d - 1 ZeroDivisionError: integer division or modulo by zero</p>
4066, <p>When uploading my file I get the following error:</p>  <p>Could not parse '#VALUE!' into expected type of Double</p>  <p>Please advise</p>  <p>Thank you</p>
4066, <p>Thank you !! I found it.</p>
4066, <p>I've checked the team numbers and</p> <p>I've check all the combinations of teams</p>  <p>Q1. &nbsp;Does this simply mean I have 201 incorrect combinations of teams?</p> <p>Q2. If I leave as is and these 201 missing combinations are not actual matches will my submission be still scored?</p>  <p>Many thanks</p>
4066, <p>Hi</p> <p>Don't understand why I&nbsp;receive an error indicating that only one key cannot be found should there at least be two if I have all 2278 rows.</p>
4066, <p>I switched it back around. &nbsp;</p> <p>It looks like&nbsp;both submissions uploaded ok now.</p>  <p>Many thanks for the great response time on my question.</p>  <p>Finally my first Kaggle entry!! :))</p>
4066, <p>Hi - If I only seem to have a problem with one key after I make two submissions and select two uploads to be submitted are all the other correct 2277 keys with probabilities still measured and ranked; and are both my submissions added to the leaderboard after I've selected them?</p>  <p>many thanks</p> <p>Anthony</p>
4066, <p>Hi&nbsp;</p> <p>Not that it would make a difference in my score. &nbsp;</p> <p>However if I have two submissions would there then at least be two rows in the .csv leaderboard file that correspond to both my entries?</p>  <p>I noticed some parrticipants were listed twice in the .csv file presumably for each of their submisstions.&nbsp;</p>  <p>Thanks!</p>
4066, <p>Hi if you are able to extend the deadline at least until sometime later today that would be great.</p>  <p>Thanks</p>
4066, <p>I had to find some control data (i.e.- no seizures)</p>
4066, <p>Hi</p> <p>To be clear am I to submit at file with:</p>  <p><strong>2278 entires (matchups)</strong></p> <p>ID = 1104_1129 &nbsp; &nbsp; &nbsp; &nbsp; or &nbsp; &nbsp; &nbsp; &nbsp; 2014_1104_1129 &nbsp; &nbsp;?</p>  <p>or&nbsp;</p>  <p>2278 entries x 4 (2011 - 2014) ~ <strong>9112 entries</strong> &nbsp;?</p>  <p>ID= 2014_1104_1129</p>  <p>Thanks</p>
4066, <p>Any more insight as to what this error means? Thanks</p>  <p>Column '' was not expected (Line 1 Column 15)</p>
4066, <p>Thanks for your quick reply!&nbsp; I just changed the way I made the .csv file and that worked.</p>
4066, <p>Can any one tell me why I might be receiving this error?</p>  <p>ERROR: Unable to find 104266 required key values in the 'QuoteNumber' column ERROR: Unable to find the required key value '173838' in the 'QuoteNumber' column ERROR: Unable to find the required key value '173841' in the  ....</p>  <p>I'm submitting the classifications on the test file with 173836 rows not including the header file.  Why would the system be looking for rows in the test set that are not there?</p>  <p>Thanks!</p>
4080, ----
4104, <p>[quote=taras sereda;90014]</p>  <p>Can somebody clarify which pass should bu used?  PATH_TO_JSON = &quot;path/to/data/from/process_html.py&quot;</p>  <p>I've used path to directory where I've downloaded all archives {01234} but script fails when parsing compleats.</p>  <p>Probably I'm missing something. I've got the following error.</p>  <pre><code>Traceback (most recent call last):   File &quot;/Users/taras-sereda/PycharmProjects/dato/classifier.py&quot; line 33 in &lt;module&gt;     sf = sf.unpack('X1'column_name_prefix='')   File &quot;/usr/local/lib/python2.7/site-packages/graphlab/data_structures/sframe.py&quot; line 4678 in unpack     new_sf = self[unpack_column].unpack(column_name_prefix column_types na_value limit)   File &quot;/usr/local/lib/python2.7/site-packages/graphlab/data_structures/sarray.py&quot; line 2692 in unpack     raise TypeError(&quot;Only SArray of dict/list/array type supports unpack&quot;) TypeError: Only SArray of dict/list/array type supports unpack [INFO] Stopping the server connection. </code></pre>  <p>[/quote]</p>  <p>My guess is that you need to run process_html.py first (takes about 8 hours for me) and you will get the json files at the output. However I have no ideas what to do after that. In the classifier.py the code tries to open something as csv file in  PATH_TO_JSON making little sense. Any hints please?</p>
4107, <p>I am wondering too.&nbsp;</p>
4107, <p>PCA worked well with 12&nbsp;</p> <p>yet the GMM in R is totally different from that&nbsp;in sk-learn.&nbsp;</p>
4107, <p>p.hats &lt;- predict.glm(train.glm newdata = testData type = &quot;response&quot;)<br>Error in eval(expr envir enclos) : object 'feature1' not found</p>
4107, <p>Looks powerfull nice work!&nbsp;</p>
4107, <p>Wow fantastic article and thanks for sharing!&nbsp;</p>
4107, <p>Oh come on! It's been two days when could we keep on going?&nbsp;</p> <p>Sorry for complaining and good to see everything continues :P</p>
4107, <p>Hi</p> <p>I am an advertising researcher&nbsp;and want to work in the&nbsp;project in Matlab or Java. Anybody that is interested are more than welcome.</p> <p>Thanks!</p>
4107, <p>[quote=pi_informatics;58472]</p> <p>Check the rules:</p> <p>OPEN-SOURCE CODE</p> <p>A Submission will be ineligible to win a prize if it was developed using code containing or depending on software licensed under an open source license:</p> <p>* other than an Open Source Initiative-approved license (see &lt;http://opensource.org/&gt;); or<br>* an open source license that prohibits commercial use.</p> <p>* using Matlab or any closed source software (except BLAS) will not be qualified.</p> <p>[/quote]</p> <p>1. Good call that implies&nbsp;extra exffort for&nbsp;rewriting functions in Matlab or Java when it is necessary XD</p> <p>2. But it is fun after all so let's do it!&nbsp;</p>
4107, <p>Mosel: E-123 at (477) of `santa_test.mos': `toys' is not defined.</p>  <p>Isn't mosel defines variables in a way that's similar to python?&nbsp;</p> <p>What should we do if further actions are required.&nbsp;</p>
4107, <p>Thanks! and&nbsp;</p> <p>could you show me&nbsp;</p> <p>How to &quot;install and configure the Xpress-MATLAB interface&quot;&nbsp;with the&nbsp;santa_updated_XpressTools_Matlab_2.zip?&nbsp;</p>  <p>cuz I have trouble running this:&nbsp;</p> <p>&gt;&gt; moselexec 'santa_greedy_matlab.mos'<br>Undefined function 'moselexec'</p>
4107, <p>I installed Xpress7.7 in &quot;/opt/xpressmp/&quot; and&nbsp;got&nbsp;a matlab 2014b if that matters I find&nbsp;the &quot;add path&quot; button but got lost and don't know which folder&nbsp;to add...&nbsp;</p>
4107, <p>Ah my installation doesn't have a matlab folder all are listed below and looks confusing -&nbsp;</p> <p>$pwd<br>/opt/xpressmp<br>$ls -al<br>total 704<br>drwxr-xr-x 14 root wheel 476 Dec 22 11:54 .<br>drwxr-xr-x 5 root wheel 170 Dec 21 23:25 ..<br>drwxr-xr-x 19 502 staff 646 Dec 22 11:54 bin<br>drwxr-xr-x 17 502 staff 578 Dec 22 11:54 docs<br>drwxr-xr-x 24 502 staff 816 Dec 22 11:54 dso<br>drwxr-xr-x 11 502 staff 374 Dec 22 11:54 examples<br>drwxr-xr-x 22 502 staff 748 Dec 22 11:54 include<br>drwxr-xr-x 27 502 staff 918 Dec 22 11:54 lib<br>-rw-r--r-- 1 502 staff 48842 Apr 28 2014 license.txt<br>drwxr-xr-x 25 502 staff 850 Dec 22 11:54 licenses<br>drwxr-xr-x 5 502 staff 170 Dec 22 11:54 readme<br>-rw-r--r-- 1 502 staff 27483 Apr 28 2014 readme.html<br>-rw-r--r-- 1 502 staff 275446 Apr 28 2014 relnotes.html<br>-rw-r--r-- 1 502 staff 458 Apr 28 2014 version.txt</p>
4107, <p>Although it may drain the performance a bit setting up a large virtual memory should help.&nbsp;</p>
4107, <p>How could one integrate the Mosel with matlab I can't seem to get it working by executing &quot;execmosel&quot;...</p>
4107, <p>Er.. ok thanks for telling.&nbsp;</p>
4107, <p>Wait... after executing&nbsp;</p> <p>nothing seems to happen&nbsp;</p> <p>how could one&nbsp;tell if it is running and how long&nbsp;</p> <p>one supposed to wait?&nbsp;</p>   <p>&gt;&gt; moselexec 'santa_greedy_matlab.mos'<br>&gt;&gt; ls</p>
4107, <p>Let us share our last line in the submission and talk about what works and doesn't.&nbsp;</p> <p>ToyId &nbsp; &nbsp; &nbsp; ElfId &nbsp; StartTime &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Duration</p> <p>2032434 &nbsp;455 &nbsp; &nbsp;2529 2 20 11 25 189892</p>
4107, <p>I kinda&nbsp;interested in finding someone to discuss with currently working on python with heuristic strategy and algorithms. Currently 167 and working on a&nbsp;better version.&nbsp;</p>
4107, <p>It is sooo HUGE that my chrome says &quot;7 days left&quot; for downloading this but once I finish my work shift the download would be stop and I will never finish downloading it.&nbsp;</p>  <p>Any tricks/ hacks for getting the downloading job done?&nbsp;</p>  <p>Thanks!&nbsp;</p>
4107, <p>OK&nbsp;folks it seems that the wget being discussed in the forum could do the continuing trick with the follwing command:&nbsp;</p> <p>wget -c url<br>wget --continue url<br>wget --continue [options] url</p> <p>I am going to give it a shot when the downloading need to be resumed&nbsp;after a black out.&nbsp;</p>
4107, <p>Brilliant benchmark! But there is one thing that I don't understand.&nbsp;</p> <p>Could you perhaps explain why you applied the log transform to all &quot;P values&quot;?&nbsp;</p> <p>[quote=Jamie Ross;71934]</p> <p>The model knows what to do it was specified at the train step.</p> <p>[/quote]</p>
4117, ----
4120, ----
4123, ----
4125, ----
4167, ----
4172, ----
4174, ----
4175, ----
4184, ----
4193, ----
4194, ----
4195, <p>I recommend using a sequential palette from Color Brewer. It helps to convey ordered information such as the per capita internet usage plotted above.</p>
4209, ----
4272, ----
4278, ----
4280, ----
4315, ----
4319, ----
4353, ----
4360, ----
4363, ----
4366, ----
4378, ----
4383, ----
4400, ----
4406, ----
4407, <p>Hello folks</p> <p>I am not able to understand the meaning of&nbsp;last 101 columns in&nbsp;<strong>events.csv </strong>file. &nbsp;<span class="x_GRcorrect">which</span> are as&nbsp;<span>&nbsp;</span><strong>count_1</strong><span>&nbsp;</span><strong>count_2</strong><span> ...&nbsp;</span><strong>count_100</strong><span>&nbsp;</span><strong>count_other.&nbsp;</strong></p> <p><span>&nbsp;</span></p> <p><span>Thanks</span></p>
4407, <p>Thanks john&nbsp;for clearing this. !!</p> <p>&nbsp;</p>
4407, <p>I could'nt understand the data.</p> <p>there are no descriptions how i am suppose to understand it.</p> <p>Can anyone help me understand it. if this is not against the rules.</p>
4407, <p>I couldn't find any column name with target of default and not default details</p> <p>Should i create a column by myself.</p> <p>if a loss value is =0 then person didnt default.</p> <p>and if loss value is&gt;0 person default.</p>
4407, <p>I have a doubt</p> <p>in jan 2010 the product which is sold in 2005 can also come and which is sold in 2008 dec can also come.</p> <p>than how can we understand that which module product we are referring to</p> <p>i mean M1 produced in 2005 or M1 producrd in 2008?</p> <p>Or can u give me some overview so that i can understand the question well.</p> <p>Thanks in advance</p>
4407, <p>Hi:</p>  <p>Could someone please explain me what is  -Model 2: Logistic Regression(scikit). Dataset: Log(X+1) X is dataset then what is X+1 and log(X+1) means log of entire dataset's variable?</p>
4407, <p>Hi:</p>  <p>Could someone please explain me what is  -Model 2: Logistic Regression(scikit). Dataset: Log(X+1) X is dataset then what is X+1 and log(X+1) means log of entire dataset's variable?</p>
4407, <p>Hi All:</p>  <p>Please pardon me if I am asking wrong question:</p>  <p>I am trying to learn ensemble models where we use 2 3 different models and we get some better results then single model. Please suggest me some link or posts or book where I can get a head start I have already tried Scikit ensemble models but I am looking where I can learn it thoroughly and can tweak it according to my need.</p>  <p>It will be greatfull if someone help me over this.</p>  <p>PS: I have already done the google work but not found any satisfactory results.</p>  <p>Thanks!</p>
4407, <p>Thanks!</p>
4413, ----
4427, ----
4438, ----
4447, ----
4450, ----
4451, ----
4453, <p>It is clear that registration is required for the hackathon competition.&nbsp; Is registration required for the 2 month deep dive competition?&nbsp; Or will the deep dive competition be held as other competitions at Kaggle -- a login at Kaggle will suffice?</p> <p>Also are there any restrictions on only the hackathon participants are allowed participate in the 2 month deep dive?</p>
4453, <p>Why does prediction have negative values in submission.csv?&nbsp;</p> <p>Since the prediction is a probability (quoting from &quot;evaluation&quot; page -- The resulting submission format looks like like the following where &quot;pred&quot; represents the predicted probability that the first team will win)  shouldn't the prediction be a value between 0 and 1 (both inclusive)?&nbsp;</p> <p>Just checking</p>
4453, <p>Never mind.&nbsp; The predictions are indeed between 0 and 1 for considered results.</p>
4453, <p>Is there a way to get the scores for all submissions from a team and also see the submissions (like in previous contests)?&nbsp; I am trying to figure out if I missed out submitting/selecting some submissions.</p>
4453, <p>Jeff I asked as the raw data reported only one entry from me.</p>
4453, <p>Hey</p> <p>While the competition is over and the winner's have been announced on Kaggle home page Kaggle displays &quot;Closed - Validating Final Results&quot; next to this competition under the &quot;Result&quot; tab in user's profile.</p> <p>Out of curiosity what's happening?</p> <p>Cheers</p>
4453, <p>I tried running the evaluation script and there is an extra closing parenthesis on line 85 of SantasHelpers_Evaluation_Metric.py and &quot;import math&quot; is missing from the file.</p>
4453, <p>With leakage or evaluation error (w/ kaggle backend) 0 error is achievable.&nbsp; Now if BAYZ have achieved 0 error with neither or even by identifying leakage then kudos to them.</p>
4453, <p>In a different thread Boris from BAYZ team explains what his team did to get 0 error.</p>
4455, ----
4456, 
4456, <p>Dear all</p> <p>I am the author of FeatureHashing .</p> <p>Here is a&nbsp;<a href="https://gist.github.com/wush978/1d8c18b57b6844dd0084">demo code</a> to train the logistic regression with&nbsp;<a href="http://cran.r-project.org/web/packages/glmnet/index.html">glmnet</a> and gradient boosted decision tree with&nbsp;<a href="http://cran.r-project.org/web/packages/xgboost/index.html">gxboost</a> on the <a href="http://data.computational-advertising.org/">ipinyou dataset</a></p> <p>Hope that helps.These example will be included in the documentation of the next patch.</p>
4456, <p>Thanks for your asking.</p>  <p>I correct the link above. But please do not use them directly. Replace the formula and the variable name of the data.frame to adapt to your dataset.</p>  <p>Your question indicates some incomplete explanation of the document.</p> <p>[quote=Hitesh Sharma;63119]</p> <p>A few additional questions. Please pardon my ignorance as I'm a novice.</p> <p>1) It seems that the feature hasher by default returns a transposed matrix. Shouldn't it be transposed again before training so that the rows represent the samples and the columns the new features.</p>  <p>There is a parameter `transpose` to decide whether to transpose the returned matrix or not. The default value is `TRUE` and I am considering whether to change it in the next patch or not.</p>  <p>2) From the sample I'm trying to understand how the features are hashed and mapped.</p> <p><code>&gt; m &lt;- hashed.model.matrix(~ . CO2 2^6 keep.hashing_mapping = TRUE)<br>&gt; mapping &lt;- as.list(attr(m &quot;mapping&quot;))<br></code><code>&gt; mapping &lt;- unlist(as.list(attr(m &quot;mapping&quot;)))<br>&gt; <br>&gt; mapping %% 2^6<br> PlantQn1 PlantQn2 PlantQn3 uptake TypeMississippi Treatmentchilled PlantMn1 PlantMn2 PlantMn3 <br> 33 37 6 48 16 18 22 1 20 <br> PlantQc1 PlantQc2 PlantQc3 Treatmentnonchilled PlantMc1 PlantMc2 PlantMc3 conc TypeQuebec <br> 5 26 45 31 54 41 41 30 6 <br></code></p> <p>From the above my understanding is that value of Qn1 for Plant will be 33 Qn2 be 37 etc. This implies that in the one hot encoded vector of 64 features the 33rd and 37th values will be set to 1.</p> <p><code>Let's look at the CO2 data.</code><code></code><code>&gt; head(CO2)<br>Grouped Data: uptake ~ conc | Plant<br> Plant Type Treatment conc uptake<br>1 Qn1 Quebec nonchilled 95 16.0<br>2 Qn1 Quebec nonchilled 175 30.4<br>3 Qn1 Quebec nonchilled 250 34.8<br>4 Qn1 Quebec nonchilled 350 37.2<br>5 Qn1 Quebec nonchilled 500 35.3<br>6 Qn1 Quebec nonchilled 675 39.2<br></code></p> <p>Comparing the above with the first column of the feature hash I don't see the 33rd element set as 1 (this is what QN1 above was mapped to).</p> <p><code>&gt; dgm = as(m &quot;dgCMatrix&quot;)<br></code></p> <p><code>&gt; dgm[1]<br> [1] 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 95 -1 0 -1 0 0 0 0 0 0 0 0 0 0 0 0 0 0<br>[49] -16 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0<br></code></p> <p>And lastly why are there elements like -16 and 95.</p> <p>It's possible that I'm not just understanding how feature hashing works. So please pardon my ignorance and the questions asked.</p> <p>Thanks.</p> <p>[/quote]</p>  <p>At first I need to address that the implementation uses a binary hashed function to correct the sign of the result. Therefore&nbsp;there are 1 and -1 in the returned matrix. You can read the second formula in here:&nbsp;<a href="http://en.wikipedia.org/wiki/Feature_hashing#Feature_vectorization_using_the_hashing_trick">feature vectorization using the hashing trick</a></p> <p>Second&nbsp;the range of `%% 2^6` is 0 ~ 2^6 - 1 so we need to shift the index by 1 Because the starting index in R is 1</p> <p>The first row of CO2 is:</p> <p>Plant Type Treatment conc uptake<br>1 Qn1 Quebec nonchilled 95 16</p> <p>The mapping of them are:</p> <p>&gt; mapping[&quot;Treatmentnonchilled&quot;] %% 2^6 + 1<br>Treatmentnonchilled<br> 32<br>&gt; mapping[&quot;TypeQuebec&quot;] %% 2^6 + 1<br>TypeQuebec<br> 7<br>&gt; mapping[&quot;PlantQn1&quot;] %% 2^6 + 1<br>PlantQn1<br> 34<br>&gt; mapping[&quot;conc&quot;] %% 2^6 + 1<br>conc<br> 31<br>&gt; mapping[&quot;uptake&quot;] %% 2^6 + 1<br>uptake<br> 49</p>  <p>Therefore&nbsp;</p> <p>&gt; dgm[c(731323449)1]<br>&lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;<br> 1 95 -1 -1 -16</p> <p>Note that the variable conc and uptake are numeric so the value is 95 and -16.</p> <p>Hope that helps.</p>
4459, ----
4466, ----
4467, <p>Ben I'm not sure what you mean by reverse-engineering the test set construction. We don't have access to the full records so no one can ever know how many extra records have been removed. Those extra records could never be reverse-engineered.</p>
4467, <p>[quote=Domcastro;39232]</p> <p>[quote=Jose M.;39143]</p> <p>By the way I'm obtaining a 6879 % of accuracy on the train set using the &quot;last quoted plan benchmark&quot;. This is very different from the accuracy obtained on the test set as reported in the leaderboard. Are you obtaining the same figure?</p> <p>[/quote]</p> <p>This has confused me - I thought the last row was the plan they purchased - wouldn't that give 100% accuracy?</p> <p>[/quote]</p> <p>He most likely meant the&nbsp;last quoted plan before the purchase point.</p>
4467, <p>The Leaderboard page states that the rankings are currently based on approximately on 30% of the test data. The final ranking will be determined by the other 70%. So we already have access to all of the test data they will use but we don't know exactly which rows. This is obviously so we cannot try to overfit our submissions by using the leaderboard scoring.</p>
4467, <p>[quote=Hubert Wassner;39611]</p> <p>ok so should we consider the cost as an input or an output of the prediction ?</p> <p>[/quote]</p> <p>Well I know I consider every field we are given to be input. Any information given before the final shopping point is potentially useful.</p>
4467, <p>It really doesn't matter if any of this demographic data is accurate for our purposes. If people lied / made mistakes in these training records then they will lie / make mistakes in real world examples as well. If your analysis can identify differences between the buying behaviors of people who change demographic data that is great otherwise you can just treat it as noise.</p> <p>Just act the same way you would if your predictions were being done in real time. If someone has changed their age in the last 3 different shopping points how would you use that age in your predictions? If some gave the same age 5 times and now changed it how would you use that age in your predictions? Maybe you will ignore it maybe you will use the most recent maybe you will use the average (or more likely the mode). I suggest using multiple&nbsp;techniques and use&nbsp;the one that provides the most accurate results for you.</p> <p>Personally I just treat the most recent answer as the &quot;correct&quot; one for right now but as I run out of low hanging fruit to analyze I will probably&nbsp;start looking into whether those changes can help my predictions. I may end up throwing out any field that has changed from my decision trees if I find out it creates too much noise but I haven't gotten that far yet.</p>
4467, <p>For a situation where options change so infrequently a set of percentages would also probably provide the best&nbsp;information. It may be useful to know that Group A has a 33% chance of changing from 0 to 1 if the standard chance is 5% even though you still think it will most likely stay at 0. My current implementation attempts to calculate this percentage but because of this scoring it is only useful when a specific change crosses 50% (which it hasn't yet but I'm just getting started).</p> <p>But the reality is that we have no idea why these predictions are useful to the client.&nbsp;Maybe predicting a change is only financially beneficial if they know every future group option. My bet would be that this method is just easier to score though.</p>
4467, <p>I am not using the weight at all in the early part of my work. My only goal is to give high target scores to as many of the fire loss policies as I can. The weight factor will make guessing highly weighted policies more important than lower weighted factors but to be honest my algorithms are trying their best to analyze every record anyway. Its not like I am spending effort to devote more computing resources to highly weighted policies. Perhaps later I will find a way to game the weights to slightly alter my target scores but I doubt it.</p> <p>I should say that I am looking at the weights as a possible predictive feature though. Realize there is the chance the criteria Liberty Mutual is using to create the weight could have predictive power.</p>
4467, <p>The value &quot;Z&quot; could be any value.&nbsp;It is a true null. It doesn't necessarily mean that the value is not applicable just that the value is missing. Unless an admin wants to give you more information that is all the competition provides as an explanation.</p> <p>It could still potentially have predictive power though. Imagine a situation where someone who is less diligent about filling out all fields on a policy application is also less diligent about home maintenance that could lead to a fire. But then again it could just be NULL because of poor data translation or multiple data sources that don't all have the same attributes.</p>
4467, <p>The Data section claims var4 is a nominal variable meaning they are not ordered. It is likely they are grouped in some way considering their choice of using values like C1D1D2 instead of simply 123... but that doesn't mean the numeric part implies an order.</p> <p>Although just because Kaggle claims these are nominal variables you are under no obligation to believe them. You could try to determine if&nbsp;a trend can be found within some of the&nbsp;subgroups. You could also try grouping all values by their letter prefix and see if that provides more predictive power than using them separately. The latter example could still be helpful even if they are nominal which the former technique would imply the variable is at least partially ordinal.</p>
4467, <p>Notice that it says the letter is a higher level and the number is the lower level not that there is a ranking between the letters and numbers. So there is no implied rank between A and D or between 1 and 6.</p>
4467, <p>I hope that an admin can address some of the concerns raised in this thread. I really want to&nbsp;compete in this project but also feel that a team large enough to do significant hand labeling will have a real advantage. That advantage is still there even if&nbsp;a new set of test data is published closer to the submission deadline because hand labeling can now provide up to 130k more training images (over four times more than the standard training set).</p> <p>While I think people are probably underestimating the time it would take for amateurs to hand label 130k images into 121 categories (over 2000 hours even at 1 per minute) just hand labeling certain category groupings or at least ruling out large groups of categories could provide quite an advantage.</p> <p>Computer&nbsp;assisted hand labeling and then over-fitting seems to be the way to win this competition but I would&nbsp;love for someone to be able to convince me otherwise. I simply don't have the manpower to be the team who has the most time to hand label.</p>
4467, <p>Use the train data when training your model. It will have the revenue field. You should probably split the training data into sets you use for training and sets you use for testing your own models.</p>  <p>The test data is what you use to create your predictions so only Kaggle knows the values.</p>
4470, ----
4471, ----
4477, ----
4481, <p>Hello</p> <p>I have followed the submission format provided on corresponding ChaLearn page yet this error message keeps coming up! Of course I did not type in the scores manually so there appears to be no reason why that particular record would stand out in any way.</p> <p>What could be the issue here?</p>
4487, ----
4488, ----
4493, <p>Hi</p> <p>I had similar of making 2 submissions but not selecting for final scoring. I have written a post requesting the admins to select the submission in case you have made 1 or 2 selections since you can anyways select a maximum of 2 selections.</p> <p>Please pitch in you support if you agree&nbsp;</p> <p>Link:&nbsp;http://www.kaggle.com/c/march-machine-learning-mania/forums/t/7467/request-to-admins-missed-final-selection-for-scoring</p>
4493, <p>Dear Admins</p> <p>I had made 2 submissions a few hours before the deadline. But the next morning when I woke up I saw an email form the admin asking me to select my submission for scoring. Only then I realized that the submission should be selected (clicked in check box).</p> <p>Since a person can have a maximum of 2 selections for final scoring I was under the impression that the 2 submission of mine will be scored. I thought only if you made 3 or more submissions you nee to select 2 for final scoring. I admit that this was a mistake on my part.&nbsp;However having spent the last few months on the March Mania problem it is a shame that my submission will not be scored.&nbsp;</p> <p>I think there would be other participants like me who might have missed selecting the submission. In order to help our cause <strong>without breaking the rules</strong> of the competition I kindly request the admins to:</p> <p>&quot;<strong><em>If the participant has made only 2 or only 1 submissions the submissions can be score because as per the rules a maximum of two selection is allowed per person</em></strong>.&quot;</p> <p>I believe that this will be fair because in this case the participant was intending to submit more than 2 submissions anyways. Only if the participant made more tan 2 submission he/she would have to select this best 2 for final scoring.&nbsp;</p> <p>Regards</p> <p>Nilos</p>
4493, <p>This was one of my fav competitions and I would love to be back again next year. I had no knowledge of basketball before this competition but now in the hind sight I think I have found where my model gave me a big loss and would love to rectify them next year. The bigger picture would be to expand the model to predict results of different sports.</p> <p>And yeah I would love to see 2nd and 3rd prize too.</p>
4493, <p>[quote=Dr. Pain;40900]</p> <p>It will be interesting to see how this plays out over the remaining games but this suggests that being at the top of the leaderboard (at this point) has a large random element.</p> <p>[/quote]</p> <p>On the contrary I think breaking into the top of the leader-board especially the top 6 is going to very difficult. If I look at the last five score refreshes the worst position of someone in the current top 6 was position 11 which was three refresh back. If I look at the last two refreshes the worst position of someone in the current top 6 was position no. 9. So that the top positions have kind of stabilized.</p>
4493, <p>I have 2 questions:</p> <p>1. Will the sample submission file be updated for this year on day 132 once the 68 teams for this years tournament are announced?</p> <p>2. Is 14th March the last date for submission of our entries stage 1 or stage 2 of the competition?</p>
4493, <p>Hi Jeff</p> <p>Would you be re-uploading the mapping of the team names and the team id or is the mapping between team names and team id remaining unchanged?</p> <p>Regards</p> <p>N</p>
4493, <p>Hi Jeff</p> <p>You have uploaded the 2015 files in the data section. Wanted to check with you if these are any different from the ones you have uploaded in this thread because I have downloaded the 2015 files from this thread and started working on my final prediction. Let me know if there is any change between the 2015 files you have uploaded here and the same files you have uploaded in the data section.</p>  <p>By the way I really appreciate the good job you have done throughout the competition :)</p> <p>Regards</p> <p>N</p>
4493, <p>Last year the winning score was 0.529 by&nbsp;One shining MGF. This year I scored exactly 0.529 and I am not even in the top 200 :-). This year the winning score happens to be 0.438</p> <p>I am curious to know what led to this significant improvement over least year. Are we understanding basketball better and/or are we betting better.</p>
4493, <p>I am looking for a suitable team mate too</p> <p>My email id: nilotpalsinha@gmail.com</p>
4493, <p>I am looking to partner too. My current leaderboard position is 460/916 and would like to go higher by collaborating with a suitable partner:)</p> <p>email id: nilotpal.sinha@gmail.com</p>
4493, <p>@Ekrem</p> <p>I understand that our task is to predict the annual revenue. So the revenue given in the training data set must be some transformation of the annual revenue of the training restaurants. Or is it the transformed revenue of the restaurants since its opening date?</p> <p>Can you please clarify this?</p>
4493, <p>@Admin</p> <p>Are you there? Can you&nbsp;address the above questions?</p>
4493, <p>I stand with Jose.</p> <p>What is the procedure to impeach the competition admin? This is torture !!!! Any human rights activists around.</p>
4493, <p>@Wendy Kan</p> <p>Seriously!!!!!! How can you answer so early? I was hoping that you would answer 5 minutes before the competition deadline :(</p>
4493, <p>I am looking to team up .... always competed as an individual till date.  I am Ex Data Scientist :)</p>
4493, <p>@Garrett - Lets do it</p>
4493, <p>Very sad to hear this.. May god give enough strength to his family to take this pain.. RIP.. Regards</p>
4493, <p>Hello</p> <p>Need support from the system administrator as I am getting submission errors. I have used the same program and have made 10 submissions without any error. No change has been made in the program  In fact I have tried using the sample submission format as well.. That too is giving an error. Pl. help</p>  <p>Regards</p>  <p>Anjana</p>
4493, <p>Thanks for your revert.. What should be the format...</p>
4493, <p>Thanks... I am able to resolve the problem...</p>
4493, <p>Hello</p> <p>Interesting post by Andy &quot;First shot: Sentiment Analysis in R&quot;..&nbsp; You can follow the same steps and instead of &quot;Na&#239;ve Bayes&quot; try with &quot;rpart&quot;  using &quot;neg&quot; and &quot;vPos&quot; to get a LB score of .66772.</p>
4493, <p>Oh.. Yes... Typical characteristic of late entrants :-)&nbsp;</p>
4493, <p>Hello..&nbsp; Competition is over.. Sharing some tricks which worked for me and would appreciate views/feedback.</p> <p>I am sure most of us in this competition would have realized that &quot;weather.csv&quot; file is very important from prediction perspective and hence there is nothing new which you will find in the script. However what really worked for me was &quot;Data Exploration&quot;.</p> <p>Brief on the variables which helped me to end at &quot;64th&quot; rank in this competition. Some of the values which were considered for variable creation&nbsp;are based on the open literature available .</p> <p>&nbsp;DewPoint:Sunrise (Interaction Variable) -&#224; Both the fields are available in &#8220;weather.csv&#8221; file.</p> <p>CodeSum_New &nbsp;(Type &#8211; Binary) &nbsp;----- &gt; In &nbsp;&#8220;weather.csv&#8221; file If CodeSum&nbsp; field has &#8220;RA&#8221;&nbsp; code then that day is marked as&#8221;1&#8221; in both test and train data set. &nbsp;This field was created considering the fact that mosquitoes are active on a dry day.</p> <p>Tmax_Tmin (Type &#8211; Binary)&nbsp; ----- &gt; Difference is calculated between&nbsp; two fields i.e. &#8220;Tmax&#8221; and &#8220;Tmin&#8221; from &#8220;weather.csv&#8221; file . If the difference is between 21-41  then that day is marked as &#8220;1&#8221; in both test and train data set.</p> <p>Sunset (Type &#8211; Binary)&nbsp; ---- &gt;&nbsp; If &#8220;Sunset&#8221; time in &#8220;weather.csv&#8221; file is beyond 16:47 on a specific day that day is marked as &#8220;1&#8221; in both test and train data set.</p> <p>Tmax_Tmin:Sunset (Interaction Variable)</p> <p>Dry (Type &#8211; Binary)&nbsp;&nbsp;&nbsp; --&#224; If &#8220;DewPoint&#8221; in &#8220;weather.csv&#8221; file is less than 53 then that day is marked as &#8220;1&#8221; in both train and test data set.</p> <p>&nbsp;Dry:Sunset (Interaction Variable)</p> <p>Dry_Wet_Diff (Type &#8211; Binary)&nbsp;&nbsp;&nbsp; --&#224; Difference is calculated between &#8220;DewPoint&#8221; and &#8220;WetBulb&#8221; fields available in &#8220;weather.csv&#8221; file. If a difference is greater than &#8220;5&#8221; than that day is marked as &#8220;1&#8221; both in test and train data set.</p> <p>Dry: Dry_Wet_Diff (Interaction Variable)</p> <p>Dry:Tmax_Tmin(Interaction Variable)</p> <p>Hot (Type &#8211; Binary)&nbsp;&nbsp;&nbsp; --&#224; if value of &#8220;Tmax&#8221; is greater than 40 then that day is marked as &#8220;1&#8221; in both  test and train data set.<br>Dewpoint AvgSpeedTmax -&#224; Fieldsfrom &#8220;weather.csv&#8221; file.</p> <p>Day (Type &#8211; Numeric )&nbsp;&nbsp;&nbsp; -&#224; &#8220;Day&#8221; extracted from the Date field.</p> <p>Month (Type-Numeric) -&#224; &#8220;Month&#8221; extracted from the Date field.</p> <p>NumMosquitos WnvDOYPresent week dayofyear Species2 &#224; Fields created in the code</p> <p>Result_Speed (Type &#8211; Binary)&nbsp;&nbsp;&nbsp; -&#224; All the days where &nbsp;&#8220;ResultSpeed&#8221; value is less than &#8220;11&#8221; in &#8220;weather.csv&#8221; are marked as &#8220;1&#8221; in both test and train data set.</p> <p>Win_Speed(Type &#8211; Binary)&nbsp;&nbsp;&nbsp; &#224; All the days where&nbsp; &#8220;AvgSpeed&#8221; value is greater than &#8220;10&#8221; in &#8220;weather.csv&#8221; are marked as &#8220;1&#8221; in both test and train data set.</p> <p>Street_Effect (Type &#8211; Binary)&nbsp;&nbsp;&nbsp;&nbsp; &#224; Train data set was filtered for WnvPresent =1. Any street which has appeared more than once in this filtered data set was considered as the risky street and the same was marked as &nbsp;&#8220;1&#8221; in both test and train data set.</p> <p>spray_effect (Type &#8211; Binary)&nbsp;&nbsp;&nbsp; &nbsp;-&gt; &#8220;spray.csv&#8221; file has data for the year 2011 and 2013. As per the information available on internet &#8220;Culex&#8221; virus is generally active for 14 days. So if a spray has been done on a specific day as per the data available in &#8220;spray.csv&#8221; file then the effect was considered for +14 days from that specific date and these days were marked as &#8220;0&#8221; in the train data set. No data is available for spray for the years available in test data set. However as spray file has data for 2011 it was assumed that the concerned agency started the spray work from 2011 and hence the similar days were assumed for 2012 and 2014 for creation of this field in the test data set.</p> <p>Risk (Type &#8211; Binary ) &#224; As per the literature available  peak season for this virus growth is from last week of July till first week of September. If spray is not done in this period (i.e. spray_effect=0) and if a day as &#8220;Dry&#8221; i.e. Dry=1 then the risk of virus is high. All such days were marked as &#8220;1&#8221; in both train and test data set.</p> <p>Month_Cat (Type &#8211; Binary ) -&gt; Any period before June i.e. Month &lt;7 or beyond September i.e. Month &gt;9  when the chances Wnv presence is minimal is categorized as &#8220;1&#8221; and rest all other months in the given data set were categorized as &#8220;0&#8221;.<br>Longitude Latitude -- &gt; Fields available in the train and test file.<br>&nbsp;</p>
4493, <p>Hi..</p> <p>&nbsp;For variables like Tmax_Tmin and Sunset &quot;1&quot; was used as a proxy for treating &quot;season&quot; in a different way however for some other variables like Dry Hot it was more to indicate the possibility of a mosquito is high as it was mentioned that mosquito is more active in hotdry&nbsp; weather.</p> <p>Regarding your views on  if it an effective method or not even I had the similar views  when I started working on this competition using various algorithms/models etc however the same was not giving good score on LB... Hence thought of going back to basics and exploring the data..</p>
4493, <p>Instead of xrange  use &quot;range&quot; as the same is not defined from Python 3.x onwards...</p>
4493, <p>Hello</p> <p>As per the syntax answer is &quot;yes&quot; however I am not sure if range is more memory efficient or not.. Regards</p>
4493, <p>Hello</p>  <p>I am not able to submit the file.. Getting error that first column should be &quot;String&quot;.. How do I make first column as &quot;String&quot; in excel.</p> <p>Earlier was getting an SYLK error.. Changed the first column from ID to Id..</p> <p>Need help pl.</p>
4493, <p>I have tried the text function as well .. Text(A2&quot;0&quot;).. Still the same error.. Need help from competition admin pl.</p>
4493, <p>Yes pl. Thanks</p>
4493, 
4493, <p>Hello </p>  <p>I have merged the train data with other files  taking at least one common field into consideration  remove the fields with lot of categories update NA with &quot;0&quot; .</p>  <p>Now if I run RF (even with ntree=20) or NN with &quot;no hidden layer&quot;  execution just goes for ever.. </p>  <p>I have tried classification tree.. But the result is not good on LB..</p>  <p>Any suggestion/trick to handle this problem pl..</p>
4493, <p>Hi WhizWilde -- Thanks for your suggestion on cluster.. I will definitely try that ...</p>  <p>Hello Andrew -When I try to execute the same script on my machine it takes around 45-60 minutes.. Not sure about the configuration of the machine where it takes less than 10 minutes .. I have a 8GB m/c... </p>  <p>Hi Signipinnis - :-).. Believe me yesterday night I did the same.. However my m/c was not very happy with the idea that it is expected to work while others are sleeping and within an hour it just hanged :-)... </p>
4495, <p>exciting analysis thank you!</p>
4495, <p>When looking at the data it appears some features has extremely large values.</p> <p>For example column 617 has min value&nbsp;2792379252744499744573751296 and max value&nbsp;73014636354119001351722496950272. i am wondering what meaning could this column possibly mean.</p> <p>Hopefully this is not some registration&nbsp;number</p>
4495, <p>Thanks for sharing the code ! I am sure we'll learn a lot from it.</p> <p>I am new to this field and I am wondering if anyone is willing to share how they approached feature selection. It seems rather difficult to select some of the features mentioned above using a brute force approach such as&nbsp;(f274-f528)/(f528-f527+1).</p>
4520, ----
4521, ----
4526, ----
4554, ----
4567, ----
4568, ----
4571, <p>I have 2 related questions:</p> <p>1) How can i see the post-competition private/public leaderboard for the submissions sent after the deadline in a given kaggle competition?<br><br> 2) In this specific competition would some company be interested in a say 0.347 solution?. <br>If you count the bits needed to go from the 0.45 trivial &quot;benchmark&quot; solution down to 0.379 winners solution and then count the supplementary bits needed to go to 0.347 you will know that this is not a rehash/optimization of current ideas floating in the field there is a too big difference and also you can't expect to go down to near 0 in this field.<br> Also data is quite &quot;doctored&quot; in this competition's day 31 comparing with previous days ((on top of the already known holiday) in the following sense: probability of (assuming same random sampling) of day 31 happening is below 2^-5000 while between other consecutive days is above 0.01 in a reasonable optimal model) so one expects better numbers in production &quot;un-doctored&quot; data between train/test&nbsp; periods.<br>Or maybe the organizers should say at end of competition something like &quot;our best algorithm already gets 0.xxx on this combination of train/test data&quot;.</p>
4571, <p>Does Kaggle allow me to look/show to others the &quot;after the deadline&quot; leaderboard for a competition? Also why did Kaggle ended 2 most valuable competitions in same day same hour i am asking beacause i was &quot;busy&quot; with the datascience bowl competition but i would have appreciated even something as short 12 hours to do a reasonable code here. I even submitted a default solution hoping to have time to compete here during a long CNN train but unfortunately i didn't have a chance because i was overseeing &gt; 10 computers. Didn't they explain to the data owners that ending &quot;same hour&quot; with another big one reduces their chances to get all &quot;available&quot; efforts?</p>
4571, <p>The Points are missing from profile ? Only the ranking is shown now.</p>
4571, <p>Yes quite generous percentages in low participation competitions. Whats wrong with keeping the same % across all?</p>
4571, <p>Isn't today just the Merger and 1st submission deadline like the nice progress bar in the top of page shows? Why would they need the model for first submission they need the model for the last submission the best one right ? Why would i merge teams today if its the last day? The real deadline its 8.3 days from now or at least that's what it writes in my browser. I mean until now 1st submission deadline you just had to have 1 submission (even the default one ) to be still in the game for the last week the most heated week of the competition right? Or is different for this one? Also it was somewhere in the forum that you can upload model in the last 2 weeks so it seems to me that there is still a week left? It will be nice to clarify this now if today its the last day because there is no point to continue work tomorrow if the competition finnishes today. ( and in fact all my work until now will be pretty pointless because i can't finnish it until 1st Submission deadline) In my opinion the correct approach would be to have Merger and 1st submission deadline today then next week the teams can improve models with validation and after the deadline the private Leaderboard calculation with test data.</p>
4571, <p>I think they made a mistake somehow they are ending this on the &quot;Merger and 1st submission&quot; day not letting the newly formed teams to compete with joined forces after the Merger and until Deadline. But they could correct it: on the timeline page which i read just now it says &quot;The organizers reserve the right to update the contest timeline if they deem it necessary&quot;. So they could make the appropriate changes. Especially in this long competition i guess they wouldn't want to end it in a rush and let the teams obtain the most improved and useful results.</p>
4571, <p>Well not a problem there will be more competitions. I was only folowing the timeline progress bar  from the top of the forum or leaderboard pages and it says only &quot;Merger and 1st submission&quot; and  &quot;Deadline for new entry &amp; team mergers&quot; which normally means competition doesn't end on the first submission or on new entry. </p>
4571, <p>You can obtain almost full map of hotels and cities exactly to better than 3.2 meter accuracy without needing any extra data and just mantain arbitrary lat long up to some unknown rotation to the real  world lat long.  Obtain also the overlapping relations between the destinations because you can match the hotels by coords. Earth radius deducted from grid search: 3963.0000 miles (strange choice??) Computations was started initially from all the destinations who have  maximum ONE length of distance from any city. These are candidate for being &quot;single hotels&quot; top 5 by number of cities pointed by:</p>  <p>dest 43062 htmax= 1 ndist= 111 ncty= 111 nhit= 246</p>  <p>dest 3789 htmax= 1 ndist= 137 ncty= 137 nhit= 287</p>  <p>dest 3470 htmax= 1 ndist= 121 ncty= 121 nhit= 309</p>  <p>dest 25174 htmax= 1 ndist= 146 ncty= 146 nhit= 310</p>  <p>dest 9703 htmax= 1 ndist= 72 ncty= 72 nhit= 600</p>  <p>dest 21063 htmax= 1 ndist= 213 ncty= 213 nhit= 617</p>  <p>dest 33865 htmax= 1 ndist= 115 ncty= 115 nhit= 759</p>  <p>dest 48752 htmax= 1 ndist= 279 ncty= 279 nhit= 826</p>  <p>The destinations (single hotels) i started with were :</p>  <p>h1=25174</p>  <p>h2=33865</p>  <p>h3=48752</p>  <p>Then pick the cities c1c2c3 that have the following distance matrix  to the above 3 hotels</p>  <p>e(c1h1)= 1371.0242 e(c1h2)= 1680.7709 e(c1h3)= 1022.7044</p>  <p>e(c2h1)=  710.2743 e(c2h2)= 1767.2299 e(c2h3)= 1446.413</p>  <p>e(c3h1)= 2140.0445 e(c3h2)= 1084.3085 e(c3h3)= 1768.1506</p>  <p>(with e(ab) i note the spherical distance on the sphere in miles)</p>  <p>knowing radius R (you will do gridsearch later to the 3963 value) you obtain a nonlinear system that has at most 16 full real solutions  (and many more complex) for a 3x3 distance matrix but there are 4 symmetries so there are at most 4 distinct full real solutions <br> but usually only at most 2-3 converge to 100 decimals (i used mpfr). matching with full tethraedrons to other city and hotels you select the correct  base solution with 3 further single hotels h4h5h6 which have each 3 distances  with c1c2c3 and six further cities c4c5c6c7c8c9 which have each  3 distances with h1h2h3 so you obtain accurate to 4 decimals:</p>  <p>e(c1c2)= 803.595455 e(c1c3)= 883.753673 e(c2c3)= 1452.09340</p>  <p>e(h1h2)= 2441.36694 e(h1h3)= 1561.38509 e(h2h3)= 2697.01204</p>  <p>choose h4 to have known</p>  <p>e(c1h4)  1375.3989</p>  <p>e(c2h4)   710.6864</p>  <p>e(c3h4)  2142.2537</p>  <p>choose h5 to have known</p>  <p>e(c1h5)  1036.4751</p>  <p>e(c2h5)  1473.8628</p>  <p>e(c3h5)  1770.4498</p>  <p>choose h6 to have known</p>  <p>e(c1h6)  1550.5012</p>  <p>e(c2h6)  1740.3875</p>  <p>e(c3h6)  2358.7705</p>  <p>choose c4 to have known</p>  <p>e(c4h1)     2.1402</p>  <p>e(c4h2)  2443.3882</p>  <p>e(c4h3)  1562.3011</p>  <p>choose c5 to have known</p>  <p>e(c5h1)  340.5342</p>  <p>e(c5h2)  2108.4495</p>  <p>e(c5h3)  1626.4935</p>  <p>choose c6 to have known</p>  <p>e(c6h1)  2450.4404</p>  <p>e(c6h2)   914.0866</p>  <p>e(c6h3)  2118.8973</p>  <p>choose c7 to have known</p>  <p>e(c7h1)  2022.6995</p>  <p>e(c7h2)   627.9322</p>  <p>e(c7h3)  2653.9268</p>  <p>choose c8 to have known</p>  <p>e(c8h1)   329.1878</p>  <p>e(c8h2)  2331.2786</p>  <p>e(c8h3)  1856.1279</p>  <p>choose c9 to have known</p>  <p>e(c9h1)  2406.3413</p>  <p>e(c9h2)   161.6393</p>  <p>e(c9h3)  2764.0730</p>  <p>Tetrahedron systems have only 2 solutions you start with 3 known points and add one which has 3 distances known to them and then select the correct solution closest to be on the sphere So you now have a solid base of 9 cities and 6 hotels </p>  <p>Now its time to do the gridsearch for earth radius R by minimizing the error of the extra 12 distances between the 6 hotels and 9 cities  not described above (c4-c9 known distances to h4-h6) and also minimizing their distance to the sphere. I just steped 0.0001 from 3950 to 3970 miles for a simple gridsearch.</p>  <p>you can conquer the rest of the world by doing thetraedrons at first and even 2 distances at latest stage and selecting based on destination center. But its nontrivial you will have to mantain a database of edges (&gt; 12 million) and some structuring so you don't do full  exhaustive 3 edge search you will get &gt; 2 million hotels (nonduplicated) Why i am not in TOP 3? oh i started my code 3 days ago and i just needed few more hours (after the deadline) Well that explains the 66% bookings from the problem once you know the exact hotel from the booking and its exact cluster history</p>
4571, <p>Kevin what is your exact error? Which DLL is missing? </p>  <p>For example if VCOMP120.DLL is missing try this link for <a href="https://www.microsoft.com/en-us/download/details.aspx?id=40784">Visual C++ Redistributable Packages for Visual Studio 2013</a></p>  <p>Or reduce the version number if you need VS2012 or VS2010.</p>
4571, <p>Wow! That's really a lot of models! Can't wait for this bit from the main competition page to become a reality: &quot;<strong>The winning models will be open sourced.</strong>&quot; :)</p>  <p>Congratulations!</p>
4594, ----
4603, ----
4650, ----
4657, ----
4676, ----
4699, ----
4704, ----
4729, <p>[quote=Ben Hamner;127378]</p>  <p>On the side I've been playing with two frameworks for comparing ranking systems</p>  <ul> <li>Predicting a users normalized ranking for the next competition (0=first 1=last)</li> <li>Predicting the probability a user will win the next competition</li> </ul>  <p>I don't have anything useful to share on that now but I think those are potentially interesting avenues of investigation.</p>  <p>[/quote]</p>  <p>This would be very interesting information to have regardless of weather it's used for a ranking system. Other useful information would be comparison of users based on their different skill sets: vision problem geeks vs. NLP stars rankers vs. classifiers vs. regressors etc. </p>
4729, <p>[quote=Kele XU;102976]</p>  <p>You can reduce the eta to about 0.01 then run about 6000 nrounds. It's quite easy to get about 0.96840 for single model.</p>  <p>[/quote]</p>  <p>I've tried this and am still getting results in the 0.9682x range. Any suggestion on what a good choice for other parameters might be or at least how to tune them on our own?</p>
4729, <p>[quote=Radu Stoicescu;117709]</p>  <p>There are only 274 sets and 60 days of competition manually solving 4-5 sets a day is more than possible it's inviting.</p>  <p>[/quote]</p>  <p>Or we could all join a single team massively parallelize the job and be done with the whole thing in an hour.  ;) I'd be happy with my share of $500. :D</p>
4746, ----
4751, ----
4754, ----
4758, ----
4770, ----
4775, ----
4794, ----
4813, ----
4816, ----
4818, ----
4821, ----
4837, ----
4852, ----
4853, <p>How can we make sure that the results are reproducible even if we set the constant seeds ?</p>  <p><a href="https://www.kaggle.com/wiki/WinningModelDocumentationTemplate">https://www.kaggle.com/wiki/WinningModelDocumentationTemplate</a> Results are exactly reproducible. Set seeds to any random number generators to constant values along with anything else necessary to exactly reproduce results and make sure these are saved in your code.</p>
4853, <p>William  Based on your explanation of the flow for the second stage Could you please clarify the following questions</p>  <p>Q1 - Will you release the new test set and the validation set answers after the deadline on Monday ? Q2 - In the case of using the validation dataset to re-train the model(s) if we are not allowed to change the workflow or the parameters of the model(s) in stage 1  how we can/should generate two or more different submission  during the stage 2                                   a - without changing either the workflow of the data pre-processing or                                    b - changing/updating (optimizing)  the hyper-parameters of the model(s) submitted <br>                                          in the first stage</p>  <p>Thanks </p>
4857, ----
4862, ----
4879, ----
4899, ----
4901, ----
4916, ----
4938, ----
4963, ----
4973, ----
4982, ----
4986, ----
4996, ----
5024, ----
5033, ----
5043, ----
5056, ----
5078, ----
5080, ----
5081, ----
5100, ----
5120, ----
5140, ----
5145, ----
5169, ----
5174, <p>+1 forget about deadline and fail to submit. If those who provided benchmark submission or rather all-zeros submission when competition starts can take part but me not I think such deadline is not good idea.</p>
5183, ----
5190, ----
5200, ----
5208, ----
5229, <p>HI How can you combine xgboost models to improve on AUC score? I'm looking for an example in python.</p>  <p>Thank you</p>
5229, <p>It does on my 16GB ram desktop (I think 8GB is the minimal for this competition)</p>
5229, <p>Looking forward to learn from the leading &amp; winning solutions </p>  <p>thanks for a great competition</p>  <p>(last job is still running - check it after competition is over :-) )</p>
5229, <p>congratulations to the winners !!!</p>  <p>It has been a great competition a great challenge and even greater opportunity (at least for me) to learn: 1) image processing and preparation 2) deep learning using theano &amp; lasagne (could not get neon code to work...) 3) truly cant wait to read both methodology of the winners and perhaps take a pick at some code...</p>  <p>please share your solutions</p>  <p>thanks a lot for competition admins &amp; everyone else who shared !</p>
5229, <p>As to your question what is the field customers meaning - I assume that it relates to the number of buying customers within the store on the given date</p>
5229, <p>Your eta is probably too high so your model is not converging try using a lower value</p>
5229, <p>I think that the reason for the better performance of the validation vs. Train score is the sensitivity of the scoring function to extream values as the odds of getting more of those extreams go larger with the sample size</p>
5229, <p>I agree in previous competitions with big number of click-&amp;-submiters of overfitted results there were rank changes of more than 250 places on public lb relativly to private lb.</p>
5229, <p>I agree with inversion. This is not just a time series model as there are external regressors that must be taken into account while forming the model. Don't know which articles were you reffering to but i assume that those articles refffered to classic time series models like phisical measures and not economic parameters that are effected by parameters other than time.</p>
5229, <p>Funny i use these as predictors in my day job as forecasting  maneger still dod not think of adding them to my model :-)</p>
5229, <p>I agree the RMSPE function does favor lower predictions over higher ones. I also took this approach deliberately with some improvement measured.</p>
5229, <p>Fantastic ! so simple so elegant and apparently very robust solution!</p>  <p>Congrats  Bohdan!  very well done and surely deserved the master tier</p>
5237, ----
5255, ----
5275, ----
5296, ----
5303, ----
5313, ----
5342, ----
