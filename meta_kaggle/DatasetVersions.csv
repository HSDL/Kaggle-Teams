"Id","DatasetId","VersionNumber","Name","Slug","Overview","Description","CreationDate","VersionNotes","ArchiveFileName","ArchiveFileSizeBytes","NumVotes"
6,6,1,"2013 American Community Survey","2013-american-community-survey","Find insights in the 2013 American Community Survey","The [American Community Survey](http://www.census.gov/programs-surveys/acs/) is an ongoing survey from the US Census Bureau. In this survey, approximately 3.5 million households per year are asked detailed questions about who they are and how they live. Many [topics](http://www.census.gov/programs-surveys/acs/guidance/subjects.html) are covered, including ancestry, education, work, transportation, internet use, and residency.

The responses reveal a fascinating, granular snapshot into the lives of many Americans.

[![Number of Households Surveyed](https://www.kaggle.io/svf/30288/6679caba9ecb435abbb518d96ca1fcd3/number_of_households_surveyed.png)](https://www.kaggle.com/benhamner/2013-american-community-survey/number-of-households-surveyed-map)

We''re publishing this data on scripts to make it easy for you to explore this rich dataset, share your work, and collaborate with other data scientists. No data download or local environment needed! We''ve also added shapefiles to simplify publishing maps.

What surprising insights can you find in this data? We look forward to seeing and sharing what you discover on scripts!

## Data Description

Here''s a [data dictionary](http://www2.census.gov/programs-surveys/acs/tech_docs/pums/data_dict/PUMSDataDict13.txt).

There are two types of survey data provided, **housing** and **population**.

For the housing data, each row is a housing unit, and the characteristics are properties like rented vs. owned, age of home, etc.

For the population data, each row is a person and the characteristics are properties like age, gender, whether they work, method/length of commute, etc.

Each data set is divided in two pieces, ""a"" and ""b"" (where ""a"" contains states 1 to 25 and ""b"" contains states 26 to 50).

**Both data sets have *weights* associated with them.** Weights are included to account for the fact that individuals are not sampled with equal probably (people who have a greater chance of being sampled have a lower weight to reflect this).

 - Weight variable for the housing data: WGTP
 - Weight variable for the population data: PWGTP

In Kaggle Scripts, these files can be accessed at:

 - `../input/pums/ss13husa.csv` (housing, a)
 - `../input/pums/ss13husb.csv` (housing, b)
 - `../input/pums/ss13pusa.csv` (population, a)
 - `../input/pums/ss13pusb.csv` (population, b)

You can download the data from the census website:

 - [housing](http://www2.census.gov/acs2013_1yr/pums/csv_hus.zip)
 - [population](http://www2.census.gov/acs2013_1yr/pums/csv_pus.zip)

In scripts, they are accessed at:

 - `../input/shapefiles/pums/tl_2013_[state]_puma10.[extension].`

The shapefiles can also be downloaded [here](ftp://ftp2.census.gov/geo/tiger/TIGER2013/PUMA/).

[<img style=""float: left; height:4em; margin-right:1em;"" src=""https://kaggle2.blob.core.windows.net/competitions/kaggle/3333/media/datacamp_logo2016.png"">](https://www.datacamp.com/courses/data-exploration-with-kaggle-scripts) DataCamp and Kaggle have teamed up to bring you the basics of [Data Exploration With Kaggle Scripts](https://www.datacamp.com/courses/data-exploration-with-kaggle-scripts). Take the free, interactive course [here](https://www.datacamp.com/courses/data-exploration-with-kaggle-scripts) and start building your data science portfolio.","2015-07-18 00:51:12","Initial Release","2013-american-community-survey.zip",906423713,51
7,7,1,"May 2015 Reddit Comments","reddit-comments-may-2015","Get personal with a dataset of comments from May 2015","Recently Reddit released [an enormous dataset](https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/) containing all ~1.7 billion of their publicly available comments. The full dataset is an unwieldy 1+ terabyte uncompressed, so we've decided to host a small portion of the comments here for Kagglers to explore. (You don't even need to leave your browser!)

You can find all the comments from May 2015 on scripts for your natural language processing pleasure. What had redditors laughing, bickering, and NSFW-ing this spring?

Who knows? Top visualizations may just end up on [Reddit](https://www.reddit.com/r/dataisbeautiful).

## Data Description

The database has one table, `May2015`, with the following fields:

 - created_utc
 - ups
 - subreddit_id
 - link_id
 - name
 - score_hidden
 - author_flair_css_class
 - author_flair_text
 - subreddit
 - id
 - removal_reason
 - gilded
 - downs
 - archived
 - author
 - score
 - retrieved_on
 - body
 - distinguished
 - edited
 - controversiality
 - parent_id","2015-08-04 23:59:00","Initial release","reddit-comments-may-2015.7z",8483353425,52
8,8,1,"Ocean Ship Logbooks (1750-1850)","climate-data-from-ocean-ships","Explore changing climatology with data from early shipping logs","In the mid-eighteenth to nineteenth centuries, navigating the open ocean was an imprecise and often dangerous feat. In order to calculate their daily progress and avoid running/sailing into the unknown, a ship's crew kept a detailed logbook with data on winds, waves, and any remarkable weather.

Handwritten in archived logbooks, these rich datasets were nearly impossible to study until the European Union funded their digitization in 2001. You can visit the EU project [website](https://pendientedemigracion.ucm.es/info/cliwoc/) for detailed information on the countries and ships included.

We're hosting the full 1750-1850 dataset on Kaggle to promote the exploration of this unique and invaluable climatology resource. 

## Data Description

This data comes from the [Climatological Database for the World's Oceans 1750-1850 (CLIWOC), version 1.5 data release](http://pendientedemigracion.ucm.es/info/cliwoc/cliwoc15.htm).

The primary data file is CLIWOC15.csv. The columns in this table [are described on this page](http://pendientedemigracion.ucm.es/info/cliwoc/content/CLIWOC15all.htm) (scroll down to the table that starts with ""Field abbreviation""). It includes 280,280 observational records of ship locations weather data, and other associated information.

The ancillary data files are described on the above site.","2015-08-18 21:53:00","Initial release","climate-data-from-ocean-ships.7z",13502076,54
9,9,1,"Meta Kaggle","meta-kaggle","The dataset on Kaggle, on Kaggle","We aren't saying this dataset is the Rosetta Stone of machine learning competitions, but we do think there is a lot to learn from (and a lot of fun to be had by) releasing some of our most interesting tables on Kaggle community and competition activity.

Strategizing to become a Master? Wondering who, where, and what goes in to a winning team? Deciding between evaluation metrics for your next data science project? We hope the scripts published here will enrich and entertain Kagglers, spark some lively conversations, and act as a resource for the larger machine learning community.

*Don't see a table you're eager to explore? We'll be periodically refreshing and adding to this dataset. Show us what you've got, and what you could do with more!*

This data (available through Kaggle Scripts as CSV files and a SQLite database) contains the tables listed below.

Note that this data is not a complete dump: rows, columns, and tables have been filtered out, and it is a small subset of the data that we can release publicly. Over time, we'll add more of the tables that we can release publicly to it.

## Data Schema

**[CompetitionHostSegments](https://www.kaggle.com/benhamner/meta-kaggle/competitionhostsegments-table)**

 - Id
 - Name
 - Title
 - OrderId

**[Competitions](https://www.kaggle.com/benhamner/meta-kaggle/competitions-table-sample)**

 - Id
 - CompetitionName
 - Title
 - BriefDescription
 - DateEnabled
 - Deadline
 - HasLeaderboard
 - SolutionNumRows
 - EvaluationAlgorithmId
 - LeaderboardPercentage
 - MaxDailySubmissions
 - NumScoredSubmissions
 - NumPrizes
 - SiteId
 - LeaderboardDisplayFormat
 - FinalLeaderboardHasBeenVerified
 - ValidationSetId
 - HasHeaderImage
 - RewardTypeId
 - RewardQuantity
 - CompetitionHostSegmentId
 - SolutionLastColumnWasLeaderboardUsageIndicator
 - ModelSubmissionDeadline
 - EnableSubmissionModelHashes
 - EnableSubmissionModelAttachments
 - MaxTeamSize
 - HyperParameters
 - CanQualifyTalent
 - EvaluationAlgorithmParameters

**[EvaluationAlgorithms](https://www.kaggle.com/benhamner/meta-kaggle/evaluationalgorithms-table)**

 - Id
 - Abbreviation
 - Name
 - Description
 - IsMax

**[RewardTypes](https://www.kaggle.com/benhamner/meta-kaggle/rewardtypes-table)**

 - Id
 - Name
 - Description

**[Sites](https://www.kaggle.com/benhamner/meta-kaggle/sites-table)**

 - Id
 - ShortName
 - LongName
 - CanonicalHostName

**[Submissions](https://www.kaggle.com/benhamner/meta-kaggle/submissions-table-sample)**

 - Id
 - SubmittedUserId
 - DateSubmitted
 - TeamId
 - PrivateScore
 - PublicScore
 - IsSelected
 - ScoreStatus
 - IsAfterDeadline
 - DateScored
 - ScoringDurationMilliseconds

**[TeamMemberships](https://www.kaggle.com/benhamner/meta-kaggle/teammemberships-table-sample)**

 - Id
 - TeamId
 - UserId

**[Teams](https://www.kaggle.com/benhamner/meta-kaggle/teams-table-sample)**

 - Id
 - TeamName
 - CompetitionId
 - TeamLeaderId
 - Score
 - Ranking
 - ScoreFirstSubmittedDate
 - IsBenchmark
 - GithubRepoLink

**[Users](https://www.kaggle.com/benhamner/meta-kaggle/users-table-sample)**

 - Id
 - UserName
 - DisplayName
 - RegisterDate
 - Points
 - Ranking
 - Tier
 - HighestRanking

**[ValidationSets](https://www.kaggle.com/benhamner/meta-kaggle/validationsets-table)**

 - Id
 - Name
 - SetValue
 - CompetitionId","2015-09-08 19:01:00","Initial release","meta-kaggle-release-2015-09-17-21-19-05.zip",103368152,46
10,10,1,"Hillary Clinton's Emails","hillary-clinton-emails","Uncover the political landscape in Hillary Clinton's emails","Throughout 2015, Hillary Clinton has been embroiled in [controversy](https://en.wikipedia.org/wiki/Hillary_Clinton_email_controversy) over the use of personal email accounts on non-government servers during her time as the United States Secretary of State. Some political experts and opponents maintain that Clinton's use of personal email accounts to conduct Secretary of State affairs is in violation of protocols and federal laws that ensure appropriate recordkeeping of government activity. Hillary's campaign has provided their own four sentence summary of her email use [here](https://www.hillaryclinton.com/hillarys-emails-four-sentences/). 

There have been a number of Freedom of Information [lawsuits](https://en.wikipedia.org/wiki/Hillary_Clinton_email_controversy#Freedom_of_Information_lawsuits) filed over the State Department's failure to fully release the emails sent and received on Clinton's private accounts. On Monday, August 31, the State Department released nearly 7,000 pages of Clinton's heavily redacted emails (its biggest release of emails to date). 

The documents were released by the State Department as PDFs. We've cleaned and normalized the released documents and are hosting them for public analysis. Kaggle's choice to host this dataset is not meant to express any particular political affiliation or intent. 

## Data Description

hillary-clinton-emails-release-*.zip contains four CSV tables and a SQLite database. The released archive is timestamped since the conversion process from the raw data is messy and imprecise, and it may be updated on a regular basis. Here's the [code that creates this data release](https://github.com/benhamner/hillary-clinton-emails).

### Emails.csv

 - **Id** - unique identifier for internal reference
 - **DocNumber** - FOIA document number
 - **MetadataSubject** - Email SUBJECT field (from the FOIA metadata)
 - **MetadataTo** - Email TO field (from the FOIA metadata)
 - **MetadataFrom** - Email FROM field (from the FOIA metadata)
 - **SenderPersonId** - PersonId of the email sender (linking to Persons table)
 - **MetadataDateSent** - Date the email was sent (from the FOIA metadata)
 - **MetadataDateReleased** - Date the email was released (from the FOIA metadata)
 - **MetadataPdfLink** - Link to the original PDF document (from the FOIA metadata)
 - **MetadataCaseNumber** - Case number (from the FOIA metadata)
 - **MetadataDocumentClass** - Document class (from the FOIA metadata)
 - **ExtractedSubject** - Email SUBJECT field (extracted from the PDF)
 - **ExtractedTo** - Email TO field (extracted from the PDF)
 - **ExtractedFrom** - Email FROM field (extracted from the PDF)
 - **ExtractedCc** - Email CC field (extracted from the PDF)
 - **ExtractedDateSent** - Date the email was sent (extracted from the PDF)
 - **ExtractedCaseNumber** - Case number (extracted from the PDF)
 - **ExtractedDocNumber** - Doc number (extracted from the PDF)
 - **ExtractedDateReleased** - Date the email was released (extracted from the PDF)
 - **ExtractedReleaseInPartOrFull** - Whether the email was partially censored (extracted from the PDF)
 - **ExtractedBodyText** - Attempt to only pull out the text in the body that the email sender wrote (extracted from the PDF)
 - **RawText** - Raw email text (extracted from the PDF)

### Persons.csv

 - **Id** - unique identifier for internal reference
 - **Name** - person's name

### Aliases.csv

 - **Id** - unique identifier for internal reference
 - **Alias** - text in the From/To email fields that refers to the person
 - **PersonId** - person that the alias refers to

### EmailReceivers.csv

 - **Id** - unique identifier for internal reference
 - **EmailId** - Id of the email
 - **PersonId** - Id of the person that received the email

### database.sqlite

This SQLite database contains all of the above tables (Emails, Persons, Aliases, and EmailReceivers) with their corresponding fields. You can see the schema and ingest code under scripts/sqlImport.sql","2015-09-11 01:56:00","Initial release","hillary-clinton-emails-release-2015-09-11-01-39-01.zip",12759116,83
11,11,1,"US Dept of Education: College Scorecard","college-scorecard","Raise the curtain on the true cost of higher education","It's no secret that US university students often graduate with debt repayment obligations that far outstrip their employment and income prospects. While it's understood that students from elite colleges tend to earn more than graduates from less prestigious universities, the finer relationships between future income and university attendance are quite murky. In an effort to make educational investments less speculative, the US Department of Education has matched information from the student financial aid system with federal tax returns to create the [College Scorecard dataset](https://collegescorecard.ed.gov/data/).

Kaggle is hosting the College Scorecard dataset in order to facilitate shared learning and collaboration. Insights from this dataset can help make the returns on higher education more transparent and, in turn, more fair.

## Data Description

[Here's a script showing an exploratory overview of some of the data](https://www.kaggle.com/benhamner/d/kaggle/college-scorecard/exploring-the-us-college-scorecard-data).

college-scorecard-release-*.zip contains a compressed version of the same data available through Kaggle Scripts.

It consists of three components:

 - [All the raw data files released in version 1.40 of the college scorecard data](https://collegescorecard.ed.gov/data/)
 - Scorecard.csv, a single CSV file with all the years data combined. In it, we've converted categorical variables represented by integer keys in the original data to their labels and added a Year column
 - database.sqlite, a SQLite database containing a single Scorecard table that contains the same information as Scorecard.csv

*New to data exploration in R? Take the free, interactive DataCamp course, ""[Data Exploration With Kaggle Scripts](https://www.datacamp.com/courses/data-exploration-with-kaggle-scripts),"" to learn the basics of visualizing data with ggplot. You'll also create your first Kaggle Scripts along the way.*","2015-09-25 00:05:00","Initial release","college-scorecard-release-2015-09-23-15-08-57.zip",601451381,52
12,12,1,"NIPS 2015 Papers","nips-2015-papers","Explore and analyze this year's NIPS papers","[Neural Information Processing Systems (NIPS)](https://nips.cc/) is one of the top machine learning conferences in the world. It covers topics ranging from deep learning and computer vision to cognitive science and reinforcement learning. 

[![Wordcloud](https://kaggle2.blob.core.windows.net/competitions/kaggle/4817/media/unnamed-chunk-9-1.png)](https://www.kaggle.com/benhamner/nips-2015-papers/exploring-the-nips-2015-papers)

This year, Kaggle is hosting the NIPS 2015 paper dataset to facilitate and showcase exploratory analytics on the NIPS data. We've extracted the paper text from the raw PDF files and are releasing that both in CSV files and as a SQLite database. Here's a [quick script](https://www.kaggle.com/benhamner/nips-2015-papers/exploring-the-nips-2015-papers) that gives an overview of what's included in the data.

We encourage you to explore this data and share what you find through Kaggle Scripts!

## Data Description

[Overview of the data in Kaggle Scripts](https://www.kaggle.com/benhamner/nips-2015-papers/exploring-the-nips-2015-papers).

nips-2015-papers-release-*.zip (downloadable from the link above) contains the below files/folders. All this data's available through Kaggle Scripts as well, and you can create a new script to immediately start exploring the data in R, Python, Julia, or SQLite.

This dataset is available in two formats: three CSV files and a single SQLite database (consisting of three tables with content identical to the CSV files).

You can see the code used to create this dataset on [Github](https://github.com/benhamner/nips-2015-papers).

### Papers.csv

This file contains one row for each of the 403 NIPS papers from this year's conference. It includes the following fields

 - **Id** - unique identifier for the paper (equivalent to the one in NIPS's system)
 - **Title** - title of the paper
 - **EventType** - whether it's a poster, oral, or spotlight presentation
 - **PdfName** - filename for the PDF document
 - **Abstract** - text for the abstract (scraped from the NIPS website)
 - **PaperText** - raw text from the PDF document (created using the tool pdftotext)

### Authors.csv

This file contains id's and names for each of the authors on this year's NIPS papers.

 - **Id** - unique identifier for the author (equivalent to the one in NIPS's system)
 - **Name** - author's name

### PaperAuthors.csv

This file links papers to their corresponding authors.

 - **Id** - unique identifier
 - **PaperId** - id for the paper
 - **AuthorId** - id for the author

### database.sqlite

This SQLite database contains the tables with equivalent data and formatting as the Papers.csv, Authors.csv, and PaperAuthors.csv files.

### pdfs

This folder contains the raw pdf files for each of the papers.","2015-12-09 06:16:00","Initial release","nips-2015-papers-release-2015-12-08-21-53-18.zip",355902222,19
13,13,1,"US Baby Names","us-baby-names","Explore naming trends from babies born in the US","US Social Security applications are a great way to track trends in how babies born in the US are named.

Data.gov releases two datasets that are helplful for this: one at the [national level](https://catalog.data.gov/dataset/baby-names-from-social-security-card-applications-national-level-data) and another [at the state level](https://catalog.data.gov/dataset/baby-names-from-social-security-card-applications-data-by-state-and-district-of-). Note that only names with at least 5 babies born in the same year (/ state) are included in this dataset for privacy.

[![benjamin](https://www.kaggle.io/svf/153725/b6c3f30368aeb2b277016b0582d1eab6/nameOverTime.png)](https://www.kaggle.com/benhamner/d/kaggle/us-baby-names/babies-named-benjamin-over-time)

I've taken the raw files here and combined/normalized them into two CSV files (one for each dataset) as well as a SQLite database with two equivalently-defined tables. The code that did these transformations is [available here](https://github.com/benhamner/us-baby-names/).

*New to data exploration in R? Take the free, interactive DataCamp course, ""[Data Exploration With Kaggle Scripts](https://www.datacamp.com/courses/data-exploration-with-kaggle-scripts),"" to learn the basics of visualizing data with ggplot. You'll also create your first Kaggle Scripts along the way.*","2015-12-18 23:16:00","Initial release","us-baby-names-release-2015-12-18-00-53-48.zip",179313640,75
14,14,1,"SF Salaries","sf-salaries","Explore San Francisco city employee salary data","One way to understand how a city government works is by looking at who it employs and how its employees are compensated. This data contains the names, job title, and compensation for San Francisco city employees on an annual basis from 2011 to 2014.

[![salary distribution](https://www.kaggle.io/svf/157898/60445b7da739a3ae25f3f8b26328036d/salaryDistribution.png)](https://www.kaggle.com/benhamner/d/kaggle/sf-salaries/exploring-the-sf-city-salary-data)

## Exploration Ideas

To help get you started, here are some data exploration ideas:

 - How have salaries changed over time between different groups of people?
 - How are base pay, overtime pay, and benefits allocated between different groups?
 - Is there any evidence of pay discrimination based on gender in this dataset?
 - How is budget allocated based on different groups and responsibilities?

Have other ideas you're curious for someone else to explore? Post them in [this forum thread](https://www.kaggle.com/forums/f/977/sf-salaries/t/18264/sf-salaries-dataset).

## Data Description

sf-salaries-release-*.zip (downloadable via the ""Download Data"" link in the header above) contains a CSV table and a SQLite database (with the same data as the CSV file). Here's the [code that creates this data release](https://github.com/benhamner/sf-salaries).

The original source for this data is [here](http://transparentcalifornia.com/salaries/san-francisco/). We've taken the raw files here and combined/normalized them into a single CSV file as well as a SQLite database with an equivalently-defined table.

### Salaries.csv

 - **Id**
 - **EmployeeName**
 - **JobTitle**
 - **BasePay**
 - **OvertimePay**
 - **OtherPay**
 - **Benefits**
 - **TotalPay**
 - **TotalPayBenefits**
 - **Year**
 - **Notes**
 - **Agency**
 - **Status**","2015-12-21 19:40:00","Initial release","sf-salaries-release-2015-12-21-03-21-32.zip",11958977,76
16,16,1,"First GOP Debate Twitter Sentiment","first-gop-debate-twitter-sentiment","Analyze tweets on the first 2016 GOP Presidential Debate","*This data originally came from [Crowdflower's Data for Everyone library](http://www.crowdflower.com/data-for-everyone).*

As the original source says,

> We looked through tens of thousands of tweets about the early August GOP debate in Ohio and asked contributors to do both sentiment analysis and data categorization. Contributors were asked if the tweet was relevant, which candidate was mentioned, what subject was mentioned, and then what the sentiment was for a given tweet. We've removed the non-relevant messages from the uploaded dataset.

The data we're providing on Kaggle is a slightly reformatted version of the original source. It includes both a CSV file and SQLite database. The code that does these transformations is [available on GitHub](https://github.com/benhamner/crowdflower-first-gop-debate-twitter-sentiment)","2015-12-28 06:04:45","Initial release","first-gop-debate-twitter-sentiment-release-2015-12-28-05-51-00.zip",2519310,47
17,17,1,"Twitter US Airline Sentiment","twitter-airline-sentiment","Analyze how travelers in February 2015 expressed their feelings on Twitter","*This data originally came from [Crowdflower's Data for Everyone library](http://www.crowdflower.com/data-for-everyone).*

As the original source says,

> A sentiment analysis job about the problems of each major U.S. airline. Twitter data was scraped from February of 2015 and contributors were asked to first classify positive, negative, and neutral tweets, followed by categorizing negative reasons (such as ""late flight"" or ""rude service"").

The data we're providing on Kaggle is a slightly reformatted version of the original source. It includes both a CSV file and SQLite database. The code that does these transformations is [available on GitHub](https://github.com/benhamner/crowdflower-airline-twitter-sentiment)

For example, it contains whether the sentiment of the tweets in this set was positive, neutral, or negative for six US airlines:

[![airline sentiment graph](https://www.kaggle.io/svf/136065/a6e055ee6d877d2f7784dc42a15ecc43/airlineSentimentPlot.png)](https://www.kaggle.com/benhamner/d/crowdflower/twitter-airline-sentiment/exploring-airline-twitter-sentiment-data)

The fields in the Tweets.csv file / Tweets database table are:

 - tweet_id
 - airline_sentiment
 - airline\_sentiment\_confidence
 - negativereason
 - negativereason_confidence
 - airline
 - airline\_sentiment\_gold
 - name
 - negativereason_gold
 - retweet_count
 - text
 - tweet_coord
 - tweet_created
 - tweet_location
 - user_timezone","2016-01-07 00:38:08","Initial release","airline-twitter-sentiment-release-2016-01-07-00-13-30.zip",2680450,87
18,18,1,"Amazon Fine Food Reviews","amazon-fine-food-reviews","Analyze ~500,000 food reviews from Amazon","*This data was originally published on [SNAP](http://snap.stanford.edu/data/web-FineFoods.html).*

The Amazon Fine Food Reviews dataset consists of 568,454 food reviews Amazon users left up to October 2012.

[![wordcloud](https://www.kaggle.io/svf/137051/2ba35b1344041b4964fe12365b577999/wordcloud.png)](https://www.kaggle.com/benhamner/d/snap/amazon-fine-food-reviews/reviews-wordcloud)

This dataset consists of a single CSV file, Reviews.csv, and a corresponding SQLite table named Reviews in database.sqlite. The columns in the table are:

 - **Id**
 - **ProductId** - unique identifier for the product
 - **UserId** - unqiue identifier for the user
 - **ProfileName**
 - **HelpfulnessNumerator** - number of users who found the review helpful
 - **HelpfulnessDenominator** - number of users who indicated whether they found the review helpful
 - **Score** - rating between 1 and 5
 - **Time** - timestamp for the review
 - **Summary** - brief summary of the review
 - **Text** - text of the review

See [this SQLite query](https://www.kaggle.com/benhamner/d/snap/amazon-fine-food-reviews/data-sample) for a quick sample of the dataset.

If you publish articles based on this dataset, please cite the following paper:

 - J. McAuley and J. Leskovec. [From amateurs to connoisseurs: modeling the evolution of user expertise through online reviews](http://i.stanford.edu/~julian/pdfs/www13.pdf). WWW, 2013.","2016-01-08 21:12:10","*This data was originally published on [SNAP](http://snap.stanford.edu/data/web-FineFoods.html).*

If you publish articles based on this dataset, please cite the following paper:
 - J. McAuley and J. Leskovec. [From amateurs to connoisseurs: modeling the evolution of user expertise through online reviews](http://i.stanford.edu/~julian/pdfs/www13.pdf). WWW, 2013.","amazon-fine-foods-release-2016-01-08-20-34-54.zip",250825998,78
19,19,1,"Iris","iris","Classify iris plants into three species in this classic dataset","The Iris dataset was used in Fisher's classic 1936 paper, [The Use of Multiple Measurements in Taxonomic Problems](http://rcs.chemometrics.ru/Tutorials/classification/Fisher.pdf).

It includes three iris species with 50 samples each as well as some properties about each flower. One flower species is linearly separable from the other two, but the other two are not linearly separable from each other.

The columns in this dataset are:

 - Id
 - SepalLengthCm
 - SepalWidthCm
 - PetalLengthCm
 - PetalWidthCm
 - Species

[![Sepal Width vs. Sepal Length](https://www.kaggle.io/svf/138327/e401fb2cc596451b1e4d025aaacda95f/sepalWidthvsLength.png)](https://www.kaggle.com/benhamner/d/uciml/iris/sepal-width-vs-length)","2016-01-12 00:33:31","Initial release","iris-release-2016-01-12-01-48-31.zip",3862,75
20,20,1,"World Food Facts","world-food-facts","Explore nutrition facts from foods around the world","This data comes from the [Open Food Facts database](http://world.openfoodfacts.org/data), a free, open, and collaborative database of food products around the world.

It contains a single table, `FoodFacts`, in CSV form in `FoodFacts.csv` and in SQLite form in `database.sqlite`.

The columns in `FoodFacts` are as follows:

 - **code** (text)
 - **url** (text)
 - **creator** (text)
 - **created\_t** (text)
 - **created\_datetime** (text)
 - **last\_modified\_t** (text)
 - **last\_modified\_datetime** (text)
 - **product\_name** (text)
 - **generic\_name** (text)
 - **quantity** (text)
 - **packaging** (text)
 - **packaging\_tags** (text)
 - **brands** (text)
 - **brands\_tags** (text)
 - **categories** (text)
 - **categories\_tags** (text)
 - **categories\_en** (text)
 - **origins** (text)
 - **origins\_tags** (text)
 - **manufacturing\_places** (text)
 - **manufacturing\_places\_tags** (text)
 - **labels** (text)
 - **labels\_tags** (text)
 - **labels\_en** (text)
 - **emb\_codes** (text)
 - **emb\_codes\_tags** (text)
 - **first\_packaging\_code\_geo** (text)
 - **cities** (text)
 - **cities\_tags** (text)
 - **purchase\_places** (text)
 - **stores** (text)
 - **countries** (text)
 - **countries\_tags** (text)
 - **countries\_en** (text)
 - **ingredients\_text** (text)
 - **allergens** (text)
 - **allergens\_en** (text)
 - **traces** (text)
 - **traces\_tags** (text)
 - **traces\_en** (text)
 - **serving\_size** (text)
 - **no\_nutriments** (numeric)
 - **additives\_n** (numeric)
 - **additives** (text)
 - **additives\_tags** (text)
 - **additives\_en** (text)
 - **ingredients\_from\_palm\_oil\_n** (numeric)
 - **ingredients\_from\_palm\_oil** (numeric)
 - **ingredients\_from\_palm\_oil\_tags** (text)
 - **ingredients\_that\_may\_be\_from\_palm\_oil\_n** (numeric)
 - **ingredients\_that\_may\_be\_from\_palm\_oil** (numeric)
 - **ingredients\_that\_may\_be\_from\_palm\_oil\_tags** (text)
 - **nutrition\_grade\_uk** (numeric)
 - **nutrition\_grade\_fr** (text)
 - **pnns\_groups\_1** (text)
 - **pnns\_groups\_2** (text)
 - **states** (text)
 - **states\_tags** (text)
 - **states\_en** (text)
 - **main\_category** (text)
 - **main\_category\_en** (text)
 - **image\_url** (text)
 - **image\_small\_url** (text)
 - **energy\_100g** (numeric)
 - **energy\_from\_fat\_100g** (numeric)
 - **fat\_100g** (numeric)
 - **saturated\_fat\_100g** (numeric)
 - **butyric\_acid\_100g** (numeric)
 - **caproic\_acid\_100g** (numeric)
 - **caprylic\_acid\_100g** (numeric)
 - **capric\_acid\_100g** (numeric)
 - **lauric\_acid\_100g** (numeric)
 - **myristic\_acid\_100g** (numeric)
 - **palmitic\_acid\_100g** (numeric)
 - **stearic\_acid\_100g** (numeric)
 - **arachidic\_acid\_100g** (numeric)
 - **behenic\_acid\_100g** (numeric)
 - **lignoceric\_acid\_100g** (numeric)
 - **cerotic\_acid\_100g** (numeric)
 - **montanic\_acid\_100g** (numeric)
 - **melissic\_acid\_100g** (numeric)
 - **monounsaturated\_fat\_100g** (numeric)
 - **polyunsaturated\_fat\_100g** (numeric)
 - **omega\_3\_fat\_100g** (numeric)
 - **alpha\_linolenic\_acid\_100g** (numeric)
 - **eicosapentaenoic\_acid\_100g** (numeric)
 - **docosahexaenoic\_acid\_100g** (numeric)
 - **omega\_6\_fat\_100g** (numeric)
 - **linoleic\_acid\_100g** (numeric)
 - **arachidonic\_acid\_100g** (numeric)
 - **gamma\_linolenic\_acid\_100g** (numeric)
 - **dihomo\_gamma\_linolenic\_acid\_100g** (numeric)
 - **omega\_9\_fat\_100g** (numeric)
 - **oleic\_acid\_100g** (numeric)
 - **elaidic\_acid\_100g** (numeric)
 - **gondoic\_acid\_100g** (numeric)
 - **mead\_acid\_100g** (numeric)
 - **erucic\_acid\_100g** (numeric)
 - **nervonic\_acid\_100g** (numeric)
 - **trans\_fat\_100g** (numeric)
 - **cholesterol\_100g** (numeric)
 - **carbohydrates\_100g** (numeric)
 - **sugars\_100g** (numeric)
 - **sucrose\_100g** (numeric)
 - **glucose\_100g** (numeric)
 - **fructose\_100g** (numeric)
 - **lactose\_100g** (numeric)
 - **maltose\_100g** (numeric)
 - **maltodextrins\_100g** (numeric)
 - **starch\_100g** (numeric)
 - **polyols\_100g** (numeric)
 - **fiber\_100g** (numeric)
 - **proteins\_100g** (numeric)
 - **casein\_100g** (numeric)
 - **serum\_proteins\_100g** (numeric)
 - **nucleotides\_100g** (numeric)
 - **salt\_100g** (numeric)
 - **sodium\_100g** (numeric)
 - **alcohol\_100g** (numeric)
 - **vitamin\_a\_100g** (numeric)
 - **beta\_carotene\_100g** (numeric)
 - **vitamin\_d\_100g** (numeric)
 - **vitamin\_e\_100g** (numeric)
 - **vitamin\_k\_100g** (numeric)
 - **vitamin\_c\_100g** (numeric)
 - **vitamin\_b1\_100g** (numeric)
 - **vitamin\_b2\_100g** (numeric)
 - **vitamin\_pp\_100g** (numeric)
 - **vitamin\_b6\_100g** (numeric)
 - **vitamin\_b9\_100g** (numeric)
 - **vitamin\_b12\_100g** (numeric)
 - **biotin\_100g** (numeric)
 - **pantothenic\_acid\_100g** (numeric)
 - **silica\_100g** (numeric)
 - **bicarbonate\_100g** (numeric)
 - **potassium\_100g** (numeric)
 - **chloride\_100g** (numeric)
 - **calcium\_100g** (numeric)
 - **phosphorus\_100g** (numeric)
 - **iron\_100g** (numeric)
 - **magnesium\_100g** (numeric)
 - **zinc\_100g** (numeric)
 - **copper\_100g** (numeric)
 - **manganese\_100g** (numeric)
 - **fluoride\_100g** (numeric)
 - **selenium\_100g** (numeric)
 - **chromium\_100g** (numeric)
 - **molybdenum\_100g** (numeric)
 - **iodine\_100g** (numeric)
 - **caffeine\_100g** (numeric)
 - **taurine\_100g** (numeric)
 - **ph\_100g** (numeric)
 - **fruits\_vegetables\_nuts\_100g** (numeric)
 - **collagen\_meat\_protein\_ratio\_100g** (numeric)
 - **cocoa\_100g** (numeric)
 - **chlorophyl\_100g** (numeric)
 - **carbon\_footprint\_100g** (numeric)
 - **nutrition\_score\_fr\_100g** (numeric)
 - **nutrition\_score\_uk\_100g** (numeric)","2016-01-13 02:01:43","Initial release","world-food-facts-release-2016-01-13-03-19-37.zip",52601980,93
21,21,1,"Health Insurance Marketplace","health-insurance-marketplace","Explore health and dental plans data in the US Health Insurance Marketplace","The Health Insurance Marketplace Public Use Files contain data on health and dental plans offered to individuals and small businesses through the US Health Insurance Marketplace.

[![median plan premiums](https://www.kaggle.io/svf/161766/6468386b85217299ff3e2ae957d7d603/medianPlanPremiums.png)](https://www.kaggle.com/benhamner/d/hhsgov/health-insurance-marketplace/median-monthly-premiums-by-state)

## Exploration Ideas

To help get you started, here are some data exploration ideas:

 - How do plan rates and benefits vary across states?
 - How do plan benefits relate to plan rates?
 - How do plan rates vary by age?
 - How do plans vary across insurance network providers?

See [this forum thread](https://www.kaggle.com/forums/f/1015/health-insurance-marketplace/t/19023/exploration-ideas) for more ideas, and post there if you want to add your own ideas or answer some of the open questions!

## Data Description

This data was originally prepared and released by the [Centers for Medicare & Medicaid Services (CMS)](https://www.cms.gov/cciio/resources/data-resources/marketplace-puf.html). Please read the [CMS Disclaimer-User Agreement](https://www.cms.gov/CCIIO/Resources/Data-Resources/Downloads/Data-Disclaimer-User-Agreement.pdf) before using this data.

Here, we've processed the data to facilitate analytics. This processed version has three components:

### 1. Original versions of the data

The original versions of the 2014, 2015, 2016 data are available in the ""raw"" directory of the download and ""../input/raw"" on Kaggle Scripts. Search for ""dictionaries"" on [this page](https://www.cms.gov/cciio/resources/data-resources/marketplace-puf.html) to find the data dictionaries describing the individual raw files.

### 2. Combined CSV files that contain

In the top level directory of the download (""../input"" on Kaggle Scripts), there are six CSV files that contain the combined at across all years:

 - **BenefitsCostSharing.csv**
 - **BusinessRules.csv**
 - **Network.csv**
 - **PlanAttributes.csv**
 - **Rate.csv**
 - **ServiceArea.csv**

Additionally, there are two CSV files that facilitate joining data across years:

 - **Crosswalk2015.csv** - joining 2014 and 2015 data
 - **Crosswalk2016.csv** - joining 2015 and 2016 data

### 3. SQLite database

The ""database.sqlite"" file contains tables corresponding to each of the processed CSV files.

The code to create the processed version of this data is [available on GitHub](https://github.com/benhamner/health-insurance-marketplace).","2016-01-20 16:32:33","Initial release","health-insurance-marketplace-release-2016-01-20-15-52-37.zip",734858920,101
22,22,1,"2015 Notebook UX Survey","2015-notebook-ux-survey","Understand user perspectives on Jupyter Notebooks","At the end of 2015, the Jupyter Project conducted a UX Survey for Jupyter Notebook users. This dataset, Survey.csv, contains the raw responses.

See the [Google Group Thread](https://groups.google.com/forum/?utm_medium=eamail&utm_source=footer#!msg/jupyter/XCzJ02Rzj0Y/syumf3EgFgAJ) for more context around this dataset.

[![Jupyter Notebook Use](https://www.kaggle.io/svf/144704/f2121c0697a95da9f593496f7ffa867f/JupyterNotebookUse.png)](https://www.kaggle.com/benhamner/d/jupyter/2015-notebook-ux-survey/exploring-jupyter-notebook-survey-data)","2016-01-20 22:51:48","Initial release","2015-notebook-ux-survey-release-2016-01-20-19-29-13.zip",198773,37
23,23,1,"World Development Indicators","world-development-indicators","Explore country development indicators from around the world","The World Development Indicators from the World Bank contain over a thousand annual indicators of economic development from hundreds of countries around the world.

Here's a [list of the available indicators](https://www.kaggle.com/benhamner/d/worldbank/world-development-indicators/indicators-in-data) along with a [list of the available countries](https://www.kaggle.com/benhamner/d/worldbank/world-development-indicators/countries-in-the-wdi-data).

For example, this data includes the life expectancy at birth from many countries around the world:

[![Life expactancy at birth map](https://www.kaggle.io/svf/148550/984799c02510d42a0ef29b5770284d00/map.png)](https://www.kaggle.com/benhamner/d/worldbank/world-development-indicators/r-rworldmap-visualization-example)

The dataset hosted here is a slightly transformed verion of the raw files [available here](http://data.worldbank.org/data-catalog/world-development-indicators) to facilitate analytics.","2016-01-28 06:30:59","Initial release","world-development-indicators-release-2016-01-28-06-31-53.zip",385864788,84
24,24,1,"2016 US Election","2016-us-election","Explore data related to the 2016 US Election","This contains data relevant for the 2016 US Presidential Election, including up-to-date primary results.

[![nh-dem](https://www.kaggle.io/svf/162809/cf105fb56b206d457b5bde1a8e546365/New%20Hampshire_Democrat.png)](https://www.kaggle.com/benhamner/d/benhamner/2016-us-election/new-hampshire-democratic-primary-results)

[![ia-rep](https://www.kaggle.io/svf/162801/87335e996f9415489f5088c670c34f29/Iowa_Republican.png)](https://www.kaggle.com/benhamner/d/benhamner/2016-us-election/iowa-republican-primary-results)

## Exploration Ideas

 - What candidates within the Republican party have results that are the most anti-correlated?
 - Which Republican candidate is Hillary Clinton most correlated with based on county voting patterns? What about Bernie Sanders?
 - What insights can you discover by mapping this data?

Do you have answers or other exploration ideas? Add your ideas to [this forum post](https://www.kaggle.com/forums/f/1078/2016-us-election/t/19071/exploration-ideas) and share your insights through [Kaggle Scripts](https://www.kaggle.com/benhamner/2016-us-election/scripts)!

Do you think that we should augment this dataset with more data sources? [Let us know here](https://www.kaggle.com/forums/f/1078/2016-us-election/t/19072/additional-data-sources)!

## Data Description

The 2016 US Election dataset contains several main files and folders at the moment. You may download the entire archive via the ""Download Data"" link at the top of the page, or interact with the data in Kaggle Scripts through the `../input` directory.

 - **[PrimaryResults.csv](https://www.kaggle.com/benhamner/d/benhamner/2016-us-election/primary-results-sample-data)**: main primary results file
   - State: state where the primary or caucus was held
   - StateAbbreviation: two letter state abbreviation
   - County: county where the results come from
   - Party: Democrat or Republican
   - Candidate: name of the candidate
   - Votes: number of votes the candidate received in the corresponding state and county (may be missing)
   - FractionVotes: fraction of votes the president received in the corresponding state, county, and primary
 - **database.sqlite**: SQLite database containing the PrimaryResults table with identical data and schema
 - **county_shapefiles**: directory containing county shapefiles at three different resolutions for mapping

## Original Data Sources

 - [Iowa County-Level Results](http://overflow.solutions/datasets/2016-iowa-caucus-data-sets/)
 - [New Hampshire County-Level Results](https://numeracy.co/projects/2n9KPEk6ShS)
 - [County Shapefiles](https://www.census.gov/geo/maps-data/data/cbf/cbf_counties.html)","2016-02-19 06:37:00","Initial release","2016-presidential-election-2016-02-19-05-04-51.zip",16443244,4
25,24,2,"2016 US Election","2016-us-election","Explore data related to the 2016 US Election","***Data updated on Thursday, Feb 25**: This data update includes the SC and NV results so far as well, along with county demographic information from the US Census. Note that breaking changes were made to the schema of the data as well.*

This contains data relevant for the 2016 US Presidential Election, including up-to-date primary results.

[![sc-rep](https://www.kaggle.io/svf/166413/03fc1f5985d4b7458794e813418f0bac/South%20Carolina_Republican.png)](https://www.kaggle.com/benhamner/d/benhamner/2016-us-election/sc-republican-primary-results)

[![nv-dem](https://www.kaggle.io/svf/166450/4f86d7fc845c9ae1a562e0827347ebc8/Nevada_Democrat.png)](https://www.kaggle.com/benhamner/d/benhamner/2016-us-election/nv-democrat-primary-results)

## Exploration Ideas

 - What candidates within the Republican party have results that are the most anti-correlated?
 - Which Republican candidate is Hillary Clinton most correlated with based on county voting patterns? What about Bernie Sanders?
 - What insights can you discover by mapping this data?

Do you have answers or other exploration ideas? Add your ideas to [this forum post](https://www.kaggle.com/forums/f/1078/2016-us-election/t/19071/exploration-ideas) and share your insights through [Kaggle Scripts](https://www.kaggle.com/benhamner/2016-us-election/scripts)!

Do you think that we should augment this dataset with more data sources? Submit a pull request to this repo, or [let us know here](https://www.kaggle.com/forums/f/1078/2016-us-election/t/19072/additional-data-sources)!

## Data Description

The 2016 US Election dataset contains several main files and folders at the moment. You may download the entire archive via the ""Download Data"" link at the top of the page, or interact with the data in Kaggle Scripts through the `../input` directory.

 - **[primary_results.csv](https://www.kaggle.com/benhamner/d/benhamner/2016-us-election/primary-results-sample-data)**: main primary results file
   - state: state where the primary or caucus was held
   - state_abbreviation: two letter state abbreviation
   - county: county where the results come from
   - fips: [FIPS county code](https://en.wikipedia.org/wiki/FIPS_county_code)
   - party: Democrat or Republican
   - candidate: name of the candidate
   - votes: number of votes the candidate received in the corresponding state and county (may be missing)
   - fraction_votes: fraction of votes the president received in the corresponding state, county, and primary
 - **county_facts.csv**: demographic data on counties from US census
 - **county\_facts\_dictionary.csv**: description of the columns in county_facts
 - **database.sqlite**: SQLite database containing the primary\_results, county\_facts, and county\_facts\_dictionary tables with identical data and schema
 - **county_shapefiles**: directory containing county shapefiles at three different resolutions for mapping

## Original Data Sources

 - [Primary Results from CNN](http://www.cnn.com/election/primaries/counties/ia/Dem)
 - [New Hampshire County-Level Results](https://numeracy.co/projects/2n9KPEk6ShS)
 - [County Shapefiles](https://www.census.gov/geo/maps-data/data/cbf/cbf_counties.html)
 - [County QuickFacts](http://quickfacts.census.gov/qfd/download_data.html)","2016-02-26 02:37:08","Added NV Democrat, NV Republican, SC Republican results. Added US Census QuickFacts data.","2016_presidential_election_2016-02-26-02-28-37.zip",17259404,2
26,12,2,"NIPS 2015 Papers","nips-2015-papers","Explore and analyze this year's NIPS papers","[Neural Information Processing Systems (NIPS)](https://nips.cc/) is one of the top machine learning conferences in the world. It covers topics ranging from deep learning and computer vision to cognitive science and reinforcement learning. 

[![Wordcloud](https://kaggle2.blob.core.windows.net/competitions/kaggle/4817/media/unnamed-chunk-9-1.png)](https://www.kaggle.com/benhamner/nips-2015-papers/exploring-the-nips-2015-papers)

This year, Kaggle is hosting the NIPS 2015 paper dataset to facilitate and showcase exploratory analytics on the NIPS data. We've extracted the paper text from the raw PDF files and are releasing that both in CSV files and as a SQLite database. Here's a [quick script](https://www.kaggle.com/benhamner/nips-2015-papers/exploring-the-nips-2015-papers) that gives an overview of what's included in the data.

We encourage you to explore this data and share what you find through Kaggle Scripts!

## Data Description

[Overview of the data in Kaggle Scripts](https://www.kaggle.com/benhamner/nips-2015-papers/exploring-the-nips-2015-papers).

nips-2015-papers-release-*.zip (downloadable from the link above) contains the below files/folders. All this data's available through Kaggle Scripts as well, and you can create a new script to immediately start exploring the data in R, Python, Julia, or SQLite.

This dataset is available in two formats: three CSV files and a single SQLite database (consisting of three tables with content identical to the CSV files).

You can see the code used to create this dataset on [Github](https://github.com/benhamner/nips-2015-papers).

### Papers.csv

This file contains one row for each of the 403 NIPS papers from this year's conference. It includes the following fields

 - **Id** - unique identifier for the paper (equivalent to the one in NIPS's system)
 - **Title** - title of the paper
 - **EventType** - whether it's a poster, oral, or spotlight presentation
 - **PdfName** - filename for the PDF document
 - **Abstract** - text for the abstract (scraped from the NIPS website)
 - **PaperText** - raw text from the PDF document (created using the tool pdftotext)

### Authors.csv

This file contains id's and names for each of the authors on this year's NIPS papers.

 - **Id** - unique identifier for the author (equivalent to the one in NIPS's system)
 - **Name** - author's name

### PaperAuthors.csv

This file links papers to their corresponding authors.

 - **Id** - unique identifier
 - **PaperId** - id for the paper
 - **AuthorId** - id for the author

### database.sqlite

This SQLite database contains the tables with equivalent data and formatting as the Papers.csv, Authors.csv, and PaperAuthors.csv files.

### pdfs

This folder contains the raw pdf files for each of the papers.","2016-02-29 02:51:00","Added accepted_papers.html to output. Many input files had minor updates as well which propagated through","release-2016-02-29-02-26-09.zip",351994648,18
27,24,3,"2016 US Election","2016-us-election","Explore data related to the 2016 US Election","***Data updated on Thursday, Feb 25**: This data update includes the SC and NV results so far as well, along with county demographic information from the US Census. Note that breaking changes were made to the schema of the data as well.*

This contains data relevant for the 2016 US Presidential Election, including up-to-date primary results.

[![sc-rep](https://www.kaggle.io/svf/166413/03fc1f5985d4b7458794e813418f0bac/South%20Carolina_Republican.png)](https://www.kaggle.com/benhamner/d/benhamner/2016-us-election/sc-republican-primary-results)

[![nv-dem](https://www.kaggle.io/svf/166450/4f86d7fc845c9ae1a562e0827347ebc8/Nevada_Democrat.png)](https://www.kaggle.com/benhamner/d/benhamner/2016-us-election/nv-democrat-primary-results)

## Exploration Ideas

 - What candidates within the Republican party have results that are the most anti-correlated?
 - Which Republican candidate is Hillary Clinton most correlated with based on county voting patterns? What about Bernie Sanders?
 - What insights can you discover by mapping this data?

Do you have answers or other exploration ideas? Add your ideas to [this forum post](https://www.kaggle.com/forums/f/1078/2016-us-election/t/19071/exploration-ideas) and share your insights through [Kaggle Scripts](https://www.kaggle.com/benhamner/2016-us-election/scripts)!

Do you think that we should augment this dataset with more data sources? Submit a pull request to this repo, or [let us know here](https://www.kaggle.com/forums/f/1078/2016-us-election/t/19072/additional-data-sources)!

## Data Description

The 2016 US Election dataset contains several main files and folders at the moment. You may download the entire archive via the ""Download Data"" link at the top of the page, or interact with the data in Kaggle Scripts through the `../input` directory.

 - **[primary_results.csv](https://www.kaggle.com/benhamner/d/benhamner/2016-us-election/primary-results-sample-data)**: main primary results file
   - state: state where the primary or caucus was held
   - state_abbreviation: two letter state abbreviation
   - county: county where the results come from
   - fips: [FIPS county code](https://en.wikipedia.org/wiki/FIPS_county_code)
   - party: Democrat or Republican
   - candidate: name of the candidate
   - votes: number of votes the candidate received in the corresponding state and county (may be missing)
   - fraction_votes: fraction of votes the president received in the corresponding state, county, and primary
 - **county_facts.csv**: demographic data on counties from US census
 - **county\_facts\_dictionary.csv**: description of the columns in county_facts
 - **database.sqlite**: SQLite database containing the primary\_results, county\_facts, and county\_facts\_dictionary tables with identical data and schema
 - **county_shapefiles**: directory containing county shapefiles at three different resolutions for mapping

## Original Data Sources

 - [Primary Results from CNN](http://www.cnn.com/election/primaries/counties/ia/Dem)
 - [New Hampshire County-Level Results](https://numeracy.co/projects/2n9KPEk6ShS)
 - [County Shapefiles](https://www.census.gov/geo/maps-data/data/cbf/cbf_counties.html)
 - [County QuickFacts](http://quickfacts.census.gov/qfd/download_data.html)","2016-02-29 07:01:04","Added NV Democrat, NV Republican, SC Republican results. Added US Census QuickFacts data.","2016_presidential_election_2016-02-29-06-06-28.zip",17259427,0
28,24,4,"2016 US Election","2016-us-election","Explore data related to the 2016 US Election","This contains data relevant for the 2016 US Presidential Election, including up-to-date primary results.

[![nh-dem](https://www.kaggle.io/svf/162809/cf105fb56b206d457b5bde1a8e546365/New%20Hampshire_Democrat.png)](https://www.kaggle.com/benhamner/d/benhamner/2016-us-election/new-hampshire-democratic-primary-results)

[![ia-rep](https://www.kaggle.io/svf/162801/87335e996f9415489f5088c670c34f29/Iowa_Republican.png)](https://www.kaggle.com/benhamner/d/benhamner/2016-us-election/iowa-republican-primary-results)

## Exploration Ideas

 - What candidates within the Republican party have results that are the most anti-correlated?
 - Which Republican candidate is Hillary Clinton most correlated with based on county voting patterns? What about Bernie Sanders?
 - What insights can you discover by mapping this data?

Do you have answers or other exploration ideas? Add your ideas to [this forum post](https://www.kaggle.com/forums/f/1078/2016-us-election/t/19071/exploration-ideas) and share your insights through [Kaggle Scripts](https://www.kaggle.com/benhamner/2016-us-election/scripts)!

Do you think that we should augment this dataset with more data sources? [Let us know here](https://www.kaggle.com/forums/f/1078/2016-us-election/t/19072/additional-data-sources)!

## Data Description

The 2016 US Election dataset contains several main files and folders at the moment. You may download the entire archive via the ""Download Data"" link at the top of the page, or interact with the data in Kaggle Scripts through the `../input` directory.

 - **[PrimaryResults.csv](https://www.kaggle.com/benhamner/d/benhamner/2016-us-election/primary-results-sample-data)**: main primary results file
   - State: state where the primary or caucus was held
   - StateAbbreviation: two letter state abbreviation
   - County: county where the results come from
   - Party: Democrat or Republican
   - Candidate: name of the candidate
   - Votes: number of votes the candidate received in the corresponding state and county (may be missing)
   - FractionVotes: fraction of votes the president received in the corresponding state, county, and primary
 - **database.sqlite**: SQLite database containing the PrimaryResults table with identical data and schema
 - **county_shapefiles**: directory containing county shapefiles at three different resolutions for mapping

## Original Data Sources

 - [Iowa County-Level Results](http://overflow.solutions/datasets/2016-iowa-caucus-data-sets/)
 - [New Hampshire County-Level Results](https://numeracy.co/projects/2n9KPEk6ShS)
 - [County Shapefiles](https://www.census.gov/geo/maps-data/data/cbf/cbf_counties.html)","2016-02-29 07:26:15","Added NV Democrat, NV Republican, SC Republican results. Added US Census QuickFacts data.","2016_presidential_election_2016-02-29-07-23-46.zip",17263143,19
29,25,1,"Exploring R Datasets","exploring-r-datasets","The rich assortment of datasets available in R","What factors are linked to extramarital affairs?  
Do tall parents have tall children?  
Can you visualize army mortality better than Florence Nightingale?  
Is the speed of light constant?  
*How does a beaver’s body temperature vary over time?*  

A variety of R packages come preloaded with a wide selection of example data to explore. The datasets can be historic, iconic, and occasionally bizarre. Best of all, you’re a one-line import away from the data science circus that is R datasets.

![Ice cream sales][1]

Find answers to questions you never wanted to ask. Ask questions about oddities you never wanted to find. Check out the full list of available datasets [here](https://vincentarelbundock.github.io/Rdatasets/datasets.html) (see the ‘DOC’ links). Our favorite collections are:

library(datasets)  
library(boot)  
library(car)  
library(Ecdata)  
library(HistData)  
library(MASS)  

  [1]: https://www.kaggle.io/svf/176490/dd42da33353f41359d805ee220388be8/Rplot001.png","2016-03-08 06:37:00","Initial release","Rdatasets.zip",4023459,43
30,26,1,"The History of Baseball","the-history-of-baseball","A complete history of major league baseball stats from 1871 to 2015","Baffled why your team traded for that 34-year-old pitcher? Convinced you can create a new and improved version of [WAR][1]? Wondering what made the 1907 Cubs great and if can they do it again? 

The History of Baseball is a reformatted version of the famous [Lahman’s Baseball Database][2]. It contains Major League Baseball’s complete batting and pitching statistics from 1871 to 2015, plus fielding statistics, standings, team stats, park stats, player demographics, managerial records, awards, post-season data, and more.

[Scripts][3], Kaggle’s free, in-browser analytics tool, makes it easy to share detailed sabermetrics, predict the next hall of fame inductee, illustrate how speed scores runs, or publish a definitive analysis on why the Los Angeles Dodgers will never win another World Series. 

We have more ideas for analysis than games in a season, but here are a few we’d really love to see: 

- Is there a most error-prone position?
- When do players at different positions peak?
- Are the best performers selected for all-star game?
- How many walks does it take for a starting pitcher to get pulled?
- Do players with a high ground into double play (GIDP) have a lower batting average?
- Which players are the most likely to choke during the post-season?
- Why should or shouldn’t the National League adopt the designated hitter rule?

Data Tables
-----------

- **player** - Player names, DOB, and biographical info
- **batting** - batting statistics
- **pitching** - pitching statistics
- **fielding** - fielding statistics
- **all_star** - All-Star appearances
- **hall\_of\_fame** - Hall of Fame voting data
- **manager** - managerial statistics
- **team** - yearly stats and standings 
- **batting_postseason** - post-season batting statistics
- **pitching_postseason** - post-season pitching statistics
- **team_franchise** - franchise information
- **fielding_outfield** - outfield position data
- **fielding_postseason** - post-season fieldinf data
- **manager_half** - split season data for managers
- **team_half** - split season data for teams
- **salary** - player salary data
- **postseason** - post-season series information
- **manager_award** - awards won by managers 
- **player_award** - awards won by players
- **manager\_award\_vote** - award voting for manager awards
- **player\_award\_vote** - award voting for player awards
- **appearances** - details on the positions a player appeared at
- **college** - list of colleges that players attended
- **player_college** - list of players and the colleges they attended
- **home_game** - home games with attendance 
- **park** - parks with locations

See the full [SQLite schema][4].


  [1]: http://www.fangraphs.com/library/misc/war/
  [2]: http://www.seanlahman.com/baseball-archive/statistics/
  [3]: https://www.kaggle.com/kaggle/the-history-of-baseball/scripts
  [4]: https://github.com/benhamner/baseball/blob/master/src/import.sql","2016-03-08 22:41:02","Initial release","baseball_2016-03-08-22-23-12.zip",20968731,55
31,27,1,"World University Rankings","world-university-rankings","Investigate the best universities in the world","Of all the universities in the world, which are the best?

Ranking universities is a difficult, political, and controversial practice. There are hundreds of different national and international university ranking systems, many of which disagree with each other. This dataset contains three global university rankings from very different places.

<h2>University Ranking Data</h2>

The [Times Higher Education World University Ranking][1] is widely regarded as one of the most influential and widely observed university measures. Founded in the United Kingdom in 2010, it has been criticized for its commercialization and for undermining non-English-instructing institutions.

The [Academic Ranking of World Universities][2], also known as the Shanghai Ranking, is an equally influential ranking. It was founded in China in 2003 and has been criticized for focusing on raw research power and for undermining humanities and quality of instruction.

The [Center for World University Rankings][3], is a less well know listing that comes from Saudi Arabia, it was founded in 2012.

 - How do these rankings compare to each other?
 - Are the various criticisms levied against these rankings fair or not?
 - How does your alma mater fare against the world?

<h2>Supplementary Data</h2>

To further extend your analyses, we've also included two sets of supplementary data. 

The first of these is a set of data on [educational attainment][4] around the world. It comes from The World Data Bank and comprises information from the UNESCO Institute for Statistics and the Barro-Lee Dataset. How does national educational attainment relate to the quality of each nation's universities?

The second supplementary dataset contains information about [public and private direct expenditure on education across nations][5]. This data comes from the National Center for Education Statistics. It represents expenditure as a percentage of gross domestic product. Does spending more on education lead to better international university rankings?

<h2>Data Fields</h2>

<h3>timesData.csv</h3>

 - **world_rank** - world rank for the university. Contains rank ranges and equal ranks (eg. =94 and 201-250).
 - **university_name** - name of university.
 - **country** - country of each university.
 - **teaching** - university score for teaching (the learning environment).
 - **international** - university score international outlook (staff, students, research).
 - **research** - university score for research (volume, income and reputation).
 - **citations** - university score for citations (research influence).
 - **income** - university score for industry income (knowledge transfer).
 - **total_score** - total score for university, used to determine rank.
 - **num_students** - number of students at the university.
 - **student_staff_ratio** - Number of students divided by number of staff.
 - **international_students** - Percentage of students who are international.
 - **female_male_ratio** - Female student to Male student ratio.
 - **year** - year of the ranking (2011 to 2016 included).

[Official Website][6]<br />
[Methodology][7]<br />
[Wikipedia][8]

<h3>school_and_country_table.csv</h3>

A lookup table for university against country from the timesData file. This is to be used to populate country data for the shanghai rankings, which are missing countries.

<h3>shanghaiData.csv</h3>

 - **world_rank** - world rank for university. Contains rank ranges and equal ranks (eg. 101-152).
 - **university_name** - name of university.
 - **national_rank** - rank of university within its country.
 - **total_score** - total score, used to determine rank.
 - **alumni** - Alumni Score, based on the number of alumni of an institution winning nobel prizes and fields medals.
 - **award** - Award Score, based on the number of staff of an institution winning Nobel Prizes in Physics, Chemistry, Medicine, and Economics and Fields Medals in Mathematics. 
 - **hici** - HiCi Score, based on the number of Highly Cited Researchers selected by Thomson Reuters.
 - **ns** - N&S Score, based on the number of papers published in Nature and Science.
 - **pub** - PUB Score, based on total number of papers indexed in the Science Citation Index-Expanded and Social Science Citation Index.
 - **pcp** - PCP Score, the weighted scores of the above five indicators divided by the number of full time academic staff.
 - **year** - year of ranking (2005 to 2015).

[Official Website][9]<br />
[Methodology][10]<br />
[Wikipedia][11]

<h3>cwurData.csv</h3>

 - **world_rank** - world rank for university. 
 - **university_name** - name of university.
 - **country** - country of each university.
 - **national_rank** - rank of university within its country.
 - **quality_of_education** - rank for quality of education.
 - **alumni_employment** - rank for alumni employment.
 - **quality_of_faculty** - rank for quality of faculty.
 - **publications** - rank for publications.
 - **influence** - rank for influence.
 - **citations** - rank for citations.
 - **broad_impact** - rank for broad impact (only available for 2014 and 2015)
 - **patents** - rank for patents.
 - **score** - total score, used for determining world rank.
 - **year** - year of ranking (2012 to 2015).

[Official Website][12] <br />
[Methodology][13]<br />
[Wikipedia][14]

<h3>educational_attainment_supplementary_data.csv</h3>

 - **Barro-Lee Dataset (2014)** - Average years of schooling; Educational attainment among age groups and genders; Population distributions. 144 countries. 1985-2010 in 5 year intervals.
 - **UNESCO Institute for Statistics (UIS)** - Mean years of schooling; Percentage of population (age 25+) by educational attainment. 152 countries. 1985-2015.

[more info][15]

<h3>education_expenditure_supplementary_data.csv</h3>

 - **country** - A set of OECD countries, plus Brazil and the Russian Federation, plus an OECD average.
 - **institute_type** - All Institutes (including preprimary education and subsidies to households, not separately shown), Elementary and secondary institutions
(excludes preprimary), Higher education institutions.
 - **direct_expenditure_type** - Public direct expenditure, private direct expenditure, or total (public + private) direct expenditure. (Private and total data are only available for 2011)
 - **1995, 2000, 2005, 2009, 2010, 2011** - years

[more info][16]

  [1]: https://www.timeshighereducation.com/world-university-rankings
  [2]: http://www.shanghairanking.com
  [3]: http://cwur.org
  [4]: http://datatopics.worldbank.org/Education/wDataQuery/QProjections.aspx
  [5]: http://nces.ed.gov/programs/digest/d14/tables/dt14_605.20.asp
  [6]: https://www.timeshighereducation.com/world-university-rankings
  [7]: https://www.timeshighereducation.com/news/ranking-methodology-2016
  [8]: https://en.wikipedia.org/wiki/Times_Higher_Education_World_University_Rankings
  [9]: http://www.shanghairanking.com
  [10]: http://www.shanghairanking.com/ARWU-Methodology-2015.html
  [11]: https://en.wikipedia.org/wiki/Academic_Ranking_of_World_Universities
  [12]: http://cwur.org
  [13]: http://cwur.org/methodology/
  [14]: https://en.wikipedia.org/wiki/College_and_university_rankings#Center_for_World_University_Rankings
  [15]: http://datatopics.worldbank.org/Education/wDataQuery/QProjections.aspx
  [16]: http://nces.ed.gov/programs/digest/d14/tables/dt14_605.20.asp","2016-03-09 04:10:56","Initial release","world-university-rankings.zip",1450836,4
32,28,1,"Death in the United States","mortality","Learn more about the leading causes of death in 2014","Every year the [CDC](http://www.cdc.gov/) releases the country’s most detailed report on death in the United States under the [National Vital Statistics Systems](http://www.cdc.gov/nchs/nvss.htm). This [mortality dataset](http://www.cdc.gov/nchs/deaths.htm) is a record of every death in the country for the year 2014, which includes detailed information about causes of death and the demographic background of the deceased.

It's been said that ""statistics are human beings with the tears wiped off."" This is especially true with this dataset. Each death record represents somebody's loved one, often connected with a lifetime of memories and sometimes tragically too short.

Putting the sensitive nature of the topic aside, analyzing mortality data is essential to understanding the complex circumstances of death across the country. The US Government uses this data to determine life expectancy and understand how death in the U.S. differs from the rest of the world. Whether you’re looking for macro trends or analyzing unique circumstances, we challenge you to use this dataset to find your own answers to one of life’s great mysteries. 

# Overview

This dataset is a collection of tables and is available in both CSV and SQLite formats. It was reformatted from its original fixed-width DUSMCPUB file format into a relational structure.

Each row in the DeathRecords table is an individual death record. Each death record has a one-to-many relationship with the EntityAxisConditions and RecordAxisConditions tables via a DeathRecordId key. Both of these conditions tables contain [ICD-10 codes](https://en.wikipedia.org/wiki/ICD-10#List) that indicate cause of death for each person. The difference between the tables is that the EntityAxisConditions are a sequential list of causes (as indicated on their death certificate), whereas the RecordAxisConditions is a set of unordered causes. 

A more detailed overview of the data can be found [here](http://www.cdc.gov/nchs/data/dvs/Record_Layout_2014.pdf). 

All data comes from the CDC’s [National Vital Statistics Systems](http://www.cdc.gov/nchs/data_access/vitalstatsonline.htm#Mortality_Multiple), with the exception of the [Icd10Code](http://www.who.int/classifications/icd/en/), which are sourced from the World Health Organization.

# Main Tables
##  DeathRecords

Primary table containing a single row per death record with these columns:

- **Id** (*integer primary key*) - Main identifier, used for joining with DeathRecordId in EntityAxisConditions and RecordAxisConditions tables.
- **ResidentStatus** (*integer*) - (e.g. 1 =  Residents, 2 = Intrastate resident, etc)
- **Education1989Revision** (*integer*) - Years of education using the 1989 revision format (e.g. 8 = 8 years of elementary education)
- **Education2003Revision** (*integer*) - Years of education using the 2003 revision code (e.g. 8 = Doctorate or professional degree)
- **EducationReportingFlag** (*integer*) - (0 = 1989 revision was used on death certificate, 1 = 2003 revision was used)
- **MonthOfDeath** (*integer*) - Month of death (e.g. 1 = January, 12 = December)
- **Sex** (*text*) - (M = Male, F = Female)
- **AgeType** (*integer*) - Units for the **Age** column (e.g. 1 = Years, 2 = Months)
- **Age** (*integer*) - Age at death (in **AgeType** units)
- **AgeSubstitutionFlag** (*integer*) - (1 = Calculated age is substituted for reported age)
- **AgeRecode52** (*integer*) - **Age** recoded into 52 bins (e.g. 1 = Under 1 hour)
- **AgeRecode27** (*integer*) - **Age** recoded into 27 bins (e.g. 1 = Under 1 month)
- **AgeRecode12** (*integer*) - **Age** recoded into 12 bins (e.g. 1 = Under 1 year)
- **InfantAgeRecode22** (*integer*) - In the event of an infant, **Age** recoded into 22 bins (e.g. 1 = Under 1 hour)
- **PlaceOfDeathAndDecedentsStatus** (*integer*) - (e.g. 6 = Nursing home/long term care)
- **MaritalStatus** (*text*) - (e.g. M = married, D = divorced, W = widowed)
- **DayOfWeekOfDeath** (*text*) - (e.g. 1 = Sunday, 7 = Saturday)
- **CurrentDataYear** (*text*) - Year on death record. Always 2014 for this dataset.
- **InjuryAtWork** (*text*) - Was the person injured at work? (Y = yes, N = no, U = unknown)
- **MannerOfDeath** (*integer*) - (e.g. 1 = Accident, 2 = Suicides) 
- **MethodOfDisposition** (*text*) - (e.g. B = burial, C = cremation)
- **Autopsy** (*text*) - Was an autopsy performed? (Y = Yes, N = No, U = Unknown)
- **ActivityCode** (*integer*) - (e.g. 0 = While engaged in sports activity, 1 = While engaged in leisure activity)
- **PlaceOfInjury** (*integer*) - (e.g. 0 = Home, 1 = Residential institution)
- **Icd10Code** (*text*) - [ICD-10 code](https://en.wikipedia.org/wiki/ICD-10#List) for the underlying cause of death (e.g. I251 = Atherosclerotic heart disease)
- **CauseRecode358** (*integer*) - Cause of death recoded into 358 bins
- **CauseRecode113** (*integer*) - Cause of death recoded into 113 bins
- **InfantCauseRecode130** (*integer*) - Infant cause of death recoded into 130 bins
- **CauseRecode39** (*integer*) - Cause of death recoded into 39 bins
- **NumberOfEntityAxisConditions** (*integer*) - Number of entries for this death record in the EntityAxisConditions table
- **NumberOfRecordAxisConditions** (*integer*) - Number of entries for this death record in the RecordAxisConditions table
- **Race** (*integer*) - Reported race (e.g. 1 = White, 2 = Black)
- **BridgedRaceFlag** (*integer*) - (e.g. 1 = Race is bridged)
- **RaceImputationFlag** (*integer*) - (e.g. 1 = Unknown race is imputed)
- **RaceRecode3** (*integer*) - **Race** recoded into 3 bins (e.g. 2 = Races other than White or Black)
- **RaceRecode5** (*integer*) - **Race** recoded into 5 bins (e.g. 4 = Asian or Pacific Islander)
- **HispanicOrigin** (*integer*) - (e.g. 220 = Central and South American)
- **HispanicOriginRaceRecode** (*integer*) - **HispanicOrigin** / **Race** recoded (e.g. 1 = Mexican)

## EntityAxisConditions
Ordered list of causes of death on a death certificate with these columns:

- **Id** (*integer primary key*)
- **DeathRecordId** (*integer*) - identifies the corresponding DeathRecords entry
- **Part** (*integer*) - (e.g. 1 = part one on death certificate, 2 = part two on death certificate)
- **Line** (*integer*) - If *Part* is 1, indicates line number on death certificate
- **Sequence** (*integer*) - Sequence of condition within part/line 
- **Icd10Code** (*text*) - [ICD-10 code](https://en.wikipedia.org/wiki/ICD-10#List) for the condition (e.g. I251 = Atherosclerotic heart disease)

## RecordAxisConditions
Unordered list of causes of death on a death certificate with these columns:

- **Id** (*integer primary key*)
- **DeathRecordId** (*integer*) - identifies the corresponding DeathRecords entry
- **Icd10Code** (*text*) - [ICD-10 code](https://en.wikipedia.org/wiki/ICD-10#List) for the condition (e.g. I251 = Atherosclerotic heart disease)

# Lookup Tables

There are many columns in the DeathRecords table that contain various codes (e.g. ""ResidentStatus"",  ""Education1989Revision"",  ""Education2003Revision"", etc.). We have provided lookup tables to help interpret these codes. The lookup table is named identically to the column name. Each lookup table has the exact same two-column structure of ""Code"" and ""Description"" allowing for simple database joins like [this one](https://www.kaggle.com/jeffmoser/d/cdc/mortality/top-8-days-of-the-week-to-die).

 - **ActivityCode**
 - **AgeRecode12**
 - **AgeRecode27**
 - **AgeRecode52**
 - **AgeType**
 - **BridgedRaceFlag**
 - **DayOfWeekOfDeath**
 - **Education1989Revision**
 - **Education2003Revision**
 - **EducationReportingFlag**
 - **HispanicOrigin**
 - **HispanicOriginRaceRecode**
 - **Icd10Code** 
 - **InfantAgeRecode22**
 - **MannerOfDeath**
 - **MaritalStatus**
 - **MethodOfDisposition**
 - **PlaceOfDeathAndDecedentsStatus**
 - **PlaceOfInjury**
 - **Race** 
 - **RaceImputationFlag** 
 - **RaceRecode3**
 - **RaceRecode5**
 - **ResidentStatus**
","2016-03-10 04:41:24","Initial release","DeathRecords.zip",312169811,53
33,29,1,"Climate Change: Earth Surface Temperature Data","climate-change-earth-surface-temperature-data","Exploring global temperatures since 1750","Some say climate change is the biggest threat of our age while others say it’s a myth based on dodgy science. We are turning some of the data over to you so you can form your own view.

[![us-climate-change](https://www.kaggle.io/svf/195335/062aeda1473b00ec4b8505d5133dfc2c/__results___files/figure-html/unnamed-chunk-4-1.png)](https://www.kaggle.com/jagelves/d/berkeleyearth/climate-change-earth-surface-temperature-data/continental-us-climate-change-1850-2013)

Even more than with other data sets that Kaggle has featured, there’s a huge amount of data cleaning and preparation that goes into putting together a long-time study of climate trends. Early data was collected by technicians using mercury thermometers, where any variation in the visit time impacted measurements. In the 1940s, the construction of airports caused many weather stations to be moved. In the 1980s, there was a move to electronic thermometers that are said to have a cooling bias.

Given this complexity, there are a range of organizations that collate climate trends data. The three most cited land and ocean temperature data sets are NOAA’s MLOST, NASA’s GISTEMP and the UK’s HadCrut. 

We have repackaged the data from a newer compilation put together by the [Berkeley Earth][1], which is affiliated with Lawrence Berkeley National Laboratory. The Berkeley Earth Surface Temperature Study combines 1.6 billion temperature reports from 16 pre-existing archives. It is nicely packaged and allows for slicing into interesting subsets (for example by country). They publish the source data and the code for the transformations they applied. They also use methods that allow weather observations from shorter time series to be included, meaning fewer observations need to be thrown away. 

In this dataset, we have include several files:

Global Land and Ocean-and-Land Temperatures (**GlobalTemperatures.csv**):  
 
 - Date: starts in 1750 for average land temperature and 1850 for max and min land temperatures and global ocean and land temperatures
 - LandAverageTemperature: global average land temperature in celsius  
 - LandAverageTemperatureUncertainty: the 95% confidence interval around the average  
 - LandMaxTemperature: global average maximum land temperature in celsius  
 - LandMaxTemperatureUncertainty: the 95% confidence interval around the maximum land temperature  
 - LandMinTemperature:  global average minimum land temperature in celsius  
 - LandMinTemperatureUncertainty: the 95% confidence interval around the minimum land temperature  
 - LandAndOceanAverageTemperature: global average land and ocean temperature in celsius  
 - LandAndOceanAverageTemperatureUncertainty: the 95% confidence interval around the global average land and ocean temperature  

Other files include:  

 - Global Average Land Temperature by Country (**GlobalLandTemperaturesByCountry.csv**)  
 - Global Average Land Temperature by State (**GlobalLandTemperaturesByState.csv**)  
 - Global Land Temperatures By Major City (**GlobalLandTemperaturesByMajorCity.csv**)  
 - Global Land Temperatures By City (**GlobalLandTemperaturesByCity.csv**)

The raw data comes from the [Berkeley Earth data page][2].


  [1]: http://berkeleyearth.org/about/
  [2]: http://berkeleyearth.org/data/","2016-03-10 20:18:01","Initial release","GlobalLandTemperatures.zip",88046254,74
34,27,2,"World University Rankings","world-university-rankings","Investigate the best universities in the world","Of all the universities in the world, which are the best?

Ranking universities is a difficult, political, and controversial practice. There are hundreds of different national and international university ranking systems, many of which disagree with each other. This dataset contains three global university rankings from very different places.

<h2>University Ranking Data</h2>

The [Times Higher Education World University Ranking][1] is widely regarded as one of the most influential and widely observed university measures. Founded in the United Kingdom in 2010, it has been criticized for its commercialization and for undermining non-English-instructing institutions.

The [Academic Ranking of World Universities][2], also known as the Shanghai Ranking, is an equally influential ranking. It was founded in China in 2003 and has been criticized for focusing on raw research power and for undermining humanities and quality of instruction.

The [Center for World University Rankings][3], is a less well know listing that comes from Saudi Arabia, it was founded in 2012.

 - How do these rankings compare to each other?
 - Are the various criticisms levied against these rankings fair or not?
 - How does your alma mater fare against the world?

<h2>Supplementary Data</h2>

To further extend your analyses, we've also included two sets of supplementary data. 

The first of these is a set of data on [educational attainment][4] around the world. It comes from The World Data Bank and comprises information from the UNESCO Institute for Statistics and the Barro-Lee Dataset. How does national educational attainment relate to the quality of each nation's universities?

The second supplementary dataset contains information about [public and private direct expenditure on education across nations][5]. This data comes from the National Center for Education Statistics. It represents expenditure as a percentage of gross domestic product. Does spending more on education lead to better international university rankings?

<h2>Data Fields</h2>

<h3>timesData.csv</h3>

 - **world_rank** - world rank for the university. Contains rank ranges and equal ranks (eg. =94 and 201-250).
 - **university_name** - name of university.
 - **country** - country of each university.
 - **teaching** - university score for teaching (the learning environment).
 - **international** - university score international outlook (staff, students, research).
 - **research** - university score for research (volume, income and reputation).
 - **citations** - university score for citations (research influence).
 - **income** - university score for industry income (knowledge transfer).
 - **total_score** - total score for university, used to determine rank.
 - **num_students** - number of students at the university.
 - **student_staff_ratio** - Number of students divided by number of staff.
 - **international_students** - Percentage of students who are international.
 - **female_male_ratio** - Female student to Male student ratio.
 - **year** - year of the ranking (2011 to 2016 included).

[Official Website][6]<br />
[Methodology][7]<br />
[Wikipedia][8]

<h3>school_and_country_table.csv</h3>

A lookup table for university against country from the timesData file. This is to be used to populate country data for the shanghai rankings, which are missing countries.

<h3>shanghaiData.csv</h3>

 - **world_rank** - world rank for university. Contains rank ranges and equal ranks (eg. 101-152).
 - **university_name** - name of university.
 - **national_rank** - rank of university within its country.
 - **total_score** - total score, used to determine rank.
 - **alumni** - Alumni Score, based on the number of alumni of an institution winning nobel prizes and fields medals.
 - **award** - Award Score, based on the number of staff of an institution winning Nobel Prizes in Physics, Chemistry, Medicine, and Economics and Fields Medals in Mathematics. 
 - **hici** - HiCi Score, based on the number of Highly Cited Researchers selected by Thomson Reuters.
 - **ns** - N&S Score, based on the number of papers published in Nature and Science.
 - **pub** - PUB Score, based on total number of papers indexed in the Science Citation Index-Expanded and Social Science Citation Index.
 - **pcp** - PCP Score, the weighted scores of the above five indicators divided by the number of full time academic staff.
 - **year** - year of ranking (2005 to 2015).

[Official Website][9]<br />
[Methodology][10]<br />
[Wikipedia][11]

<h3>cwurData.csv</h3>

 - **world_rank** - world rank for university. 
 - **university_name** - name of university.
 - **country** - country of each university.
 - **national_rank** - rank of university within its country.
 - **quality_of_education** - rank for quality of education.
 - **alumni_employment** - rank for alumni employment.
 - **quality_of_faculty** - rank for quality of faculty.
 - **publications** - rank for publications.
 - **influence** - rank for influence.
 - **citations** - rank for citations.
 - **broad_impact** - rank for broad impact (only available for 2014 and 2015)
 - **patents** - rank for patents.
 - **score** - total score, used for determining world rank.
 - **year** - year of ranking (2012 to 2015).

[Official Website][12] <br />
[Methodology][13]<br />
[Wikipedia][14]

<h3>educational_attainment_supplementary_data.csv</h3>

 - **Barro-Lee Dataset (2014)** - Average years of schooling; Educational attainment among age groups and genders; Population distributions. 144 countries. 1985-2010 in 5 year intervals.
 - **UNESCO Institute for Statistics (UIS)** - Mean years of schooling; Percentage of population (age 25+) by educational attainment. 152 countries. 1985-2015.

[more info][15]

<h3>education_expenditure_supplementary_data.csv</h3>

 - **country** - A set of OECD countries, plus Brazil and the Russian Federation, plus an OECD average.
 - **institute_type** - All Institutes (including preprimary education and subsidies to households, not separately shown), Elementary and secondary institutions
(excludes preprimary), Higher education institutions.
 - **direct_expenditure_type** - Public direct expenditure, private direct expenditure, or total (public + private) direct expenditure. (Private and total data are only available for 2011)
 - **1995, 2000, 2005, 2009, 2010, 2011** - years

[more info][16]

  [1]: https://www.timeshighereducation.com/world-university-rankings
  [2]: http://www.shanghairanking.com
  [3]: http://cwur.org
  [4]: http://datatopics.worldbank.org/Education/wDataQuery/QProjections.aspx
  [5]: http://nces.ed.gov/programs/digest/d14/tables/dt14_605.20.asp
  [6]: https://www.timeshighereducation.com/world-university-rankings
  [7]: https://www.timeshighereducation.com/news/ranking-methodology-2016
  [8]: https://en.wikipedia.org/wiki/Times_Higher_Education_World_University_Rankings
  [9]: http://www.shanghairanking.com
  [10]: http://www.shanghairanking.com/ARWU-Methodology-2015.html
  [11]: https://en.wikipedia.org/wiki/Academic_Ranking_of_World_Universities
  [12]: http://cwur.org
  [13]: http://cwur.org/methodology/
  [14]: https://en.wikipedia.org/wiki/College_and_university_rankings#Center_for_World_University_Rankings
  [15]: http://datatopics.worldbank.org/Education/wDataQuery/QProjections.aspx
  [16]: http://nces.ed.gov/programs/digest/d14/tables/dt14_605.20.asp","2016-03-11 22:48:16","Unified data schema","world-university-ranking.zip",1451100,47
37,31,1,"2016 March ML Mania Predictions","2016-march-ml-mania","Forecasting the 2016 NCAA Basketball Tournament","Kaggle’s [March Machine Learning Mania](https://www.kaggle.com/c/march-machine-learning-mania-2016) competition challenged data scientists to predict winners and losers of the men's 2016 NCAA basketball tournament. This dataset contains the 1070 selected predictions of all Kaggle participants. These predictions were collected and locked in prior to the start of the tournament.

How can this data be used? You can pivot it to look at both Kaggle and NCAA teams alike. You can look at who will win games, which games will be close, which games are hardest to forecast, or which Kaggle teams are gambling vs. sticking to the data.

[![First round predictions][1]](https://www.kaggle.com/wcukierski/d/wcukierski/2016-march-ml-mania/official-first-round-predictions)

*The NCAA tournament is a single-elimination tournament that begins with 68 teams. There are four games, usually called the “play-in round,” before the traditional bracket action starts. Due to competition timing, these games are included in the prediction files but should not be used in analysis, as it’s possible that the prediction was submitted after the play-in round games were over.*

## Data Description

Each Kaggle team could submit up to two prediction files. The prediction files in the dataset are in the 'predictions' folder and named according to:

> TeamName\_TeamId\_SubmissionId.csv

The file format contains a probability prediction for every possible game between the 68 teams. This is necessary to cover every possible tournament outcome. Each team has a unique numerical Id (given in Teams.csv). Each game has a unique Id column created by concatenating the year and the two team Ids. The format is the following:

> Id,Pred  
> 2016\_1112\_1114,0.6  
> 2016\_1112\_1122,0  
> ...  

The team with the lower numerical Id is always listed first. “Pred” represents the probability that the team with the lower Id beats the team with the higher Id. For example, ""2016\_1112\_1114,0.6"" indicates team 1112 has a 0.6 probability of beating team 1114.

For convenience, we have included the data files from the 2016 March Mania competition dataset in the Scripts environment (you may find TourneySlots.csv and TourneySeeds.csv useful for determining matchups, see [the documentation][2]). However, the focus of this dataset is on Kagglers' predictions.


  [1]: https://www.kaggle.io/svf/183298/a13d9a2275f63f167b9c1d4b25330646/kaggle_first_round_2016.png
  [2]: https://www.kaggle.com/c/march-machine-learning-mania-2016/data","2016-03-16 00:00:00","Initial release","2016-march-machine-learning-mania-predictions.zip",29094002,35
38,24,5,"2016 US Election","2016-us-election","Explore data related to the 2016 US Election","This contains data relevant for the 2016 US Presidential Election, including up-to-date primary results.

[![sc-rep](https://www.kaggle.io/svf/166413/03fc1f5985d4b7458794e813418f0bac/South%20Carolina_Republican.png)](https://www.kaggle.com/benhamner/d/benhamner/2016-us-election/sc-republican-primary-results)

[![nv-dem](https://www.kaggle.io/svf/166450/4f86d7fc845c9ae1a562e0827347ebc8/Nevada_Democrat.png)](https://www.kaggle.com/benhamner/d/benhamner/2016-us-election/nv-democrat-primary-results)

## Exploration Ideas

 - What candidates within the Republican party have results that are the most anti-correlated?
 - Which Republican candidate is Hillary Clinton most correlated with based on county voting patterns? What about Bernie Sanders?
 - What insights can you discover by mapping this data?

Do you have answers or other exploration ideas? Add your ideas to [this forum post](https://www.kaggle.com/forums/f/1078/2016-us-election/t/19071/exploration-ideas) and share your insights through [Kaggle Scripts](https://www.kaggle.com/benhamner/2016-us-election/scripts)!

Do you think that we should augment this dataset with more data sources? Submit a pull request to this repo, or [let us know here](https://www.kaggle.com/forums/f/1078/2016-us-election/t/19072/additional-data-sources)!

## Data Description

The 2016 US Election dataset contains several main files and folders at the moment. You may download the entire archive via the ""Download Data"" link at the top of the page, or interact with the data in Kaggle Scripts through the `../input` directory.

 - **[primary_results.csv](https://www.kaggle.com/benhamner/d/benhamner/2016-us-election/primary-results-sample-data)**: main primary results file
   - state: state where the primary or caucus was held
   - state_abbreviation: two letter state abbreviation
   - county: county where the results come from
   - fips: [FIPS county code](https://en.wikipedia.org/wiki/FIPS_county_code)
   - party: Democrat or Republican
   - candidate: name of the candidate
   - votes: number of votes the candidate received in the corresponding state and county (may be missing)
   - fraction_votes: fraction of votes the president received in the corresponding state, county, and primary
 - **county_facts.csv**: demographic data on counties from US census
 - **county\_facts\_dictionary.csv**: description of the columns in county_facts
 - **database.sqlite**: SQLite database containing the primary\_results, county\_facts, and county\_facts\_dictionary tables with identical data and schema
 - **county_shapefiles**: directory containing county shapefiles at three different resolutions for mapping

## Original Data Sources

 - [Primary Results from CNN](http://www.cnn.com/election/primaries/counties/ia/Dem)
 - [New Hampshire County-Level Results](https://numeracy.co/projects/2n9KPEk6ShS)
 - [County Shapefiles](https://www.census.gov/geo/maps-data/data/cbf/cbf_counties.html)
 - [County QuickFacts](http://quickfacts.census.gov/qfd/download_data.html)","2016-03-25 22:40:52","Added a slew of new state results","2016_presidential_election_2016-03-25-21-27-54.zip",17567111,55
45,9,2,"Meta Kaggle","meta-kaggle","The dataset on Kaggle, on Kaggle","We aren't saying this dataset is the Rosetta Stone of machine learning competitions, but we do think there is a lot to learn from (and a lot of fun to be had by) releasing some of our most interesting tables on Kaggle community and competition activity.

Strategizing to become a Master? Wondering who, where, and what goes in to a winning team? Deciding between evaluation metrics for your next data science project? We hope the scripts published here will enrich and entertain Kagglers, spark some lively conversations, and act as a resource for the larger machine learning community.

[![private leaderboard performance over time](https://www.kaggle.io/svf/214627/b025ac60d2bd52ce7321c5257b7d891b/private_leaderboard_aggregate_performance_over_time.png)](https://www.kaggle.com/benhamner/d/kaggle/meta-kaggle/kaggle-leaderboard-performance-over-time)

This data (available through Kaggle Scripts as CSV files and a SQLite database) contains the tables listed below.

Note that this data is not a complete dump: rows, columns, and tables have been filtered out, and it is a small subset of the data that we can release publicly. Over time, we'll add more of the tables that we can release publicly to it.

[![wordcloud](https://www.kaggle.io/svf/214832/ebc77bde9dd3c079bcc52573fcedbd3e/__results___files/figure-html/unnamed-chunk-4-1.png)](https://www.kaggle.com/benhamner/d/kaggle/meta-kaggle/kaggle-forum-post-wordclouds)","2016-04-19 23:44:45","Updated with data for completed competitions as of April 19, 2016 and added data on forums (three new tables: Forums, ForumMessages, and ForumTopics).","meta-kaggle.zip",176636475,6
49,33,1,"US Consumer Finance Complaints","us-consumer-finance-complaints","US consumer complaints on financial products and company responses","Each week [the CFPB](http://www.consumerfinance.gov/data-research/consumer-complaints/) sends thousands of consumers’ complaints about financial products and services to companies for response. Those complaints are published here after the company responds or after 15 days, whichever comes first. By adding their voice, consumers help improve the financial marketplace.","2016-04-26 22:33:46","Initial release","us-consumer-finance-complaint-database.zip",94858347,15
51,34,1,"Lending Club Loan Data","lending-club-loan-data","Analyze Lending Club's issued loans","These files contain complete loan data for all loans issued through the 2007-2015, including the current loan status (Current, Late, Fully Paid, etc.) and latest payment information. The file containing loan data through the ""present"" contains complete loan data for all loans issued through the previous completed calendar quarter. ","2016-05-02 21:41:52","Initial release","lending-club-loan-data.zip",251313372,61
52,9,3,"Meta Kaggle","meta-kaggle","The dataset on Kaggle, on Kaggle","We aren't saying this dataset is the Rosetta Stone of machine learning competitions, but we do think there is a lot to learn from (and a lot of fun to be had by) releasing some of our most interesting tables on Kaggle community and competition activity.

Strategizing to become a Master? Wondering who, where, and what goes in to a winning team? Deciding between evaluation metrics for your next data science project? We hope the scripts published here will enrich and entertain Kagglers, spark some lively conversations, and act as a resource for the larger machine learning community.

[![private leaderboard performance over time](https://www.kaggle.io/svf/214627/b025ac60d2bd52ce7321c5257b7d891b/private_leaderboard_aggregate_performance_over_time.png)](https://www.kaggle.com/benhamner/d/kaggle/meta-kaggle/kaggle-leaderboard-performance-over-time)

This data (available through Kaggle Scripts as CSV files and a SQLite database) contains the tables listed below.

Note that this data is not a complete dump: rows, columns, and tables have been filtered out, and it is a small subset of the data that we can release publicly. Over time, we'll add more of the tables that we can release publicly to it.

[![wordcloud](https://www.kaggle.io/svf/214832/ebc77bde9dd3c079bcc52573fcedbd3e/__results___files/figure-html/unnamed-chunk-4-1.png)](https://www.kaggle.com/benhamner/d/kaggle/meta-kaggle/kaggle-forum-post-wordclouds)","2016-05-04 03:24:39","Updated data as of May 3, 2016 & added tables for Kaggle Dataset & Kaggle Scripts","meta-kaggle.zip",386836300,8
58,35,1,"Speed Dating Experiment","speed-dating-experiment","What attributes influence the selection of a romantic partner?","What influences love at first sight? (Or, at least, love in the first four minutes?) This [dataset][1] was compiled by Columbia Business School professors Ray Fisman and Sheena Iyengar for their paper [Gender Differences in Mate Selection: Evidence From a Speed Dating Experiment][2].

Data was gathered from participants in experimental speed dating events from 2002-2004. During the events, the attendees would have a four minute ""first date"" with every other participant of the opposite sex.  At the end of their four minutes, participants were asked if they would like to see their date again. They were also asked to rate their date on six attributes: Attractiveness, Sincerity, Intelligence, Fun, Ambition, and Shared Interests. 

The dataset also includes questionnaire data gathered from participants at different points in the process. These fields include: demographics, dating habits, self-perception across key attributes, beliefs on what others find valuable in a mate, and lifestyle information. See the Speed Dating Data Key document below for details.

For more analysis from Iyengar and Fisman, read [Racial Preferences in Dating][3].

Data Exploration Ideas
----------------------

 - What are the least desirable attributes in a male partner? Does this differ for female partners? 
 - How important do people think attractiveness is in potential mate selection vs. its real impact? 
 - Are shared interests more important than a shared racial background?
 - Can people accurately predict their own perceived value in the dating market? 
 - In terms of getting a second date, is it better to be someone's first speed date of the night or their last? 

  [1]: http://www.stat.columbia.edu/~gelman/arm/examples/speed.dating/
  [2]: http://faculty.chicagobooth.edu/emir.kamenica/documents/genderDifferences.pdf
  [3]: http://faculty.chicagobooth.edu/emir.kamenica/documents/racialpreferences.pdf","2016-05-09 15:14:15","Initial release","speed-dating-experiment.zip",393107,44
64,36,1,"The General Social Survey (GSS)","general-social-survey","Longitudinal study of popular beliefs, attitudes, morality & behaviors in the US","​​The GSS gathers data on contemporary American society in order to monitor and explain trends and constants in attitudes, behaviors, and attributes.  Hundreds of trends have been tracked since 1972. In addition, since the GSS adopted questions from earlier surveys, trends can be followed for up to 70 years.

The GSS contains a standard core of demographic, behavioral, and attitudinal questions, plus topics of special interest. Among the topics covered are civil liberties, crime and violence, intergroup tolerance, morality, national spending priorities, psychological well-being, social mobility, and stress and traumatic events.

Altogether the GSS is the single best source for sociological and attitudinal trend data covering the United States. It allows researchers to examine the structure and functioning of society in general as well as the role played by relevant subgroups and to compare the United States to other nations. ([Source][1])

This dataset is a csv version of the [Cumulative Data File][2], a cross-sectional sample of the GSS from 1972-current.

  [1]: http://gss.norc.org/About-The-GSS
  [2]: http://gss.norc.org/get-the-data/spss","2016-05-10 15:44:47","Initial release","the-general-social-survey-gss.zip",155042889,5
65,37,1,"How ISIS Uses Twitter","how-isis-uses-twitter","Analyze how pro-ISIS fanboys have been using Twitter since the 2015 Paris Attacks.","We scraped over 17,000 tweets from 100+ pro-ISIS fanboys from all over the world since the November 2015 Paris Attacks. We are working with content producers and influencers to develop effective counter-messaging measures against violent extremists at home and abroad. In order to maximize our impact, we need assistance in quickly analyzing message frames. 

The dataset includes the following:

 1. Name
 2. Username
 3. Description
 4. Location
 5. Number of followers at the time the tweet was downloaded
 6. Number of statuses by the user when the tweet was downloaded
 7. Date and timestamp of the tweet
 8. The tweet itself

Based on this data, here are some useful ways of deriving insights and analysis: 

 - **Social Network Cluster Analysis**: Who are the major players in the pro-ISIS twitter network? Ideally, we would like this visualized via a cluster network with the biggest influencers scaled larger than smaller influencers. 
 - **Keyword Analysis**: Which keywords derived from the name, username, description, location, and tweets were the most commonly used by ISIS fanboys? Examples include: ""baqiyah"", ""dabiq"", ""wilayat"", ""amaq""
 - **Data Categorization of Links**: Which websites are pro-ISIS fanboys linking to? Categories include: Mainstream Media, Altermedia, Jihadist Websites, Image Upload, Video Upload, 
 - **Sentiment Analysis**: Which clergy do pro-ISIS fanboys quote the most and which ones do they hate the most? Search the tweets for names of prominent clergy and classify the tweet as positive, negative, or neutral and if negative, include the reasons why.  Examples of clergy they like the most: ""Anwar Awlaki"", ""Ahmad Jibril"", ""Ibn Taymiyyah"", ""Abdul Wahhab"". Examples of clergy that they hate the most: ""Hamza Yusuf"", ""Suhaib Webb"", ""Yaser Qadhi"", ""Nouman Ali Khan"", ""Yaqoubi"". 
 - **Timeline View**: Visualize all the tweets over a timeline and identify peak moments

Further Reading: ""[ISIS Has a Twitter Strategy and It is Terrifying \[Infographic\]][2]""


**About Fifth Tribe**

*[Fifth Tribe][1] is a digital agency based out of DC that serves businesses, non-profits, and government agencies. We provide our clients with product development, branding, web/mobile development, and digital marketing services. Our client list includes Oxfam, Ernst and Young, Kaiser Permanente, Aetna Innovation Health, the U.S. Air Force, and the U.S. Peace Corps. Along with Goldman Sachs International and IBM, we serve on the Private Sector Committee of the Board of the Global Community Engagement and Resilience Fund (GCERF), the first global effort to support local, community-level initiatives aimed at strengthening resilience against violent extremism. In December 2014, we won the anti-ISIS ""Hedaya Hack"" organized by Affinis Labs and hosted at the ""Global Countering Violent Extremism (CVE) Expo "" in Abu Dhabi. Since then, we've been actively involved in working with the open-source community and community content producers in developing counter-messaging campaigns and tools.* 



  [1]: http://www.fifthtribe.com
  [2]: https://medium.com/fifth-tribe-stories/isis-has-a-twitter-strategy-and-it-is-terrifying-7cc059ccf51b#.m3zeluykl","2016-05-13 20:05:17","Initial release: Tweets are not encoded. Please use version 2 or 3 of the dataset. ","how-isis-uses-twitter.zip",3230493,0
66,37,2,"How ISIS Uses Twitter","how-isis-uses-twitter","Analyze how pro-ISIS fanboys have been using Twitter since the 2015 Paris Attacks.","We scraped over 17,000 tweets from 100+ pro-ISIS fanboys from all over the world since the November 2015 Paris Attacks. We are working with content producers and influencers to develop effective counter-messaging measures against violent extremists at home and abroad. In order to maximize our impact, we need assistance in quickly analyzing message frames. 

The dataset includes the following:

 1. Name
 2. Username
 3. Description
 4. Location
 5. Number of followers at the time the tweet was downloaded
 6. Number of statuses by the user when the tweet was downloaded
 7. Date and timestamp of the tweet
 8. The tweet itself

Based on this data, here are some useful ways of deriving insights and analysis: 

 - **Social Network Cluster Analysis**: Who are the major players in the pro-ISIS twitter network? Ideally, we would like this visualized via a cluster network with the biggest influencers scaled larger than smaller influencers. 
 - **Keyword Analysis**: Which keywords derived from the name, username, description, location, and tweets were the most commonly used by ISIS fanboys? Examples include: ""baqiyah"", ""dabiq"", ""wilayat"", ""amaq""
 - **Data Categorization of Links**: Which websites are pro-ISIS fanboys linking to? Categories include: Mainstream Media, Altermedia, Jihadist Websites, Image Upload, Video Upload, 
 - **Sentiment Analysis**: Which clergy do pro-ISIS fanboys quote the most and which ones do they hate the most? Search the tweets for names of prominent clergy and classify the tweet as positive, negative, or neutral and if negative, include the reasons why.  Examples of clergy they like the most: ""Anwar Awlaki"", ""Ahmad Jibril"", ""Ibn Taymiyyah"", ""Abdul Wahhab"". Examples of clergy that they hate the most: ""Hamza Yusuf"", ""Suhaib Webb"", ""Yaser Qadhi"", ""Nouman Ali Khan"", ""Yaqoubi"". 
 - **Timeline View**: Visualize all the tweets over a timeline and identify peak moments

Further Reading: ""[ISIS Has a Twitter Strategy and It is Terrifying \[Infographic\]][2]""


**About Fifth Tribe**

*[Fifth Tribe][1] is a digital agency based out of DC that serves businesses, non-profits, and government agencies. We provide our clients with product development, branding, web/mobile development, and digital marketing services. Our client list includes Oxfam, Ernst and Young, Kaiser Permanente, Aetna Innovation Health, the U.S. Air Force, and the U.S. Peace Corps. Along with Goldman Sachs International and IBM, we serve on the Private Sector Committee of the Board of the Global Community Engagement and Resilience Fund (GCERF), the first global effort to support local, community-level initiatives aimed at strengthening resilience against violent extremism. In December 2014, we won the anti-ISIS ""Hedaya Hack"" organized by Affinis Labs and hosted at the ""Global Countering Violent Extremism (CVE) Expo "" in Abu Dhabi. Since then, we've been actively involved in working with the open-source community and community content producers in developing counter-messaging campaigns and tools.* 



  [1]: http://www.fifthtribe.com
  [2]: https://medium.com/fifth-tribe-stories/isis-has-a-twitter-strategy-and-it-is-terrifying-7cc059ccf51b#.m3zeluykl","2016-05-13 22:04:46","Added .db version of the tweets which contains foreign languages properly encoded.","how-isis-uses-twitter.zip",2854046,1
67,38,1,"Annual Nominal Fish Catches","ices-fish-catch","Explore the impact of overfishing in the Northeast Atlantic region.","The datasets provides data of annual nominal catches of more than 200 species of fish and shellfish in the Northeast Atlantic region, which are officially submitted by 20 [International Council for the Exploration of the Sea (ICES)][1] member countries between 2006 and 2014.

  [1]: http://www.ices.dk","2016-05-15 15:14:18","Initial release","ices-fish-catch.zip",779479,1
68,37,3,"How ISIS Uses Twitter","how-isis-uses-twitter","Analyze how ISIS fanboys have been using Twitter since 2015 Paris Attacks","We scraped over 17,000 tweets from 100+ pro-ISIS fanboys from all over the world since the November 2015 Paris Attacks. We are working with content producers and influencers to develop effective counter-messaging measures against violent extremists at home and abroad. In order to maximize our impact, we need assistance in quickly analyzing message frames. 

The dataset includes the following:

 1. Name
 2. Username
 3. Description
 4. Location
 5. Number of followers at the time the tweet was downloaded
 6. Number of statuses by the user when the tweet was downloaded
 7. Date and timestamp of the tweet
 8. The tweet itself

Based on this data, here are some useful ways of deriving insights and analysis: 

 - **Social Network Cluster Analysis**: Who are the major players in the pro-ISIS twitter network? Ideally, we would like this visualized via a cluster network with the biggest influencers scaled larger than smaller influencers. 
 - **Keyword Analysis**: Which keywords derived from the name, username, description, location, and tweets were the most commonly used by ISIS fanboys? Examples include: ""baqiyah"", ""dabiq"", ""wilayat"", ""amaq""
 - **Data Categorization of Links**: Which websites are pro-ISIS fanboys linking to? Categories include: Mainstream Media, Altermedia, Jihadist Websites, Image Upload, Video Upload, 
 - **Sentiment Analysis**: Which clergy do pro-ISIS fanboys quote the most and which ones do they hate the most? Search the tweets for names of prominent clergy and classify the tweet as positive, negative, or neutral and if negative, include the reasons why.  Examples of clergy they like the most: ""Anwar Awlaki"", ""Ahmad Jibril"", ""Ibn Taymiyyah"", ""Abdul Wahhab"". Examples of clergy that they hate the most: ""Hamza Yusuf"", ""Suhaib Webb"", ""Yaser Qadhi"", ""Nouman Ali Khan"", ""Yaqoubi"". 
 - **Timeline View**: Visualize all the tweets over a timeline and identify peak moments

Further Reading: ""[ISIS Has a Twitter Strategy and It is Terrifying \[Infographic\]][2]""


**About Fifth Tribe**

*[Fifth Tribe][1] is a digital agency based out of DC that serves businesses, non-profits, and government agencies. We provide our clients with product development, branding, web/mobile development, and digital marketing services. Our client list includes Oxfam, Ernst and Young, Kaiser Permanente, Aetna Innovation Health, the U.S. Air Force, and the U.S. Peace Corps. Along with Goldman Sachs International and IBM, we serve on the Private Sector Committee of the Board of the Global Community Engagement and Resilience Fund (GCERF), the first global effort to support local, community-level initiatives aimed at strengthening resilience against violent extremism. In December 2014, we won the anti-ISIS ""Hedaya Hack"" organized by Affinis Labs and hosted at the ""Global Countering Violent Extremism (CVE) Expo "" in Abu Dhabi. Since then, we've been actively involved in working with the open-source community and community content producers in developing counter-messaging campaigns and tools.* 



  [1]: http://www.fifthtribe.com
  [2]: https://medium.com/fifth-tribe-stories/isis-has-a-twitter-strategy-and-it-is-terrifying-7cc059ccf51b#.m3zeluykl","2016-05-16 15:30:51","Converted SQLite database to a UTF-8 CSV file and XLSX spreadsheet (since the CSV is tough to correctly import into Excel)","how-isis-uses-twitter.zip",2572725,4
69,39,1,"101 Innovations - Research Tools Survey","101-innovations-research-tools-survey","Explore global research practices and opinions on scholarly communication","Many new websites and online tools have come into existence to support scholarly communication in all phases of the research workflow. To what extent researchers are using these and more traditional tools has been largely unknown. This 2015-2016 survey aimed to fill that gap. 

![Demographics of 20,663 survey respondents][1]

The survey captured information on tool usage for 17 research activities, stance towards open access and open science, and expectations of the most important development in scholarly communication. Respondents’ demographics included research roles, country of affiliation, research discipline and year of first publication.
The online survey employed an open, non-probability sample. A largely self-selected group of 20,663 researchers, librarians, editors, publishers and other groups involved in research took the survey, which was available in seven languages. The survey was open from May 10, 2015 to February 10, 2016.

This data set contains:

 - Full raw (anonymized) and cleaned data files (csv, each file containing 20,663 records and 178 variables)
 - Variable lists for raw and cleaned data files (csv)
 - Readme file (txt)



The dataset is also deposited in Zenodo: [http://dx.doi.org/10.5281/zenodo.49583][2]

The full description of survey methodology is in a data publication in F1000 Research: [http://dx.doi.org/10.12688/f1000research.8414.1][3]

More information on the project this survey is part of can be found here: [http://101innovations.wordpress.com][4]



Contact:

 - Jeroen Bosman: [http://orcid.org/0000-0001-5796-2727][5] /
   j.bosman@uu.nl
 - Bianca Kramer: [http://orcid.org/0000-0002-5965-6560][6] /
   b.m.r.kramer@uu.nl

  [1]: https://101innovations.files.wordpress.com/2016/04/survey-demographics-doughnut.png
  [2]: http://dx.doi.org/10.5281/zenodo.49583
  [3]: http://dx.doi.org/10.12688/f1000research.8414.1
  [4]: http://101innovations.wordpress.com
  [5]: http://orcid.org/0000-0001-5796-2727
  [6]: http://orcid.org/0000-0002-5965-6560","2016-05-16 21:44:11","Initial release","101-innovations-research-tools-survey.zip",5107276,2
71,39,2,"101 Innovations - Research Tools Survey","101-innovations-research-tools-survey","Explore global research practices and opinions on scholarly communication","Many new websites and online tools have come into existence to support scholarly communication in all phases of the research workflow. To what extent researchers are using these and more traditional tools has been largely unknown. This 2015-2016 survey aimed to fill that gap. 

![Demographics of 20,663 survey respondents][1]

The survey captured information on tool usage for 17 research activities, stance towards open access and open science, and expectations of the most important development in scholarly communication. Respondents’ demographics included research roles, country of affiliation, research discipline and year of first publication.
The online survey employed an open, non-probability sample. A largely self-selected group of 20,663 researchers, librarians, editors, publishers and other groups involved in research took the survey, which was available in seven languages. The survey was open from May 10, 2015 to February 10, 2016.

This data set contains:

 - Full raw (anonymized) and cleaned data files (csv, each file containing 20,663 records and 178 variables)
 - Variable lists for raw and cleaned data files (csv)
 - Readme file (txt)



The dataset is also deposited in Zenodo: [http://dx.doi.org/10.5281/zenodo.49583][2]

The full description of survey methodology is in a data publication in F1000 Research: [http://dx.doi.org/10.12688/f1000research.8414.1][3]

More information on the project this survey is part of can be found here: [http://101innovations.wordpress.com][5]

**[edited to add]** For quick visual exploration of the data, check out the interactive dashboard on Silk: [http://dashboard101innovations.silk.co/][4]

Contact:

 - Jeroen Bosman: [http://orcid.org/0000-0001-5796-2727][6] /
   j.bosman@uu.nl
 - Bianca Kramer: [http://orcid.org/0000-0002-5965-6560][7] /
   b.m.r.kramer@uu.nl


  [1]: https://101innovations.files.wordpress.com/2016/04/survey-demographics-doughnut.png
  [2]: http://dx.doi.org/10.5281/zenodo.49583
  [3]: http://dx.doi.org/10.12688/f1000research.8414.1
  [4]: http://dashboard101innovations.silk.co/
  [5]: http://101innovations.wordpress.com
  [6]: http://orcid.org/0000-0001-5796-2727
  [7]: http://orcid.org/0000-0002-5965-6560","2016-05-17 20:09:03","Simplified file names","101-innovations-research-tools-survey.zip",5106270,11
72,37,4,"How ISIS Uses Twitter","how-isis-uses-twitter","Analyze how ISIS fanboys have been using Twitter since 2015 Paris Attacks","We scraped over 17,000 tweets from 100+ pro-ISIS fanboys from all over the world since the November 2015 Paris Attacks. We are working with content producers and influencers to develop effective counter-messaging measures against violent extremists at home and abroad. In order to maximize our impact, we need assistance in quickly analyzing message frames. 

The dataset includes the following:

 1. Name
 2. Username
 3. Description
 4. Location
 5. Number of followers at the time the tweet was downloaded
 6. Number of statuses by the user when the tweet was downloaded
 7. Date and timestamp of the tweet
 8. The tweet itself

Based on this data, here are some useful ways of deriving insights and analysis: 

 - **Social Network Cluster Analysis**: Who are the major players in the pro-ISIS twitter network? Ideally, we would like this visualized via a cluster network with the biggest influencers scaled larger than smaller influencers. 
 - **Keyword Analysis**: Which keywords derived from the name, username, description, location, and tweets were the most commonly used by ISIS fanboys? Examples include: ""baqiyah"", ""dabiq"", ""wilayat"", ""amaq""
 - **Data Categorization of Links**: Which websites are pro-ISIS fanboys linking to? Categories include: Mainstream Media, Altermedia, Jihadist Websites, Image Upload, Video Upload, 
 - **Sentiment Analysis**: Which clergy do pro-ISIS fanboys quote the most and which ones do they hate the most? Search the tweets for names of prominent clergy and classify the tweet as positive, negative, or neutral and if negative, include the reasons why.  Examples of clergy they like the most: ""Anwar Awlaki"", ""Ahmad Jibril"", ""Ibn Taymiyyah"", ""Abdul Wahhab"". Examples of clergy that they hate the most: ""Hamza Yusuf"", ""Suhaib Webb"", ""Yaser Qadhi"", ""Nouman Ali Khan"", ""Yaqoubi"". 
 - **Timeline View**: Visualize all the tweets over a timeline and identify peak moments

Further Reading: ""[ISIS Has a Twitter Strategy and It is Terrifying \[Infographic\]][2]""


**About Fifth Tribe**

*[Fifth Tribe][1] is a digital agency based out of DC that serves businesses, non-profits, and government agencies. We provide our clients with product development, branding, web/mobile development, and digital marketing services. Our client list includes Oxfam, Ernst and Young, Kaiser Permanente, Aetna Innovation Health, the U.S. Air Force, and the U.S. Peace Corps. Along with Goldman Sachs International and IBM, we serve on the Private Sector Committee of the Board of the Global Community Engagement and Resilience Fund (GCERF), the first global effort to support local, community-level initiatives aimed at strengthening resilience against violent extremism. In December 2014, we won the anti-ISIS ""Hedaya Hack"" organized by Affinis Labs and hosted at the ""Global Countering Violent Extremism (CVE) Expo "" in Abu Dhabi. Since then, we've been actively involved in working with the open-source community and community content producers in developing counter-messaging campaigns and tools.* 



  [1]: http://www.fifthtribe.com
  [2]: https://medium.com/fifth-tribe-stories/isis-has-a-twitter-strategy-and-it-is-terrifying-7cc059ccf51b#.m3zeluykl","2016-05-17 23:53:32","Individual files consistent with the previous version. This should fix the archive download.","how-isis-uses-twitter.zip",2572697,23
76,42,1,"World of Warcraft Avatar History","warcraft-avatar-history","Track the players of this popular online game","Overview
--------

The World of Warcraft Avatar History Dataset is a collection of records that detail information about player characters in the game over time. It includes information about their character level, race, class, location, and social guild. The Kaggle version of this dataset includes only the information from 2008 (and the dataset in general only includes information from the 'Horde' faction of players in the game from a single game server).

 - Full Dataset Source and Information:
   [http://mmnet.iis.sinica.edu.tw/dl/wowah/][1] 
 - Code used to clean the data: [https://github.com/myles-oneill/WoWAH-parser][2] 

Ideas for Using the Dataset
---------------------------

From the perspective of game system designers, players' behavior is one of the most important factors they must consider when designing game systems. To gain a fundamental understanding of the game play behavior of online gamers, exploring users' game play time provides a good starting point. This is because the concept of game play time is applicable to all genres of games and it enables us to model the system workload as well as the impact of system and network QoS on users' behavior. It can even help us predict players' loyalty to specific games.

Open Questions
--------------

 - Understand user gameplay behavior (game sessions, movement, leveling)
 - Understand user interactions (guilds)
 - Predict players unsubscribing from the game based on activity
 - What are the most popular zones in WoW, what level players tend to inhabit each?

  [1]: http://mmnet.iis.sinica.edu.tw/dl/wowah/
  [2]: https://github.com/myles-oneill/WoWAH-parser
","2016-05-19 23:46:20","Initial release","warcraft-avatar-history.zip",80924144,5
77,43,1,"Game of Thrones","game-of-thrones","Explore deaths and battles from this fantasy world","Overview
--------

Game of Thrones is a hit fantasy tv show based on the equally famous book series ""A Song of Fire and Ice"" by George RR Martin. The show is well known for its vastly complicated political landscape, large number of characters, and its frequent character deaths.

Data Sources
------------

This dataset combines three sources of data, all of which are based on information from the book series.

 - Firstly, there is **battles.csv** which contains Chris Albon's ""The
   War of the Five Kings"" Dataset, which can be found here:
   https://github.com/chrisalbon/war_of_the_five_kings_dataset . Its a
   great collection of all of the battles in the series.

 - Secondly we have **character-deaths.csv** from Erin Pierce and Ben
   Kahle. This dataset was created as a part of their Bayesian Survival
   Analysis which can be found here:
   http://allendowney.blogspot.com/2015/03/bayesian-survival-analysis-for-game-of.html

 - Finally we have a more comprehensive character dataset with
   **character-predictions.csv**. This comes from the team at A Song of Ice and Data who scraped it from  http://awoiaf.westeros.org/ . It
   also includes their predictions on which character will die, the
   methodology of which can be found here:
   https://got.show/machine-learning-algorithm-predicts-death-game-of-thrones

What insights about the complicated political landscape of this fantasy world can you find in this data?

Of course, it goes without saying that this dataset contains spoilers ;)","2016-05-20 01:32:31","Initial release","game-of-thrones.zip",65992,56
79,45,1,"Detailed NFL Play-by-Play Data 2015","nflplaybyplay2015","An NFL dataset generated by the nflscrapR R-package & primed for analysis","# Introduction 

The lack of publicly available National Football League (NFL) data sources has been a major obstacle in the creation of modern, reproducible research in football analytics.  While clean play-by-play data is available via open-source software packages in other sports (e.g. nhlscrapr for hockey; PitchF/x data in baseball; the NBA API for basketball), the equivalent datasets are not freely available for researchers interested in the statistical analysis of the NFL.  To solve this issue, a group of [Carnegie Mellon University statistical researchers](http://www.stat.cmu.edu) led by recent graduate, Maksim Horowitz, built and released [nflscrapR](https://github.com/maksimhorowitz/nflscrapR) an R package which uses an API maintained by the NFL to scrape, clean, parse, and output clean datasets at the individual play, player, game, and season levels.  These datasets allow for the advancement of NFL research in the public domain by allowing analysts to develop from a common source in order to create reproducible NFL research, similar to what is being done currently in other professional sports. 

## 2015 NFL Play-by-Play Dataset

The dataset made available on Kaggle contains all the regular season plays from the 2015-2016 NFL season.  The dataset contain **46,129** rows and **63** columns.  Each play is broken down into great detail containing information on; game situation, players involved and results.  Detailed information about the dataset can be found in the **nflscrapR** documentation.


## Downloading and Installing nflscrapR: 
### Use the following code in your R console:

    # Must install the devtools package using the below code
    install.packages('devtools')
    library(devtools)
    # For now you must install nflscrapR from github
    if (!is.element(""nflscrapR"", installed.packages())) {
        # Print Installing nflscrapR
        devtools::install_github(repo = ""maksimhorowitz/nflscrapR"")
    }

    library(nflscrapR)","2016-05-20 19:30:33","Initial release","nflplaybyplay2015.zip",2651592,9
82,42,2,"World of Warcraft Avatar History","warcraft-avatar-history","Track the players of this popular online game","Overview
--------

The World of Warcraft Avatar History Dataset is a collection of records that detail information about player characters in the game over time. It includes information about their character level, race, class, location, and social guild. The Kaggle version of this dataset includes only the information from 2008 (and the dataset in general only includes information from the 'Horde' faction of players in the game from a single game server).

 - Full Dataset Source and Information:
   [http://mmnet.iis.sinica.edu.tw/dl/wowah/][1] 
 - Code used to clean the data: [https://github.com/myles-oneill/WoWAH-parser][2] 

Ideas for Using the Dataset
---------------------------

From the perspective of game system designers, players' behavior is one of the most important factors they must consider when designing game systems. To gain a fundamental understanding of the game play behavior of online gamers, exploring users' game play time provides a good starting point. This is because the concept of game play time is applicable to all genres of games and it enables us to model the system workload as well as the impact of system and network QoS on users' behavior. It can even help us predict players' loyalty to specific games.

Open Questions
--------------

 - Understand user gameplay behavior (game sessions, movement, leveling)
 - Understand user interactions (guilds)
 - Predict players unsubscribing from the game based on activity
 - What are the most popular zones in WoW, what level players tend to inhabit each?

Wrath of the Lich King
----------------------

An expansion to World of Warcraft, ""Wrath of the Lich King"" (Wotlk) was released on November 13, 2008. It introduced new zones for players to go to, a new character class (the death knight), and a new level cap of 80 (up from 70 previously). This event intersects nicely with the dataset and is probably interesting to investigate.

Map
---

This dataset doesn't include a shapefile (if you know of one that exists, let me know!) to show where the zones the dataset talks about are. Here is a list of zones an information from this version of the game, including their recommended levels: http://wowwiki.wikia.com/wiki/Zones_by_level_(original) . 

  [1]: http://mmnet.iis.sinica.edu.tw/dl/wowah/
  [2]: https://github.com/myles-oneill/WoWAH-parser","2016-05-20 23:47:42","Fixed a bug in the code generating the data. Now the races, classes, and zones are able to have multiple words each.","warcraft-avatar-history.zip",91748261,12
83,47,1,"Airplane Crashes Since 1908","airplane-crashes-since-1908","Full history of airplane crashes throughout the world, from 1908-present"," Analysis of the public dataset:
""Airplane Crashes and Fatalities Since 1908"" (Full history of airplane crashes throughout the world, from 1908-present) hosted by Open Data by Socrata available at:

https://opendata.socrata.com/Government/Airplane-Crashes-and-Fatalities-Since-1908/q2te-8cvq

Questions

 1. Yearly how many planes crashed? how many people were on board? how many survived? how many died?
 2. Highest number of crashes by operator and Type of aircrafts.
 3. ‘Summary’ field has the details about the crashes. Find the reasons of the crash and categorize them in different clusters i.e Fire, shot down, weather (for the ‘Blanks’ in the data category can be UNKNOWN) you are open to make clusters of your choice but they should not exceed 7.
4. Find the number of crashed aircrafts and number of deaths against each category from above step.
 5. Find any interesting trends/behaviors that you encounter when you analyze the dataset.

My solution is in the pdf attached. I was told that other candidates solved it in a better way. Any suggestion is welcome. Thanks.
----------
Following code used for question 6

 6.`Air = read.csv(""3-Airplane_Crashes_Since_1908.txt"")
Air$Fatalities[is.na(Air$Fatalities)] = 0
corpus = Corpus(VectorSource(Air$Summary))
corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, PlainTextDocument)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, stopwords(""english""))
dtm = DocumentTermMatrix(corpus)
dtm = removeSparseTerms(dtm, 0.95)
inspect(dtm[1:10, 501:510])
findFreqTerms(dtm,100)

dtm_tfxidf = weightTfIdf(dtm)
inspect(dtm_tfxidf[1:10, 501:510])

### k-means (this uses euclidean distance)
m = as.matrix(dtm_tfxidf)
rownames(m) = 1:nrow(m)

preproc = preProcess(m)
m_norm = predict(preproc, m)
cl = kmeans(m_norm, centers = 7)

table(cl$cluster)

### show clusters using the first 2 principal components
plot(prcomp(m_norm)$x, col=cl$cl)

findFreqTerms(dtm[cl$cluster==1,], 50)
findFreqTerms(dtm[cl$cluster==2,], 50)
findFreqTerms(dtm[cl$cluster==3,], 50)
findFreqTerms(dtm[cl$cluster==4,], 50)
findFreqTerms(dtm[cl$cluster==5,], 50)
findFreqTerms(dtm[cl$cluster==6,], 50)
findFreqTerms(dtm[cl$cluster==7,], 50)

sum(Air$Fatalities[which(cl$cluster==1)])
sum(Air$Fatalities[which(cl$cluster==2)])
sum(Air$Fatalities[which(cl$cluster==3)])
sum(Air$Fatalities[which(cl$cluster==4)])
sum(Air$Fatalities[which(cl$cluster==5)])
sum(Air$Fatalities[which(cl$cluster==6)])
sum(Air$Fatalities[which(cl$cluster==7)])

inspect(corpus[which(cl$cluster==1)])
`","2016-05-21 08:23:53","Initial release","airplane-crashes-since-1908.zip",500698,0
84,47,2,"Airplane Crashes Since 1908","airplane-crashes-since-1908","Full history of airplane crashes throughout the world, from 1908-present"," Analysis of the public dataset:
""Airplane Crashes and Fatalities Since 1908"" (Full history of airplane crashes throughout the world, from 1908-present) hosted by Open Data by Socrata available at:

https://opendata.socrata.com/Government/Airplane-Crashes-and-Fatalities-Since-1908/q2te-8cvq

Questions

 1. Yearly how many planes crashed? how many people were on board? how many survived? how many died?
 2. Highest number of crashes by operator and Type of aircrafts.
 3. ‘Summary’ field has the details about the crashes. Find the reasons of the crash and categorize them in different clusters i.e Fire, shot down, weather (for the ‘Blanks’ in the data category can be UNKNOWN) you are open to make clusters of your choice but they should not exceed 7.
4. Find the number of crashed aircrafts and number of deaths against each category from above step.
 5. Find any interesting trends/behaviors that you encounter when you analyze the dataset.


----------


My solution

 1. The following bar charts display the answers requested by point 2.a of the assignment, in particular:
•	the planes crashed per year
•	people aboard per year during crashes
•	people dead per year during crashes
•	people survived per year during crashes
![enter image description here][1]
 2. The following answers regard point 2.b of the assignment
•	Highest number of crashes by operator: Aeroflot with 179 crashes
•	By Type of aircraft:
	Douglas DC-3 with 334 crashes
 3. List item
 4. List item
 5. I have identified 7 clusters using k-means clustering technique on a matrix obtained by a text corpus created by using Text Analysis (plain text, remove punctuation, to lower, etc.)
The following table summarize for each cluster the number of crashes and death.

	Cluster 1	Cluster 2	Cluster 3	Cluster 4	Cluster 5	Cluster 6	Cluster 7
N. of crashes per cluster	258	500	211	1014	2749	195	341
N. of deaths per cluster	6368	9408	3513	14790	58826	4439	8135

The following picture shows clusters using the first 2 principal components:


  [1]: https://www.kaggle.com/saurograndi/airplane-crashes-since-1908/downloads/plots.png","2016-05-21 14:40:25","-","airplane-crashes-since-1908.zip",696376,0
85,47,3,"Airplane Crashes Since 1908","airplane-crashes-since-1908","Full history of airplane crashes throughout the world, from 1908-present","Analysis of the public dataset:
""Airplane Crashes and Fatalities Since 1908"" (Full history of airplane crashes throughout the world, from 1908-present) hosted by Open Data by Socrata available at:

https://opendata.socrata.com/Government/Airplane-Crashes-and-Fatalities-Since-1908/q2te-8cvq

Questions

 1. Yearly how many planes crashed? how many people were on board? how many survived? how many died?
 2. Highest number of crashes by operator and Type of aircrafts.
 3. ‘Summary’ field has the details about the crashes. Find the reasons of the crash and categorize them in different clusters i.e Fire, shot down, weather (for the ‘Blanks’ in the data category can be UNKNOWN) you are open to make clusters of your choice but they should not exceed 7.
4. Find the number of crashed aircrafts and number of deaths against each category from above step.
 5. Find any interesting trends/behaviors that you encounter when you analyze the dataset.


----------


My solution

 1. The following bar charts display the answers requested by point 1. of the assignment, in particular:
•	the planes crashed per year
•	people aboard per year during crashes
•	people dead per year during crashes
•	people survived per year during crashes
![enter image description here][1]
 2. The following answers regard point 2 of the assignment
•	Highest number of crashes by operator: Aeroflot with 179 crashes
•	By Type of aircraft:
	Douglas DC-3 with 334 crashes
 3. List item
 4. List item
 5. I have identified 7 clusters using k-means clustering technique on a matrix obtained by a text corpus created by using Text Analysis (plain text, remove punctuation, to lower, etc.)
The following table summarize for each cluster the number of crashes and death.

	Cluster 1	Cluster 2	Cluster 3	Cluster 4	Cluster 5	Cluster 6	Cluster 7
N. of crashes per cluster	258	500	211	1014	2749	195	341
N. of deaths per cluster	6368	9408	3513	14790	58826	4439	8135

The following picture shows clusters using the first 2 principal components:
![enter image description here][2]
For each clusters I will summarize the most used words and I will try to identify the causes of the crash

Cluster 1 (258)
aircraft 	crashed  	plane    	shortly  	taking
No many information about this cluster can be deducted using Text Analysis

Cluster 2 (500)
aircraft 	airport  	altitude 	crashed  	crew     		due     
engine   	failed   	failure  	fire     	flight  		landing
lost     	pilot    	plane    	runway   	takeoff  		taking
Engine failure on the runway after landing or takeoff

Cluster 3 (211)
aircraft 	crashed  	fog
Crash caused by fog

Cluster 4 (1014)
aircraft   	airport    	attempting 	cargo      	crashed   
fire       	land       	landing    	miles      	pilot     
plane      	route      	runway     	struck     	takeoff
Struck a cargo during landing or takeoff

Cluster 5 (2749)
accident   	aircraft   	airport    	altitude   	approach  
attempting 	cargo      	conditions 	control    	crashed   
crew       	due        	engine     	failed     	failure   
feet       	fire       	flight     	flying     	fog       
ground     	killed     	land       	landing    	lost      
low        	miles      	mountain   	pilot      	plane     
poor       	route      	runway     	short      	shortly   
struck     	takeoff    	taking     	weather	
Struck a cargo due to engine failure or bad weather conditions mainly fog

Cluster 6 (195)
aircraft 	crashed  	engine   	failure  	fire     	flight  
left     	pilot    	plane    	runway		
Engine failure on the runway

Cluster 7 (341)
accident 	aircraft 	altitude 	cargo    	control  	crashed
crew     	due      	engine   	failure  	flight   	landing
loss     	lost     	pilot    	plane    	takeoff	
Engine failure during landing or takeoff


----------
Better solutions is welcome. Thanks.

  [1]: https://www.kaggle.com/saurograndi/airplane-crashes-since-1908/downloads/plots.png
  [2]: https://www.kaggle.com/saurograndi/airplane-crashes-since-1908/downloads/plots_2.png","2016-05-21 14:52:45","-","airplane-crashes-since-1908.zip",799438,16
86,48,1,"SEPTA - Regional Rail","on-time-performance","Predict  Arrival Times of Philadelphia's Regional Trains. ","**SEPTA - Southeastern Pennsylvania Transportation Authority** 

The SEPTA Regional Rail system consists of commuter rail service on 13 branches to more than 150 active stations in Philadelphia, Pennsylvania, and its suburbs and satellite cities.  

SEPTA uses On-Time Performance (OTP)  to measure service reliability. OTP identifies the number of trains for all rail lines that arrive at their scheduled destination at the scheduled time.  However, by industry standard,  a train may arrive up to 5 minutes and 59 seconds after its scheduled time and still be considered on-time.  

SEPTA has established an annual goal of 91% for Regional Rail On-Time Performance.  How well are they doing?  Is it even a meaningful measure?

The columns in this dataset are:

 - train_id
 - direction   *('N' or 'S'   direction is demarcated as either Northbound or Southbound)*
 - origin        *(See map below - you'll see 'Warminster', 'Glenside',...'Airport Terminal..')*
 - next_station  *(Treat this as station stop)* 
 - date
 - status    *('On Time', '5 min', '10 min'.  This is a status on train lateness)*
 - timeStamp


**What To Look For...**

Ah, as a commuter you care more about the performance of the train(s) you plan to take.  OTP maybe 91% or above; but, if the train **you take** runs late, you'll spend extra time on the tracks.  If it consistently runs late, you may be able to take your time getting to the station.  

As regular commuters, we instinctively look for patterns. For example, during the Tuesday morning rush, do some trains run consistently late?   Do some trains always run consistently late across all days in the schedule, pointing to a possible schedule changed?  Which trains can you count on being reliable?  How long does it take to get to the Airport from Elkins Park, between 9am and 11am, during the week, during the weekend?  

----------


Below is a map of the system and station stops.  This dataset contains data on the Regional Rail Lines.

![SEPTA map][1]

SEPTA train schedules can be found [here](http://www.septa.org/schedules/rail/).  Note different Weekday, Saturday, and Sunday schedules.   



  [1]: https://raw.githubusercontent.com/mchirico/mchirico.github.io/master/p/images/septaMap.png","2016-05-24 01:52:03","Initial release","septaotp.zip",33471687,2
87,48,2,"SEPTA - Regional Rail","on-time-performance","Predict  Arrival Times of Philadelphia's Regional Trains. ","**SEPTA - Southeastern Pennsylvania Transportation Authority** 

The SEPTA Regional Rail system consists of commuter rail service on 13 branches to more than 150 active stations in Philadelphia, Pennsylvania, and its suburbs and satellite cities.  

SEPTA uses On-Time Performance (OTP)  to measure service reliability. OTP identifies the number of trains for all rail lines that arrive at their scheduled destination at the scheduled time.  However, by industry standard,  a train may arrive up to 5 minutes and 59 seconds after its scheduled time and still be considered on-time.  

SEPTA has established an annual goal of 91% for Regional Rail On-Time Performance.  How well are they doing?  Is it even a meaningful measure?

The columns in this dataset are:

 - train_id
 - direction   *('N' or 'S'   direction is demarcated as either Northbound or Southbound)*
 - origin        *(See map below - you'll see 'Warminster', 'Glenside',...'Airport Terminal..')*
 - next_station  *(Treat this as station stop)* 
 - date
 - status    *('On Time', '5 min', '10 min'.  This is a status on train lateness)*
 - timeStamp


**What To Look For...**

Ah, as a commuter you care more about the performance of the train(s) you plan to take.  OTP maybe 91% or above; but, if the train **you take** runs late, you'll spend extra time on the tracks.  If it consistently runs late, maybe the schedule should be changed.

Look for patterns. For example, during the Tuesday morning rush do some trains run consistently late?   How long does it take to get to from point A to point B in the system?  Performance is VERY important to SEPTA. 

----------


Below is a map of the system and station stops.  This dataset contains data on the Regional Rail Lines.

![SEPTA map][1]

SEPTA train schedules can be found [here](http://www.septa.org/schedules/rail/).  Note different Weekday, Saturday, and Sunday schedules.   



  [1]: https://raw.githubusercontent.com/mchirico/mchirico.github.io/master/p/images/septaMap.png","2016-05-24 19:17:09","SQLite database name change was made, so that docker images will read the data. In addition, uploading yesterday's remaining data.","septaotp.zip",37278665,2
88,48,3,"SEPTA - Regional Rail","on-time-performance","Predict  Arrival Times of Philadelphia's Regional Trains. ","**SEPTA - Southeastern Pennsylvania Transportation Authority** 

The SEPTA Regional Rail system consists of commuter rail service on 13 branches to more than 150 active stations in Philadelphia, Pennsylvania, and its suburbs and satellite cities.  

SEPTA uses On-Time Performance (OTP)  to measure service reliability. OTP identifies the number of trains for all rail lines that arrive at their scheduled destination at the scheduled time.  However, by industry standard,  a train may arrive up to 5 minutes and 59 seconds after its scheduled time and still be considered on-time.  

SEPTA has established an annual goal of 91% for Regional Rail On-Time Performance.  How well are they doing?  Is it even a meaningful measure?

The columns in this dataset are:

 - train_id
 - direction   *('N' or 'S'   direction is demarcated as either Northbound or Southbound)*
 - origin        *(See map below - you'll see 'Warminster', 'Glenside',...'Airport Terminal..')*
 - next_station  *(Treat this as station stop)* 
 - date
 - status    *('On Time', '5 min', '10 min'.  This is a status on train lateness)*
 - timeStamp


**What To Look For...**

Ah, as a commuter you care more about the performance of the train(s) you plan to take.  OTP maybe 91% or above; but, if the train **you take** runs late, you'll spend extra time on the tracks.  If it consistently runs late, maybe the schedule should be changed.

Look for patterns. For example, during the Tuesday morning rush do some trains run consistently late?   How long does it take to get to from point A to point B in the system?  Performance is VERY important to SEPTA. 

----------


Below is a map of the system and station stops.  This dataset contains data on the Regional Rail Lines.

![SEPTA map][1]

SEPTA train schedules can be found [here](http://www.septa.org/schedules/rail/).  Note different Weekday, Saturday, and Sunday schedules.   



  [1]: https://raw.githubusercontent.com/mchirico/mchirico.github.io/master/p/images/septaMap.png","2016-05-24 21:39:09","Trouble reading last dataset...trying to load this again.","on-time-performance.zip",37379435,0
90,48,4,"SEPTA - Regional Rail","on-time-performance","Predict  Arrival Times of Philadelphia's Regional Trains. ","**SEPTA - Southeastern Pennsylvania Transportation Authority** 

The SEPTA Regional Rail system consists of commuter rail service on 13 branches to more than 150 active stations in Philadelphia, Pennsylvania, and its suburbs and satellite cities.  

SEPTA uses On-Time Performance (OTP)  to measure service reliability. OTP identifies the number of trains for all rail lines that arrive at their scheduled destination at the scheduled time.  However, by industry standard,  a train may arrive up to 5 minutes and 59 seconds after its scheduled time and still be considered on-time.  

SEPTA has established an annual goal of 91% for Regional Rail On-Time Performance.  How well are they doing?  Is it even a meaningful measure?

The columns in this dataset are:

 - train_id
 - direction   *('N' or 'S'   direction is demarcated as either Northbound or Southbound)*
 - origin        *(See map below - you'll see 'Warminster', 'Glenside',...'Airport Terminal..')*
 - next_station  *(Treat this as station stop)* 
 - date
 - status    *('On Time', '5 min', '10 min'.  This is a status on train lateness)*
 - timeStamp


**What To Look For...**

Ah, as a commuter you care more about the performance of the train(s) you plan to take.  OTP maybe 91% or above; but, if the train **you take** runs late, you'll spend extra time on the tracks.  If it consistently runs late, maybe the schedule should be changed.

Look for patterns. For example, during the Tuesday morning rush do some trains run consistently late?   How long does it take to get to from point A to point B in the system?  Performance is VERY important to SEPTA. 

----------


Below is a map of the system and station stops.  This dataset contains data on the Regional Rail Lines.

![SEPTA map][1]

SEPTA train schedules can be found [here](http://www.septa.org/schedules/rail/).  Note different Weekday, Saturday, and Sunday schedules.   



  [1]: https://raw.githubusercontent.com/mchirico/mchirico.github.io/master/p/images/septaMap.png","2016-05-25 00:48:43","database.sqlite.zip was created incorrectly in the previous version.  Data was updated from the last few hours. Still testing.","on-time-performance.zip",37508589,0
91,48,5,"SEPTA - Regional Rail","on-time-performance","Predict  Arrival Times of Philadelphia's Regional Trains. ","**SEPTA - Southeastern Pennsylvania Transportation Authority** 

The SEPTA Regional Rail system consists of commuter rail service on 13 branches to more than 150 active stations in Philadelphia, Pennsylvania, and its suburbs and satellite cities.  

SEPTA uses On-Time Performance (OTP)  to measure service reliability. OTP identifies the number of trains for all rail lines that arrive at their scheduled destination at the scheduled time.  However, by industry standard,  a train may arrive up to 5 minutes and 59 seconds after its scheduled time and still be considered on-time.  

SEPTA has established an annual goal of 91% for Regional Rail On-Time Performance.  How well are they doing?  Is it even a meaningful measure?

The columns in this dataset are:

 - train_id
 - direction   *('N' or 'S'   direction is demarcated as either Northbound or Southbound)*
 - origin        *(See map below - you'll see 'Warminster', 'Glenside',...'Airport Terminal..')*
 - next_station  *(Think of this as the station stop, at timeStamp)* 
 - date
 - status    *('On Time', '5 min', '10 min'.  This is a status on train lateness)*
 - timeStamp


**What To Look For...**

Ah, as a commuter you care more about the performance of the train(s) you plan to take.  OTP maybe 91% or above; but, if the train **you take** runs late, you'll spend extra time on the tracks.  If it consistently runs late, maybe the schedule should be changed.

Look for patterns. For example, during the Tuesday morning rush do some trains run consistently late?   How long does it take to get to from point A to point B in the system?  Performance is VERY important to SEPTA. 

----------


Below is a map of the system and station stops.  This dataset contains data on the Regional Rail Lines.

![SEPTA map][1]

SEPTA train schedules can be found [here](http://www.septa.org/schedules/rail/).  Note different Weekday, Saturday, and Sunday schedules.   



  [1]: https://raw.githubusercontent.com/mchirico/mchirico.github.io/master/p/images/septaMap.png","2016-05-25 01:12:48","Running through complete check of upload process.","on-time-performance.zip",37441627,1
92,48,6,"SEPTA - Regional Rail","on-time-performance","Predict  Arrival Times of Philadelphia's Regional Trains. ","**SEPTA - Southeastern Pennsylvania Transportation Authority** 

The SEPTA Regional Rail system consists of commuter rail service on 13 branches to more than 150 active stations in Philadelphia, Pennsylvania, and its suburbs and satellite cities.  

SEPTA uses On-Time Performance (OTP)  to measure service reliability. OTP identifies the number of trains for all rail lines that arrive at their scheduled destination at the scheduled time.  However, by industry standard,  a train may arrive up to 5 minutes and 59 seconds after its scheduled time and still be considered on-time.  

SEPTA has established an annual goal of 91% for Regional Rail On-Time Performance.  How well are they doing?  Is it even a meaningful measure?

The columns in this dataset are:

 - train_id
 - direction   *('N' or 'S'   direction is demarcated as either Northbound or Southbound)*
 - origin        *(See map below - you'll see 'Warminster', 'Glenside',...'Airport Terminal..')*
 - next_station  *(Think of this as the station stop, at timeStamp)* 
 - date
 - status    *('On Time', '5 min', '10 min'.  This is a status on train lateness. 999 is a suspended train)*
 - timeStamp


**What To Look For...**

Ah, as a commuter you care more about the performance of the train(s) you plan to take.  OTP maybe 91% or above; but, if the train **you take** runs late, you'll spend extra time on the tracks.  If it consistently runs late, maybe the schedule should be changed.

Look for patterns. For example, during the Tuesday morning rush do some trains run consistently late?   How long does it take to get to from point A to point B in the system?  Performance is VERY important to SEPTA. 

----------


Below is a map of the system and station stops.  This dataset contains data on the Regional Rail Lines.

![SEPTA map][1]

SEPTA train schedules can be found [here](http://www.septa.org/schedules/rail/).  Note different Weekday, Saturday, and Sunday schedules.   



  [1]: https://raw.githubusercontent.com/mchirico/mchirico.github.io/master/p/images/septaMap.png","2016-05-25 19:16:13","2016-05-25 3:00 PM (EST) update ... may take up to 90 minutes for Docker image to recognize.","on-time-performance.zip",37877264,4
97,52,1,"Drosophila Melanogaster Genome","drosophila-melanogaster-genome","Explore the annotated genome of the common fruit fly","Drosophila Melanogaster
-----------------------

Drosophila Melanogaster, the common fruit fly, is a model organism which has been extensively used in entymological research. It is one of the most studied organisms in biological research, particularly in genetics and developmental biology. 

When its not being used for scientific research, *D. melanogaster* is a common pest in homes, restaurants, and anywhere else that serves food. They are not to be confused with Tephritidae flys (also known as fruit flys). 

https://en.wikipedia.org/wiki/Drosophila_melanogaster

About the Genome
----------------

This genome was first sequenced in 2000. It contains four pairs of chromosomes (2,3,4 and X/Y). More than 60% of the genome appears to be functional non-protein-coding DNA. 

![D. melanogaster chromosomes][1]

The genome is maintained and frequently updated at [FlyBase][2]. This dataset is sourced from the UCSC Genome Bioinformatics download page. It uses the August 2014 version of the D. melanogaster genome (dm6, BDGP Release 6 + ISO1 MT). http://hgdownload.soe.ucsc.edu/downloads.html#fruitfly

Files were modified by Kaggle to be a better fit for analysis on Scripts. This primarily involved turning files into CSV format, with a header row, as well as converting the genome itself from 2bit format into a FASTA sequence file.

Bioinformatics
--------------

Genomic analysis can be daunting to data scientists who haven't had much experience with bioinformatics before. We have tried to give basic explanations to each of the files in this dataset, as well as links to further reading on the biological basis for each. If you haven't had the chance to study much biology before, some light reading (ie wikipedia) on the following topics may be helpful to understand the nuances of the data provided here: [Genetics][3], [Genomics][4] ([Sequencing][5]/[Genome Assembly][6]), [Chromosomes][7], [DNA][8], [RNA][9] ([mRNA][10]/[miRNA][11]), [Genes][12], [Alleles][13], [Exons][14], [Introns][15], [Transcription][16], [Translation][17], [Peptides][18], [Proteins][19], [Gene Regulation][20], [Mutation][21], [Phylogenetics][22], and [SNPs][23]. 

Of course, if you've got some idea of the basics already - don't be afraid to jump right in! 

Learning Bioinformatics
-----------------------

There are a lot of great resources for learning bioinformatics on the web. One cool site is [Rosalind][24] - a platform that gives you bioinformatic coding challenges to complete. You can use Kaggle Scripts on this dataset to easily complete the challenges on Rosalind (and see [Myles' solutions here][25] if you get stuck). We have set up [Biopython][26] on Kaggle's docker image which is a great library to help you with your analyses. Check out their [tutorial here][27] and we've also created [a python notebook with some of the tutorial applied to this dataset][28] as a reference.



Files in this Dataset
---------------------
<hr>
**Drosophila Melanogaster Genome**

 - genome.fa

The assembled genome itself is presented here in [FASTA format][29]. Each chromosome is a different sequence of nucleotides. Repeats from RepeatMasker and Tandem Repeats Finder (with period of 12 or less) are show in lower case; non-repeating sequence is shown in upper case.
<hr>
**Meta Information**

There are 3 additional files with meta information about the genome.

 - meta-cpg-island-ext-unmasked.csv

This file contains descriptive information about CpG Islands in the genome.

https://en.wikipedia.org/wiki/CpG_site

 - meta-cytoband.csv

This file describes the positions of cytogenic bands on each chromosome.

https://en.wikipedia.org/wiki/Cytogenetics

 - meta-simple-repeat.csv

This file describes simple tandem repeats in the genome.

https://en.wikipedia.org/wiki/Repeated_sequence_(DNA)
https://en.wikipedia.org/wiki/Tandem_repeat
<hr>
**Drosophila Melanogaster mRNA Sequences**

Messenger RNA (mRNA) is an intermediate molecule created as part of the cellular process of converting genomic information into proteins. Some mRNA are never translated into proteins and have functional roles in the cell on their own. Collectively, organism mRNA information is known as a Transcriptome. mRNA files included in this dataset give insight into the activity of genes in the organism.

https://en.wikipedia.org/wiki/Messenger_RNA

 - mrna-genbank.fa

This file includes all mRNA sequences from GenBank associated with Drosophila Melanogaster.

http://www.ncbi.nlm.nih.gov/genbank/

 - mrna-refseq.fa

This file includes all mRNA sequences from RefSeq associated with Drosophila Melanogaster.

http://www.ncbi.nlm.nih.gov/refseq/
<hr>
**Gene Predictions**

A gene is a segment of DNA on the genome which, through mRNA, is used to create proteins in the organism. Knowing which parts of DNA are coding (genes) or non-coding is difficult, and a number of different systems for prediction exist. This dataset includes a number of different gene prediction systems applied to the drosophila melanogaster genome.

https://en.wikipedia.org/wiki/Gene_prediction

 - genes-augustus.csv

AUGUSTUS is a piece of software that predicts genes ab initio using Hidden Markov Models.
http://www.ncbi.nlm.nih.gov/pmc/articles/PMC441517/


 - genes-genscan.csv

GENSCAN is an older ab initio software for predicting genes. 
http://genes.mit.edu/GENSCANinfo.html

 - genes-ensembl.csv
 - ensembl-gtp.csv
 - ensembl-pep.csv
 - ensembl-source.csv
 - ensembl-to-gene-name.csv

Ensembl provides gene annotation generated by their software Genebuild. This process combines automatic annotation alongside manual curation. 
http://uswest.ensembl.org/info/genome/genebuild/genome_annotation.html

We have also included some supplementary files for these, including predicted protein peptide sequences for each predicted gene.

 - genes-refseq.csv
 - genes-xeno-refseq.csv
 - refseq-link.csv
 - refseq-summary.csv

We have included two RefSeq gene predictions in this dataset. The first is based solely on information from the drosophila melanogaster genome. The second (genes-xeno-refseq.csv) uses genes from other organisms as a basis for predicting genes in drosophila melanogaster.

RefSeq RNAs were aligned against the D. melanogaster genome using blat; those with an alignment of less than 15% were discarded. When a single RNA aligned in multiple places, the alignment having the highest base identity was identified. Only alignments having a base identity level within 0.1% of the best and at least 96% base identity with the genomic sequence were kept.

We have also included supplementary files for these which include information about the genes that have been identified.

http://www.ncbi.nlm.nih.gov/refseq/
<hr>

What can you do with this data?
-------------------------------

Genomic data is the foundation of bioinformatics, and there is an incredible array of things you can do with this data. A good place to start is to look at some of the meta supplementary files alongside the genomic sequence itself. 

We have a number of different gene prediction systems in the dataset, how do they compare to each other? How do they compare to the mRNA data? 

Working back from the refseq-summary.csv file, you can look at genes that code for particular proteins - can you find these genes in the genome?

How much of the genome codes for the mRNA's found in our mRNA data? Of the mRNA's we have, how many map to the predicted genes and the predicted peptided sequence data? How much of the mRNA seems to be protein-coding vs how much looks like it is miRNA? Can you find pre-mRNA or splice variants within the mRNA data? Does meta information like cytogenic bands or CpG sites correspond with splice variants or a lack of mRNA altogether?

Those are just some of many ideas that could get you started.

Looking for Feedback
--------------------

This is the first genomic dataset on Kaggle and we are looking for feedback from our community about how interesting this dataset is to them, or if there are ways we could improve it to better suit analysis. Please post suggestions for supplementary data, future genomes we could host, bioinformatics packages we should include on scripts, and any other feedback on the dataset forum.


  [1]: https://upload.wikimedia.org/wikipedia/commons/1/1d/Drosophila-chromosome-diagram.jpg
  [2]: http://flybase.org
  [3]: https://en.wikipedia.org/wiki/Genetics
  [4]: https://en.wikipedia.org/wiki/Genomics
  [5]: https://en.wikipedia.org/wiki/Sequencing
  [6]: https://en.wikipedia.org/wiki/Sequence_assembly
  [7]: https://en.wikipedia.org/wiki/Chromosome
  [8]: https://en.wikipedia.org/wiki/DNA
  [9]: https://en.wikipedia.org/wiki/RNA
  [10]: https://en.wikipedia.org/wiki/Messenger_RNA
  [11]: https://en.wikipedia.org/wiki/MicroRNA
  [12]: https://en.wikipedia.org/wiki/Gene
  [13]: https://en.wikipedia.org/wiki/Allele
  [14]: https://en.wikipedia.org/wiki/Exon
  [15]: https://en.wikipedia.org/wiki/Intron
  [16]: https://en.wikipedia.org/wiki/Transcription_(genetics)
  [17]: https://en.wikipedia.org/wiki/Translation_(biology)
  [18]: https://en.wikipedia.org/wiki/Peptide
  [19]: https://en.wikipedia.org/wiki/Protein
  [20]: https://en.wikipedia.org/wiki/Regulation_of_gene_expression
  [21]: https://en.wikipedia.org/wiki/Mutation
  [22]: https://en.wikipedia.org/wiki/Phylogenetics
  [23]: https://en.wikipedia.org/wiki/Single-nucleotide_polymorphism
  [24]: http://rosalind.info/problems/list-view/
  [25]: https://www.kaggle.com/mylesoneill/d/mylesoneill/drosophila-melanogaster-genome/rosalind-problem-solutions
  [26]: http://biopython.org
  [27]: http://biopython.org/DIST/docs/tutorial/Tutorial.html
  [28]: https://www.kaggle.com/mylesoneill/d/mylesoneill/drosophila-melanogaster-genome/getting-started-with-biopython
  [29]: https://en.wikipedia.org/wiki/FASTA_format","2016-05-26 03:21:22","Initial release","drosophila-melanogaster-genome.zip",132707073,12
98,48,7,"SEPTA - Regional Rail","on-time-performance","Predict  Arrival Times of Philadelphia's Regional Trains. ","## SEPTA - Southeastern Pennsylvania Transportation Authority ##
 

The SEPTA Regional Rail system consists of commuter rail service on 13 branches to more than 150 active stations in Philadelphia, Pennsylvania, and its suburbs and satellite cities.  

SEPTA uses On-Time Performance (OTP)  to measure service reliability. OTP identifies the number of trains for all rail lines that arrive at their scheduled destination at the scheduled time.  However, by industry standard,  a train may arrive up to 5 minutes and 59 seconds after its scheduled time and still be considered on-time.  

SEPTA has established an annual goal of 91% for Regional Rail On-Time Performance.  How well are they doing?  Is it even a meaningful measure?


#Data Description

**otp.csv**

 - **train_id**
 - **direction**   *('N' or 'S'   direction is demarcated as either Northbound or Southbound)[^1](https://www.kaggle.com/forums/f/1300/septa-regional-rail/t/21259/why-are-all-trains-originating-in-thorndale-marked-as-southbound)*
 - **origin**        *(See map below - you'll see 'Warminster', 'Glenside',...'Airport Terminal..')*
 - **next_station**  *(Think of this as the station stop, at timeStamp)* 
 - **date**
 - **status**    *('On Time', '5 min', '10 min'.  This is a status on train lateness. 999 is a suspended train)*
 - **timeStamp**


----------


**trainView.csv - GPS Train data (early release)**


Most GPS coordinates are based on track telemetry; however, cars are being equipped with GPS units. 

- **train_id**
- **status**
- **next_station**
- **service**
- **dest**
- **lon**
- **lat**
- **source**
- **track_change**
- **track**
- **date**
- **timeStamp0**   First timeStamp at coordinates.
- **timeStamp1**    Last timeStamp at coordinates.

You can look at the example [here](https://www.kaggle.com/septa/d/septa/on-time-performance/trainview-gps-data-early-beta) on how to track a single train.

**What To Look For...**

Ah, as a commuter you care more about the performance of the train(s) you plan to take.  OTP maybe 91% or above; but, if the train **you take** runs late, you'll spend extra time on the tracks.  If it consistently runs late, maybe the schedule should be changed.

Look for patterns. For example, during the Tuesday morning rush do some trains run consistently late?   How long does it take to get to from point A to point B in the system?  Performance is VERY important to SEPTA. 

----------


Below is a map of the system and station stops.  This dataset contains data on the Regional Rail Lines.

![SEPTA map][1]

SEPTA train schedules can be found [here](http://www.septa.org/schedules/rail/).  Note different Weekday, Saturday, and Sunday schedules.   



  [1]: https://raw.githubusercontent.com/mchirico/mchirico.github.io/master/p/images/septaMap.png","2016-05-27 22:30:50","Updated opt, and added early beta of trainView.csv, which contains GPS (lat,lon) train data.","on-time-performance.zip",57783005,4
99,38,2,"Annual Nominal Fish Catches","ices-fish-catch","Explore the impact of overfishing in the Northeast Atlantic region.","The datasets provides data of annual nominal catches of more than 200 species of fish and shellfish in the Northeast Atlantic region, which are officially submitted by 20 [International Council for the Exploration of the Sea (ICES)][1] member countries between 2006 and 2014.

  [1]: http://www.ices.dk","2016-05-29 08:12:32","The datasets provides data of annual nominal catches of more than 200 species of fish and shellfish in the Northeast Atlantic region, which are officially submitted by 20 [International Council for the Exploration of the Sea (ICES)][1] member countries between 2006 and 2014.

  [1]: http://www.ices.dk","ices-fish-catch.zip",779227,3
100,48,8,"SEPTA - Regional Rail","on-time-performance","Predict  Arrival Times of Philadelphia's Regional Trains. ","## SEPTA - Southeastern Pennsylvania Transportation Authority ##
 

The SEPTA Regional Rail system consists of commuter rail service on 13 branches to more than 150 active stations in Philadelphia, Pennsylvania, and its suburbs and satellite cities.  

SEPTA uses On-Time Performance (OTP)  to measure service reliability. OTP identifies the number of trains for all rail lines that arrive at their scheduled destination at the scheduled time.  However, by industry standard,  a train may arrive up to 5 minutes and 59 seconds after its scheduled time and still be considered on-time.  

SEPTA has established an annual goal of 91% for Regional Rail On-Time Performance.  How well are they doing?  Is it even a meaningful measure?


#Data Description

**otp.csv**

 - **train_id**
 - **direction**   *('N' or 'S'   direction is demarcated as either Northbound or Southbound)[^1](https://www.kaggle.com/forums/f/1300/septa-regional-rail/t/21259/why-are-all-trains-originating-in-thorndale-marked-as-southbound)*
 - **origin**        *(See map below - you'll see 'Warminster', 'Glenside',...'Airport Terminal..')*
 - **next_station**  *(Think of this as the station stop, at timeStamp)* 
 - **date**
 - **status**    *('On Time', '5 min', '10 min'.  This is a status on train lateness. 999 is a suspended train)*
 - **timeStamp**


----------


**trainView.csv - GPS Train data (early release)**


Most GPS coordinates are based on track telemetry; however, cars are being equipped with GPS units. 

- **train_id**
- **status**
- **next_station**
- **service**
- **dest**
- **lon**
- **lat**
- **source**
- **track_change**
- **track**
- **date**
- **timeStamp0**   First timeStamp at coordinates.
- **timeStamp1**    Last timeStamp at coordinates.

You can look at the example [here](https://www.kaggle.com/septa/d/septa/on-time-performance/trainview-gps-data-early-beta) on how to track a single train.

**What To Look For...**

Ah, as a commuter you care more about the performance of the train(s) you plan to take.  OTP maybe 91% or above; but, if the train **you take** runs late, you'll spend extra time on the tracks.  If it consistently runs late, maybe the schedule should be changed.

Look for patterns. For example, during the Tuesday morning rush do some trains run consistently late?   How long does it take to get to from point A to point B in the system?  Performance is VERY important to SEPTA. 

----------


Below is a map of the system and station stops.  This dataset contains data on the Regional Rail Lines.

![SEPTA map][1]

SEPTA train schedules can be found [here](http://www.septa.org/schedules/rail/).  Note different Weekday, Saturday, and Sunday schedules.   



  [1]: https://raw.githubusercontent.com/mchirico/mchirico.github.io/master/p/images/septaMap.png","2016-05-30 16:23:39","Updated data to 2016-05-30. Missing rows discovered in trainView.csv have now been added.","on-time-performance.zip",59875661,1
101,48,9,"SEPTA - Regional Rail","on-time-performance","Predict  Arrival Times of Philadelphia's Regional Trains.","## SEPTA - Southeastern Pennsylvania Transportation Authority ##
 

The SEPTA Regional Rail system consists of commuter rail service on 13 branches to more than 150 active stations in Philadelphia, Pennsylvania, and its suburbs and satellite cities.  

SEPTA uses On-Time Performance (OTP)  to measure service reliability. OTP identifies the number of trains for all rail lines that arrive at their scheduled destination at the scheduled time.  However, by industry standard,  a train may arrive up to 5 minutes and 59 seconds after its scheduled time and still be considered on-time.  

SEPTA has established an annual goal of 91% for Regional Rail On-Time Performance.  How well are they doing?  Is it even a meaningful measure?


#Data Description

**otp.csv**

 - **train_id**
 - **direction**   *('N' or 'S'   direction is demarcated as either Northbound or Southbound)[^1](https://www.kaggle.com/forums/f/1300/septa-regional-rail/t/21259/why-are-all-trains-originating-in-thorndale-marked-as-southbound)*
 - **origin**        *(See map below - you'll see 'Warminster', 'Glenside',...'Airport Terminal..')*
 - **next_station**  *(Think of this as the station stop, at timeStamp)* 
 - **date**
 - **status**    *('On Time', '5 min', '10 min'.  This is a status on train lateness. 999 is a suspended train)*
 - **timeStamp**


----------


**trainView.csv - GPS Train data (early release)**


Most GPS coordinates are based on track telemetry; however, cars are being equipped with GPS units. 

- **train_id**
- **status**
- **next_station**
- **service**
- **dest**
- **lon**
- **lat**
- **source**
- **track_change**
- **track**
- **date**
- **timeStamp0**   First timeStamp at coordinates.
- **timeStamp1**    Last timeStamp at coordinates.

You can look at the example [here](https://www.kaggle.com/septa/d/septa/on-time-performance/trainview-gps-data-early-beta) on how to track a single train.

**What To Look For...**

Ah, as a commuter you care more about the performance of the train(s) you plan to take.  OTP maybe 91% or above; but, if the train **you take** runs late, you'll spend extra time on the tracks.  If it consistently runs late, maybe the schedule should be changed.

Look for patterns. For example, during the Tuesday morning rush do some trains run consistently late?   How long does it take to get to from point A to point B in the system?  Performance is VERY important to SEPTA. 

----------


Below is a map of the system and station stops.  This dataset contains data on the Regional Rail Lines.

![SEPTA map][1]

SEPTA train schedules can be found [here](http://www.septa.org/schedules/rail/).  Note different Weekday, Saturday, and Sunday schedules.   



  [1]: https://raw.githubusercontent.com/mchirico/mchirico.github.io/master/p/images/septaMap.png","2016-06-02 01:08:20","Bringing records current to 2016-06-01. No structural changes.","on-time-performance.zip",62161976,1
102,53,1,"2016 New Coder Survey","2016-new-coder-survey-","A survey of 15,000+ people who are new to software development","[Free Code Camp](https://www.freecodecamp.com) is an open source community where you learn to code and build projects for nonprofits. 

[CodeNewbie.org](https://www.codenewbie.org) is the most supportive community of people learning to code.

Together, we surveyed more than 15,000 people who are actively learning to code. We reached them through the twitter accounts and email lists of various organizations that help people learn to code.

Our goal was to understand these people's motivations in learning to code, how they're learning to code, their demographics, and their socioeconomic background.

We've written in depth about this dataset here: https://medium.freecodecamp.com/we-asked-15-000-people-who-they-are-and-how-theyre-learning-to-code-4104e29b2781#.5mxwnyk80","2016-06-03 00:27:28","Initial release","2016-new-coder-survey-.zip",1968261,20
103,54,1,"Cheltenham's Facebook Group","cheltenham-s-facebook-group","Discover How a Community Leverages Facebook","Facebook is becoming an essential tool for more than just family and friends.  Discover how Cheltenham Township, a diverse community just outside of Philadelphia, deals with major issues such as the Bill Cosby trial, everyday traffic issues, sewer I/I problems and lost cats and dogs.  Communities work when they're connected and exchanging information.  What and who are the essential forces making a positive impact, and when and how do conversational threads get directed or misdirected?  Who trolls, who entertains and who informs?    


**Data Sources**

There are only 4 files in the dataset. It contains every post made to the public Facebook group [**Unofficial Cheltenham Township**](https://www.facebook.com/groups/25160801076/).

 
**post.csv**

These are the main posts you will see on the page.  It might help to take a quick look at the [page](https://www.facebook.com/groups/25160801076/).  Commas in the **msg** field have been replaced with {COMMA}, and apostrophes have been replaced with {APOST}. 

 - **pid**  Main Post id
 - **id**    Id of the user posting
 - **name**  User's name
 - **timeStamp**
 - **shares**
 - **url**
 - **msg**  Text of the message posted. 
 - **likes** Number of likes

**comment.csv**

These are comments to the main post. Note, Facebook postings have comments, and comments on comments.  

 - **pid**  Matches Main Post identifier in post.csv
 - **cid**  Comment Id.
 - **timeStamp**  
 - **id**  Id of user commenting
 - **name**  Name of user commenting
 - **rid**   Id of user responding to first comment
 - **msg**  Message

**like.csv**

These are likes and responses.  The two keys in this file (pid,cid) will join to post and comment respectively.

 - **pid**  Matches Main Post identifier in post.csv
 - **cid**  Matches Comments id.
 - **response**  Response such as LIKE, ANGRY etc.
 - **id**  The id of user responding
 - **name** Name of the user responding

**member.csv**

These are all the members in the group.  Some members never, or rarely, post or comment.

 - **id** Id of the member
 - **name** Name of the member
 - **url**  Url of the member
  ","2016-06-03 02:49:04","Initial release","cheltenham-s-facbook-group.zip",13117771,1
104,54,2,"Cheltenham's Facebook Group","cheltenham-s-facebook-group","Discover How a Community Leverages Facebook","Facebook is becoming an essential tool for more than just family and friends.  Discover how Cheltenham Township, a diverse community just outside of Philadelphia, deals with major issues such as the Bill Cosby trial, everyday traffic issues, sewer I/I problems and lost cats and dogs.  Communities work when they're connected and exchanging information.  What and who are the essential forces making a positive impact, and when and how do conversational threads get directed or misdirected?  Who trolls, who entertains and who informs?    


**Data Sources**

There are only 4 files in the dataset. It contains every post made to the public Facebook group [**Unofficial Cheltenham Township**](https://www.facebook.com/groups/25160801076/).

 
**post.csv**

These are the main posts you will see on the page.  It might help to take a quick look at the [page](https://www.facebook.com/groups/25160801076/).  Commas in the **msg** field have been replaced with {COMMA}, and apostrophes have been replaced with {APOST}. 

 - **pid**  Main Post id
 - **id**    Id of the user posting
 - **name**  User's name
 - **timeStamp**
 - **shares**
 - **url**
 - **msg**  Text of the message posted. 
 - **likes** Number of likes

**comment.csv**

These are comments to the main post. Note, Facebook postings have comments, and comments on comments.  

 - **pid**  Matches Main Post identifier in post.csv
 - **cid**  Comment Id.
 - **timeStamp**  
 - **id**  Id of user commenting
 - **name**  Name of user commenting
 - **rid**   Id of user responding to first comment
 - **msg**  Message

**like.csv**

These are likes and responses.  The two keys in this file (pid,cid) will join to post and comment respectively.

 - **pid**  Matches Main Post identifier in post.csv
 - **cid**  Matches Comments id.
 - **response**  Response such as LIKE, ANGRY etc.
 - **id**  The id of user responding
 - **name** Name of the user responding

**member.csv**

These are all the members in the group.  Some members never, or rarely, post or comment.

 - **id** Id of the member
 - **name** Name of the member
 - **url**  Url of the member
  ","2016-06-03 11:21:02","Populated dataset with current records.","cheltenham-s-facebook-group.zip",13140477,0
105,54,3,"Cheltenham's Facebook Group","cheltenham-s-facebook-group","Discover How a Community Leverages Facebook","Facebook is becoming an essential tool for more than just family and friends.  Discover how Cheltenham Township, a diverse community just outside of Philadelphia, deals with major issues such as the Bill Cosby trial, everyday traffic issues, sewer I/I problems and lost cats and dogs.  Communities work when they're connected and exchanging information.  What and who are the essential forces making a positive impact, and when and how do conversational threads get directed or misdirected?  Who trolls, who entertains and who informs?    


**Data Sources**

There are only 4 files in the dataset. It contains every post made to the public Facebook group [**Unofficial Cheltenham Township**](https://www.facebook.com/groups/25160801076/).

 
**post.csv**

These are the main posts you will see on the page.  It might help to take a quick look at the [page](https://www.facebook.com/groups/25160801076/).  Commas in the **msg** field have been replaced with {COMMA}, and apostrophes have been replaced with {APOST}. 

 - **pid**  Main Post id
 - **id**    Id of the user posting
 - **name**  User's name
 - **timeStamp**
 - **shares**
 - **url**
 - **msg**  Text of the message posted. 
 - **likes** Number of likes

**comment.csv**

These are comments to the main post. Note, Facebook postings have comments, and comments on comments.  

 - **pid**  Matches Main Post identifier in post.csv
 - **cid**  Comment Id.
 - **timeStamp**  
 - **id**  Id of user commenting
 - **name**  Name of user commenting
 - **rid**   Id of user responding to first comment
 - **msg**  Message

**like.csv**

These are likes and responses.  The two keys in this file (pid,cid) will join to post and comment respectively.

 - **pid**  Matches Main Post identifier in post.csv
 - **cid**  Matches Comments id.
 - **response**  Response such as LIKE, ANGRY etc.
 - **id**  The id of user responding
 - **name** Name of the user responding

**member.csv**

These are all the members in the group.  Some members never, or rarely, post or comment.

 - **id** Id of the member
 - **name** Name of the member
 - **url**  URL of the member","2016-06-04 14:02:13","Issue with LIKES limit being set too low, clipping the number of likes.  This issue maybe address in this version.","cheltenham-s-facebook-group.zip",13141060,0
106,54,4,"Cheltenham's Facebook Groups","cheltenham-s-facebook-group","Discover How a Community Leverages Facebook","Facebook is becoming an essential tool for more than just family and friends.  Discover how Cheltenham Township (USA), a diverse community just outside of Philadelphia, deals with major issues such as the Bill Cosby trial, everyday traffic issues, sewer I/I problems and lost cats and dogs.  Communities work when they're connected and exchanging information.  What and who are the essential forces making a positive impact, and when and how do conversational threads get directed or misdirected?  Who trolls, who entertains and who informs?    


**Data Sources**

There are only 4 files in the dataset. It contains every post made to the public Facebook group [**Unofficial Cheltenham Township**](https://www.facebook.com/groups/25160801076/).

 
**post.csv**

These are the main posts you will see on the page.  It might help to take a quick look at the [page](https://www.facebook.com/groups/25160801076/).  Commas in the **msg** field have been replaced with {COMMA}, and apostrophes have been replaced with {APOST}. 

 - **pid**  Main Post id
 - **id**    Id of the user posting
 - **name**  User's name
 - **timeStamp**
 - **shares**
 - **url**
 - **msg**  Text of the message posted. 
 - **likes** Number of likes

**comment.csv**

These are comments to the main post. Note, Facebook postings have comments, and comments on comments.  

 - **pid**  Matches Main Post identifier in post.csv
 - **cid**  Comment Id.
 - **timeStamp**  
 - **id**  Id of user commenting
 - **name**  Name of user commenting
 - **rid**   Id of user responding to first comment
 - **msg**  Message

**like.csv**

These are likes and responses.  The two keys in this file (pid,cid) will join to post and comment respectively.

 - **pid**  Matches Main Post identifier in post.csv
 - **cid**  Matches Comments id.
 - **response**  Response such as LIKE, ANGRY etc.
 - **id**  The id of user responding
 - **name** Name of the user responding

**member.csv**

These are all the members in the group.  Some members never, or rarely, post or comment.

 - **id** Id of the member
 - **name** Name of the member
 - **url**  URL of the member","2016-06-05 02:38:39","Bumped up LIKES limit across all posts.","cheltenham-s-facebook-groups.zip",13228526,0
107,54,5,"Cheltenham's Facebook Groups","cheltenham-s-facebook-group","Discover How a Community Leverages Facebook","Facebook is becoming an essential tool for more than just family and friends.  Discover how Cheltenham Township (USA), a diverse community just outside of Philadelphia, deals with major issues such as the Bill Cosby trial, everyday traffic issues, sewer I/I problems and lost cats and dogs.  And yes, theft.  

Communities work when they're connected and exchanging information.  What and who are the essential forces making a positive impact, and when and how do conversational threads get directed or misdirected?     


**Data Sources**

There are only 4 files in the dataset. But, it contains every post made to the following 3 public Facebook groups

 - [**Unofficial Cheltenham Township**](https://www.facebook.com/groups/25160801076/)
 - [**Elkins Park Happenings!**](https://www.facebook.com/groups/117291968282998/)
 - [**Free Speech Zone**](https://www.facebook.com/groups/1443890352589739/)

 
**post.csv**

These are the main posts you will see on the page.  It might help to take a quick look at the [page](https://www.facebook.com/groups/25160801076/).  Commas in the **msg** field have been replaced with {COMMA}, and apostrophes have been replaced with {APOST}. 


 - **gid** Group id (3 different Facebook groups) 
 -  **pid**  Main Post id
 -  **id**    Id of the user posting
 -  **name**  User's name
 -  **timeStamp**
 -  **shares**
 -  **url**
 -  **msg**  Text of the message posted. 
 -  **likes** Number of likes

**comment.csv**

These are comments to the main post. Note, Facebook postings have comments, and comments on comments.  

 - **gid**  Group id
 - **pid**  Matches Main Post identifier in post.csv
 - **cid**  Comment Id.
 - **timeStamp**  
 - **id**  Id of user commenting
 - **name**  Name of user commenting
 - **rid**   Id of user responding to first comment
 - **msg**  Message

**like.csv**

These are likes and responses.  The two keys in this file (pid,cid) will join to post and comment respectively.

 - **gid** Group id
 - **pid**  Matches Main Post identifier in post.csv
 - **cid**  Matches Comments id.
 - **response**  Response such as LIKE, ANGRY etc.
 - **id**  The id of user responding
 - **name** Name of the user responding

**member.csv**

These are all the members in the group.  Some members never, or rarely, post or comment.

 - **gid** Group id 
 - **id** Id of the member
 - **name** Name of the member
 - **url**  URL of the member","2016-06-06 23:24:00","This version breaks the existing format. Now 3 Facebook groups are included in this dataset. These are the 3 main groups of Cheltenham.","cheltenham-s-facebook-group.zip",32423971,0
108,54,6,"Cheltenham's Facebook Groups","cheltenham-s-facebook-group","Discover How a Community Leverages Facebook","Facebook is becoming an essential tool for more than just family and friends.  Discover how Cheltenham Township (USA), a diverse community just outside of Philadelphia, deals with major issues such as the Bill Cosby trial, everyday traffic issues, sewer I/I problems and lost cats and dogs.  And yes, theft.  

Communities work when they're connected and exchanging information.  What and who are the essential forces making a positive impact, and when and how do conversational threads get directed or misdirected?     


**Data Sources**

There are only 4 files in the dataset. But, it contains every post made to the following 3 public Facebook groups

 - [**Unofficial Cheltenham Township**](https://www.facebook.com/groups/25160801076/)
 - [**Elkins Park Happenings!**](https://www.facebook.com/groups/117291968282998/)
 - [**Free Speech Zone**](https://www.facebook.com/groups/1443890352589739/)

 
**post.csv**

These are the main posts you will see on the page.  It might help to take a quick look at the [page](https://www.facebook.com/groups/25160801076/).  Commas in the **msg** field have been replaced with {COMMA}, and apostrophes have been replaced with {APOST}. 


 - **gid** Group id (3 different Facebook groups) 
 -  **pid**  Main Post id
 -  **id**    Id of the user posting
 -  **name**  User's name
 -  **timeStamp**
 -  **shares**
 -  **url**
 -  **msg**  Text of the message posted. 
 -  **likes** Number of likes

**comment.csv**

These are comments to the main post. Note, Facebook postings have comments, and comments on comments.  

 - **gid**  Group id
 - **pid**  Matches Main Post identifier in post.csv
 - **cid**  Comment Id.
 - **timeStamp**  
 - **id**  Id of user commenting
 - **name**  Name of user commenting
 - **rid**   Id of user responding to first comment
 - **msg**  Message

**like.csv**

These are likes and responses.  The two keys in this file (pid,cid) will join to post and comment respectively.

 - **gid** Group id
 - **pid**  Matches Main Post identifier in post.csv
 - **cid**  Matches Comments id.
 - **response**  Response such as LIKE, ANGRY etc.
 - **id**  The id of user responding
 - **name** Name of the user responding

**member.csv**

These are all the members in the group.  Some members never, or rarely, post or comment.

 - **gid** Group id 
 - **id** Id of the member
 - **name** Name of the member
 - **url**  URL of the member","2016-06-07 00:14:39","Missed post.csv in the last dataset.","cheltenham-s-facebook-group.zip",33928599,1
109,54,7,"Cheltenham's Facebook Groups","cheltenham-s-facebook-group","Discover How a Community Leverages Facebook","Facebook is becoming an essential tool for more than just family and friends.  Discover how Cheltenham Township (USA), a diverse community just outside of Philadelphia, deals with major issues such as the Bill Cosby trial, everyday traffic issues, sewer I/I problems and lost cats and dogs.  And yes, theft.  

Communities work when they're connected and exchanging information.  What and who are the essential forces making a positive impact, and when and how do conversational threads get directed or misdirected?     


**Data Sources**

There are only 4 files in the dataset. But, it contains every post made to the following 3 public Facebook groups

 - [**Unofficial Cheltenham Township**](https://www.facebook.com/groups/25160801076/)
 - [**Elkins Park Happenings!**](https://www.facebook.com/groups/117291968282998/)
 - [**Free Speech Zone**](https://www.facebook.com/groups/1443890352589739/)

 
**post.csv**

These are the main posts you will see on the page.  It might help to take a quick look at the [page](https://www.facebook.com/groups/25160801076/).  Commas in the **msg** field have been replaced with {COMMA}, and apostrophes have been replaced with {APOST}. 


 - **gid** Group id (3 different Facebook groups) 
 -  **pid**  Main Post id
 -  **id**    Id of the user posting
 -  **name**  User's name
 -  **timeStamp**
 -  **shares**
 -  **url**
 -  **msg**  Text of the message posted. 
 -  **likes** Number of likes

**comment.csv**

These are comments to the main post. Note, Facebook postings have comments, and comments on comments.  

 - **gid**  Group id
 - **pid**  Matches Main Post identifier in post.csv
 - **cid**  Comment Id.
 - **timeStamp**  
 - **id**  Id of user commenting
 - **name**  Name of user commenting
 - **rid**   Id of user responding to first comment
 - **msg**  Message

**like.csv**

These are likes and responses.  The two keys in this file (pid,cid) will join to post and comment respectively.

 - **gid** Group id
 - **pid**  Matches Main Post identifier in post.csv
 - **cid**  Matches Comments id.
 - **response**  Response such as LIKE, ANGRY etc.
 - **id**  The id of user responding
 - **name** Name of the user responding

**member.csv**

These are all the members in the group.  Some members never, or rarely, post or comment.

 - **gid** Group id 
 - **id** Id of the member
 - **name** Name of the member
 - **url**  URL of the member","2016-06-07 12:19:36","Eliminate issues with duplicates post.csv and updated data.","cheltenham-s-facebook-group.zip",33900923,2
111,54,8,"Cheltenham's Facebook Groups","cheltenham-s-facebook-group","Discover How a Community Leverages Facebook","Facebook is becoming an essential tool for more than just family and friends.  Discover how Cheltenham Township (USA), a diverse community just outside of Philadelphia, deals with major issues such as the Bill Cosby trial, everyday traffic issues, sewer I/I problems and lost cats and dogs.  And yes, theft.  

Communities work when they're connected and exchanging information.  What and who are the essential forces making a positive impact, and when and how do conversational threads get directed or misdirected?     


**Data Sources**

There are only 4 files in the dataset. But, it contains every post made to the following 3 public Facebook groups

 - [**Unofficial Cheltenham Township**](https://www.facebook.com/groups/25160801076/)
 - [**Elkins Park Happenings!**](https://www.facebook.com/groups/117291968282998/)
 - [**Free Speech Zone**](https://www.facebook.com/groups/1443890352589739/)

 
**post.csv**

These are the main posts you will see on the page.  It might help to take a quick look at the [page](https://www.facebook.com/groups/25160801076/).  Commas in the **msg** field have been replaced with {COMMA}, and apostrophes have been replaced with {APOST}. 


 - **gid** Group id (3 different Facebook groups) 
 -  **pid**  Main Post id
 -  **id**    Id of the user posting
 -  **name**  User's name
 -  **timeStamp**
 -  **shares**
 -  **url**
 -  **msg**  Text of the message posted. 
 -  **likes** Number of likes

**comment.csv**

These are comments to the main post. Note, Facebook postings have comments, and comments on comments.  

 - **gid**  Group id
 - **pid**  Matches Main Post identifier in post.csv
 - **cid**  Comment Id.
 - **timeStamp**  
 - **id**  Id of user commenting
 - **name**  Name of user commenting
 - **rid**   Id of user responding to first comment
 - **msg**  Message

**like.csv**

These are likes and responses.  The two keys in this file (pid,cid) will join to post and comment respectively.

 - **gid** Group id
 - **pid**  Matches Main Post identifier in post.csv
 - **cid**  Matches Comments id.
 - **response**  Response such as LIKE, ANGRY etc.
 - **id**  The id of user responding
 - **name** Name of the user responding

**member.csv**

These are all the members in the group.  Some members never, or rarely, post or comment.

 - **gid** Group id 
 - **id** Id of the member
 - **name** Name of the member
 - **url**  URL of the member","2016-06-09 04:18:47","Updated data to 2016-06-09.  ","cheltenham-s-facebook-group.zip",34108776,1
112,56,1,"The Gravitational Waves Discovery Data","the-gravitational-waves-discovery-data","The GW150914 Gravitational Waves event data","On February 11th 2016 LIGO-Virgo collaboration gave the announce of the discovery of Gravitational Waves, just 100 years after the Einstein’s paper on their prediction. The LIGO Scientific Collaboration (LSC) and the Virgo Collaboration prepared a web page to inform the broader community about a confirmed astrophysical event observed by the gravitational-wave detectors, and to make the data around that time available for others to analyze: https://losc.ligo.org/events/GW150914/

You can find much more information on the [LOSC][1] web site, and a good starting tutorial at the following link:

https://losc.ligo.org/tutorial00/

These data sets contain 32 secs of data sampled at 4096Hz an 16384Hz around the GW event detected on 14/09/2015.

Longer sets of data can be downloaded here

    https://losc.ligo.org/s/events/GW150914/H-H1_LOSC_4_V1-1126257414-4096.hdf5
    https://losc.ligo.org/s/events/GW150914/L-L1_LOSC_4_V1-1126257414-4096.hdf5
    https://losc.ligo.org/s/events/GW150914/H-H1_LOSC_16_V1-1126257414-4096.hdf5
    https://losc.ligo.org/s/events/GW150914/L-L1_LOSC_16_V1-1126257414-4096.hdf5


**How to acknowledge use of this data:** 
If your research used data from one of the data releases, please cite as:

 
 - LIGO Scientific Collaboration, ""LIGO Open Science Center release of    S5"", 2014, DOI 10.7935/K5WD3XHR    
 - LIGO Scientific Collaboration, ""LIGO Open Science Center release of S6"", 2015, DOI 10.7935/K5RN35SD    
 - LIGO Scientific Collaboration, ""LIGO Open Science Center release of GW150914"", 2016, DOI10.7935/K5MW2F23

and please include the statement ""This research has made use of data, software and/or web tools obtained from the LIGO Open Science Center (https://losc.ligo.org), a service of LIGO Laboratory and the LIGO Scientific Collaboration. LIGO is funded by the U.S. National Science Foundation.""

If you would also like to cite a published paper,
M Vallisneri et al. ""The LIGO Open Science Center"", proceedings of the 10th LISA Symposium, 
University of Florida, Gainesville, May 18-23, 2014; also arxiv:1410.4839

**Publications**
We request that you let the LOSC team know if you publish (or intend to publish) a paper using data released from this site. If you would like, we may be able to review your work prior to publication, as we do for our colleagues in the LIGO Scientific Collaboration.
**Credits**
LOSC Development: The LOSC Team and The LIGO Scientific Collaboration

The data products made available through the LOSC web service are created and maintained by LIGO Lab and the LIGO Scientific Collaboration. The development of this web page was a team effort, with all members of the LOSC team making contributions in most areas. In addition to the team members listed below, a large number of individuals in the LIGO Scientific Collaboration have contributed content and advice. The LOSC team includes:

    Alan Weinstein: LOSC Director
    Roy Williams: LOSC Developer, web services and data base architecture
    Jonah Kanner: LOSC Developer, tutorials, documentation, data set curation
    Michele Vallisneri: LOSC Developer, data quality curation
    Branson Stephens: LOSC Developer, event database and web site architecture

Please send any comments, questions, or concerns to: losc@ligo.org


  [1]: https://losc.ligo.org/about/","2016-06-09 08:45:04","Initial release","the-gravitational-waves-discovery-data.zip",9894343,15
113,54,9,"Cheltenham's Facebook Groups","cheltenham-s-facebook-group","Discover How a Community Leverages Facebook","Facebook is becoming an essential tool for more than just family and friends.  Discover how Cheltenham Township (USA), a diverse community just outside of Philadelphia, deals with major issues such as the Bill Cosby trial, everyday traffic issues, sewer I/I problems and lost cats and dogs.  And yes, theft.  

Communities work when they're connected and exchanging information.  What and who are the essential forces making a positive impact, and when and how do conversational threads get directed or misdirected?     


**Data Sources**

There are only 4 files in the dataset. But, it contains every post made to the following 3 public Facebook groups

 - [**Unofficial Cheltenham Township**](https://www.facebook.com/groups/25160801076/)
 - [**Elkins Park Happenings!**](https://www.facebook.com/groups/117291968282998/)
 - [**Free Speech Zone**](https://www.facebook.com/groups/1443890352589739/)

 
**post.csv**

These are the main posts you will see on the page.  It might help to take a quick look at the [page](https://www.facebook.com/groups/25160801076/).  Commas in the **msg** field have been replaced with {COMMA}, and apostrophes have been replaced with {APOST}. 


 - **gid** Group id (3 different Facebook groups) 
 -  **pid**  Main Post id
 -  **id**    Id of the user posting
 -  **name**  User's name
 -  **timeStamp**
 -  **shares**
 -  **url**
 -  **msg**  Text of the message posted. 
 -  **likes** Number of likes

**comment.csv**

These are comments to the main post. Note, Facebook postings have comments, and comments on comments.  

 - **gid**  Group id
 - **pid**  Matches Main Post identifier in post.csv
 - **cid**  Comment Id.
 - **timeStamp**  
 - **id**  Id of user commenting
 - **name**  Name of user commenting
 - **rid**   Id of user responding to first comment
 - **msg**  Message

**like.csv**

These are likes and responses.  The two keys in this file (pid,cid) will join to post and comment respectively.

 - **gid** Group id
 - **pid**  Matches Main Post identifier in post.csv
 - **cid**  Matches Comments id.
 - **response**  Response such as LIKE, ANGRY etc.
 - **id**  The id of user responding
 - **name** Name of the user responding

**member.csv**

These are all the members in the group.  Some members never, or rarely, post or comment.

 - **gid** Group id 
 - **id** Id of the member
 - **name** Name of the member
 - **url**  URL of the member","2016-06-10 03:45:17","Updated data to late 2016-06-09","cheltenham-s-facebook-group.zip",34297115,1
114,48,10,"SEPTA - Regional Rail","on-time-performance","Predict  Arrival Times of Philadelphia's Regional Trains.","## SEPTA - Southeastern Pennsylvania Transportation Authority ##
 

The SEPTA Regional Rail system consists of commuter rail service on 13 branches to more than 150 active stations in Philadelphia, Pennsylvania, and its suburbs and satellite cities.  

SEPTA uses On-Time Performance (OTP)  to measure service reliability. OTP identifies the number of trains for all rail lines that arrive at their scheduled destination at the scheduled time.  However, by industry standard,  a train may arrive up to 5 minutes and 59 seconds after its scheduled time and still be considered on-time.  

SEPTA has established an annual goal of 91% for Regional Rail On-Time Performance.  How well are they doing?  Is it even a meaningful measure?


#Data Description

**otp.csv**

 - **train_id**
 - **direction**   *('N' or 'S'   direction is demarcated as either Northbound or Southbound)[^1](https://www.kaggle.com/forums/f/1300/septa-regional-rail/t/21259/why-are-all-trains-originating-in-thorndale-marked-as-southbound)*
 - **origin**        *(See map below - you'll see 'Warminster', 'Glenside',...'Airport Terminal..')*
 - **next_station**  *(Think of this as the station stop, at timeStamp)* 
 - **date**
 - **status**    *('On Time', '5 min', '10 min'.  This is a status on train lateness. 999 is a suspended train)*
 - **timeStamp**


----------


**trainView.csv - GPS Train data (early release)**


Most GPS coordinates are based on track telemetry; however, cars are being equipped with GPS units. 

- **train_id**
- **status**
- **next_station**
- **service**
- **dest**
- **lon**
- **lat**
- **source**
- **track_change**
- **track**
- **date**
- **timeStamp0**   First timeStamp at coordinates.
- **timeStamp1**    Last timeStamp at coordinates.

You can look at the example [here](https://www.kaggle.com/septa/d/septa/on-time-performance/trainview-gps-data-early-beta) on how to track a single train.

**What To Look For...**

Ah, as a commuter you care more about the performance of the train(s) you plan to take.  OTP maybe 91% or above; but, if the train **you take** runs late, you'll spend extra time on the tracks.  If it consistently runs late, maybe the schedule should be changed.

Look for patterns. For example, during the Tuesday morning rush do some trains run consistently late?   How long does it take to get to from point A to point B in the system?  Performance is VERY important to SEPTA. 

----------


Below is a map of the system and station stops.  This dataset contains data on the Regional Rail Lines.

![SEPTA map][1]

SEPTA train schedules can be found [here](http://www.septa.org/schedules/rail/).  Note different Weekday, Saturday, and Sunday schedules.   



  [1]: https://raw.githubusercontent.com/mchirico/mchirico.github.io/master/p/images/septaMap.png","2016-06-10 19:46:22","Uploading recent data, but otp.csv may still have missing trains that cross midnight boundaries.  The problem maybe fixed on the next data update scheduled for June 30th.  The file trainView.csv should be complete.","on-time-performance.zip",72065664,2
116,57,1,"SF Bay Area Bike Share","sf-bay-area-bike-share","Anonymized bike trip data from August 2013 to August 2015","The [Bay Area Bike Share](http://www.bayareabikeshare.com/) enables quick, easy, and affordable bike trips around the San Francisco Bay Area. They make [regular open data releases](http://www.bayareabikeshare.com/open-data) (this dataset is a [transformed version](https://github.com/benhamner/bay-area-bike-share) of the data from this link), plus maintain a real-time API.

[![sf-trips](https://www.kaggle.io/svf/266282/f38a21e1d561640de079ba6a8e0935ea/sf_trips.png)](https://www.kaggle.com/benhamner/d/benhamner/sf-bay-area-bike-share/san-francisco-trip-map/)

# Exploration Ideas

 - How does weather impact bike trips?
 - How do bike trip patterns vary by time of day and the day of the week?","2016-06-14 04:07:05","Initial release","sf-bay-area-bike-share.zip",692440704,6
117,42,3,"World of Warcraft Avatar History","warcraft-avatar-history","Track the players of this popular online game","Overview
--------

The World of Warcraft Avatar History Dataset is a collection of records that detail information about player characters in the game over time. It includes information about their character level, race, class, location, and social guild. The Kaggle version of this dataset includes only the information from 2008 (and the dataset in general only includes information from the 'Horde' faction of players in the game from a single game server).

 - Full Dataset Source and Information:
   [http://mmnet.iis.sinica.edu.tw/dl/wowah/][1] 
 - Code used to clean the data: [https://github.com/myles-oneill/WoWAH-parser][2] 

Ideas for Using the Dataset
---------------------------

From the perspective of game system designers, players' behavior is one of the most important factors they must consider when designing game systems. To gain a fundamental understanding of the game play behavior of online gamers, exploring users' game play time provides a good starting point. This is because the concept of game play time is applicable to all genres of games and it enables us to model the system workload as well as the impact of system and network QoS on users' behavior. It can even help us predict players' loyalty to specific games.

Open Questions
--------------

 - Understand user gameplay behavior (game sessions, movement, leveling)
 - Understand user interactions (guilds)
 - Predict players unsubscribing from the game based on activity
 - What are the most popular zones in WoW, what level players tend to inhabit each?

Wrath of the Lich King
----------------------

An expansion to World of Warcraft, ""Wrath of the Lich King"" (Wotlk) was released on November 13, 2008. It introduced new zones for players to go to, a new character class (the death knight), and a new level cap of 80 (up from 70 previously). This event intersects nicely with the dataset and is probably interesting to investigate.

Map
---

This dataset doesn't include a shapefile (if you know of one that exists, let me know!) to show where the zones the dataset talks about are. Here is a list of zones an information from this version of the game, including their recommended levels: http://wowwiki.wikia.com/wiki/Zones_by_level_(original) . 

**Update (Version 3)**: [dmi3kno][3] has generously put together some supplementary zone information files which have now been included in this dataset. Some notes about the files:

*Note that some zone names contain Chinese characters. Unicode names are preserved as a key to the original dataset. What this addition will allow is to understand properties of the zones a bit better - their relative location to each other, competititive properties, type of gameplay and, hopefully, their contribution to character leveling. Location coordinates contain some redundant (and possibly duplicate) records as they are collected from different sources. Working with uncleaned location coordinate data will allow users to demonstrate their data wrangling skills (both working with strings and spatial data).*


  [1]: http://mmnet.iis.sinica.edu.tw/dl/wowah/
  [2]: https://github.com/myles-oneill/WoWAH-parser
  [3]: https://www.kaggle.com/dmi3kno","2016-06-14 09:10:17","Added locations.csv, location_coords.csv, and zones.csv - [files created and shared by dmi3kno][1].
  [1]: https://www.kaggle.com/forums/f/1289/world-of-warcraft-avatar-history/t/21641/additional-datasets-to-analyze-zone-information","warcraft-avatar-history.zip",91838629,16
118,54,10,"Cheltenham's Facebook Groups","cheltenham-s-facebook-group","Discover How a Community Leverages Facebook","Facebook is becoming an essential tool for more than just family and friends.  Discover how Cheltenham Township (USA), a diverse community just outside of Philadelphia, deals with major issues such as the Bill Cosby trial, everyday traffic issues, sewer I/I problems and lost cats and dogs.  And yes, theft.  

Communities work when they're connected and exchanging information.  What and who are the essential forces making a positive impact, and when and how do conversational threads get directed or misdirected?     


**Data Sources**

There are only 4 files in the dataset. But, it contains every post made to the following 3 public Facebook groups

 - [**Unofficial Cheltenham Township**](https://www.facebook.com/groups/25160801076/)
 - [**Elkins Park Happenings!**](https://www.facebook.com/groups/117291968282998/)
 - [**Free Speech Zone**](https://www.facebook.com/groups/1443890352589739/)

 
**post.csv**

These are the main posts you will see on the page.  It might help to take a quick look at the [page](https://www.facebook.com/groups/25160801076/).  Commas in the **msg** field have been replaced with {COMMA}, and apostrophes have been replaced with {APOST}. 


 - **gid** Group id (3 different Facebook groups) 
 -  **pid**  Main Post id
 -  **id**    Id of the user posting
 -  **name**  User's name
 -  **timeStamp**
 -  **shares**
 -  **url**
 -  **msg**  Text of the message posted. 
 -  **likes** Number of likes

**comment.csv**

These are comments to the main post. Note, Facebook postings have comments, and comments on comments.  

 - **gid**  Group id
 - **pid**  Matches Main Post identifier in post.csv
 - **cid**  Comment Id.
 - **timeStamp**  
 - **id**  Id of user commenting
 - **name**  Name of user commenting
 - **rid**   Id of user responding to first comment
 - **msg**  Message

**like.csv**

These are likes and responses.  The two keys in this file (pid,cid) will join to post and comment respectively.

 - **gid** Group id
 - **pid**  Matches Main Post identifier in post.csv
 - **cid**  Matches Comments id.
 - **response**  Response such as LIKE, ANGRY etc.
 - **id**  The id of user responding
 - **name** Name of the user responding

**member.csv**

These are all the members in the group.  Some members never, or rarely, post or comment.

 - **gid** Group id 
 - **id** Id of the member
 - **name** Name of the member
 - **url**  URL of the member","2016-06-15 16:03:30","Updated data to 2016-06-15","cheltenham-s-facebook-group.zip",34941632,0
119,55,1,"The Enron Email Dataset","enron-email-dataset","500,000+ emails from 150 employees of the Enron Corporation","The Enron email dataset contains approximately 500,000 emails generated by employees of the Enron Corporation. It was obtained by the Federal Energy Regulatory Commission during its investigation of Enron's collapse.

This is the May 7, 2015 Version of dataset, as published at [https://www.cs.cmu.edu/~./enron/][1]


  [1]: https://www.cs.cmu.edu/~./enron/","2016-06-16 16:38:12","Initial release","enron-email-dataset.zip",406377645,0
120,55,2,"The Enron Email Dataset","enron-email-dataset","500,000+ emails from 150 employees of the Enron Corporation","The Enron email dataset contains approximately 500,000 emails generated by employees of the Enron Corporation. It was obtained by the Federal Energy Regulatory Commission during its investigation of Enron's collapse.

This is the May 7, 2015 Version of dataset, as published at [https://www.cs.cmu.edu/~./enron/][1]


  [1]: https://www.cs.cmu.edu/~./enron/","2016-06-16 20:55:19","Line endings fix","enron-email-dataset.zip",404531234,8
121,54,11,"Cheltenham's Facebook Groups","cheltenham-s-facebook-group","Discover How a Community Leverages Facebook","Facebook is becoming an essential tool for more than just family and friends.  Discover how Cheltenham Township (USA), a diverse community just outside of Philadelphia, deals with major issues such as the Bill Cosby trial, everyday traffic issues, sewer I/I problems and lost cats and dogs.  And yes, theft.  

Communities work when they're connected and exchanging information.  What and who are the essential forces making a positive impact, and when and how do conversational threads get directed or misdirected?     


**Data Sources**

There are only 4 files in the dataset. But, it contains every post made to the following 3 public Facebook groups

 - [**Unofficial Cheltenham Township**](https://www.facebook.com/groups/25160801076/)
 - [**Elkins Park Happenings!**](https://www.facebook.com/groups/117291968282998/)
 - [**Free Speech Zone**](https://www.facebook.com/groups/1443890352589739/)

 
**post.csv**

These are the main posts you will see on the page.  It might help to take a quick look at the [page](https://www.facebook.com/groups/25160801076/).  Commas in the **msg** field have been replaced with {COMMA}, and apostrophes have been replaced with {APOST}. 


 - **gid** Group id (3 different Facebook groups) 
 -  **pid**  Main Post id
 -  **id**    Id of the user posting
 -  **name**  User's name
 -  **timeStamp**
 -  **shares**
 -  **url**
 -  **msg**  Text of the message posted. 
 -  **likes** Number of likes

**comment.csv**

These are comments to the main post. Note, Facebook postings have comments, and comments on comments.  

 - **gid**  Group id
 - **pid**  Matches Main Post identifier in post.csv
 - **cid**  Comment Id.
 - **timeStamp**  
 - **id**  Id of user commenting
 - **name**  Name of user commenting
 - **rid**   Id of user responding to first comment
 - **msg**  Message

**like.csv**

These are likes and responses.  The two keys in this file (pid,cid) will join to post and comment respectively.

 - **gid** Group id
 - **pid**  Matches Main Post identifier in post.csv
 - **cid**  Matches Comments id.
 - **response**  Response such as LIKE, ANGRY etc.
 - **id**  The id of user responding
 - **name** Name of the user responding

**member.csv**

These are all the members in the group.  Some members never, or rarely, post or comment.

 - **gid** Group id 
 - **id** Id of the member
 - **name** Name of the member
 - **url**  URL of the member","2016-06-19 16:25:34","Data was updated to 2016-06-19.  In addition, previous versions of like.csv had misaligned columns, which should now be fixed.  ","cheltenham-s-facebook-group.zip",35110539,0
122,9,4,"Meta Kaggle","meta-kaggle","The dataset on Kaggle, on Kaggle","We aren't saying this dataset is the Rosetta Stone of machine learning competitions, but we do think there is a lot to learn from (and a lot of fun to be had by) releasing some of our most interesting tables on Kaggle community and competition activity.

Strategizing to become a Master? Wondering who, where, and what goes in to a winning team? Deciding between evaluation metrics for your next data science project? We hope the scripts published here will enrich and entertain Kagglers, spark some lively conversations, and act as a resource for the larger machine learning community.

[![private leaderboard performance over time](https://www.kaggle.io/svf/214627/b025ac60d2bd52ce7321c5257b7d891b/private_leaderboard_aggregate_performance_over_time.png)](https://www.kaggle.com/benhamner/d/kaggle/meta-kaggle/kaggle-leaderboard-performance-over-time)

This data (available through Kaggle Scripts as CSV files and a SQLite database) contains the tables listed below.

Note that this data is not a complete dump: rows, columns, and tables have been filtered out, and it is a small subset of the data that we can release publicly. Over time, we'll add more of the tables that we can release publicly to it.

[![wordcloud](https://www.kaggle.io/svf/214832/ebc77bde9dd3c079bcc52573fcedbd3e/__results___files/figure-html/unnamed-chunk-4-1.png)](https://www.kaggle.com/benhamner/d/kaggle/meta-kaggle/kaggle-forum-post-wordclouds)","2016-06-22 20:02:17","Updated data as ofJune 21, 2016","meta-kaggle.zip",454894092,0
123,9,5,"Meta Kaggle","meta-kaggle","The dataset on Kaggle, on Kaggle","We aren't saying this dataset is the Rosetta Stone of machine learning competitions, but we do think there is a lot to learn from (and a lot of fun to be had by) releasing some of our most interesting tables on Kaggle community and competition activity.

Strategizing to become a Master? Wondering who, where, and what goes in to a winning team? Deciding between evaluation metrics for your next data science project? We hope the scripts published here will enrich and entertain Kagglers, spark some lively conversations, and act as a resource for the larger machine learning community.

[![private leaderboard performance over time](https://www.kaggle.io/svf/214627/b025ac60d2bd52ce7321c5257b7d891b/private_leaderboard_aggregate_performance_over_time.png)](https://www.kaggle.com/benhamner/d/kaggle/meta-kaggle/kaggle-leaderboard-performance-over-time)

This data (available through Kaggle Scripts as CSV files and a SQLite database) contains the tables listed below.

Note that this data is not a complete dump: rows, columns, and tables have been filtered out, and it is a small subset of the data that we can release publicly. Over time, we'll add more of the tables that we can release publicly to it.

[![wordcloud](https://www.kaggle.io/svf/214832/ebc77bde9dd3c079bcc52573fcedbd3e/__results___files/figure-html/unnamed-chunk-4-1.png)](https://www.kaggle.com/benhamner/d/kaggle/meta-kaggle/kaggle-forum-post-wordclouds)","2016-06-22 23:37:21","Updated as of June 22, 2016. Fixed forum inclusion criteria so general forum topics / messages are present as well","meta-kaggle.zip",459337304,4
124,58,1,"Trade statistics japan 2016 1-4","tradestatisticsjapan2016-1to4","CSV format Trade statistics japan 2016 1-4  with HS code and Country code","CSV format Trade statistics japan 2016 1-4  with HS code and Country code
original data http://www.customs.go.jp/toukei/info/index_e.htm

## dataset layout ( summary )

 - exp_imp    1:export 2:import
 - Year
 - Country
 - Value     1000 Yen
 - month
 - hs2    2 digit HS code
 - hs4    4 digit HS code
 - hs6|    6 digit HS code
 - hs9|    9 digit HS code (japanse local code)

## country code etc
 - [country code ( github )](https://github.com/zanjibar/data4nihon/blob/master/02/Country_eng.csv)
 - [2 digit HS code ( github )](https://github.com/zanjibar/data4nihon/blob/master/02/hs2_eng.csv)","2016-06-24 14:43:10","Initial release","tradestatisticsjapan2016-1to4.zip",5894611,4
126,54,12,"Cheltenham's Facebook Groups","cheltenham-s-facebook-group","Discover How a Community Leverages Facebook","Facebook is becoming an essential tool for more than just family and friends.  Discover how Cheltenham Township (USA), a diverse community just outside of Philadelphia, deals with major issues such as the Bill Cosby trial, everyday traffic issues, sewer I/I problems and lost cats and dogs.  And yes, theft.  

Communities work when they're connected and exchanging information.  What and who are the essential forces making a positive impact, and when and how do conversational threads get directed or misdirected?     


**Data Sources**

There are only 4 files in the dataset. But, it contains every post made to the following 3 public Facebook groups

 - [**Unofficial Cheltenham Township**](https://www.facebook.com/groups/25160801076/)
 - [**Elkins Park Happenings!**](https://www.facebook.com/groups/117291968282998/)
 - [**Free Speech Zone**](https://www.facebook.com/groups/1443890352589739/)

 
**post.csv**

These are the main posts you will see on the page.  It might help to take a quick look at the [page](https://www.facebook.com/groups/25160801076/).  Commas in the **msg** field have been replaced with {COMMA}, and apostrophes have been replaced with {APOST}. 


 - **gid** Group id (3 different Facebook groups) 
 -  **pid**  Main Post id
 -  **id**    Id of the user posting
 -  **name**  User's name
 -  **timeStamp**
 -  **shares**
 -  **url**
 -  **msg**  Text of the message posted. 
 -  **likes** Number of likes

**comment.csv**

These are comments to the main post. Note, Facebook postings have comments, and comments on comments.  

 - **gid**  Group id
 - **pid**  Matches Main Post identifier in post.csv
 - **cid**  Comment Id.
 - **timeStamp**  
 - **id**  Id of user commenting
 - **name**  Name of user commenting
 - **rid**   Id of user responding to first comment
 - **msg**  Message

**like.csv**

These are likes and responses.  The two keys in this file (pid,cid) will join to post and comment respectively.

 - **gid** Group id
 - **pid**  Matches Main Post identifier in post.csv
 - **cid**  Matches Comments id.
 - **response**  Response such as LIKE, ANGRY etc.
 - **id**  The id of user responding
 - **name** Name of the user responding

**member.csv**

These are all the members in the group.  Some members never, or rarely, post or comment.

 - **gid** Group id 
 - **id** Id of the member
 - **name** Name of the member
 - **url**  URL of the member","2016-06-24 22:55:50","Making dataset current to 2016-06-24","cheltenham-s-facebook-group.zip",35525019,0
127,54,13,"Cheltenham's Facebook Groups","cheltenham-s-facebook-group","Discover How a Community Leverages Facebook","Facebook is becoming an essential tool for more than just family and friends.  Discover how Cheltenham Township (USA), a diverse community just outside of Philadelphia, deals with major issues such as the Bill Cosby trial, everyday traffic issues, sewer I/I problems and lost cats and dogs.  And yes, theft.  

Communities work when they're connected and exchanging information.  What and who are the essential forces making a positive impact, and when and how do conversational threads get directed or misdirected?     


**Use Any Facebook Public Group**

You can leverage the examples here for **any** public Facebook group.  For an example of the source code used to collect this data, and a quick start docker image, take a look at the following project: [facebook-group-scrape](https://github.com/mchirico/facebook-group-scrape).  


**Data Sources**

There are only 4 files in the dataset. But, it contains every post made to the following 3 public Facebook groups

 - [**Unofficial Cheltenham Township**](https://www.facebook.com/groups/25160801076/)
 - [**Elkins Park Happenings!**](https://www.facebook.com/groups/117291968282998/)
 - [**Free Speech Zone**](https://www.facebook.com/groups/1443890352589739/)

 
**post.csv**

These are the main posts you will see on the page.  It might help to take a quick look at the [page](https://www.facebook.com/groups/25160801076/).  Commas in the **msg** field have been replaced with {COMMA}, and apostrophes have been replaced with {APOST}. 


 - **gid** Group id (3 different Facebook groups) 
 -  **pid**  Main Post id
 -  **id**    Id of the user posting
 -  **name**  User's name
 -  **timeStamp**
 -  **shares**
 -  **url**
 -  **msg**  Text of the message posted. 
 -  **likes** Number of likes

**comment.csv**

These are comments to the main post. Note, Facebook postings have comments, and comments on comments.  

 - **gid**  Group id
 - **pid**  Matches Main Post identifier in post.csv
 - **cid**  Comment Id.
 - **timeStamp**  
 - **id**  Id of user commenting
 - **name**  Name of user commenting
 - **rid**   Id of user responding to first comment
 - **msg**  Message

**like.csv**

These are likes and responses.  The two keys in this file (pid,cid) will join to post and comment respectively.

 - **gid** Group id
 - **pid**  Matches Main Post identifier in post.csv
 - **cid**  Matches Comments id.
 - **response**  Response such as LIKE, ANGRY etc.
 - **id**  The id of user responding
 - **name** Name of the user responding

**member.csv**

These are all the members in the group.  Some members never, or rarely, post or comment.

 - **gid** Group id 
 - **id** Id of the member
 - **name** Name of the member
 - **url**  URL of the member","2016-06-25 13:24:19","Yesterday's dataset load appeared to be incomplete, so this is an attempt to fix that. ","cheltenham-s-facebook-group.zip",35844140,2
128,58,2,"Japan Trade statistics CSV FORMAT","tradestatisticsjapan2016-1to4","with HS code,Countrhy code","CSV format Trade statistics japan 2016 jan - May  with HS code and Country code
original data http://www.customs.go.jp/toukei/info/index_e.htm

## dataset layout ( summary )

 - exp_imp    1:export 2:import
 - Year
 - Country
 - Value     1000 Yen
 - month
 - hs2    2 digit HS code
 - hs4    4 digit HS code
 - hs6    6 digit HS code
 - hs9   9 digit HS code (japanse local code)

## country code etc
 - [country code ( github )](https://github.com/zanjibar/data4nihon/blob/master/02/Country_eng.csv)
 - [2 digit HS code ( github )](https://github.com/zanjibar/data4nihon/blob/master/02/hs2_eng.csv)
 - [4 digit HS code ( github )](https://github.com/zanjibar/data4nihon/blob/master/02/hs4_eng.csv)","2016-06-29 15:25:58","Trade statistics japan 2016  Jan to May","japan-trade-statistics-csv-format.zip",7543892,1
129,24,6,"2016 US Election","2016-us-election","Explore data related to the 2016 US Election","This contains data relevant for the 2016 US Presidential Election, including up-to-date primary results.

[![sc-rep](https://www.kaggle.io/svf/166413/03fc1f5985d4b7458794e813418f0bac/South%20Carolina_Republican.png)](https://www.kaggle.com/benhamner/d/benhamner/2016-us-election/sc-republican-primary-results)

[![nv-dem](https://www.kaggle.io/svf/166450/4f86d7fc845c9ae1a562e0827347ebc8/Nevada_Democrat.png)](https://www.kaggle.com/benhamner/d/benhamner/2016-us-election/nv-democrat-primary-results)

## Exploration Ideas

 - What candidates within the Republican party have results that are the most anti-correlated?
 - Which Republican candidate is Hillary Clinton most correlated with based on county voting patterns? What about Bernie Sanders?
 - What insights can you discover by mapping this data?

Do you have answers or other exploration ideas? Add your ideas to [this forum post](https://www.kaggle.com/forums/f/1078/2016-us-election/t/19071/exploration-ideas) and share your insights through [Kaggle Scripts](https://www.kaggle.com/benhamner/2016-us-election/scripts)!

Do you think that we should augment this dataset with more data sources? Submit a pull request to this repo, or [let us know here](https://www.kaggle.com/forums/f/1078/2016-us-election/t/19072/additional-data-sources)!

## Data Description

The 2016 US Election dataset contains several main files and folders at the moment. You may download the entire archive via the ""Download Data"" link at the top of the page, or interact with the data in Kaggle Scripts through the `../input` directory.

 - **[primary_results.csv](https://www.kaggle.com/benhamner/d/benhamner/2016-us-election/primary-results-sample-data)**: main primary results file
   - state: state where the primary or caucus was held
   - state_abbreviation: two letter state abbreviation
   - county: county where the results come from
   - fips: [FIPS county code](https://en.wikipedia.org/wiki/FIPS_county_code)
   - party: Democrat or Republican
   - candidate: name of the candidate
   - votes: number of votes the candidate received in the corresponding state and county (may be missing)
   - fraction_votes: fraction of votes the president received in the corresponding state, county, and primary
 - **county_facts.csv**: demographic data on counties from US census
 - **county\_facts\_dictionary.csv**: description of the columns in county_facts
 - **database.sqlite**: SQLite database containing the primary\_results, county\_facts, and county\_facts\_dictionary tables with identical data and schema
 - **county_shapefiles**: directory containing county shapefiles at three different resolutions for mapping

## Original Data Sources

 - [Primary Results from CNN](http://www.cnn.com/election/primaries/counties/ia/Dem)
 - [New Hampshire County-Level Results](https://numeracy.co/projects/2n9KPEk6ShS)
 - [County Shapefiles](https://www.census.gov/geo/maps-data/data/cbf/cbf_counties.html)
 - [County QuickFacts](http://quickfacts.census.gov/qfd/download_data.html)","2016-06-30 20:57:58","Added many new state results","2016-us-election.zip",17990256,0
130,24,7,"2016 US Election","2016-us-election","Explore data related to the 2016 US Election","This contains data relevant for the 2016 US Presidential Election, including up-to-date primary results.

[![sc-rep](https://www.kaggle.io/svf/166413/03fc1f5985d4b7458794e813418f0bac/South%20Carolina_Republican.png)](https://www.kaggle.com/benhamner/d/benhamner/2016-us-election/sc-republican-primary-results)

[![nv-dem](https://www.kaggle.io/svf/166450/4f86d7fc845c9ae1a562e0827347ebc8/Nevada_Democrat.png)](https://www.kaggle.com/benhamner/d/benhamner/2016-us-election/nv-democrat-primary-results)

## Exploration Ideas

 - What candidates within the Republican party have results that are the most anti-correlated?
 - Which Republican candidate is Hillary Clinton most correlated with based on county voting patterns? What about Bernie Sanders?
 - What insights can you discover by mapping this data?

Do you have answers or other exploration ideas? Add your ideas to [this forum post](https://www.kaggle.com/forums/f/1078/2016-us-election/t/19071/exploration-ideas) and share your insights through [Kaggle Scripts](https://www.kaggle.com/benhamner/2016-us-election/scripts)!

Do you think that we should augment this dataset with more data sources? Submit a pull request to this repo, or [let us know here](https://www.kaggle.com/forums/f/1078/2016-us-election/t/19072/additional-data-sources)!

## Data Description

The 2016 US Election dataset contains several main files and folders at the moment. You may download the entire archive via the ""Download Data"" link at the top of the page, or interact with the data in Kaggle Scripts through the `../input` directory.

## Original Data Sources

 - [Primary Results from CNN](http://www.cnn.com/election/primaries/counties/ia/Dem)
 - [New Hampshire County-Level Results](https://numeracy.co/projects/2n9KPEk6ShS)
 - [County Shapefiles](https://www.census.gov/geo/maps-data/data/cbf/cbf_counties.html)
 - [County QuickFacts](http://quickfacts.census.gov/qfd/download_data.html)","2016-06-30 22:33:04","Fixed double-nested county_shapefiles directory in the previous version","2016-us-election.zip",17989392,0
131,24,8,"2016 US Election","2016-us-election","Explore data related to the 2016 US Election","This contains data relevant for the 2016 US Presidential Election, including up-to-date primary results.

[![clinton](https://www.kaggle.io/svf/283669/c4b0b01015e30a34141596ce2eb62d75/hillary_clinton.png)](https://www.kaggle.com/benhamner/d/benhamner/2016-us-election/hillary-clinton-county-results-map)

[![sc-rep](https://www.kaggle.io/svf/166413/03fc1f5985d4b7458794e813418f0bac/South%20Carolina_Republican.png)](https://www.kaggle.com/benhamner/d/benhamner/2016-us-election/hillary-clinton-county-results-map)

## Exploration Ideas

 - What candidates within the Republican party have results that are the most anti-correlated?
 - Which Republican candidate is Hillary Clinton most correlated with based on county voting patterns? What about Bernie Sanders?
 - What insights can you discover by mapping this data?

Do you have answers or other exploration ideas? Add your ideas to [this forum post](https://www.kaggle.com/forums/f/1078/2016-us-election/t/19071/exploration-ideas) and share your insights through [Kaggle Scripts](https://www.kaggle.com/benhamner/2016-us-election/scripts)!

Do you think that we should augment this dataset with more data sources? Submit a pull request to this repo, or [let us know here](https://www.kaggle.com/forums/f/1078/2016-us-election/t/19072/additional-data-sources)!

## Data Description

The 2016 US Election dataset contains several main files and folders at the moment. You may download the entire archive via the ""Download Data"" link at the top of the page, or interact with the data in Kaggle Scripts through the `../input` directory.

## Original Data Sources

 - [Primary Results from CNN](http://www.cnn.com/election/primaries/counties/ia/Dem)
 - [New Hampshire County-Level Results](https://numeracy.co/projects/2n9KPEk6ShS)
 - [County Shapefiles](https://www.census.gov/geo/maps-data/data/cbf/cbf_counties.html)
 - [County QuickFacts](http://quickfacts.census.gov/qfd/download_data.html)","2016-07-01 04:12:24","Fixed some missing counties (e.g. O'Brien, IA)","2016-us-election.zip",18203349,18
132,59,1,"Breast Cancer Proteomes","breastcancerproteomes","Dividing breast cancer patients into separate sub-classes","**Context:**
This data set contains published iTRAQ proteome profiling of 77 breast cancer samples generated by the Clinical Proteomic Tumor Analysis Consortium (NCI/NIH). It contains expression values for ~12.000 proteins for each sample, with missing values present when a given protein could not be quantified in a given sample.

**Content:**
File: 77_cancer_proteomes_CPTAC_itraq.csv

 - RefSeq_accession_number: RefSeq protein ID (each protein has a unique
   ID in a RefSeq database)
 - gene_symbol: a symbol unique to each gene (every protein is encoded
   by some gene)
 - gene_name: a full name of that gene
 Remaining columns: log2 iTRAQ ratios for each sample (protein
   expression data, most important), three last columns are from healthy
   individuals

File: clinical_data_breast_cancer.csv

First column ""Complete TCGA ID"" is used to match the sample IDs in the main cancer proteomes file (see example script).
All other columns have self-explanatory names, contain data about the cancer classification of a given sample using different methods. 'PAM50 mRNA' classification is being used in the example script. 


File: PAM50_proteins.csv

Contains the list of genes and proteins used by the PAM50 classification system. The column RefSeqProteinID contains the protein IDs that can be matched with the IDs in the main protein expression data set.

**Past Research:**
The original study: http://www.nature.com/nature/journal/v534/n7605/full/nature18003.html (paywall warning)

In brief: the data were used to assess how the mutations in the DNA are affecting the protein expression landscape in breast cancer. Genes in our DNA are first transcribed into RNA molecules which then are translated into proteins. Changing the information content of DNA has impact on the behavior of the proteome, which is the main functional unit of cells, taking care of cell division, DNA repair, enzymatic reactions and signaling etc.

**Inspiration:**

This is an interesting study and I myself wanted to use this breast cancer proteome data set for other types of analyses using machine learning that I am performing as a part of my PhD. However, I though that the Kaggle community (or at least that part with biomedical interests) would enjoy playing with it. I added a simple K-means clustering example for that data with some comments, the same approach as used in the original paper.
One thing is that there is a panel of genes, the PAM50 which is used to classify breast cancers into subtypes. This panel was originally based on the RNA expression data which is (in my opinion) not as robust as the measurement of mRNA's final product, the protein. Perhaps using this data set, someone could find a different set of proteins (they all have unique NP_/XP_ identifiers) that would divide the data set even more robustly? Perhaps into a higher numbers of clusters with very distinct protein expression signatures?

**Example K-means analysis script:**
http://pastebin.com/A0Wj41DP","2016-07-03 16:15:59","Initial release","breastcancerproteomes.zip",6027106,0
133,59,2,"Breast Cancer Proteomes","breastcancerproteomes","Dividing breast cancer patients into separate sub-classes","**Context:**
This data set contains published iTRAQ proteome profiling of 77 breast cancer samples generated by the Clinical Proteomic Tumor Analysis Consortium (NCI/NIH). It contains expression values for ~12.000 proteins for each sample, with missing values present when a given protein could not be quantified in a given sample.

**Content:**
File: 77_cancer_proteomes_CPTAC_itraq.csv

 - RefSeq_accession_number: RefSeq protein ID (each protein has a unique
   ID in a RefSeq database)
 - gene_symbol: a symbol unique to each gene (every protein is encoded
   by some gene)
 - gene_name: a full name of that gene
 Remaining columns: log2 iTRAQ ratios for each sample (protein
   expression data, most important), three last columns are from healthy
   individuals

File: clinical_data_breast_cancer.csv

First column ""Complete TCGA ID"" is used to match the sample IDs in the main cancer proteomes file (see example script).
All other columns have self-explanatory names, contain data about the cancer classification of a given sample using different methods. 'PAM50 mRNA' classification is being used in the example script. 


File: PAM50_proteins.csv

Contains the list of genes and proteins used by the PAM50 classification system. The column RefSeqProteinID contains the protein IDs that can be matched with the IDs in the main protein expression data set.

**Past Research:**
The original study: http://www.nature.com/nature/journal/v534/n7605/full/nature18003.html (paywall warning)

In brief: the data were used to assess how the mutations in the DNA are affecting the protein expression landscape in breast cancer. Genes in our DNA are first transcribed into RNA molecules which then are translated into proteins. Changing the information content of DNA has impact on the behavior of the proteome, which is the main functional unit of cells, taking care of cell division, DNA repair, enzymatic reactions and signaling etc.

**Inspiration:**

This is an interesting study and I myself wanted to use this breast cancer proteome data set for other types of analyses using machine learning that I am performing as a part of my PhD. However, I though that the Kaggle community (or at least that part with biomedical interests) would enjoy playing with it. I added a simple K-means clustering example for that data with some comments, the same approach as used in the original paper.
One thing is that there is a panel of genes, the PAM50 which is used to classify breast cancers into subtypes. This panel was originally based on the RNA expression data which is (in my opinion) not as robust as the measurement of mRNA's final product, the protein. Perhaps using this data set, someone could find a different set of proteins (they all have unique NP_/XP_ identifiers) that would divide the data set even more robustly? Perhaps into a higher numbers of clusters with very distinct protein expression signatures?

**Example K-means analysis script:**
http://pastebin.com/A0Wj41DP","2016-07-03 16:22:12","Removed some unnecessary columns to make the file cleaner.","breastcancerproteomes.zip",5675295,0
134,59,3,"Breast Cancer Proteomes","breastcancerproteomes","Dividing breast cancer patients into separate sub-classes","**Context:**
This data set contains published iTRAQ proteome profiling of 77 breast cancer samples generated by the Clinical Proteomic Tumor Analysis Consortium (NCI/NIH). It contains expression values for ~12.000 proteins for each sample, with missing values present when a given protein could not be quantified in a given sample.

**Content:**

**File:** 77_cancer_proteomes_CPTAC_itraq.csv

 - RefSeq_accession_number: RefSeq protein ID (each protein has a unique
   ID in a RefSeq database)
 - gene_symbol: a symbol unique to each gene (every protein is encoded
   by some gene)
 - gene_name: a full name of that gene
 Remaining columns: log2 iTRAQ ratios for each sample (protein
   expression data, most important), three last columns are from healthy
   individuals

**File:** clinical_data_breast_cancer.csv

First column ""Complete TCGA ID"" is used to match the sample IDs in the main cancer proteomes file (see example script).
All other columns have self-explanatory names, contain data about the cancer classification of a given sample using different methods. 'PAM50 mRNA' classification is being used in the example script. 


**File:** PAM50_proteins.csv

Contains the list of genes and proteins used by the PAM50 classification system. The column RefSeqProteinID contains the protein IDs that can be matched with the IDs in the main protein expression data set.

**Past Research:**
The original study: http://www.nature.com/nature/journal/v534/n7605/full/nature18003.html (paywall warning)

In brief: the data were used to assess how the mutations in the DNA are affecting the protein expression landscape in breast cancer. Genes in our DNA are first transcribed into RNA molecules which then are translated into proteins. Changing the information content of DNA has impact on the behavior of the proteome, which is the main functional unit of cells, taking care of cell division, DNA repair, enzymatic reactions and signaling etc. They performed K-means clustering on the protein data to divide the breast cancer patients into sub-types, each having unique protein expression signature. They found that the best clustering was achieved using 3 clusters (original PAM50 gene set yields four different subtypes using RNA data).

**Inspiration:**

This is an interesting study and I myself wanted to use this breast cancer proteome data set for other types of analyses using machine learning that I am performing as a part of my PhD. However, I though that the Kaggle community (or at least that part with biomedical interests) would enjoy playing with it. I added a simple K-means clustering example for that data with some comments, the same approach as used in the original paper.
One thing is that there is a panel of genes, the PAM50 which is used to classify breast cancers into subtypes. This panel was originally based on the RNA expression data which is (in my opinion) not as robust as the measurement of mRNA's final product, the protein. Perhaps using this data set, someone could find a different set of proteins (they all have unique NP_/XP_ identifiers) that would divide the data set even more robustly? Perhaps into a higher numbers of clusters with very distinct protein expression signatures?

**Example K-means analysis script:**
http://pastebin.com/A0Wj41DP","2016-07-03 16:24:56","Re-uploaded the data.","breastcancerproteomes.zip",5680320,5
135,9,6,"Meta Kaggle","meta-kaggle","Kaggle's public data on competitions, users, submission scores, and scripts","We aren't saying this dataset is the Rosetta Stone of machine learning competitions, but we do think there is a lot to learn from (and a lot of fun to be had by) releasing some of our most interesting tables on Kaggle community and competition activity.

Strategizing to become a Master? Wondering who, where, and what goes in to a winning team? Deciding between evaluation metrics for your next data science project? We hope the scripts published here will enrich and entertain Kagglers, spark some lively conversations, and act as a resource for the larger machine learning community.

[![private leaderboard performance over time](https://www.kaggle.io/svf/214627/b025ac60d2bd52ce7321c5257b7d891b/private_leaderboard_aggregate_performance_over_time.png)](https://www.kaggle.com/benhamner/d/kaggle/meta-kaggle/kaggle-leaderboard-performance-over-time)

This data (available through Kaggle Scripts as CSV files and a SQLite database) contains the tables listed below.

Note that this data is not a complete dump: rows, columns, and tables have been filtered out, and it is a small subset of the data that we can release publicly. Over time, we'll add more of the tables that we can release publicly to it.

[![wordcloud](https://www.kaggle.io/svf/214832/ebc77bde9dd3c079bcc52573fcedbd3e/__results___files/figure-html/unnamed-chunk-4-1.png)](https://www.kaggle.com/benhamner/d/kaggle/meta-kaggle/kaggle-forum-post-wordclouds)","2016-07-06 02:16:55","Updated as of July 5, 2016. Now quoting all string fields, which should fix some downstream CSV import issues","meta-kaggle.zip",472465721,6
137,48,11,"SEPTA - Regional Rail","on-time-performance","Predict  Arrival Times of Philadelphia's Regional Trains.","## SEPTA - Southeastern Pennsylvania Transportation Authority ##
 

The SEPTA Regional Rail system consists of commuter rail service on 13 branches to more than 150 active stations in Philadelphia, Pennsylvania, and its suburbs and satellite cities.  

SEPTA uses On-Time Performance (OTP)  to measure service reliability. OTP identifies the number of trains for all rail lines that arrive at their scheduled destination at the scheduled time.  However, by industry standard,  a train may arrive up to 5 minutes and 59 seconds after its scheduled time and still be considered on-time.  

SEPTA has established an annual goal of 91% for Regional Rail On-Time Performance.  How well are they doing?  Is it even a meaningful measure?


#Data Description

**otp.csv**

 - **train_id**
 - **direction**   *('N' or 'S'   direction is demarcated as either Northbound or Southbound)[^1](https://www.kaggle.com/forums/f/1300/septa-regional-rail/t/21259/why-are-all-trains-originating-in-thorndale-marked-as-southbound)*
 - **origin**        *(See map below - you'll see 'Warminster', 'Glenside',...'Airport Terminal..')*
 - **next_station**  *(Think of this as the station stop, at timeStamp)* 
 - **date**
 - **status**    *('On Time', '5 min', '10 min'.  This is a status on train lateness. 999 is a suspended train)*
 - **timeStamp**


----------


**trainView.csv - GPS Train data (early release)**


Most GPS coordinates are based on track telemetry; however, cars are being equipped with GPS units. 

- **train_id**
- **status**
- **next_station**
- **service**
- **dest**
- **lon**
- **lat**
- **source**
- **track_change**
- **track**
- **date**
- **timeStamp0**   First timeStamp at coordinates.
- **timeStamp1**    Last timeStamp at coordinates.

You can look at the example [here](https://www.kaggle.com/septa/d/septa/on-time-performance/trainview-gps-data-early-beta) on how to track a single train.

**What To Look For...**

Ah, as a commuter you care more about the performance of the train(s) you plan to take.  OTP maybe 91% or above; but, if the train **you take** runs late, you'll spend extra time on the tracks.  If it consistently runs late, maybe the schedule should be changed.

Look for patterns. For example, during the Tuesday morning rush do some trains run consistently late?   How long does it take to get to from point A to point B in the system?  Performance is VERY important to SEPTA. 

----------


Below is a map of the system and station stops.  This dataset contains data on the Regional Rail Lines.

![SEPTA map][1]

SEPTA train schedules can be found [here](http://www.septa.org/schedules/rail/).  Note different Weekday, Saturday, and Sunday schedules.   



  [1]: https://raw.githubusercontent.com/mchirico/mchirico.github.io/master/p/images/septaMap.png","2016-07-07 01:57:32","Updated data for SEPTA.  ","on-time-performance.zip",94564221,0
153,63,1,"Soccer database, +25K matches and players stats","soccer","The ultimate soccer database for data analysis and machine learning","## The ultimate Soccer database for data analysis and machine learning ##

> Expect new releases of the database in the next few days, fixes and new features to include: missing matches, improvement of foreign keys, more player, betting odds


 - +25,000 matches
 - +10,000 players
 - 11 European Countries with their lead championship
 - Seasons 2008 to 2016
 - Player's statistics sourced from EA Sports' FIFA video game series, including the weekly updates
 - Team line up with squad formation (X, Y coordinates)
 - Detailed events (goal types, possession, corner, cross, fouls, cards etc...) for +10,000 matches","2016-07-09 19:40:44","Initial release","soccer-database-25k-matches-and-players-stats.zip",7538876,4
154,63,2,"Soccer database, +25K matches and players stats","soccer","The ultimate soccer database for data analysis and machine learning","## The ultimate Soccer database for data analysis and machine learning ##

> Expect new releases of the database in the next few days, fixes and new features to include: missing matches, improvement of foreign keys, more player, betting odds


 - +25,000 matches
 - +10,000 players
 - 11 European Countries with their lead championship
 - Seasons 2008 to 2016
 - Player's statistics sourced from EA Sports' FIFA video game series, including the weekly updates
 - Team line up with squad formation (X, Y coordinates)
 - Detailed events (goal types, possession, corner, cross, fouls, cards etc...) for +10,000 matches","2016-07-11 18:12:07","Fixed missing matches and players. Reviewed use of foreign keys. Added historical betting odds","soccer.zip",10067074,3
155,63,3,"European Soccer Database","soccer","25k+ matches and players stats for European Professional Football","The ultimate Soccer database for data analysis and machine learning 
======================================================
----------

**What you get:** 

 - +25,000 matches
 - +10,000 players
 - 11 European Countries with their lead championship
 - Seasons 2008 to 2016
 - Player's statistics sourced from EA Sports' FIFA video game series, including the weekly updates
 - Team line up with squad formation (X, Y coordinates)
 - Betting odds from up to 10 providers
 - Detailed match events (goal types, possession, corner, cross, fouls, cards etc...) for +10,000 matches

----------","2016-07-12 18:43:25","Corrected an error with players XY positions: Home players XY was always equal to Away players XY","european-soccer-database.zip",10104684,6
156,63,4,"European Soccer Database","soccer","25k+ matches and players stats for European Professional Football","The ultimate Soccer database for data analysis and machine learning 
======================================================
----------

**What you get:** 

 - +25,000 matches
 - +10,000 players
 - 11 European Countries with their lead championship
 - Seasons 2008 to 2016
 - Player's statistics sourced from EA Sports' FIFA video game series, including the weekly updates
 - Team line up with squad formation (X, Y coordinates)
 - Betting odds from up to 10 providers
 - Detailed match events (goal types, possession, corner, cross, fouls, cards etc...) for +10,000 matches

----------","2016-07-14 19:41:28","Matches with in game events part 1","soccer.zip",13299533,1
157,63,5,"European Soccer Database","soccer","25k+ matches and players stats for European Professional Football","The ultimate Soccer database for data analysis and machine learning
-------------------------------------------------------------------

**What you get:**

 - +25,000 matches
 - +10,000 players
 - 11 European Countries with their lead championship
 - Seasons 2008 to 2016
 - Player's statistics sourced from EA Sports' FIFA video game series, including the weekly updates
 - Team line up with squad formation (X, Y coordinates)
 - Betting odds from up to 10 providers
 - Detailed match events (goal types, possession, corner, cross, fouls, cards etc...) for +10,000 matches

----------

**Original Data Source:** 

You can easily find data about soccer matches but they are usually scattered across different websites. A thorough data collection and processing has been done to make your life easier. **I must insist that you do not make any commercial use of the data**. The data was sourced from:

 - [http://football-data.mx-api.enetscores.com/][1] : scores, lineup, team formation and events

 - [http://www.football-data.co.uk/][2] : betting odds

 - [http://sofifa.com/][3] : statistics from EA Sports FIFA games. *FIFA series and all FIFA assets property of EA Sports.*

> When you have a look at the database, you will notice foreign keys for
> players and matches are the same as the original data sources. I have
> called those foreign keys ""api_id"".

----------

**Improving the dataset:** 

You will notice that some players are missing from the lineup (NULL values). This is because I have not been able to source their statistics from FIFA. This will be fixed overtime as the crawling algorithm is being improved.
The dataset will also be expanded to include international games, national cups, Champion's League and Europa League. Please ask me if you're after a specific tournament.

> If you want to help me improve this dataset please get in touch!

----------

Exploring the data:
-------------------

Now that's the fun part, there is a lot you can do with this dataset. I will be adding visuals and insights to this overview page but have a look at kernels and give it a try yourself ! Here are some ideas for you:

**The Holy Grail...**
... is obviously to predict the outcome of the game. The bookies use 3 classes (Home Win, Draw, Away Win). They get it right about 53% of the time. This is also what I've achieved so far using my own SVM. Though it may sound high for such a random sport game, you've got to known
 that the home team wins about 46% of the time. So the base case (constantly predicting Home Win) has indeed 46% precision. 

**Probabilities vs Odds**

When running a multi-classification algo like SVM you could also output a probability estimate and compare it to the betting odds. Have a look at your variance vs odds and see for what games you had completely different 

**Explore and visualize features**

With access to stats, team formations and in-game events you be able to produce some interesting insights into [The Beautiful Game][4] . Who knows, Guardiola himself may hire one of you some day!


  [1]: http://football-data.mx-api.enetscores.com/
  [2]: http://www.football-data.co.uk/
  [3]: http://sofifa.com/
  [4]: https://en.wikipedia.org/wiki/The_Beautiful_Game","2016-07-14 20:11:57","Database without in-game events","soccer.zip",10104684,25
158,54,14,"Cheltenham's Facebook Groups","cheltenham-s-facebook-group","Discover How a Community Leverages Facebook","Facebook is becoming an essential tool for more than just family and friends.  Discover how Cheltenham Township (USA), a diverse community just outside of Philadelphia, deals with major issues such as the Bill Cosby trial, everyday traffic issues, sewer I/I problems and lost cats and dogs.  And yes, theft.  

Communities work when they're connected and exchanging information.  What and who are the essential forces making a positive impact, and when and how do conversational threads get directed or misdirected?     


**Use Any Facebook Public Group**

You can leverage the examples here for **any** public Facebook group.  For an example of the source code used to collect this data, and a quick start docker image, take a look at the following project: [facebook-group-scrape](https://github.com/mchirico/facebook-group-scrape).  


**Data Sources**

There are only 4 files in the dataset. But, it contains every post made to the following 3 public Facebook groups

 - [**Unofficial Cheltenham Township**](https://www.facebook.com/groups/25160801076/)
 - [**Elkins Park Happenings!**](https://www.facebook.com/groups/117291968282998/)
 - [**Free Speech Zone**](https://www.facebook.com/groups/1443890352589739/)

 
**post.csv**

These are the main posts you will see on the page.  It might help to take a quick look at the [page](https://www.facebook.com/groups/25160801076/).  Commas in the **msg** field have been replaced with {COMMA}, and apostrophes have been replaced with {APOST}. 


 - **gid** Group id (3 different Facebook groups) 
 -  **pid**  Main Post id
 -  **id**    Id of the user posting
 -  **name**  User's name
 -  **timeStamp**
 -  **shares**
 -  **url**
 -  **msg**  Text of the message posted. 
 -  **likes** Number of likes

**comment.csv**

These are comments to the main post. Note, Facebook postings have comments, and comments on comments.  

 - **gid**  Group id
 - **pid**  Matches Main Post identifier in post.csv
 - **cid**  Comment Id.
 - **timeStamp**  
 - **id**  Id of user commenting
 - **name**  Name of user commenting
 - **rid**   Id of user responding to first comment (Fixed on the next data set: incorrectly pulled id of post)
 - **msg**  Message

**like.csv**

These are likes and responses.  The two keys in this file (pid,cid) will join to post and comment respectively.

 - **gid** Group id
 - **pid**  Matches Main Post identifier in post.csv
 - **cid**  Matches Comments id.
 - **response**  Response such as LIKE, ANGRY etc.
 - **id**  The id of user responding
 - **name** Name of the user responding

**member.csv**

These are all the members in the group.  Some members never, or rarely, post or comment.

 - **gid** Group id 
 - **id** Id of the member
 - **name** Name of the member
 - **url**  URL of the member","2016-07-15 02:06:58","Added new data and fixed an issue with duplicates in comments.","cheltenham-s-facebook-group.zip",37252243,0
159,48,12,"SEPTA - Regional Rail","on-time-performance","Predict  Arrival Times of Philadelphia's Regional Trains.","## SEPTA - Southeastern Pennsylvania Transportation Authority ##
 

The SEPTA Regional Rail system consists of commuter rail service on 13 branches to more than 150 active stations in Philadelphia, Pennsylvania, and its suburbs and satellite cities.  

SEPTA uses On-Time Performance (OTP)  to measure service reliability. OTP identifies the number of trains for all rail lines that arrive at their scheduled destination at the scheduled time.  However, by industry standard,  a train may arrive up to 5 minutes and 59 seconds after its scheduled time and still be considered on-time.  

SEPTA has established an annual goal of 91% for Regional Rail On-Time Performance.  How well are they doing?  Is it even a meaningful measure?


#Data Description

**otp.csv**

 - **train_id**
 - **direction**   *('N' or 'S'   direction is demarcated as either Northbound or Southbound)[^1](https://www.kaggle.com/forums/f/1300/septa-regional-rail/t/21259/why-are-all-trains-originating-in-thorndale-marked-as-southbound)*
 - **origin**        *(See map below - you'll see 'Warminster', 'Glenside',...'Airport Terminal..')*
 - **next_station**  *(Think of this as the station stop, at timeStamp)* 
 - **date**
 - **status**    *('On Time', '5 min', '10 min'.  This is a status on train lateness. 999 is a suspended train)*
 - **timeStamp**


----------


**trainView.csv - GPS Train data (early release)**


Most GPS coordinates are based on track telemetry; however, cars are being equipped with GPS units. 

- **train_id**
- **status**
- **next_station**
- **service**
- **dest**
- **lon**
- **lat**
- **source**
- **track_change**
- **track**
- **date**
- **timeStamp0**   First timeStamp at coordinates.
- **timeStamp1**    Last timeStamp at coordinates.

You can look at the example [here](https://www.kaggle.com/septa/d/septa/on-time-performance/trainview-gps-data-early-beta) on how to track a single train.

**What To Look For...**

Ah, as a commuter you care more about the performance of the train(s) you plan to take.  OTP maybe 91% or above; but, if the train **you take** runs late, you'll spend extra time on the tracks.  If it consistently runs late, maybe the schedule should be changed.

Look for patterns. For example, during the Tuesday morning rush do some trains run consistently late?   How long does it take to get to from point A to point B in the system?  Performance is VERY important to SEPTA. 

----------


Below is a map of the system and station stops.  This dataset contains data on the Regional Rail Lines.

![SEPTA map][1]

SEPTA train schedules can be found [here](http://www.septa.org/schedules/rail/).  Note different Weekday, Saturday, and Sunday schedules.   



  [1]: https://raw.githubusercontent.com/mchirico/mchirico.github.io/master/p/images/septaMap.png","2016-07-15 02:12:16","Refreshed the data to 2016-07-14","on-time-performance.zip",96519756,0
161,76,1,"Zika Virus Epidemic","zika-virus-epidemic","Analyze the ongoing spread of this infectious disease","An outbreak of the Zika virus, an infection transmitted mostly by the *Aedes* species mosquito (*Ae. aegypti* and *Ae. albopictus*), has been sweeping across the Americas and the Pacific since mid-2015. Although first isolated in 1947 in Uganda, a lack of previous research has challenged the scientific community to quickly understand its devastating effects as the epidemic [continues to spread][1].

**All Countries & Territories with Active Zika Virus Transmission**
<img src=""http://www.cdc.gov/zika/images/zikamain_071416_880.jpg"" width=""600"">

#The data
This dataset shares publicly available data related to the ongoing Zika epidemic. It is being provided as a resource to the scientific community engaged in the public health response. The data provided here is not official and should be considered provisional and non-exhaustive. The data in reports may change over time, reflecting delays in reporting or changes in classifications. And while accurate representation of the reported data is the objective in the machine readable files shared here, that accuracy is not guaranteed. Before using any of these data, it is advisable to review the original reports and sources, which are provided whenever possible along with further information on the [CDC Zika epidemic GitHub repo][3].

The dataset includes the following fields:

* **report_date** - The report date is the date that the report was published. The date should be specified in standard ISO format (YYYY-MM-DD).

* **location** - A location is specified for each observation following the specific names specified in the country place name database. This may be any place with a 'location_type' as listed below, e.g. city, state, country, etc. It should be specified at up to three hierarchical levels in the following format: [country]-[state/province]-[county/municipality/city], always beginning with the country name. If the data is for a particular city, e.g. Salvador, it should be specified: Brazil-Bahia-Salvador.

* **location_type** - A location code is included indicating: city, district, municipality, county, state, province, or country. If there is need for an additional 'location_type', open an Issue to create a new 'location_type'.

* **data_field** - The data field is a short description of what data is represented in the row and is related to a specific definition defined by the report from which it comes.

* **data_field_code** - This code is defined in the country data guide. It includes a two letter country code (ISO-3166 alpha-2, [list][4]), followed by a 4-digit number corresponding to a specific report type and data type.

* **time_period** - Optional. If the data pertains to a specific period of time, for example an epidemiological week, that number should be indicated here and the type of time period in the 'time_period_type', otherwise it should be NA.

* **time_period_type** - Required only if 'time_period' is specified. Types will also be specified in the country data guide. Otherwise should be NA.

* **value** - The observation indicated for the specific 'report_date', 'location', 'data_field' and when appropriate, 'time_period'.

* **unit** - The unit of measurement for the 'data_field'. This should conform to the 'data_field' unit options as described in the country-specific data guide.

If you find the data useful, please support data sharing by referencing this dataset and the original data source. If you're interested in contributing to the Zika project from GitHub, you can [read more here][5]. The source for the Zika virus structure is available [here][6].


  [1]: http://www.cdc.gov/zika/geo/active-countries.html ""continues to spread""
  [2]: http://www.cdc.gov/zika/images/zikamain_071416_880.jpg
  [3]: https://github.com/cdcepi/zika ""CDC Zika epidemic GitHub repo""
  [4]: http://www.iso.org/iso/home/standards/country_codes/country_names_and_code_elements_txt-temp.htm ""list""
  [5]: https://github.com/cdcepi/zika/blob/master/how_to_contribute.md ""read more here""
  [6]: http://www.rcsb.org/pdb/explore/explore.do?structureId=5IRE ""here""","2016-07-16 03:50:21","Initial release","zika-virus-epidemic.zip",832097,4
162,77,1,"Predicting Terrorism","predicting-terrorism","Using data to predict attacks and save lives","[ Extremists send rockets into a residential neighborhood, taking out a child and two women.][1] Aleppo, Syria.

[ Suicide bombers attack Yemeni army checkpoints, killing **10**.][2] Yemen, Mukalla.  

[ **5** killed, 9 injured in Almaty terrorist attack on police station [GRAPHIC].][3] Kazakhstan,Almaty.

These three extremists attacks happened last 24 hours. 18th of July, 2016.

Can we predict where the next terrorist attacks will probably happen?
------------------------------------------------------------------------

Maybe the best weapon to fight extremists is [information][4], and machine learning is already being used to predict and prevent terrorist attacks. The [Religion of Peace][5] website gathers events that resulted in killings and which were committed out of religious duty since 2002.

You can find a table summary of Date, Country, City, (# of people) Killed, (# of people) Injured, Description (including type of attack descriptors such as ""bomb"",""car"",""shooting"",""rocket"").

I webscraped the data using R rvest.

    # Webscraping in Terror data.
    # You can use the following gadget to find out which nodes to select in a page.
    # https://cran.r-project.org/web/packages/rvest/vignettes/selectorgadget.html
    
    library(dplyr) 
    library(rvest) #Webscraping.
    
    # Strings and declaring variables.
    #Link with data.
    data.link <-""https://www.thereligionofpeace.com/attacks/attacks.aspx?"" 
    Years <- as.character(2002:2016)
    links <- paste(data.link,""Yr="",Years,sep = """")
    terror.nodes <- terror.data <- vector(""list"",length(links))
    
    ## For loop to extract data
    for (i in 1:length(links)){ 
      terror.nodes[[i]] <- read_html(links[i]) %>% html_nodes(xpath=""//table"")
       #The 11 below is the node where the table with the data is.
      terror.data[[i]] <- as.data.frame(html_table(terror.nodes[[i]][11]))
    }
    
    #Combining datasets, formating Date and saving .csv
    terror.alldata <- do.call(""rbind"",terror.data)
    #Convert strings with dates to date format
    terror.alldata$Date <- as.Date(terror.alldata$Date,""%Y.%m.%d"")
    write.csv2(terror.alldata,""~/attacks_data.csv"")

I have not worked on the analysis yet, but we have geospacial distribution, type (hidden in the Description strings) and magnitude of the attacks. There's also the possibility of using socioeconomical data available for the places listed.
    
[1]: http://en.abna24.com/service/middle-east-west-asia/archive/2016/07/18/766603/story.html
[2]: http://www.reuters.com/article/us-yemen-security-idUSKCN0ZY0BI
[3]: https://www.rt.com/news/351790-shooting-almaty-police-kazakhstan/
[4]: https://www.washingtonpost.com/news/the-switch/wp/2016/07/10/how-artificial-intelligence-could-help-warn-us-of-another-dallas/
[5]: http://www.thereligionofpeace.com/","2016-07-19 01:47:34","Initial release","predicting-terrorism.zip",963367,0
163,77,2,"Predicting Terrorism","predicting-terrorism","Using data to predict attacks and save lives","[ Extremists send rockets into a residential neighborhood, taking out a child and two women.][1] Aleppo, Syria.

[ Suicide bombers attack Yemeni army checkpoints, killing **10**.][2] Yemen, Mukalla.  

[ **5** killed, 9 injured in Almaty terrorist attack on police station [GRAPHIC].][3] Kazakhstan,Almaty.

These three extremists attacks happened last 24 hours. 18th of July, 2016.

Can we predict where the next terrorist attacks will probably happen?
------------------------------------------------------------------------

Maybe the best weapon to fight extremists is [information][4], and machine learning is already being used to predict and prevent terrorist attacks. The [Religion of Peace][5] website gathers events that resulted in killings and which were committed out of religious duty since 2002.

You can find a table summary of Date, Country, City, (# of people) Killed, (# of people) Injured, Description (including type of attack descriptors such as ""bomb"",""car"",""shooting"",""rocket"").

I webscraped the data using R rvest.

    # Webscraping in terror data by ping_freud
    # You can use the following gadget to find out which nodes to select in a page 
    # https://cran.r-project.org/web/packages/rvest/vignettes/selectorgadget.html
    library(dplyr) #For %>% pipe flow
    library(ggplot2)
    library(rvest) #Webscraping
    
    # Building strings and declaring variables.
    # Link with data.
    data.link <-""https://www.thereligionofpeace.com/attacks/attacks.aspx?""
    Years <- as.character(2002:2016)
    links <- paste(data.link,""Yr="",Years,sep = """")
    terror.nodes <- terror.data <- vector(""list"",length(links))
    
    # For loop to extract data
    for (i in 1:length(links)){ 
      terror.nodes[[i]] <- read_html(links[i]) %>% html_nodes(xpath=""//table"")
      #11 is the node where the table with data is 
      terror.data[[i]] <- as.data.frame(html_table(terror.nodes[[i]][11])) 
      terror.data[[i]] <- terror.data[[i]][nrow(terror.data[[i]]):1,]
    }
    
    # Combines data frames
    terror.alldata <- do.call(""rbind"",terror.data)
    # Convert strings with dates to date format
    terror.alldata$Date <- as.Date(terror.alldata$Date,""%Y.%m.%d"")
    row.names(terror.alldata) <- as.character(1:nrow(terror.alldata))
    write.csv(terror.alldata,""attacks_data.csv"")

I have not worked on the analysis yet, but we have geospacial distribution, type (hidden in the Description strings) and magnitude of the attacks. There's also the possibility of using socioeconomical data available for the places listed.
    
[1]: http://en.abna24.com/service/middle-east-west-asia/archive/2016/07/18/766603/story.html
[2]: http://www.reuters.com/article/us-yemen-security-idUSKCN0ZY0BI
[3]: https://www.rt.com/news/351790-shooting-almaty-police-kazakhstan/
[4]: https://www.washingtonpost.com/news/the-switch/wp/2016/07/10/how-artificial-intelligence-could-help-warn-us-of-another-dallas/
[5]: http://www.thereligionofpeace.com/","2016-07-19 18:05:03","Version 2 - Fixed ID, included 2016, reversed chronological order","predicting-terrorism.zip",1020813,2
164,54,15,"Cheltenham's Facebook Groups","cheltenham-s-facebook-group","Discover How a Community Leverages Facebook","Facebook is becoming an essential tool for more than just family and friends.  Discover how Cheltenham Township (USA), a diverse community just outside of Philadelphia, deals with major issues such as the Bill Cosby trial, everyday traffic issues, sewer I/I problems and lost cats and dogs.  And yes, theft.  

Communities work when they're connected and exchanging information.  What and who are the essential forces making a positive impact, and when and how do conversational threads get directed or misdirected?     


**Use Any Facebook Public Group**

You can leverage the examples here for **any** public Facebook group.  For an example of the source code used to collect this data, and a quick start docker image, take a look at the following project: [facebook-group-scrape](https://github.com/mchirico/facebook-group-scrape).  


**Data Sources**

There are only 4 files in the dataset. But, it contains every post made to the following 3 public Facebook groups

 - [**Unofficial Cheltenham Township**](https://www.facebook.com/groups/25160801076/)
 - [**Elkins Park Happenings!**](https://www.facebook.com/groups/117291968282998/)
 - [**Free Speech Zone**](https://www.facebook.com/groups/1443890352589739/)

 
**post.csv**

These are the main posts you will see on the page.  It might help to take a quick look at the [page](https://www.facebook.com/groups/25160801076/).  Commas in the **msg** field have been replaced with {COMMA}, and apostrophes have been replaced with {APOST}. 


 - **gid** Group id (3 different Facebook groups) 
 -  **pid**  Main Post id
 -  **id**    Id of the user posting
 -  **name**  User's name
 -  **timeStamp**
 -  **shares**
 -  **url**
 -  **msg**  Text of the message posted. 
 -  **likes** Number of likes

**comment.csv**

These are comments to the main post. Note, Facebook postings have comments, and comments on comments.  

 - **gid**  Group id
 - **pid**  Matches Main Post identifier in post.csv
 - **cid**  Comment Id.
 - **timeStamp**  
 - **id**  Id of user commenting
 - **name**  Name of user commenting
 - **rid**   Id of user responding to first comment 
 - **msg**  Message

**like.csv**

These are likes and responses.  The two keys in this file (pid,cid) will join to post and comment respectively.

 - **gid** Group id
 - **pid**  Matches Main Post identifier in post.csv
 - **cid**  Matches Comments id.
 - **response**  Response such as LIKE, ANGRY etc.
 - **id**  The id of user responding
 - **name** Name of the user responding

**member.csv**

These are all the members in the group.  Some members never, or rarely, post or comment.

 - **gid** Group id 
 - **id** Id of the member
 - **name** Name of the member
 - **url**  URL of the member","2016-07-20 00:40:16","Comment rid (response user id) should now be correct.","cheltenham-s-facebook-group.zip",37841685,0
165,48,13,"SEPTA - Regional Rail","on-time-performance","Predict  Arrival Times of Philadelphia's Regional Trains.","## SEPTA - Southeastern Pennsylvania Transportation Authority ##
 

The SEPTA Regional Rail system consists of commuter rail service on 13 branches to more than 150 active stations in Philadelphia, Pennsylvania, and its suburbs and satellite cities.  

SEPTA uses On-Time Performance (OTP)  to measure service reliability. OTP identifies the number of trains for all rail lines that arrive at their scheduled destination at the scheduled time.  However, by industry standard,  a train may arrive up to 5 minutes and 59 seconds after its scheduled time and still be considered on-time.  

SEPTA has established an annual goal of 91% for Regional Rail On-Time Performance.  How well are they doing?  Is it even a meaningful measure?


#Data Description

**otp.csv**

 - **train_id**
 - **direction**   *('N' or 'S'   direction is demarcated as either Northbound or Southbound)[^1](https://www.kaggle.com/forums/f/1300/septa-regional-rail/t/21259/why-are-all-trains-originating-in-thorndale-marked-as-southbound)*
 - **origin**        *(See map below - you'll see 'Warminster', 'Glenside',...'Airport Terminal..')*
 - **next_station**  *(Think of this as the station stop, at timeStamp)* 
 - **date**
 - **status**    *('On Time', '5 min', '10 min'.  This is a status on train lateness. 999 is a suspended train)*
 - **timeStamp**


----------


**trainView.csv - GPS Train data (early release)**


Most GPS coordinates are based on track telemetry; however, cars are being equipped with GPS units. 

- **train_id**
- **status**
- **next_station**
- **service**
- **dest**
- **lon**
- **lat**
- **source**
- **track_change**
- **track**
- **date**
- **timeStamp0**   First timeStamp at coordinates.
- **timeStamp1**    Last timeStamp at coordinates.

You can look at the example [here](https://www.kaggle.com/septa/d/septa/on-time-performance/trainview-gps-data-early-beta) on how to track a single train.

**What To Look For...**

Ah, as a commuter you care more about the performance of the train(s) you plan to take.  OTP maybe 91% or above; but, if the train **you take** runs late, you'll spend extra time on the tracks.  If it consistently runs late, maybe the schedule should be changed.

Look for patterns. For example, during the Tuesday morning rush do some trains run consistently late?   How long does it take to get to from point A to point B in the system?  Performance is VERY important to SEPTA. 

----------


Below is a map of the system and station stops.  This dataset contains data on the Regional Rail Lines.

![SEPTA map][1]

SEPTA train schedules can be found [here](http://www.septa.org/schedules/rail/).  Note different Weekday, Saturday, and Sunday schedules.   



  [1]: https://raw.githubusercontent.com/mchirico/mchirico.github.io/master/p/images/septaMap.png","2016-07-20 00:46:24","Updated records to 2016-07-19","on-time-performance.zip",100141836,0
166,63,6,"European Soccer Database","soccer","25k+ matches and players stats for European Professional Football","The ultimate Soccer database for data analysis and machine learning
-------------------------------------------------------------------

**What you get:**

 - +25,000 matches
 - +10,000 players
 - 11 European Countries with their lead championship
 - Seasons 2008 to 2016
 - Player's statistics sourced from EA Sports' FIFA video game series, including the weekly updates
 - Team line up with squad formation (X, Y coordinates)
 - Betting odds from up to 10 providers
 - Detailed match events (goal types, possession, corner, cross, fouls, cards etc...) for +10,000 matches

----------

**Original Data Source:** 

You can easily find data about soccer matches but they are usually scattered across different websites. A thorough data collection and processing has been done to make your life easier. **I must insist that you do not make any commercial use of the data**. The data was sourced from:

 - [http://football-data.mx-api.enetscores.com/][1] : scores, lineup, team formation and events

 - [http://www.football-data.co.uk/][2] : betting odds

 - [http://sofifa.com/][3] : statistics from EA Sports FIFA games. *FIFA series and all FIFA assets property of EA Sports.*

> When you have a look at the database, you will notice foreign keys for
> players and matches are the same as the original data sources. I have
> called those foreign keys ""api_id"".

----------

**Improving the dataset:** 

You will notice that some players are missing from the lineup (NULL values). This is because I have not been able to source their statistics from FIFA. This will be fixed overtime as the crawling algorithm is being improved.
The dataset will also be expanded to include international games, national cups, Champion's League and Europa League. Please ask me if you're after a specific tournament.

> Please get in touch with me if you want to help improve this dataset

----------

Exploring the data:
-------------------

Now that's the fun part, there is a lot you can do with this dataset. I will be adding visuals and insights to this overview page but please have a look at the kernels and give it a try yourself ! Here are some ideas for you:

**The Holy Grail...**
... is obviously to predict the outcome of the game. The bookies use 3 classes (Home Win, Draw, Away Win). They get it right about 53% of the time. This is also what I've achieved so far using my own SVM. Though it may sound high for such a random sport game, you've got to know
 that the home team wins about 46% of the time. So the base case (constantly predicting Home Win) has indeed 46% precision. 

**Probabilities vs Odds**

When running a multi-class classifier like SVM you could also output a probability estimate and compare it to the betting odds. Have a look at your variance vs odds and see for what games you had very different predictions.

**Explore and visualize features**

With access to stats, team formations and in-game events you should be able to produce some interesting insights into [The Beautiful Game][4] . Who knows, Guardiola himself may hire one of you some day!


  [1]: http://football-data.mx-api.enetscores.com/
  [2]: http://www.football-data.co.uk/
  [3]: http://sofifa.com/
  [4]: https://en.wikipedia.org/wiki/The_Beautiful_Game","2016-07-20 07:21:54","Dataset with ingame events","soccer.zip",36833775,3
